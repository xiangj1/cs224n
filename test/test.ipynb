{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if(not torch.cuda.is_available()):\n",
    "    raise Exception('cuda not available')\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab) 9999\n"
     ]
    }
   ],
   "source": [
    "# Load the PTB dataset\n",
    "text = open(\"ptb.train.txt\", \"r\").read() # Replace this with the actual path to the PTB dataset\n",
    "text = text.split()\n",
    "\n",
    "# Prepare the vocabulary and create mapping from words to integers\n",
    "vocab = set(text)\n",
    "print('len(vocab)', len(vocab))\n",
    "word_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "int_to_word = {ii: word for ii, word in enumerate(vocab)}\n",
    "\n",
    "# Convert the text data to integer form\n",
    "encoded = np.array([word_to_int[word] for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    return [int_to_word[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the encoded text into chunks of length `seq_length`\n",
    "seq_length = 64\n",
    "data = []\n",
    "target = []\n",
    "for i in range(0, len(encoded) - seq_length):\n",
    "    data.append(encoded[i: i + seq_length])\n",
    "    target.append(encoded[i + seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_1048\\2365251591.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data, device=DEVICE).long()\n",
      "C:\\Users\\xiang\\AppData\\Local\\Temp\\ipykernel_1048\\2365251591.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target, device=DEVICE).long()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([887457, 64])\n",
      "torch.Size([887457])\n"
     ]
    }
   ],
   "source": [
    "# Convert the data and target to tensors\n",
    "data = torch.tensor(data, device=DEVICE).long()\n",
    "target = torch.tensor(target, device=DEVICE).long()\n",
    "\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "\n",
    "# Create a TensorDataset from data and target tensors\n",
    "dataset = TensorDataset(data, target)\n",
    "\n",
    "# Create a DataLoader from the TensorDataset\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [100/27733], Loss: 7.8342\n",
      "Epoch [1/300], Step [200/27733], Loss: 7.5727\n",
      "Epoch [1/300], Step [300/27733], Loss: 6.6153\n",
      "Epoch [1/300], Step [400/27733], Loss: 6.7830\n",
      "Epoch [1/300], Step [500/27733], Loss: 7.5079\n",
      "Epoch [1/300], Step [600/27733], Loss: 6.0225\n",
      "Epoch [1/300], Step [700/27733], Loss: 7.6855\n",
      "Epoch [1/300], Step [800/27733], Loss: 7.6249\n",
      "Epoch [1/300], Step [900/27733], Loss: 6.1625\n",
      "Epoch [1/300], Step [1000/27733], Loss: 6.5600\n",
      "Epoch [1/300], Step [1100/27733], Loss: 6.5568\n",
      "Epoch [1/300], Step [1200/27733], Loss: 5.2681\n",
      "Epoch [1/300], Step [1300/27733], Loss: 6.8257\n",
      "Epoch [1/300], Step [1400/27733], Loss: 6.2989\n",
      "Epoch [1/300], Step [1500/27733], Loss: 6.8791\n",
      "Epoch [1/300], Step [1600/27733], Loss: 5.8289\n",
      "Epoch [1/300], Step [1700/27733], Loss: 6.8065\n",
      "Epoch [1/300], Step [1800/27733], Loss: 5.7066\n",
      "Epoch [1/300], Step [1900/27733], Loss: 6.9660\n",
      "Epoch [1/300], Step [2000/27733], Loss: 6.5602\n",
      "Epoch [1/300], Step [2100/27733], Loss: 6.6538\n",
      "Epoch [1/300], Step [2200/27733], Loss: 7.3403\n",
      "Epoch [1/300], Step [2300/27733], Loss: 6.7844\n",
      "Epoch [1/300], Step [2400/27733], Loss: 6.8632\n",
      "Epoch [1/300], Step [2500/27733], Loss: 6.3765\n",
      "Epoch [1/300], Step [2600/27733], Loss: 6.7665\n",
      "Epoch [1/300], Step [2700/27733], Loss: 6.4507\n",
      "Epoch [1/300], Step [2800/27733], Loss: 6.4714\n",
      "Epoch [1/300], Step [2900/27733], Loss: 5.6744\n",
      "Epoch [1/300], Step [3000/27733], Loss: 6.8887\n",
      "Epoch [1/300], Step [3100/27733], Loss: 6.5735\n",
      "Epoch [1/300], Step [3200/27733], Loss: 6.0679\n",
      "Epoch [1/300], Step [3300/27733], Loss: 5.5814\n",
      "Epoch [1/300], Step [3400/27733], Loss: 6.2903\n",
      "Epoch [1/300], Step [3500/27733], Loss: 6.1298\n",
      "Epoch [1/300], Step [3600/27733], Loss: 6.0985\n",
      "Epoch [1/300], Step [3700/27733], Loss: 6.0478\n",
      "Epoch [1/300], Step [3800/27733], Loss: 6.7671\n",
      "Epoch [1/300], Step [3900/27733], Loss: 7.1249\n",
      "Epoch [1/300], Step [4000/27733], Loss: 6.3551\n",
      "Epoch [1/300], Step [4100/27733], Loss: 5.6902\n",
      "Epoch [1/300], Step [4200/27733], Loss: 5.6203\n",
      "Epoch [1/300], Step [4300/27733], Loss: 5.2501\n",
      "Epoch [1/300], Step [4400/27733], Loss: 5.8536\n",
      "Epoch [1/300], Step [4500/27733], Loss: 6.1529\n",
      "Epoch [1/300], Step [4600/27733], Loss: 5.0872\n",
      "Epoch [1/300], Step [4700/27733], Loss: 5.9902\n",
      "Epoch [1/300], Step [4800/27733], Loss: 6.7065\n",
      "Epoch [1/300], Step [4900/27733], Loss: 5.7210\n",
      "Epoch [1/300], Step [5000/27733], Loss: 5.7900\n",
      "Epoch [1/300], Step [5100/27733], Loss: 6.1197\n",
      "Epoch [1/300], Step [5200/27733], Loss: 6.5086\n",
      "Epoch [1/300], Step [5300/27733], Loss: 5.9655\n",
      "Epoch [1/300], Step [5400/27733], Loss: 6.2394\n",
      "Epoch [1/300], Step [5500/27733], Loss: 5.5077\n",
      "Epoch [1/300], Step [5600/27733], Loss: 6.7729\n",
      "Epoch [1/300], Step [5700/27733], Loss: 5.4834\n",
      "Epoch [1/300], Step [5800/27733], Loss: 6.5582\n",
      "Epoch [1/300], Step [5900/27733], Loss: 5.6944\n",
      "Epoch [1/300], Step [6000/27733], Loss: 5.9168\n",
      "Epoch [1/300], Step [6100/27733], Loss: 6.0450\n",
      "Epoch [1/300], Step [6200/27733], Loss: 6.1842\n",
      "Epoch [1/300], Step [6300/27733], Loss: 6.3628\n",
      "Epoch [1/300], Step [6400/27733], Loss: 6.2096\n",
      "Epoch [1/300], Step [6500/27733], Loss: 5.6232\n",
      "Epoch [1/300], Step [6600/27733], Loss: 6.5687\n",
      "Epoch [1/300], Step [6700/27733], Loss: 6.0268\n",
      "Epoch [1/300], Step [6800/27733], Loss: 5.7164\n",
      "Epoch [1/300], Step [6900/27733], Loss: 5.7220\n",
      "Epoch [1/300], Step [7000/27733], Loss: 6.2972\n",
      "Epoch [1/300], Step [7100/27733], Loss: 5.5037\n",
      "Epoch [1/300], Step [7200/27733], Loss: 5.3237\n",
      "Epoch [1/300], Step [7300/27733], Loss: 6.0813\n",
      "Epoch [1/300], Step [7400/27733], Loss: 5.7854\n",
      "Epoch [1/300], Step [7500/27733], Loss: 6.5851\n",
      "Epoch [1/300], Step [7600/27733], Loss: 4.9901\n",
      "Epoch [1/300], Step [7700/27733], Loss: 5.3136\n",
      "Epoch [1/300], Step [7800/27733], Loss: 5.3370\n",
      "Epoch [1/300], Step [7900/27733], Loss: 6.2644\n",
      "Epoch [1/300], Step [8000/27733], Loss: 6.0232\n",
      "Epoch [1/300], Step [8100/27733], Loss: 6.0487\n",
      "Epoch [1/300], Step [8200/27733], Loss: 5.8575\n",
      "Epoch [1/300], Step [8300/27733], Loss: 5.9785\n",
      "Epoch [1/300], Step [8400/27733], Loss: 6.8667\n",
      "Epoch [1/300], Step [8500/27733], Loss: 6.5329\n",
      "Epoch [1/300], Step [8600/27733], Loss: 5.9301\n",
      "Epoch [1/300], Step [8700/27733], Loss: 5.3476\n",
      "Epoch [1/300], Step [8800/27733], Loss: 6.2590\n",
      "Epoch [1/300], Step [8900/27733], Loss: 5.4067\n",
      "Epoch [1/300], Step [9000/27733], Loss: 5.3673\n",
      "Epoch [1/300], Step [9100/27733], Loss: 6.6770\n",
      "Epoch [1/300], Step [9200/27733], Loss: 5.1879\n",
      "Epoch [1/300], Step [9300/27733], Loss: 7.1123\n",
      "Epoch [1/300], Step [9400/27733], Loss: 5.2519\n",
      "Epoch [1/300], Step [9500/27733], Loss: 6.8188\n",
      "Epoch [1/300], Step [9600/27733], Loss: 6.3955\n",
      "Epoch [1/300], Step [9700/27733], Loss: 6.1973\n",
      "Epoch [1/300], Step [9800/27733], Loss: 6.2427\n",
      "Epoch [1/300], Step [9900/27733], Loss: 5.6357\n",
      "Epoch [1/300], Step [10000/27733], Loss: 6.1621\n",
      "Epoch [1/300], Step [10100/27733], Loss: 4.7508\n",
      "Epoch [1/300], Step [10200/27733], Loss: 5.8497\n",
      "Epoch [1/300], Step [10300/27733], Loss: 5.8370\n",
      "Epoch [1/300], Step [10400/27733], Loss: 6.0456\n",
      "Epoch [1/300], Step [10500/27733], Loss: 5.8672\n",
      "Epoch [1/300], Step [10600/27733], Loss: 6.5729\n",
      "Epoch [1/300], Step [10700/27733], Loss: 5.6943\n",
      "Epoch [1/300], Step [10800/27733], Loss: 5.7178\n",
      "Epoch [1/300], Step [10900/27733], Loss: 6.5346\n",
      "Epoch [1/300], Step [11000/27733], Loss: 6.5313\n",
      "Epoch [1/300], Step [11100/27733], Loss: 5.9882\n",
      "Epoch [1/300], Step [11200/27733], Loss: 6.2752\n",
      "Epoch [1/300], Step [11300/27733], Loss: 4.3852\n",
      "Epoch [1/300], Step [11400/27733], Loss: 4.4074\n",
      "Epoch [1/300], Step [11500/27733], Loss: 5.9005\n",
      "Epoch [1/300], Step [11600/27733], Loss: 6.6161\n",
      "Epoch [1/300], Step [11700/27733], Loss: 6.4194\n",
      "Epoch [1/300], Step [11800/27733], Loss: 7.0521\n",
      "Epoch [1/300], Step [11900/27733], Loss: 5.5468\n",
      "Epoch [1/300], Step [12000/27733], Loss: 5.0984\n",
      "Epoch [1/300], Step [12100/27733], Loss: 6.0767\n",
      "Epoch [1/300], Step [12200/27733], Loss: 5.9712\n",
      "Epoch [1/300], Step [12300/27733], Loss: 6.0692\n",
      "Epoch [1/300], Step [12400/27733], Loss: 5.5038\n",
      "Epoch [1/300], Step [12500/27733], Loss: 4.6038\n",
      "Epoch [1/300], Step [12600/27733], Loss: 4.7068\n",
      "Epoch [1/300], Step [12700/27733], Loss: 5.5992\n",
      "Epoch [1/300], Step [12800/27733], Loss: 5.2402\n",
      "Epoch [1/300], Step [12900/27733], Loss: 5.5841\n",
      "Epoch [1/300], Step [13000/27733], Loss: 5.2264\n",
      "Epoch [1/300], Step [13100/27733], Loss: 5.7586\n",
      "Epoch [1/300], Step [13200/27733], Loss: 6.1164\n",
      "Epoch [1/300], Step [13300/27733], Loss: 6.3402\n",
      "Epoch [1/300], Step [13400/27733], Loss: 5.2290\n",
      "Epoch [1/300], Step [13500/27733], Loss: 4.2301\n",
      "Epoch [1/300], Step [13600/27733], Loss: 4.4477\n",
      "Epoch [1/300], Step [13700/27733], Loss: 5.1728\n",
      "Epoch [1/300], Step [13800/27733], Loss: 5.1605\n",
      "Epoch [1/300], Step [13900/27733], Loss: 5.8575\n",
      "Epoch [1/300], Step [14000/27733], Loss: 5.1225\n",
      "Epoch [1/300], Step [14100/27733], Loss: 4.5039\n",
      "Epoch [1/300], Step [14200/27733], Loss: 5.6023\n",
      "Epoch [1/300], Step [14300/27733], Loss: 5.6791\n",
      "Epoch [1/300], Step [14400/27733], Loss: 6.7106\n",
      "Epoch [1/300], Step [14500/27733], Loss: 5.3173\n",
      "Epoch [1/300], Step [14600/27733], Loss: 4.6796\n",
      "Epoch [1/300], Step [14700/27733], Loss: 5.5898\n",
      "Epoch [1/300], Step [14800/27733], Loss: 5.4007\n",
      "Epoch [1/300], Step [14900/27733], Loss: 6.1145\n",
      "Epoch [1/300], Step [15000/27733], Loss: 5.6872\n",
      "Epoch [1/300], Step [15100/27733], Loss: 5.6194\n",
      "Epoch [1/300], Step [15200/27733], Loss: 5.8778\n",
      "Epoch [1/300], Step [15300/27733], Loss: 5.9995\n",
      "Epoch [1/300], Step [15400/27733], Loss: 5.2462\n",
      "Epoch [1/300], Step [15500/27733], Loss: 4.7998\n",
      "Epoch [1/300], Step [15600/27733], Loss: 6.3186\n",
      "Epoch [1/300], Step [15700/27733], Loss: 5.6105\n",
      "Epoch [1/300], Step [15800/27733], Loss: 5.2684\n",
      "Epoch [1/300], Step [15900/27733], Loss: 4.0973\n",
      "Epoch [1/300], Step [16000/27733], Loss: 4.2043\n",
      "Epoch [1/300], Step [16100/27733], Loss: 4.8479\n",
      "Epoch [1/300], Step [16200/27733], Loss: 4.9332\n",
      "Epoch [1/300], Step [16300/27733], Loss: 6.0319\n",
      "Epoch [1/300], Step [16400/27733], Loss: 4.5412\n",
      "Epoch [1/300], Step [16500/27733], Loss: 6.1597\n",
      "Epoch [1/300], Step [16600/27733], Loss: 5.7182\n",
      "Epoch [1/300], Step [16700/27733], Loss: 5.8627\n",
      "Epoch [1/300], Step [16800/27733], Loss: 5.5024\n",
      "Epoch [1/300], Step [16900/27733], Loss: 5.3131\n",
      "Epoch [1/300], Step [17000/27733], Loss: 5.3646\n",
      "Epoch [1/300], Step [17100/27733], Loss: 4.8544\n",
      "Epoch [1/300], Step [17200/27733], Loss: 4.9585\n",
      "Epoch [1/300], Step [17300/27733], Loss: 4.9107\n",
      "Epoch [1/300], Step [17400/27733], Loss: 5.9365\n",
      "Epoch [1/300], Step [17500/27733], Loss: 5.5964\n",
      "Epoch [1/300], Step [17600/27733], Loss: 6.0040\n",
      "Epoch [1/300], Step [17700/27733], Loss: 4.2634\n",
      "Epoch [1/300], Step [17800/27733], Loss: 5.4269\n",
      "Epoch [1/300], Step [17900/27733], Loss: 5.7448\n",
      "Epoch [1/300], Step [18000/27733], Loss: 5.3491\n",
      "Epoch [1/300], Step [18100/27733], Loss: 5.5998\n",
      "Epoch [1/300], Step [18200/27733], Loss: 6.8466\n",
      "Epoch [1/300], Step [18300/27733], Loss: 5.6749\n",
      "Epoch [1/300], Step [18400/27733], Loss: 5.6004\n",
      "Epoch [1/300], Step [18500/27733], Loss: 4.9303\n",
      "Epoch [1/300], Step [18600/27733], Loss: 5.0194\n",
      "Epoch [1/300], Step [18700/27733], Loss: 5.4614\n",
      "Epoch [1/300], Step [18800/27733], Loss: 5.7476\n",
      "Epoch [1/300], Step [18900/27733], Loss: 4.3720\n",
      "Epoch [1/300], Step [19000/27733], Loss: 6.0581\n",
      "Epoch [1/300], Step [19100/27733], Loss: 5.5719\n",
      "Epoch [1/300], Step [19200/27733], Loss: 5.3788\n",
      "Epoch [1/300], Step [19300/27733], Loss: 5.6833\n",
      "Epoch [1/300], Step [19400/27733], Loss: 4.6871\n",
      "Epoch [1/300], Step [19500/27733], Loss: 5.1775\n",
      "Epoch [1/300], Step [19600/27733], Loss: 5.2352\n",
      "Epoch [1/300], Step [19700/27733], Loss: 5.3612\n",
      "Epoch [1/300], Step [19800/27733], Loss: 5.1866\n",
      "Epoch [1/300], Step [19900/27733], Loss: 4.8046\n",
      "Epoch [1/300], Step [20000/27733], Loss: 5.1739\n",
      "Epoch [1/300], Step [20100/27733], Loss: 5.4586\n",
      "Epoch [1/300], Step [20200/27733], Loss: 5.5608\n",
      "Epoch [1/300], Step [20300/27733], Loss: 5.4659\n",
      "Epoch [1/300], Step [20400/27733], Loss: 6.5102\n",
      "Epoch [1/300], Step [20500/27733], Loss: 5.9107\n",
      "Epoch [1/300], Step [20600/27733], Loss: 5.5518\n",
      "Epoch [1/300], Step [20700/27733], Loss: 4.4981\n",
      "Epoch [1/300], Step [20800/27733], Loss: 5.4276\n",
      "Epoch [1/300], Step [20900/27733], Loss: 5.2660\n",
      "Epoch [1/300], Step [21000/27733], Loss: 6.1959\n",
      "Epoch [1/300], Step [21100/27733], Loss: 5.2153\n",
      "Epoch [1/300], Step [21200/27733], Loss: 5.0209\n",
      "Epoch [1/300], Step [21300/27733], Loss: 5.4701\n",
      "Epoch [1/300], Step [21400/27733], Loss: 5.0235\n",
      "Epoch [1/300], Step [21500/27733], Loss: 5.3149\n",
      "Epoch [1/300], Step [21600/27733], Loss: 4.3582\n",
      "Epoch [1/300], Step [21700/27733], Loss: 4.8706\n",
      "Epoch [1/300], Step [21800/27733], Loss: 5.4296\n",
      "Epoch [1/300], Step [21900/27733], Loss: 5.5275\n",
      "Epoch [1/300], Step [22000/27733], Loss: 6.0515\n",
      "Epoch [1/300], Step [22100/27733], Loss: 5.2961\n",
      "Epoch [1/300], Step [22200/27733], Loss: 5.8088\n",
      "Epoch [1/300], Step [22300/27733], Loss: 4.7556\n",
      "Epoch [1/300], Step [22400/27733], Loss: 5.4807\n",
      "Epoch [1/300], Step [22500/27733], Loss: 5.3834\n",
      "Epoch [1/300], Step [22600/27733], Loss: 5.2389\n",
      "Epoch [1/300], Step [22700/27733], Loss: 5.8246\n",
      "Epoch [1/300], Step [22800/27733], Loss: 6.0151\n",
      "Epoch [1/300], Step [22900/27733], Loss: 5.2112\n",
      "Epoch [1/300], Step [23000/27733], Loss: 6.2569\n",
      "Epoch [1/300], Step [23100/27733], Loss: 4.7505\n",
      "Epoch [1/300], Step [23200/27733], Loss: 5.8224\n",
      "Epoch [1/300], Step [23300/27733], Loss: 4.8912\n",
      "Epoch [1/300], Step [23400/27733], Loss: 5.4505\n",
      "Epoch [1/300], Step [23500/27733], Loss: 5.5987\n",
      "Epoch [1/300], Step [23600/27733], Loss: 4.3375\n",
      "Epoch [1/300], Step [23700/27733], Loss: 5.7078\n",
      "Epoch [1/300], Step [23800/27733], Loss: 6.5603\n",
      "Epoch [1/300], Step [23900/27733], Loss: 6.4675\n",
      "Epoch [1/300], Step [24000/27733], Loss: 5.4956\n",
      "Epoch [1/300], Step [24100/27733], Loss: 5.3284\n",
      "Epoch [1/300], Step [24200/27733], Loss: 5.0056\n",
      "Epoch [1/300], Step [24300/27733], Loss: 5.1790\n",
      "Epoch [1/300], Step [24400/27733], Loss: 5.7632\n",
      "Epoch [1/300], Step [24500/27733], Loss: 5.9037\n",
      "Epoch [1/300], Step [24600/27733], Loss: 5.4239\n",
      "Epoch [1/300], Step [24700/27733], Loss: 5.1496\n",
      "Epoch [1/300], Step [24800/27733], Loss: 5.2388\n",
      "Epoch [1/300], Step [24900/27733], Loss: 4.2203\n",
      "Epoch [1/300], Step [25000/27733], Loss: 4.7887\n",
      "Epoch [1/300], Step [25100/27733], Loss: 6.0641\n",
      "Epoch [1/300], Step [25200/27733], Loss: 5.7061\n",
      "Epoch [1/300], Step [25300/27733], Loss: 4.9389\n",
      "Epoch [1/300], Step [25400/27733], Loss: 4.7191\n",
      "Epoch [1/300], Step [25500/27733], Loss: 5.6171\n",
      "Epoch [1/300], Step [25600/27733], Loss: 4.9578\n",
      "Epoch [1/300], Step [25700/27733], Loss: 5.6809\n",
      "Epoch [1/300], Step [25800/27733], Loss: 6.5049\n",
      "Epoch [1/300], Step [25900/27733], Loss: 4.8661\n",
      "Epoch [1/300], Step [26000/27733], Loss: 4.7201\n",
      "Epoch [1/300], Step [26100/27733], Loss: 5.6636\n",
      "Epoch [1/300], Step [26200/27733], Loss: 4.5255\n",
      "Epoch [1/300], Step [26300/27733], Loss: 6.3309\n",
      "Epoch [1/300], Step [26400/27733], Loss: 5.5596\n",
      "Epoch [1/300], Step [26500/27733], Loss: 4.9773\n",
      "Epoch [1/300], Step [26600/27733], Loss: 5.0207\n",
      "Epoch [1/300], Step [26700/27733], Loss: 5.8845\n",
      "Epoch [1/300], Step [26800/27733], Loss: 4.4760\n",
      "Epoch [1/300], Step [26900/27733], Loss: 5.9262\n",
      "Epoch [1/300], Step [27000/27733], Loss: 5.9426\n",
      "Epoch [1/300], Step [27100/27733], Loss: 6.5374\n",
      "Epoch [1/300], Step [27200/27733], Loss: 5.5392\n",
      "Epoch [1/300], Step [27300/27733], Loss: 5.9495\n",
      "Epoch [1/300], Step [27400/27733], Loss: 5.7385\n",
      "Epoch [1/300], Step [27500/27733], Loss: 5.5414\n",
      "Epoch [1/300], Step [27600/27733], Loss: 4.3880\n",
      "Epoch [1/300], Step [27700/27733], Loss: 5.5708\n",
      "Epoch [2/300], Step [100/27733], Loss: 5.9084\n",
      "Epoch [2/300], Step [200/27733], Loss: 5.1294\n",
      "Epoch [2/300], Step [300/27733], Loss: 5.4318\n",
      "Epoch [2/300], Step [400/27733], Loss: 4.8707\n",
      "Epoch [2/300], Step [500/27733], Loss: 5.0964\n",
      "Epoch [2/300], Step [600/27733], Loss: 4.6786\n",
      "Epoch [2/300], Step [700/27733], Loss: 5.4819\n",
      "Epoch [2/300], Step [800/27733], Loss: 5.3174\n",
      "Epoch [2/300], Step [900/27733], Loss: 4.5551\n",
      "Epoch [2/300], Step [1000/27733], Loss: 5.8230\n",
      "Epoch [2/300], Step [1100/27733], Loss: 5.9708\n",
      "Epoch [2/300], Step [1200/27733], Loss: 5.8247\n",
      "Epoch [2/300], Step [1300/27733], Loss: 5.3595\n",
      "Epoch [2/300], Step [1400/27733], Loss: 5.8920\n",
      "Epoch [2/300], Step [1500/27733], Loss: 5.3143\n",
      "Epoch [2/300], Step [1600/27733], Loss: 4.5697\n",
      "Epoch [2/300], Step [1700/27733], Loss: 3.8275\n",
      "Epoch [2/300], Step [1800/27733], Loss: 5.7338\n",
      "Epoch [2/300], Step [1900/27733], Loss: 5.7560\n",
      "Epoch [2/300], Step [2000/27733], Loss: 6.0620\n",
      "Epoch [2/300], Step [2100/27733], Loss: 5.5377\n",
      "Epoch [2/300], Step [2200/27733], Loss: 5.0139\n",
      "Epoch [2/300], Step [2300/27733], Loss: 4.3101\n",
      "Epoch [2/300], Step [2400/27733], Loss: 3.8143\n",
      "Epoch [2/300], Step [2500/27733], Loss: 5.8822\n",
      "Epoch [2/300], Step [2600/27733], Loss: 4.4072\n",
      "Epoch [2/300], Step [2700/27733], Loss: 5.2883\n",
      "Epoch [2/300], Step [2800/27733], Loss: 4.9274\n",
      "Epoch [2/300], Step [2900/27733], Loss: 4.6639\n",
      "Epoch [2/300], Step [3000/27733], Loss: 5.4063\n",
      "Epoch [2/300], Step [3100/27733], Loss: 4.7277\n",
      "Epoch [2/300], Step [3200/27733], Loss: 5.1678\n",
      "Epoch [2/300], Step [3300/27733], Loss: 5.4038\n",
      "Epoch [2/300], Step [3400/27733], Loss: 4.9193\n",
      "Epoch [2/300], Step [3500/27733], Loss: 5.1067\n",
      "Epoch [2/300], Step [3600/27733], Loss: 3.9453\n",
      "Epoch [2/300], Step [3700/27733], Loss: 5.7793\n",
      "Epoch [2/300], Step [3800/27733], Loss: 4.5874\n",
      "Epoch [2/300], Step [3900/27733], Loss: 5.6572\n",
      "Epoch [2/300], Step [4000/27733], Loss: 4.9581\n",
      "Epoch [2/300], Step [4100/27733], Loss: 5.6109\n",
      "Epoch [2/300], Step [4200/27733], Loss: 5.7670\n",
      "Epoch [2/300], Step [4300/27733], Loss: 4.6670\n",
      "Epoch [2/300], Step [4400/27733], Loss: 4.8667\n",
      "Epoch [2/300], Step [4500/27733], Loss: 4.7798\n",
      "Epoch [2/300], Step [4600/27733], Loss: 4.3653\n",
      "Epoch [2/300], Step [4700/27733], Loss: 5.0246\n",
      "Epoch [2/300], Step [4800/27733], Loss: 5.4849\n",
      "Epoch [2/300], Step [4900/27733], Loss: 4.7300\n",
      "Epoch [2/300], Step [5000/27733], Loss: 5.6081\n",
      "Epoch [2/300], Step [5100/27733], Loss: 5.3469\n",
      "Epoch [2/300], Step [5200/27733], Loss: 4.9415\n",
      "Epoch [2/300], Step [5300/27733], Loss: 5.3112\n",
      "Epoch [2/300], Step [5400/27733], Loss: 4.6618\n",
      "Epoch [2/300], Step [5500/27733], Loss: 4.8874\n",
      "Epoch [2/300], Step [5600/27733], Loss: 5.2237\n",
      "Epoch [2/300], Step [5700/27733], Loss: 5.2774\n",
      "Epoch [2/300], Step [5800/27733], Loss: 4.5463\n",
      "Epoch [2/300], Step [5900/27733], Loss: 4.8652\n",
      "Epoch [2/300], Step [6000/27733], Loss: 3.9882\n",
      "Epoch [2/300], Step [6100/27733], Loss: 5.2542\n",
      "Epoch [2/300], Step [6200/27733], Loss: 4.3782\n",
      "Epoch [2/300], Step [6300/27733], Loss: 5.6656\n",
      "Epoch [2/300], Step [6400/27733], Loss: 3.9329\n",
      "Epoch [2/300], Step [6500/27733], Loss: 4.7149\n",
      "Epoch [2/300], Step [6600/27733], Loss: 4.4716\n",
      "Epoch [2/300], Step [6700/27733], Loss: 4.9420\n",
      "Epoch [2/300], Step [6800/27733], Loss: 4.9603\n",
      "Epoch [2/300], Step [6900/27733], Loss: 4.0089\n",
      "Epoch [2/300], Step [7000/27733], Loss: 5.3788\n",
      "Epoch [2/300], Step [7100/27733], Loss: 5.1570\n",
      "Epoch [2/300], Step [7200/27733], Loss: 4.7081\n",
      "Epoch [2/300], Step [7300/27733], Loss: 4.7337\n",
      "Epoch [2/300], Step [7400/27733], Loss: 5.0264\n",
      "Epoch [2/300], Step [7500/27733], Loss: 5.2063\n",
      "Epoch [2/300], Step [7600/27733], Loss: 4.8852\n",
      "Epoch [2/300], Step [7700/27733], Loss: 5.0759\n",
      "Epoch [2/300], Step [7800/27733], Loss: 4.5275\n",
      "Epoch [2/300], Step [7900/27733], Loss: 5.0580\n",
      "Epoch [2/300], Step [8000/27733], Loss: 4.2206\n",
      "Epoch [2/300], Step [8100/27733], Loss: 4.8472\n",
      "Epoch [2/300], Step [8200/27733], Loss: 5.4727\n",
      "Epoch [2/300], Step [8300/27733], Loss: 4.8530\n",
      "Epoch [2/300], Step [8400/27733], Loss: 5.1664\n",
      "Epoch [2/300], Step [8500/27733], Loss: 4.9225\n",
      "Epoch [2/300], Step [8600/27733], Loss: 4.8481\n",
      "Epoch [2/300], Step [8700/27733], Loss: 5.7627\n",
      "Epoch [2/300], Step [8800/27733], Loss: 3.9491\n",
      "Epoch [2/300], Step [8900/27733], Loss: 4.6208\n",
      "Epoch [2/300], Step [9000/27733], Loss: 4.9217\n",
      "Epoch [2/300], Step [9100/27733], Loss: 4.9647\n",
      "Epoch [2/300], Step [9200/27733], Loss: 4.3339\n",
      "Epoch [2/300], Step [9300/27733], Loss: 5.9181\n",
      "Epoch [2/300], Step [9400/27733], Loss: 5.2091\n",
      "Epoch [2/300], Step [9500/27733], Loss: 5.4868\n",
      "Epoch [2/300], Step [9600/27733], Loss: 4.2100\n",
      "Epoch [2/300], Step [9700/27733], Loss: 4.0250\n",
      "Epoch [2/300], Step [9800/27733], Loss: 4.5823\n",
      "Epoch [2/300], Step [9900/27733], Loss: 4.9749\n",
      "Epoch [2/300], Step [10000/27733], Loss: 5.1445\n",
      "Epoch [2/300], Step [10100/27733], Loss: 5.6861\n",
      "Epoch [2/300], Step [10200/27733], Loss: 4.9245\n",
      "Epoch [2/300], Step [10300/27733], Loss: 5.7183\n",
      "Epoch [2/300], Step [10400/27733], Loss: 4.7276\n",
      "Epoch [2/300], Step [10500/27733], Loss: 4.2376\n",
      "Epoch [2/300], Step [10600/27733], Loss: 5.3494\n",
      "Epoch [2/300], Step [10700/27733], Loss: 4.3710\n",
      "Epoch [2/300], Step [10800/27733], Loss: 5.2957\n",
      "Epoch [2/300], Step [10900/27733], Loss: 4.1325\n",
      "Epoch [2/300], Step [11000/27733], Loss: 5.8499\n",
      "Epoch [2/300], Step [11100/27733], Loss: 5.2181\n",
      "Epoch [2/300], Step [11200/27733], Loss: 5.1168\n",
      "Epoch [2/300], Step [11300/27733], Loss: 5.0176\n",
      "Epoch [2/300], Step [11400/27733], Loss: 5.7674\n",
      "Epoch [2/300], Step [11500/27733], Loss: 3.8183\n",
      "Epoch [2/300], Step [11600/27733], Loss: 4.3388\n",
      "Epoch [2/300], Step [11700/27733], Loss: 4.6187\n",
      "Epoch [2/300], Step [11800/27733], Loss: 6.1888\n",
      "Epoch [2/300], Step [11900/27733], Loss: 4.5630\n",
      "Epoch [2/300], Step [12000/27733], Loss: 5.4839\n",
      "Epoch [2/300], Step [12100/27733], Loss: 6.9080\n",
      "Epoch [2/300], Step [12200/27733], Loss: 5.6346\n",
      "Epoch [2/300], Step [12300/27733], Loss: 5.1255\n",
      "Epoch [2/300], Step [12400/27733], Loss: 6.2307\n",
      "Epoch [2/300], Step [12500/27733], Loss: 5.3641\n",
      "Epoch [2/300], Step [12600/27733], Loss: 5.4549\n",
      "Epoch [2/300], Step [12700/27733], Loss: 5.3349\n",
      "Epoch [2/300], Step [12800/27733], Loss: 4.5483\n",
      "Epoch [2/300], Step [12900/27733], Loss: 5.1758\n",
      "Epoch [2/300], Step [13000/27733], Loss: 5.8607\n",
      "Epoch [2/300], Step [13100/27733], Loss: 5.0024\n",
      "Epoch [2/300], Step [13200/27733], Loss: 4.6002\n",
      "Epoch [2/300], Step [13300/27733], Loss: 4.7521\n",
      "Epoch [2/300], Step [13400/27733], Loss: 5.2271\n",
      "Epoch [2/300], Step [13500/27733], Loss: 4.5871\n",
      "Epoch [2/300], Step [13600/27733], Loss: 4.7177\n",
      "Epoch [2/300], Step [13700/27733], Loss: 5.1580\n",
      "Epoch [2/300], Step [13800/27733], Loss: 4.9937\n",
      "Epoch [2/300], Step [13900/27733], Loss: 4.5301\n",
      "Epoch [2/300], Step [14000/27733], Loss: 6.1666\n",
      "Epoch [2/300], Step [14100/27733], Loss: 3.9416\n",
      "Epoch [2/300], Step [14200/27733], Loss: 4.9150\n",
      "Epoch [2/300], Step [14300/27733], Loss: 4.5952\n",
      "Epoch [2/300], Step [14400/27733], Loss: 4.6620\n",
      "Epoch [2/300], Step [14500/27733], Loss: 5.5014\n",
      "Epoch [2/300], Step [14600/27733], Loss: 4.7092\n",
      "Epoch [2/300], Step [14700/27733], Loss: 5.0745\n",
      "Epoch [2/300], Step [14800/27733], Loss: 5.3776\n",
      "Epoch [2/300], Step [14900/27733], Loss: 4.5347\n",
      "Epoch [2/300], Step [15000/27733], Loss: 4.6452\n",
      "Epoch [2/300], Step [15100/27733], Loss: 4.5098\n",
      "Epoch [2/300], Step [15200/27733], Loss: 4.7830\n",
      "Epoch [2/300], Step [15300/27733], Loss: 5.7298\n",
      "Epoch [2/300], Step [15400/27733], Loss: 5.6165\n",
      "Epoch [2/300], Step [15500/27733], Loss: 4.4749\n",
      "Epoch [2/300], Step [15600/27733], Loss: 4.4718\n",
      "Epoch [2/300], Step [15700/27733], Loss: 4.4400\n",
      "Epoch [2/300], Step [15800/27733], Loss: 4.8288\n",
      "Epoch [2/300], Step [15900/27733], Loss: 4.1848\n",
      "Epoch [2/300], Step [16000/27733], Loss: 4.7080\n",
      "Epoch [2/300], Step [16100/27733], Loss: 5.3778\n",
      "Epoch [2/300], Step [16200/27733], Loss: 4.5663\n",
      "Epoch [2/300], Step [16300/27733], Loss: 5.0912\n",
      "Epoch [2/300], Step [16400/27733], Loss: 5.8858\n",
      "Epoch [2/300], Step [16500/27733], Loss: 4.4016\n",
      "Epoch [2/300], Step [16600/27733], Loss: 4.4699\n",
      "Epoch [2/300], Step [16700/27733], Loss: 4.9498\n",
      "Epoch [2/300], Step [16800/27733], Loss: 5.1360\n",
      "Epoch [2/300], Step [16900/27733], Loss: 5.8820\n",
      "Epoch [2/300], Step [17000/27733], Loss: 5.2149\n",
      "Epoch [2/300], Step [17100/27733], Loss: 4.2509\n",
      "Epoch [2/300], Step [17200/27733], Loss: 5.1664\n",
      "Epoch [2/300], Step [17300/27733], Loss: 5.9835\n",
      "Epoch [2/300], Step [17400/27733], Loss: 5.1561\n",
      "Epoch [2/300], Step [17500/27733], Loss: 4.9744\n",
      "Epoch [2/300], Step [17600/27733], Loss: 5.6764\n",
      "Epoch [2/300], Step [17700/27733], Loss: 5.4387\n",
      "Epoch [2/300], Step [17800/27733], Loss: 4.7220\n",
      "Epoch [2/300], Step [17900/27733], Loss: 5.8130\n",
      "Epoch [2/300], Step [18000/27733], Loss: 4.3681\n",
      "Epoch [2/300], Step [18100/27733], Loss: 5.3672\n",
      "Epoch [2/300], Step [18200/27733], Loss: 5.5778\n",
      "Epoch [2/300], Step [18300/27733], Loss: 4.0131\n",
      "Epoch [2/300], Step [18400/27733], Loss: 4.7858\n",
      "Epoch [2/300], Step [18500/27733], Loss: 4.2536\n",
      "Epoch [2/300], Step [18600/27733], Loss: 5.6062\n",
      "Epoch [2/300], Step [18700/27733], Loss: 5.0389\n",
      "Epoch [2/300], Step [18800/27733], Loss: 5.9451\n",
      "Epoch [2/300], Step [18900/27733], Loss: 5.0829\n",
      "Epoch [2/300], Step [19000/27733], Loss: 5.4628\n",
      "Epoch [2/300], Step [19100/27733], Loss: 4.7655\n",
      "Epoch [2/300], Step [19200/27733], Loss: 4.6578\n",
      "Epoch [2/300], Step [19300/27733], Loss: 4.8285\n",
      "Epoch [2/300], Step [19400/27733], Loss: 5.1415\n",
      "Epoch [2/300], Step [19500/27733], Loss: 4.2011\n",
      "Epoch [2/300], Step [19600/27733], Loss: 4.1349\n",
      "Epoch [2/300], Step [19700/27733], Loss: 4.4481\n",
      "Epoch [2/300], Step [19800/27733], Loss: 5.2859\n",
      "Epoch [2/300], Step [19900/27733], Loss: 5.7081\n",
      "Epoch [2/300], Step [20000/27733], Loss: 4.6305\n",
      "Epoch [2/300], Step [20100/27733], Loss: 3.8533\n",
      "Epoch [2/300], Step [20200/27733], Loss: 5.3277\n",
      "Epoch [2/300], Step [20300/27733], Loss: 4.0952\n",
      "Epoch [2/300], Step [20400/27733], Loss: 4.9410\n",
      "Epoch [2/300], Step [20500/27733], Loss: 5.7599\n",
      "Epoch [2/300], Step [20600/27733], Loss: 4.4679\n",
      "Epoch [2/300], Step [20700/27733], Loss: 5.3590\n",
      "Epoch [2/300], Step [20800/27733], Loss: 4.8518\n",
      "Epoch [2/300], Step [20900/27733], Loss: 5.6102\n",
      "Epoch [2/300], Step [21000/27733], Loss: 3.8117\n",
      "Epoch [2/300], Step [21100/27733], Loss: 5.5176\n",
      "Epoch [2/300], Step [21200/27733], Loss: 5.5716\n",
      "Epoch [2/300], Step [21300/27733], Loss: 4.5923\n",
      "Epoch [2/300], Step [21400/27733], Loss: 4.0378\n",
      "Epoch [2/300], Step [21500/27733], Loss: 4.5643\n",
      "Epoch [2/300], Step [21600/27733], Loss: 4.9037\n",
      "Epoch [2/300], Step [21700/27733], Loss: 5.9367\n",
      "Epoch [2/300], Step [21800/27733], Loss: 6.4074\n",
      "Epoch [2/300], Step [21900/27733], Loss: 5.3156\n",
      "Epoch [2/300], Step [22000/27733], Loss: 5.3007\n",
      "Epoch [2/300], Step [22100/27733], Loss: 4.1041\n",
      "Epoch [2/300], Step [22200/27733], Loss: 5.2701\n",
      "Epoch [2/300], Step [22300/27733], Loss: 4.8767\n",
      "Epoch [2/300], Step [22400/27733], Loss: 4.3633\n",
      "Epoch [2/300], Step [22500/27733], Loss: 5.0940\n",
      "Epoch [2/300], Step [22600/27733], Loss: 5.3104\n",
      "Epoch [2/300], Step [22700/27733], Loss: 5.5670\n",
      "Epoch [2/300], Step [22800/27733], Loss: 4.4674\n",
      "Epoch [2/300], Step [22900/27733], Loss: 5.2655\n",
      "Epoch [2/300], Step [23000/27733], Loss: 4.5129\n",
      "Epoch [2/300], Step [23100/27733], Loss: 4.9510\n",
      "Epoch [2/300], Step [23200/27733], Loss: 4.0529\n",
      "Epoch [2/300], Step [23300/27733], Loss: 5.5750\n",
      "Epoch [2/300], Step [23400/27733], Loss: 5.7683\n",
      "Epoch [2/300], Step [23500/27733], Loss: 4.8263\n",
      "Epoch [2/300], Step [23600/27733], Loss: 4.8943\n",
      "Epoch [2/300], Step [23700/27733], Loss: 5.2264\n",
      "Epoch [2/300], Step [23800/27733], Loss: 4.9757\n",
      "Epoch [2/300], Step [23900/27733], Loss: 4.8667\n",
      "Epoch [2/300], Step [24000/27733], Loss: 4.0671\n",
      "Epoch [2/300], Step [24100/27733], Loss: 4.8013\n",
      "Epoch [2/300], Step [24200/27733], Loss: 4.7333\n",
      "Epoch [2/300], Step [24300/27733], Loss: 6.1658\n",
      "Epoch [2/300], Step [24400/27733], Loss: 4.4827\n",
      "Epoch [2/300], Step [24500/27733], Loss: 4.4148\n",
      "Epoch [2/300], Step [24600/27733], Loss: 4.8209\n",
      "Epoch [2/300], Step [24700/27733], Loss: 5.3524\n",
      "Epoch [2/300], Step [24800/27733], Loss: 5.8465\n",
      "Epoch [2/300], Step [24900/27733], Loss: 5.3062\n",
      "Epoch [2/300], Step [25000/27733], Loss: 4.6598\n",
      "Epoch [2/300], Step [25100/27733], Loss: 4.9850\n",
      "Epoch [2/300], Step [25200/27733], Loss: 4.6195\n",
      "Epoch [2/300], Step [25300/27733], Loss: 5.8495\n",
      "Epoch [2/300], Step [25400/27733], Loss: 4.7427\n",
      "Epoch [2/300], Step [25500/27733], Loss: 4.5464\n",
      "Epoch [2/300], Step [25600/27733], Loss: 5.5888\n",
      "Epoch [2/300], Step [25700/27733], Loss: 4.8890\n",
      "Epoch [2/300], Step [25800/27733], Loss: 4.1898\n",
      "Epoch [2/300], Step [25900/27733], Loss: 4.7846\n",
      "Epoch [2/300], Step [26000/27733], Loss: 5.4688\n",
      "Epoch [2/300], Step [26100/27733], Loss: 6.2638\n",
      "Epoch [2/300], Step [26200/27733], Loss: 5.5176\n",
      "Epoch [2/300], Step [26300/27733], Loss: 5.0212\n",
      "Epoch [2/300], Step [26400/27733], Loss: 3.7301\n",
      "Epoch [2/300], Step [26500/27733], Loss: 5.4330\n",
      "Epoch [2/300], Step [26600/27733], Loss: 4.7115\n",
      "Epoch [2/300], Step [26700/27733], Loss: 4.7325\n",
      "Epoch [2/300], Step [26800/27733], Loss: 4.6048\n",
      "Epoch [2/300], Step [26900/27733], Loss: 5.0683\n",
      "Epoch [2/300], Step [27000/27733], Loss: 3.9588\n",
      "Epoch [2/300], Step [27100/27733], Loss: 5.5919\n",
      "Epoch [2/300], Step [27200/27733], Loss: 5.9041\n",
      "Epoch [2/300], Step [27300/27733], Loss: 5.6046\n",
      "Epoch [2/300], Step [27400/27733], Loss: 4.6516\n",
      "Epoch [2/300], Step [27500/27733], Loss: 5.3515\n",
      "Epoch [2/300], Step [27600/27733], Loss: 4.8625\n",
      "Epoch [2/300], Step [27700/27733], Loss: 5.2157\n",
      "Epoch [3/300], Step [100/27733], Loss: 5.3823\n",
      "Epoch [3/300], Step [200/27733], Loss: 5.1259\n",
      "Epoch [3/300], Step [300/27733], Loss: 3.3164\n",
      "Epoch [3/300], Step [400/27733], Loss: 3.6168\n",
      "Epoch [3/300], Step [500/27733], Loss: 4.2299\n",
      "Epoch [3/300], Step [600/27733], Loss: 3.8547\n",
      "Epoch [3/300], Step [700/27733], Loss: 4.3053\n",
      "Epoch [3/300], Step [800/27733], Loss: 4.1534\n",
      "Epoch [3/300], Step [900/27733], Loss: 4.9558\n",
      "Epoch [3/300], Step [1000/27733], Loss: 4.9795\n",
      "Epoch [3/300], Step [1100/27733], Loss: 4.5626\n",
      "Epoch [3/300], Step [1200/27733], Loss: 4.1437\n",
      "Epoch [3/300], Step [1300/27733], Loss: 4.7038\n",
      "Epoch [3/300], Step [1400/27733], Loss: 4.3181\n",
      "Epoch [3/300], Step [1500/27733], Loss: 4.5803\n",
      "Epoch [3/300], Step [1600/27733], Loss: 3.9195\n",
      "Epoch [3/300], Step [1700/27733], Loss: 4.4780\n",
      "Epoch [3/300], Step [1800/27733], Loss: 4.7457\n",
      "Epoch [3/300], Step [1900/27733], Loss: 4.9364\n",
      "Epoch [3/300], Step [2000/27733], Loss: 4.1728\n",
      "Epoch [3/300], Step [2100/27733], Loss: 4.9615\n",
      "Epoch [3/300], Step [2200/27733], Loss: 4.1639\n",
      "Epoch [3/300], Step [2300/27733], Loss: 4.4273\n",
      "Epoch [3/300], Step [2400/27733], Loss: 3.6344\n",
      "Epoch [3/300], Step [2500/27733], Loss: 4.7128\n",
      "Epoch [3/300], Step [2600/27733], Loss: 4.2090\n",
      "Epoch [3/300], Step [2700/27733], Loss: 4.5761\n",
      "Epoch [3/300], Step [2800/27733], Loss: 4.1988\n",
      "Epoch [3/300], Step [2900/27733], Loss: 4.1880\n",
      "Epoch [3/300], Step [3000/27733], Loss: 3.9805\n",
      "Epoch [3/300], Step [3100/27733], Loss: 4.5519\n",
      "Epoch [3/300], Step [3200/27733], Loss: 4.9119\n",
      "Epoch [3/300], Step [3300/27733], Loss: 3.6844\n",
      "Epoch [3/300], Step [3400/27733], Loss: 5.7888\n",
      "Epoch [3/300], Step [3500/27733], Loss: 5.2499\n",
      "Epoch [3/300], Step [3600/27733], Loss: 5.3709\n",
      "Epoch [3/300], Step [3700/27733], Loss: 4.1555\n",
      "Epoch [3/300], Step [3800/27733], Loss: 5.6124\n",
      "Epoch [3/300], Step [3900/27733], Loss: 4.7184\n",
      "Epoch [3/300], Step [4000/27733], Loss: 4.3681\n",
      "Epoch [3/300], Step [4100/27733], Loss: 4.3216\n",
      "Epoch [3/300], Step [4200/27733], Loss: 5.0656\n",
      "Epoch [3/300], Step [4300/27733], Loss: 3.5462\n",
      "Epoch [3/300], Step [4400/27733], Loss: 3.3228\n",
      "Epoch [3/300], Step [4500/27733], Loss: 4.8687\n",
      "Epoch [3/300], Step [4600/27733], Loss: 4.7409\n",
      "Epoch [3/300], Step [4700/27733], Loss: 3.9700\n",
      "Epoch [3/300], Step [4800/27733], Loss: 4.2672\n",
      "Epoch [3/300], Step [4900/27733], Loss: 5.0342\n",
      "Epoch [3/300], Step [5000/27733], Loss: 4.1241\n",
      "Epoch [3/300], Step [5100/27733], Loss: 4.2657\n",
      "Epoch [3/300], Step [5200/27733], Loss: 4.7771\n",
      "Epoch [3/300], Step [5300/27733], Loss: 4.9468\n",
      "Epoch [3/300], Step [5400/27733], Loss: 4.0660\n",
      "Epoch [3/300], Step [5500/27733], Loss: 4.7104\n",
      "Epoch [3/300], Step [5600/27733], Loss: 5.0838\n",
      "Epoch [3/300], Step [5700/27733], Loss: 4.1926\n",
      "Epoch [3/300], Step [5800/27733], Loss: 5.0214\n",
      "Epoch [3/300], Step [5900/27733], Loss: 4.3803\n",
      "Epoch [3/300], Step [6000/27733], Loss: 4.1429\n",
      "Epoch [3/300], Step [6100/27733], Loss: 4.4915\n",
      "Epoch [3/300], Step [6200/27733], Loss: 3.9129\n",
      "Epoch [3/300], Step [6300/27733], Loss: 3.9919\n",
      "Epoch [3/300], Step [6400/27733], Loss: 4.5092\n",
      "Epoch [3/300], Step [6500/27733], Loss: 5.4730\n",
      "Epoch [3/300], Step [6600/27733], Loss: 5.3563\n",
      "Epoch [3/300], Step [6700/27733], Loss: 5.9044\n",
      "Epoch [3/300], Step [6800/27733], Loss: 4.0907\n",
      "Epoch [3/300], Step [6900/27733], Loss: 5.6737\n",
      "Epoch [3/300], Step [7000/27733], Loss: 5.4361\n",
      "Epoch [3/300], Step [7100/27733], Loss: 4.4396\n",
      "Epoch [3/300], Step [7200/27733], Loss: 4.2210\n",
      "Epoch [3/300], Step [7300/27733], Loss: 4.7939\n",
      "Epoch [3/300], Step [7400/27733], Loss: 5.5208\n",
      "Epoch [3/300], Step [7500/27733], Loss: 4.8654\n",
      "Epoch [3/300], Step [7600/27733], Loss: 4.4814\n",
      "Epoch [3/300], Step [7700/27733], Loss: 4.6817\n",
      "Epoch [3/300], Step [7800/27733], Loss: 3.8920\n",
      "Epoch [3/300], Step [7900/27733], Loss: 3.7422\n",
      "Epoch [3/300], Step [8000/27733], Loss: 3.4295\n",
      "Epoch [3/300], Step [8100/27733], Loss: 4.5205\n",
      "Epoch [3/300], Step [8200/27733], Loss: 4.6675\n",
      "Epoch [3/300], Step [8300/27733], Loss: 4.5065\n",
      "Epoch [3/300], Step [8400/27733], Loss: 5.3134\n",
      "Epoch [3/300], Step [8500/27733], Loss: 5.1455\n",
      "Epoch [3/300], Step [8600/27733], Loss: 4.1885\n",
      "Epoch [3/300], Step [8700/27733], Loss: 3.3966\n",
      "Epoch [3/300], Step [8800/27733], Loss: 4.2273\n",
      "Epoch [3/300], Step [8900/27733], Loss: 4.3005\n",
      "Epoch [3/300], Step [9000/27733], Loss: 4.4734\n",
      "Epoch [3/300], Step [9100/27733], Loss: 5.0898\n",
      "Epoch [3/300], Step [9200/27733], Loss: 4.3587\n",
      "Epoch [3/300], Step [9300/27733], Loss: 5.0006\n",
      "Epoch [3/300], Step [9400/27733], Loss: 5.3190\n",
      "Epoch [3/300], Step [9500/27733], Loss: 5.2728\n",
      "Epoch [3/300], Step [9600/27733], Loss: 3.9690\n",
      "Epoch [3/300], Step [9700/27733], Loss: 4.9011\n",
      "Epoch [3/300], Step [9800/27733], Loss: 4.9632\n",
      "Epoch [3/300], Step [9900/27733], Loss: 4.2906\n",
      "Epoch [3/300], Step [10000/27733], Loss: 3.9109\n",
      "Epoch [3/300], Step [10100/27733], Loss: 4.4583\n",
      "Epoch [3/300], Step [10200/27733], Loss: 4.8547\n",
      "Epoch [3/300], Step [10300/27733], Loss: 4.3506\n",
      "Epoch [3/300], Step [10400/27733], Loss: 4.3385\n",
      "Epoch [3/300], Step [10500/27733], Loss: 4.1372\n",
      "Epoch [3/300], Step [10600/27733], Loss: 4.0555\n",
      "Epoch [3/300], Step [10700/27733], Loss: 4.1428\n",
      "Epoch [3/300], Step [10800/27733], Loss: 5.1610\n",
      "Epoch [3/300], Step [10900/27733], Loss: 4.3794\n",
      "Epoch [3/300], Step [11000/27733], Loss: 3.7362\n",
      "Epoch [3/300], Step [11100/27733], Loss: 4.6596\n",
      "Epoch [3/300], Step [11200/27733], Loss: 4.4282\n",
      "Epoch [3/300], Step [11300/27733], Loss: 3.4812\n",
      "Epoch [3/300], Step [11400/27733], Loss: 4.2768\n",
      "Epoch [3/300], Step [11500/27733], Loss: 4.9696\n",
      "Epoch [3/300], Step [11600/27733], Loss: 4.5109\n",
      "Epoch [3/300], Step [11700/27733], Loss: 4.0651\n",
      "Epoch [3/300], Step [11800/27733], Loss: 4.5343\n",
      "Epoch [3/300], Step [11900/27733], Loss: 4.3561\n",
      "Epoch [3/300], Step [12000/27733], Loss: 4.7514\n",
      "Epoch [3/300], Step [12100/27733], Loss: 3.9372\n",
      "Epoch [3/300], Step [12200/27733], Loss: 3.3667\n",
      "Epoch [3/300], Step [12300/27733], Loss: 4.6052\n",
      "Epoch [3/300], Step [12400/27733], Loss: 4.3369\n",
      "Epoch [3/300], Step [12500/27733], Loss: 5.6724\n",
      "Epoch [3/300], Step [12600/27733], Loss: 4.8047\n",
      "Epoch [3/300], Step [12700/27733], Loss: 5.4874\n",
      "Epoch [3/300], Step [12800/27733], Loss: 4.9661\n",
      "Epoch [3/300], Step [12900/27733], Loss: 4.7957\n",
      "Epoch [3/300], Step [13000/27733], Loss: 3.9403\n",
      "Epoch [3/300], Step [13100/27733], Loss: 5.0523\n",
      "Epoch [3/300], Step [13200/27733], Loss: 3.9220\n",
      "Epoch [3/300], Step [13300/27733], Loss: 6.1393\n",
      "Epoch [3/300], Step [13400/27733], Loss: 4.1215\n",
      "Epoch [3/300], Step [13500/27733], Loss: 4.6249\n",
      "Epoch [3/300], Step [13600/27733], Loss: 4.4805\n",
      "Epoch [3/300], Step [13700/27733], Loss: 5.1068\n",
      "Epoch [3/300], Step [13800/27733], Loss: 5.5442\n",
      "Epoch [3/300], Step [13900/27733], Loss: 4.7039\n",
      "Epoch [3/300], Step [14000/27733], Loss: 4.9391\n",
      "Epoch [3/300], Step [14100/27733], Loss: 4.8957\n",
      "Epoch [3/300], Step [14200/27733], Loss: 3.7183\n",
      "Epoch [3/300], Step [14300/27733], Loss: 4.3908\n",
      "Epoch [3/300], Step [14400/27733], Loss: 3.8766\n",
      "Epoch [3/300], Step [14500/27733], Loss: 4.5476\n",
      "Epoch [3/300], Step [14600/27733], Loss: 4.0560\n",
      "Epoch [3/300], Step [14700/27733], Loss: 5.1134\n",
      "Epoch [3/300], Step [14800/27733], Loss: 5.6637\n",
      "Epoch [3/300], Step [14900/27733], Loss: 4.5172\n",
      "Epoch [3/300], Step [15000/27733], Loss: 5.2103\n",
      "Epoch [3/300], Step [15100/27733], Loss: 4.9206\n",
      "Epoch [3/300], Step [15200/27733], Loss: 4.7580\n",
      "Epoch [3/300], Step [15300/27733], Loss: 4.7664\n",
      "Epoch [3/300], Step [15400/27733], Loss: 4.4859\n",
      "Epoch [3/300], Step [15500/27733], Loss: 4.6418\n",
      "Epoch [3/300], Step [15600/27733], Loss: 4.7372\n",
      "Epoch [3/300], Step [15700/27733], Loss: 4.5698\n",
      "Epoch [3/300], Step [15800/27733], Loss: 4.5204\n",
      "Epoch [3/300], Step [15900/27733], Loss: 4.6673\n",
      "Epoch [3/300], Step [16000/27733], Loss: 3.9233\n",
      "Epoch [3/300], Step [16100/27733], Loss: 4.1179\n",
      "Epoch [3/300], Step [16200/27733], Loss: 4.8494\n",
      "Epoch [3/300], Step [16300/27733], Loss: 3.8861\n",
      "Epoch [3/300], Step [16400/27733], Loss: 5.4879\n",
      "Epoch [3/300], Step [16500/27733], Loss: 4.5958\n",
      "Epoch [3/300], Step [16600/27733], Loss: 5.3919\n",
      "Epoch [3/300], Step [16700/27733], Loss: 3.9362\n",
      "Epoch [3/300], Step [16800/27733], Loss: 4.3773\n",
      "Epoch [3/300], Step [16900/27733], Loss: 3.5534\n",
      "Epoch [3/300], Step [17000/27733], Loss: 4.8160\n",
      "Epoch [3/300], Step [17100/27733], Loss: 5.4103\n",
      "Epoch [3/300], Step [17200/27733], Loss: 4.7506\n",
      "Epoch [3/300], Step [17300/27733], Loss: 4.8989\n",
      "Epoch [3/300], Step [17400/27733], Loss: 4.8801\n",
      "Epoch [3/300], Step [17500/27733], Loss: 5.7683\n",
      "Epoch [3/300], Step [17600/27733], Loss: 4.5181\n",
      "Epoch [3/300], Step [17700/27733], Loss: 5.3716\n",
      "Epoch [3/300], Step [17800/27733], Loss: 4.6377\n",
      "Epoch [3/300], Step [17900/27733], Loss: 4.4323\n",
      "Epoch [3/300], Step [18000/27733], Loss: 5.6289\n",
      "Epoch [3/300], Step [18100/27733], Loss: 4.5595\n",
      "Epoch [3/300], Step [18200/27733], Loss: 5.2232\n",
      "Epoch [3/300], Step [18300/27733], Loss: 4.2532\n",
      "Epoch [3/300], Step [18400/27733], Loss: 5.2512\n",
      "Epoch [3/300], Step [18500/27733], Loss: 4.1642\n",
      "Epoch [3/300], Step [18600/27733], Loss: 4.1391\n",
      "Epoch [3/300], Step [18700/27733], Loss: 5.0500\n",
      "Epoch [3/300], Step [18800/27733], Loss: 5.3193\n",
      "Epoch [3/300], Step [18900/27733], Loss: 4.5117\n",
      "Epoch [3/300], Step [19000/27733], Loss: 4.9451\n",
      "Epoch [3/300], Step [19100/27733], Loss: 5.3031\n",
      "Epoch [3/300], Step [19200/27733], Loss: 4.1601\n",
      "Epoch [3/300], Step [19300/27733], Loss: 4.8177\n",
      "Epoch [3/300], Step [19400/27733], Loss: 4.5043\n",
      "Epoch [3/300], Step [19500/27733], Loss: 4.0786\n",
      "Epoch [3/300], Step [19600/27733], Loss: 4.3403\n",
      "Epoch [3/300], Step [19700/27733], Loss: 5.3543\n",
      "Epoch [3/300], Step [19800/27733], Loss: 5.5936\n",
      "Epoch [3/300], Step [19900/27733], Loss: 4.3990\n",
      "Epoch [3/300], Step [20000/27733], Loss: 4.1816\n",
      "Epoch [3/300], Step [20100/27733], Loss: 4.5986\n",
      "Epoch [3/300], Step [20200/27733], Loss: 4.8327\n",
      "Epoch [3/300], Step [20300/27733], Loss: 4.0646\n",
      "Epoch [3/300], Step [20400/27733], Loss: 4.1566\n",
      "Epoch [3/300], Step [20500/27733], Loss: 4.3820\n",
      "Epoch [3/300], Step [20600/27733], Loss: 4.6368\n",
      "Epoch [3/300], Step [20700/27733], Loss: 3.6901\n",
      "Epoch [3/300], Step [20800/27733], Loss: 4.2732\n",
      "Epoch [3/300], Step [20900/27733], Loss: 4.2432\n",
      "Epoch [3/300], Step [21000/27733], Loss: 5.0737\n",
      "Epoch [3/300], Step [21100/27733], Loss: 5.0355\n",
      "Epoch [3/300], Step [21200/27733], Loss: 3.5815\n",
      "Epoch [3/300], Step [21300/27733], Loss: 5.2135\n",
      "Epoch [3/300], Step [21400/27733], Loss: 4.5114\n",
      "Epoch [3/300], Step [21500/27733], Loss: 4.3759\n",
      "Epoch [3/300], Step [21600/27733], Loss: 5.5933\n",
      "Epoch [3/300], Step [21700/27733], Loss: 4.2180\n",
      "Epoch [3/300], Step [21800/27733], Loss: 4.5865\n",
      "Epoch [3/300], Step [21900/27733], Loss: 4.7156\n",
      "Epoch [3/300], Step [22000/27733], Loss: 4.4829\n",
      "Epoch [3/300], Step [22100/27733], Loss: 4.9281\n",
      "Epoch [3/300], Step [22200/27733], Loss: 4.9001\n",
      "Epoch [3/300], Step [22300/27733], Loss: 5.0489\n",
      "Epoch [3/300], Step [22400/27733], Loss: 4.4800\n",
      "Epoch [3/300], Step [22500/27733], Loss: 3.6991\n",
      "Epoch [3/300], Step [22600/27733], Loss: 4.5810\n",
      "Epoch [3/300], Step [22700/27733], Loss: 5.3483\n",
      "Epoch [3/300], Step [22800/27733], Loss: 4.8292\n",
      "Epoch [3/300], Step [22900/27733], Loss: 4.3834\n",
      "Epoch [3/300], Step [23000/27733], Loss: 4.2975\n",
      "Epoch [3/300], Step [23100/27733], Loss: 4.4574\n",
      "Epoch [3/300], Step [23200/27733], Loss: 4.3540\n",
      "Epoch [3/300], Step [23300/27733], Loss: 4.3632\n",
      "Epoch [3/300], Step [23400/27733], Loss: 4.9700\n",
      "Epoch [3/300], Step [23500/27733], Loss: 5.1191\n",
      "Epoch [3/300], Step [23600/27733], Loss: 5.5809\n",
      "Epoch [3/300], Step [23700/27733], Loss: 4.1917\n",
      "Epoch [3/300], Step [23800/27733], Loss: 4.8136\n",
      "Epoch [3/300], Step [23900/27733], Loss: 4.4769\n",
      "Epoch [3/300], Step [24000/27733], Loss: 3.6908\n",
      "Epoch [3/300], Step [24100/27733], Loss: 4.6544\n",
      "Epoch [3/300], Step [24200/27733], Loss: 5.6314\n",
      "Epoch [3/300], Step [24300/27733], Loss: 4.6168\n",
      "Epoch [3/300], Step [24400/27733], Loss: 4.3064\n",
      "Epoch [3/300], Step [24500/27733], Loss: 3.6245\n",
      "Epoch [3/300], Step [24600/27733], Loss: 5.7301\n",
      "Epoch [3/300], Step [24700/27733], Loss: 5.1614\n",
      "Epoch [3/300], Step [24800/27733], Loss: 4.8641\n",
      "Epoch [3/300], Step [24900/27733], Loss: 5.0587\n",
      "Epoch [3/300], Step [25000/27733], Loss: 5.7711\n",
      "Epoch [3/300], Step [25100/27733], Loss: 4.6442\n",
      "Epoch [3/300], Step [25200/27733], Loss: 5.0997\n",
      "Epoch [3/300], Step [25300/27733], Loss: 4.4841\n",
      "Epoch [3/300], Step [25400/27733], Loss: 4.8948\n",
      "Epoch [3/300], Step [25500/27733], Loss: 5.0606\n",
      "Epoch [3/300], Step [25600/27733], Loss: 3.3026\n",
      "Epoch [3/300], Step [25700/27733], Loss: 5.8631\n",
      "Epoch [3/300], Step [25800/27733], Loss: 3.8191\n",
      "Epoch [3/300], Step [25900/27733], Loss: 4.9884\n",
      "Epoch [3/300], Step [26000/27733], Loss: 4.9886\n",
      "Epoch [3/300], Step [26100/27733], Loss: 4.5815\n",
      "Epoch [3/300], Step [26200/27733], Loss: 3.9819\n",
      "Epoch [3/300], Step [26300/27733], Loss: 4.5479\n",
      "Epoch [3/300], Step [26400/27733], Loss: 4.7952\n",
      "Epoch [3/300], Step [26500/27733], Loss: 4.9886\n",
      "Epoch [3/300], Step [26600/27733], Loss: 4.8283\n",
      "Epoch [3/300], Step [26700/27733], Loss: 4.5124\n",
      "Epoch [3/300], Step [26800/27733], Loss: 6.2420\n",
      "Epoch [3/300], Step [26900/27733], Loss: 3.8759\n",
      "Epoch [3/300], Step [27000/27733], Loss: 5.8736\n",
      "Epoch [3/300], Step [27100/27733], Loss: 4.3569\n",
      "Epoch [3/300], Step [27200/27733], Loss: 5.7241\n",
      "Epoch [3/300], Step [27300/27733], Loss: 5.2615\n",
      "Epoch [3/300], Step [27400/27733], Loss: 3.9232\n",
      "Epoch [3/300], Step [27500/27733], Loss: 4.8519\n",
      "Epoch [3/300], Step [27600/27733], Loss: 4.5183\n",
      "Epoch [3/300], Step [27700/27733], Loss: 4.7294\n",
      "Epoch [4/300], Step [100/27733], Loss: 4.3325\n",
      "Epoch [4/300], Step [200/27733], Loss: 4.2508\n",
      "Epoch [4/300], Step [300/27733], Loss: 3.4829\n",
      "Epoch [4/300], Step [400/27733], Loss: 4.1246\n",
      "Epoch [4/300], Step [500/27733], Loss: 3.6207\n",
      "Epoch [4/300], Step [600/27733], Loss: 3.6320\n",
      "Epoch [4/300], Step [700/27733], Loss: 4.2772\n",
      "Epoch [4/300], Step [800/27733], Loss: 4.3386\n",
      "Epoch [4/300], Step [900/27733], Loss: 4.1513\n",
      "Epoch [4/300], Step [1000/27733], Loss: 4.5873\n",
      "Epoch [4/300], Step [1100/27733], Loss: 3.7047\n",
      "Epoch [4/300], Step [1200/27733], Loss: 4.0027\n",
      "Epoch [4/300], Step [1300/27733], Loss: 5.0619\n",
      "Epoch [4/300], Step [1400/27733], Loss: 4.1708\n",
      "Epoch [4/300], Step [1500/27733], Loss: 4.4767\n",
      "Epoch [4/300], Step [1600/27733], Loss: 3.6561\n",
      "Epoch [4/300], Step [1700/27733], Loss: 4.7720\n",
      "Epoch [4/300], Step [1800/27733], Loss: 3.6846\n",
      "Epoch [4/300], Step [1900/27733], Loss: 4.8405\n",
      "Epoch [4/300], Step [2000/27733], Loss: 4.3390\n",
      "Epoch [4/300], Step [2100/27733], Loss: 3.6439\n",
      "Epoch [4/300], Step [2200/27733], Loss: 3.9182\n",
      "Epoch [4/300], Step [2300/27733], Loss: 5.2618\n",
      "Epoch [4/300], Step [2400/27733], Loss: 5.1926\n",
      "Epoch [4/300], Step [2500/27733], Loss: 4.4307\n",
      "Epoch [4/300], Step [2600/27733], Loss: 4.5445\n",
      "Epoch [4/300], Step [2700/27733], Loss: 4.8922\n",
      "Epoch [4/300], Step [2800/27733], Loss: 5.0974\n",
      "Epoch [4/300], Step [2900/27733], Loss: 4.0484\n",
      "Epoch [4/300], Step [3000/27733], Loss: 4.2548\n",
      "Epoch [4/300], Step [3100/27733], Loss: 3.8618\n",
      "Epoch [4/300], Step [3200/27733], Loss: 4.0034\n",
      "Epoch [4/300], Step [3300/27733], Loss: 4.1616\n",
      "Epoch [4/300], Step [3400/27733], Loss: 4.0928\n",
      "Epoch [4/300], Step [3500/27733], Loss: 4.7384\n",
      "Epoch [4/300], Step [3600/27733], Loss: 4.5847\n",
      "Epoch [4/300], Step [3700/27733], Loss: 5.1933\n",
      "Epoch [4/300], Step [3800/27733], Loss: 6.2469\n",
      "Epoch [4/300], Step [3900/27733], Loss: 4.6861\n",
      "Epoch [4/300], Step [4000/27733], Loss: 4.3820\n",
      "Epoch [4/300], Step [4100/27733], Loss: 4.3743\n",
      "Epoch [4/300], Step [4200/27733], Loss: 4.3617\n",
      "Epoch [4/300], Step [4300/27733], Loss: 4.3462\n",
      "Epoch [4/300], Step [4400/27733], Loss: 5.0442\n",
      "Epoch [4/300], Step [4500/27733], Loss: 4.8599\n",
      "Epoch [4/300], Step [4600/27733], Loss: 3.3586\n",
      "Epoch [4/300], Step [4700/27733], Loss: 4.2703\n",
      "Epoch [4/300], Step [4800/27733], Loss: 4.4405\n",
      "Epoch [4/300], Step [4900/27733], Loss: 4.6535\n",
      "Epoch [4/300], Step [5000/27733], Loss: 5.4707\n",
      "Epoch [4/300], Step [5100/27733], Loss: 5.0864\n",
      "Epoch [4/300], Step [5200/27733], Loss: 2.7975\n",
      "Epoch [4/300], Step [5300/27733], Loss: 4.0308\n",
      "Epoch [4/300], Step [5400/27733], Loss: 3.2233\n",
      "Epoch [4/300], Step [5500/27733], Loss: 4.1478\n",
      "Epoch [4/300], Step [5600/27733], Loss: 4.8283\n",
      "Epoch [4/300], Step [5700/27733], Loss: 3.4749\n",
      "Epoch [4/300], Step [5800/27733], Loss: 4.1180\n",
      "Epoch [4/300], Step [5900/27733], Loss: 4.2911\n",
      "Epoch [4/300], Step [6000/27733], Loss: 3.9592\n",
      "Epoch [4/300], Step [6100/27733], Loss: 4.2620\n",
      "Epoch [4/300], Step [6200/27733], Loss: 4.7540\n",
      "Epoch [4/300], Step [6300/27733], Loss: 5.1752\n",
      "Epoch [4/300], Step [6400/27733], Loss: 4.7164\n",
      "Epoch [4/300], Step [6500/27733], Loss: 4.5411\n",
      "Epoch [4/300], Step [6600/27733], Loss: 4.3057\n",
      "Epoch [4/300], Step [6700/27733], Loss: 4.3676\n",
      "Epoch [4/300], Step [6800/27733], Loss: 3.4400\n",
      "Epoch [4/300], Step [6900/27733], Loss: 3.4850\n",
      "Epoch [4/300], Step [7000/27733], Loss: 4.6945\n",
      "Epoch [4/300], Step [7100/27733], Loss: 3.9867\n",
      "Epoch [4/300], Step [7200/27733], Loss: 3.7704\n",
      "Epoch [4/300], Step [7300/27733], Loss: 4.3261\n",
      "Epoch [4/300], Step [7400/27733], Loss: 4.3859\n",
      "Epoch [4/300], Step [7500/27733], Loss: 4.1244\n",
      "Epoch [4/300], Step [7600/27733], Loss: 4.4730\n",
      "Epoch [4/300], Step [7700/27733], Loss: 3.8071\n",
      "Epoch [4/300], Step [7800/27733], Loss: 4.1591\n",
      "Epoch [4/300], Step [7900/27733], Loss: 4.9233\n",
      "Epoch [4/300], Step [8000/27733], Loss: 4.6688\n",
      "Epoch [4/300], Step [8100/27733], Loss: 4.2293\n",
      "Epoch [4/300], Step [8200/27733], Loss: 4.7413\n",
      "Epoch [4/300], Step [8300/27733], Loss: 5.3268\n",
      "Epoch [4/300], Step [8400/27733], Loss: 5.1812\n",
      "Epoch [4/300], Step [8500/27733], Loss: 5.4197\n",
      "Epoch [4/300], Step [8600/27733], Loss: 4.1630\n",
      "Epoch [4/300], Step [8700/27733], Loss: 4.6715\n",
      "Epoch [4/300], Step [8800/27733], Loss: 3.8927\n",
      "Epoch [4/300], Step [8900/27733], Loss: 4.8346\n",
      "Epoch [4/300], Step [9000/27733], Loss: 3.1273\n",
      "Epoch [4/300], Step [9100/27733], Loss: 4.1936\n",
      "Epoch [4/300], Step [9200/27733], Loss: 4.1711\n",
      "Epoch [4/300], Step [9300/27733], Loss: 4.6329\n",
      "Epoch [4/300], Step [9400/27733], Loss: 4.3251\n",
      "Epoch [4/300], Step [9500/27733], Loss: 5.1723\n",
      "Epoch [4/300], Step [9600/27733], Loss: 3.4561\n",
      "Epoch [4/300], Step [9700/27733], Loss: 3.4770\n",
      "Epoch [4/300], Step [9800/27733], Loss: 4.4922\n",
      "Epoch [4/300], Step [9900/27733], Loss: 4.2520\n",
      "Epoch [4/300], Step [10000/27733], Loss: 3.9422\n",
      "Epoch [4/300], Step [10100/27733], Loss: 4.0661\n",
      "Epoch [4/300], Step [10200/27733], Loss: 3.9259\n",
      "Epoch [4/300], Step [10300/27733], Loss: 4.8082\n",
      "Epoch [4/300], Step [10400/27733], Loss: 4.5325\n",
      "Epoch [4/300], Step [10500/27733], Loss: 4.5164\n",
      "Epoch [4/300], Step [10600/27733], Loss: 3.5159\n",
      "Epoch [4/300], Step [10700/27733], Loss: 3.7732\n",
      "Epoch [4/300], Step [10800/27733], Loss: 4.6029\n",
      "Epoch [4/300], Step [10900/27733], Loss: 4.0117\n",
      "Epoch [4/300], Step [11000/27733], Loss: 4.4489\n",
      "Epoch [4/300], Step [11100/27733], Loss: 3.9612\n",
      "Epoch [4/300], Step [11200/27733], Loss: 5.4902\n",
      "Epoch [4/300], Step [11300/27733], Loss: 4.4757\n",
      "Epoch [4/300], Step [11400/27733], Loss: 4.4599\n",
      "Epoch [4/300], Step [11500/27733], Loss: 3.5665\n",
      "Epoch [4/300], Step [11600/27733], Loss: 4.6897\n",
      "Epoch [4/300], Step [11700/27733], Loss: 4.4388\n",
      "Epoch [4/300], Step [11800/27733], Loss: 4.6123\n",
      "Epoch [4/300], Step [11900/27733], Loss: 4.2337\n",
      "Epoch [4/300], Step [12000/27733], Loss: 5.2038\n",
      "Epoch [4/300], Step [12100/27733], Loss: 4.0432\n",
      "Epoch [4/300], Step [12200/27733], Loss: 4.6390\n",
      "Epoch [4/300], Step [12300/27733], Loss: 4.3399\n",
      "Epoch [4/300], Step [12400/27733], Loss: 5.5570\n",
      "Epoch [4/300], Step [12500/27733], Loss: 4.5211\n",
      "Epoch [4/300], Step [12600/27733], Loss: 4.1868\n",
      "Epoch [4/300], Step [12700/27733], Loss: 4.9457\n",
      "Epoch [4/300], Step [12800/27733], Loss: 4.5003\n",
      "Epoch [4/300], Step [12900/27733], Loss: 3.9291\n",
      "Epoch [4/300], Step [13000/27733], Loss: 4.7275\n",
      "Epoch [4/300], Step [13100/27733], Loss: 4.0116\n",
      "Epoch [4/300], Step [13200/27733], Loss: 5.1815\n",
      "Epoch [4/300], Step [13300/27733], Loss: 4.0206\n",
      "Epoch [4/300], Step [13400/27733], Loss: 4.0745\n",
      "Epoch [4/300], Step [13500/27733], Loss: 3.6364\n",
      "Epoch [4/300], Step [13600/27733], Loss: 3.4354\n",
      "Epoch [4/300], Step [13700/27733], Loss: 3.8363\n",
      "Epoch [4/300], Step [13800/27733], Loss: 4.5494\n",
      "Epoch [4/300], Step [13900/27733], Loss: 4.4426\n",
      "Epoch [4/300], Step [14000/27733], Loss: 4.1657\n",
      "Epoch [4/300], Step [14100/27733], Loss: 4.9656\n",
      "Epoch [4/300], Step [14200/27733], Loss: 5.8473\n",
      "Epoch [4/300], Step [14300/27733], Loss: 4.1555\n",
      "Epoch [4/300], Step [14400/27733], Loss: 4.3032\n",
      "Epoch [4/300], Step [14500/27733], Loss: 4.0632\n",
      "Epoch [4/300], Step [14600/27733], Loss: 3.8561\n",
      "Epoch [4/300], Step [14700/27733], Loss: 4.7355\n",
      "Epoch [4/300], Step [14800/27733], Loss: 4.0896\n",
      "Epoch [4/300], Step [14900/27733], Loss: 3.6865\n",
      "Epoch [4/300], Step [15000/27733], Loss: 4.3122\n",
      "Epoch [4/300], Step [15100/27733], Loss: 4.3004\n",
      "Epoch [4/300], Step [15200/27733], Loss: 4.5849\n",
      "Epoch [4/300], Step [15300/27733], Loss: 4.3511\n",
      "Epoch [4/300], Step [15400/27733], Loss: 4.8655\n",
      "Epoch [4/300], Step [15500/27733], Loss: 4.3515\n",
      "Epoch [4/300], Step [15600/27733], Loss: 5.0211\n",
      "Epoch [4/300], Step [15700/27733], Loss: 3.4787\n",
      "Epoch [4/300], Step [15800/27733], Loss: 4.6163\n",
      "Epoch [4/300], Step [15900/27733], Loss: 3.6308\n",
      "Epoch [4/300], Step [16000/27733], Loss: 4.2544\n",
      "Epoch [4/300], Step [16100/27733], Loss: 4.2874\n",
      "Epoch [4/300], Step [16200/27733], Loss: 3.9658\n",
      "Epoch [4/300], Step [16300/27733], Loss: 3.6951\n",
      "Epoch [4/300], Step [16400/27733], Loss: 5.0762\n",
      "Epoch [4/300], Step [16500/27733], Loss: 4.7616\n",
      "Epoch [4/300], Step [16600/27733], Loss: 3.4882\n",
      "Epoch [4/300], Step [16700/27733], Loss: 4.8495\n",
      "Epoch [4/300], Step [16800/27733], Loss: 4.8927\n",
      "Epoch [4/300], Step [16900/27733], Loss: 3.8842\n",
      "Epoch [4/300], Step [17000/27733], Loss: 4.6016\n",
      "Epoch [4/300], Step [17100/27733], Loss: 4.9664\n",
      "Epoch [4/300], Step [17200/27733], Loss: 5.0290\n",
      "Epoch [4/300], Step [17300/27733], Loss: 4.1970\n",
      "Epoch [4/300], Step [17400/27733], Loss: 5.3511\n",
      "Epoch [4/300], Step [17500/27733], Loss: 4.6541\n",
      "Epoch [4/300], Step [17600/27733], Loss: 5.0581\n",
      "Epoch [4/300], Step [17700/27733], Loss: 4.2228\n",
      "Epoch [4/300], Step [17800/27733], Loss: 4.4828\n",
      "Epoch [4/300], Step [17900/27733], Loss: 3.6035\n",
      "Epoch [4/300], Step [18000/27733], Loss: 4.4762\n",
      "Epoch [4/300], Step [18100/27733], Loss: 3.2689\n",
      "Epoch [4/300], Step [18200/27733], Loss: 4.4510\n",
      "Epoch [4/300], Step [18300/27733], Loss: 4.2990\n",
      "Epoch [4/300], Step [18400/27733], Loss: 4.7140\n",
      "Epoch [4/300], Step [18500/27733], Loss: 4.4873\n",
      "Epoch [4/300], Step [18600/27733], Loss: 4.3794\n",
      "Epoch [4/300], Step [18700/27733], Loss: 4.3372\n",
      "Epoch [4/300], Step [18800/27733], Loss: 4.3371\n",
      "Epoch [4/300], Step [18900/27733], Loss: 4.3649\n",
      "Epoch [4/300], Step [19000/27733], Loss: 3.3361\n",
      "Epoch [4/300], Step [19100/27733], Loss: 4.7155\n",
      "Epoch [4/300], Step [19200/27733], Loss: 4.3108\n",
      "Epoch [4/300], Step [19300/27733], Loss: 4.0316\n",
      "Epoch [4/300], Step [19400/27733], Loss: 4.8990\n",
      "Epoch [4/300], Step [19500/27733], Loss: 4.9503\n",
      "Epoch [4/300], Step [19600/27733], Loss: 4.3789\n",
      "Epoch [4/300], Step [19700/27733], Loss: 4.5647\n",
      "Epoch [4/300], Step [19800/27733], Loss: 4.2943\n",
      "Epoch [4/300], Step [19900/27733], Loss: 4.8015\n",
      "Epoch [4/300], Step [20000/27733], Loss: 4.4728\n",
      "Epoch [4/300], Step [20100/27733], Loss: 4.4288\n",
      "Epoch [4/300], Step [20200/27733], Loss: 4.7886\n",
      "Epoch [4/300], Step [20300/27733], Loss: 4.3093\n",
      "Epoch [4/300], Step [20400/27733], Loss: 4.3903\n",
      "Epoch [4/300], Step [20500/27733], Loss: 5.5355\n",
      "Epoch [4/300], Step [20600/27733], Loss: 4.3622\n",
      "Epoch [4/300], Step [20700/27733], Loss: 4.4162\n",
      "Epoch [4/300], Step [20800/27733], Loss: 4.4939\n",
      "Epoch [4/300], Step [20900/27733], Loss: 4.1054\n",
      "Epoch [4/300], Step [21000/27733], Loss: 3.5351\n",
      "Epoch [4/300], Step [21100/27733], Loss: 4.3563\n",
      "Epoch [4/300], Step [21200/27733], Loss: 4.5189\n",
      "Epoch [4/300], Step [21300/27733], Loss: 5.0933\n",
      "Epoch [4/300], Step [21400/27733], Loss: 4.0942\n",
      "Epoch [4/300], Step [21500/27733], Loss: 4.3429\n",
      "Epoch [4/300], Step [21600/27733], Loss: 4.7082\n",
      "Epoch [4/300], Step [21700/27733], Loss: 4.6543\n",
      "Epoch [4/300], Step [21800/27733], Loss: 3.2757\n",
      "Epoch [4/300], Step [21900/27733], Loss: 3.8732\n",
      "Epoch [4/300], Step [22000/27733], Loss: 4.0524\n",
      "Epoch [4/300], Step [22100/27733], Loss: 4.1729\n",
      "Epoch [4/300], Step [22200/27733], Loss: 4.0228\n",
      "Epoch [4/300], Step [22300/27733], Loss: 5.8083\n",
      "Epoch [4/300], Step [22400/27733], Loss: 4.6323\n",
      "Epoch [4/300], Step [22500/27733], Loss: 5.0232\n",
      "Epoch [4/300], Step [22600/27733], Loss: 4.2166\n",
      "Epoch [4/300], Step [22700/27733], Loss: 4.4977\n",
      "Epoch [4/300], Step [22800/27733], Loss: 5.2630\n",
      "Epoch [4/300], Step [22900/27733], Loss: 4.1261\n",
      "Epoch [4/300], Step [23000/27733], Loss: 4.4042\n",
      "Epoch [4/300], Step [23100/27733], Loss: 5.2408\n",
      "Epoch [4/300], Step [23200/27733], Loss: 4.7124\n",
      "Epoch [4/300], Step [23300/27733], Loss: 4.1772\n",
      "Epoch [4/300], Step [23400/27733], Loss: 4.7568\n",
      "Epoch [4/300], Step [23500/27733], Loss: 4.1774\n",
      "Epoch [4/300], Step [23600/27733], Loss: 4.6222\n",
      "Epoch [4/300], Step [23700/27733], Loss: 3.8822\n",
      "Epoch [4/300], Step [23800/27733], Loss: 3.7287\n",
      "Epoch [4/300], Step [23900/27733], Loss: 4.9996\n",
      "Epoch [4/300], Step [24000/27733], Loss: 5.3142\n",
      "Epoch [4/300], Step [24100/27733], Loss: 4.2108\n",
      "Epoch [4/300], Step [24200/27733], Loss: 4.5749\n",
      "Epoch [4/300], Step [24300/27733], Loss: 4.3093\n",
      "Epoch [4/300], Step [24400/27733], Loss: 5.0857\n",
      "Epoch [4/300], Step [24500/27733], Loss: 4.4633\n",
      "Epoch [4/300], Step [24600/27733], Loss: 4.3616\n",
      "Epoch [4/300], Step [24700/27733], Loss: 4.8513\n",
      "Epoch [4/300], Step [24800/27733], Loss: 3.4652\n",
      "Epoch [4/300], Step [24900/27733], Loss: 4.6926\n",
      "Epoch [4/300], Step [25000/27733], Loss: 4.9740\n",
      "Epoch [4/300], Step [25100/27733], Loss: 5.0109\n",
      "Epoch [4/300], Step [25200/27733], Loss: 4.7393\n",
      "Epoch [4/300], Step [25300/27733], Loss: 5.3259\n",
      "Epoch [4/300], Step [25400/27733], Loss: 4.2244\n",
      "Epoch [4/300], Step [25500/27733], Loss: 4.5765\n",
      "Epoch [4/300], Step [25600/27733], Loss: 5.0013\n",
      "Epoch [4/300], Step [25700/27733], Loss: 4.5060\n",
      "Epoch [4/300], Step [25800/27733], Loss: 4.4810\n",
      "Epoch [4/300], Step [25900/27733], Loss: 4.7076\n",
      "Epoch [4/300], Step [26000/27733], Loss: 5.1755\n",
      "Epoch [4/300], Step [26100/27733], Loss: 5.1852\n",
      "Epoch [4/300], Step [26200/27733], Loss: 5.0984\n",
      "Epoch [4/300], Step [26300/27733], Loss: 4.7577\n",
      "Epoch [4/300], Step [26400/27733], Loss: 4.7181\n",
      "Epoch [4/300], Step [26500/27733], Loss: 5.0075\n",
      "Epoch [4/300], Step [26600/27733], Loss: 5.0465\n",
      "Epoch [4/300], Step [26700/27733], Loss: 4.2105\n",
      "Epoch [4/300], Step [26800/27733], Loss: 3.0576\n",
      "Epoch [4/300], Step [26900/27733], Loss: 4.9070\n",
      "Epoch [4/300], Step [27000/27733], Loss: 4.8852\n",
      "Epoch [4/300], Step [27100/27733], Loss: 3.2447\n",
      "Epoch [4/300], Step [27200/27733], Loss: 4.6199\n",
      "Epoch [4/300], Step [27300/27733], Loss: 4.4642\n",
      "Epoch [4/300], Step [27400/27733], Loss: 3.4966\n",
      "Epoch [4/300], Step [27500/27733], Loss: 4.6350\n",
      "Epoch [4/300], Step [27600/27733], Loss: 4.9956\n",
      "Epoch [4/300], Step [27700/27733], Loss: 4.5060\n",
      "Epoch [5/300], Step [100/27733], Loss: 3.6697\n",
      "Epoch [5/300], Step [200/27733], Loss: 3.7431\n",
      "Epoch [5/300], Step [300/27733], Loss: 3.8915\n",
      "Epoch [5/300], Step [400/27733], Loss: 3.4150\n",
      "Epoch [5/300], Step [500/27733], Loss: 4.4266\n",
      "Epoch [5/300], Step [600/27733], Loss: 4.0008\n",
      "Epoch [5/300], Step [700/27733], Loss: 3.5256\n",
      "Epoch [5/300], Step [800/27733], Loss: 3.8959\n",
      "Epoch [5/300], Step [900/27733], Loss: 5.1381\n",
      "Epoch [5/300], Step [1000/27733], Loss: 4.7918\n",
      "Epoch [5/300], Step [1100/27733], Loss: 4.4326\n",
      "Epoch [5/300], Step [1200/27733], Loss: 3.6686\n",
      "Epoch [5/300], Step [1300/27733], Loss: 3.2764\n",
      "Epoch [5/300], Step [1400/27733], Loss: 4.8861\n",
      "Epoch [5/300], Step [1500/27733], Loss: 4.2398\n",
      "Epoch [5/300], Step [1600/27733], Loss: 4.6750\n",
      "Epoch [5/300], Step [1700/27733], Loss: 3.5105\n",
      "Epoch [5/300], Step [1800/27733], Loss: 3.9872\n",
      "Epoch [5/300], Step [1900/27733], Loss: 4.0135\n",
      "Epoch [5/300], Step [2000/27733], Loss: 3.2102\n",
      "Epoch [5/300], Step [2100/27733], Loss: 3.9344\n",
      "Epoch [5/300], Step [2200/27733], Loss: 3.7399\n",
      "Epoch [5/300], Step [2300/27733], Loss: 3.9683\n",
      "Epoch [5/300], Step [2400/27733], Loss: 3.7495\n",
      "Epoch [5/300], Step [2500/27733], Loss: 4.6382\n",
      "Epoch [5/300], Step [2600/27733], Loss: 4.2054\n",
      "Epoch [5/300], Step [2700/27733], Loss: 3.4777\n",
      "Epoch [5/300], Step [2800/27733], Loss: 3.8575\n",
      "Epoch [5/300], Step [2900/27733], Loss: 3.2961\n",
      "Epoch [5/300], Step [3000/27733], Loss: 3.9312\n",
      "Epoch [5/300], Step [3100/27733], Loss: 4.1658\n",
      "Epoch [5/300], Step [3200/27733], Loss: 4.0536\n",
      "Epoch [5/300], Step [3300/27733], Loss: 3.8010\n",
      "Epoch [5/300], Step [3400/27733], Loss: 4.5002\n",
      "Epoch [5/300], Step [3500/27733], Loss: 3.7758\n",
      "Epoch [5/300], Step [3600/27733], Loss: 4.3171\n",
      "Epoch [5/300], Step [3700/27733], Loss: 4.0275\n",
      "Epoch [5/300], Step [3800/27733], Loss: 4.5579\n",
      "Epoch [5/300], Step [3900/27733], Loss: 3.6540\n",
      "Epoch [5/300], Step [4000/27733], Loss: 3.9164\n",
      "Epoch [5/300], Step [4100/27733], Loss: 4.7357\n",
      "Epoch [5/300], Step [4200/27733], Loss: 3.8822\n",
      "Epoch [5/300], Step [4300/27733], Loss: 3.1270\n",
      "Epoch [5/300], Step [4400/27733], Loss: 4.2545\n",
      "Epoch [5/300], Step [4500/27733], Loss: 3.9372\n",
      "Epoch [5/300], Step [4600/27733], Loss: 3.5328\n",
      "Epoch [5/300], Step [4700/27733], Loss: 3.4291\n",
      "Epoch [5/300], Step [4800/27733], Loss: 5.0352\n",
      "Epoch [5/300], Step [4900/27733], Loss: 3.5034\n",
      "Epoch [5/300], Step [5000/27733], Loss: 4.1657\n",
      "Epoch [5/300], Step [5100/27733], Loss: 3.4755\n",
      "Epoch [5/300], Step [5200/27733], Loss: 3.5779\n",
      "Epoch [5/300], Step [5300/27733], Loss: 4.5625\n",
      "Epoch [5/300], Step [5400/27733], Loss: 4.0080\n",
      "Epoch [5/300], Step [5500/27733], Loss: 4.9912\n",
      "Epoch [5/300], Step [5600/27733], Loss: 3.6645\n",
      "Epoch [5/300], Step [5700/27733], Loss: 4.8149\n",
      "Epoch [5/300], Step [5800/27733], Loss: 4.6330\n",
      "Epoch [5/300], Step [5900/27733], Loss: 5.2006\n",
      "Epoch [5/300], Step [6000/27733], Loss: 4.9652\n",
      "Epoch [5/300], Step [6100/27733], Loss: 4.0281\n",
      "Epoch [5/300], Step [6200/27733], Loss: 4.2408\n",
      "Epoch [5/300], Step [6300/27733], Loss: 3.6291\n",
      "Epoch [5/300], Step [6400/27733], Loss: 3.7459\n",
      "Epoch [5/300], Step [6500/27733], Loss: 3.6351\n",
      "Epoch [5/300], Step [6600/27733], Loss: 3.9634\n",
      "Epoch [5/300], Step [6700/27733], Loss: 4.0213\n",
      "Epoch [5/300], Step [6800/27733], Loss: 4.3713\n",
      "Epoch [5/300], Step [6900/27733], Loss: 4.4562\n",
      "Epoch [5/300], Step [7000/27733], Loss: 4.5795\n",
      "Epoch [5/300], Step [7100/27733], Loss: 3.2509\n",
      "Epoch [5/300], Step [7200/27733], Loss: 4.4152\n",
      "Epoch [5/300], Step [7300/27733], Loss: 4.2557\n",
      "Epoch [5/300], Step [7400/27733], Loss: 4.1214\n",
      "Epoch [5/300], Step [7500/27733], Loss: 3.6986\n",
      "Epoch [5/300], Step [7600/27733], Loss: 4.9039\n",
      "Epoch [5/300], Step [7700/27733], Loss: 3.7505\n",
      "Epoch [5/300], Step [7800/27733], Loss: 4.2618\n",
      "Epoch [5/300], Step [7900/27733], Loss: 3.2387\n",
      "Epoch [5/300], Step [8000/27733], Loss: 4.0952\n",
      "Epoch [5/300], Step [8100/27733], Loss: 5.1754\n",
      "Epoch [5/300], Step [8200/27733], Loss: 4.0908\n",
      "Epoch [5/300], Step [8300/27733], Loss: 3.3397\n",
      "Epoch [5/300], Step [8400/27733], Loss: 4.5284\n",
      "Epoch [5/300], Step [8500/27733], Loss: 4.2432\n",
      "Epoch [5/300], Step [8600/27733], Loss: 4.0199\n",
      "Epoch [5/300], Step [8700/27733], Loss: 4.4895\n",
      "Epoch [5/300], Step [8800/27733], Loss: 3.7964\n",
      "Epoch [5/300], Step [8900/27733], Loss: 4.6859\n",
      "Epoch [5/300], Step [9000/27733], Loss: 3.2696\n",
      "Epoch [5/300], Step [9100/27733], Loss: 4.5150\n",
      "Epoch [5/300], Step [9200/27733], Loss: 3.7451\n",
      "Epoch [5/300], Step [9300/27733], Loss: 4.0552\n",
      "Epoch [5/300], Step [9400/27733], Loss: 4.5208\n",
      "Epoch [5/300], Step [9500/27733], Loss: 4.6344\n",
      "Epoch [5/300], Step [9600/27733], Loss: 4.3551\n",
      "Epoch [5/300], Step [9700/27733], Loss: 4.9112\n",
      "Epoch [5/300], Step [9800/27733], Loss: 4.5170\n",
      "Epoch [5/300], Step [9900/27733], Loss: 3.3319\n",
      "Epoch [5/300], Step [10000/27733], Loss: 4.1724\n",
      "Epoch [5/300], Step [10100/27733], Loss: 3.6950\n",
      "Epoch [5/300], Step [10200/27733], Loss: 3.7660\n",
      "Epoch [5/300], Step [10300/27733], Loss: 3.3428\n",
      "Epoch [5/300], Step [10400/27733], Loss: 3.6247\n",
      "Epoch [5/300], Step [10500/27733], Loss: 4.2901\n",
      "Epoch [5/300], Step [10600/27733], Loss: 4.7359\n",
      "Epoch [5/300], Step [10700/27733], Loss: 4.4008\n",
      "Epoch [5/300], Step [10800/27733], Loss: 4.1701\n",
      "Epoch [5/300], Step [10900/27733], Loss: 4.2793\n",
      "Epoch [5/300], Step [11000/27733], Loss: 4.0640\n",
      "Epoch [5/300], Step [11100/27733], Loss: 3.9541\n",
      "Epoch [5/300], Step [11200/27733], Loss: 3.8631\n",
      "Epoch [5/300], Step [11300/27733], Loss: 4.8520\n",
      "Epoch [5/300], Step [11400/27733], Loss: 3.6084\n",
      "Epoch [5/300], Step [11500/27733], Loss: 3.8208\n",
      "Epoch [5/300], Step [11600/27733], Loss: 3.8275\n",
      "Epoch [5/300], Step [11700/27733], Loss: 5.1439\n",
      "Epoch [5/300], Step [11800/27733], Loss: 4.5429\n",
      "Epoch [5/300], Step [11900/27733], Loss: 4.7468\n",
      "Epoch [5/300], Step [12000/27733], Loss: 4.7779\n",
      "Epoch [5/300], Step [12100/27733], Loss: 4.1944\n",
      "Epoch [5/300], Step [12200/27733], Loss: 3.8382\n",
      "Epoch [5/300], Step [12300/27733], Loss: 4.0697\n",
      "Epoch [5/300], Step [12400/27733], Loss: 3.6976\n",
      "Epoch [5/300], Step [12500/27733], Loss: 4.5342\n",
      "Epoch [5/300], Step [12600/27733], Loss: 4.4474\n",
      "Epoch [5/300], Step [12700/27733], Loss: 4.8543\n",
      "Epoch [5/300], Step [12800/27733], Loss: 4.5407\n",
      "Epoch [5/300], Step [12900/27733], Loss: 4.1582\n",
      "Epoch [5/300], Step [13000/27733], Loss: 4.2479\n",
      "Epoch [5/300], Step [13100/27733], Loss: 4.2337\n",
      "Epoch [5/300], Step [13200/27733], Loss: 4.6696\n",
      "Epoch [5/300], Step [13300/27733], Loss: 4.4854\n",
      "Epoch [5/300], Step [13400/27733], Loss: 3.8267\n",
      "Epoch [5/300], Step [13500/27733], Loss: 4.7201\n",
      "Epoch [5/300], Step [13600/27733], Loss: 3.8658\n",
      "Epoch [5/300], Step [13700/27733], Loss: 4.3189\n",
      "Epoch [5/300], Step [13800/27733], Loss: 4.4879\n",
      "Epoch [5/300], Step [13900/27733], Loss: 4.6844\n",
      "Epoch [5/300], Step [14000/27733], Loss: 4.5615\n",
      "Epoch [5/300], Step [14100/27733], Loss: 3.8639\n",
      "Epoch [5/300], Step [14200/27733], Loss: 4.0761\n",
      "Epoch [5/300], Step [14300/27733], Loss: 3.7029\n",
      "Epoch [5/300], Step [14400/27733], Loss: 4.7283\n",
      "Epoch [5/300], Step [14500/27733], Loss: 4.8823\n",
      "Epoch [5/300], Step [14600/27733], Loss: 4.8524\n",
      "Epoch [5/300], Step [14700/27733], Loss: 4.0464\n",
      "Epoch [5/300], Step [14800/27733], Loss: 4.4639\n",
      "Epoch [5/300], Step [14900/27733], Loss: 5.1789\n",
      "Epoch [5/300], Step [15000/27733], Loss: 5.5375\n",
      "Epoch [5/300], Step [15100/27733], Loss: 3.4364\n",
      "Epoch [5/300], Step [15200/27733], Loss: 4.2771\n",
      "Epoch [5/300], Step [15300/27733], Loss: 4.2571\n",
      "Epoch [5/300], Step [15400/27733], Loss: 4.9683\n",
      "Epoch [5/300], Step [15500/27733], Loss: 4.2669\n",
      "Epoch [5/300], Step [15600/27733], Loss: 4.3990\n",
      "Epoch [5/300], Step [15700/27733], Loss: 5.0225\n",
      "Epoch [5/300], Step [15800/27733], Loss: 4.1879\n",
      "Epoch [5/300], Step [15900/27733], Loss: 4.6110\n",
      "Epoch [5/300], Step [16000/27733], Loss: 4.4125\n",
      "Epoch [5/300], Step [16100/27733], Loss: 4.5484\n",
      "Epoch [5/300], Step [16200/27733], Loss: 4.3468\n",
      "Epoch [5/300], Step [16300/27733], Loss: 4.1645\n",
      "Epoch [5/300], Step [16400/27733], Loss: 4.1535\n",
      "Epoch [5/300], Step [16500/27733], Loss: 3.8328\n",
      "Epoch [5/300], Step [16600/27733], Loss: 4.0219\n",
      "Epoch [5/300], Step [16700/27733], Loss: 3.6644\n",
      "Epoch [5/300], Step [16800/27733], Loss: 3.5586\n",
      "Epoch [5/300], Step [16900/27733], Loss: 3.8064\n",
      "Epoch [5/300], Step [17000/27733], Loss: 4.0683\n",
      "Epoch [5/300], Step [17100/27733], Loss: 5.2715\n",
      "Epoch [5/300], Step [17200/27733], Loss: 5.0227\n",
      "Epoch [5/300], Step [17300/27733], Loss: 4.4101\n",
      "Epoch [5/300], Step [17400/27733], Loss: 3.6512\n",
      "Epoch [5/300], Step [17500/27733], Loss: 3.6141\n",
      "Epoch [5/300], Step [17600/27733], Loss: 4.1698\n",
      "Epoch [5/300], Step [17700/27733], Loss: 4.7699\n",
      "Epoch [5/300], Step [17800/27733], Loss: 4.3841\n",
      "Epoch [5/300], Step [17900/27733], Loss: 4.2507\n",
      "Epoch [5/300], Step [18000/27733], Loss: 5.0574\n",
      "Epoch [5/300], Step [18100/27733], Loss: 4.3169\n",
      "Epoch [5/300], Step [18200/27733], Loss: 4.1283\n",
      "Epoch [5/300], Step [18300/27733], Loss: 4.6184\n",
      "Epoch [5/300], Step [18400/27733], Loss: 4.0479\n",
      "Epoch [5/300], Step [18500/27733], Loss: 4.8882\n",
      "Epoch [5/300], Step [18600/27733], Loss: 4.1869\n",
      "Epoch [5/300], Step [18700/27733], Loss: 3.5179\n",
      "Epoch [5/300], Step [18800/27733], Loss: 4.0824\n",
      "Epoch [5/300], Step [18900/27733], Loss: 3.0763\n",
      "Epoch [5/300], Step [19000/27733], Loss: 4.3234\n",
      "Epoch [5/300], Step [19100/27733], Loss: 2.8261\n",
      "Epoch [5/300], Step [19200/27733], Loss: 4.9603\n",
      "Epoch [5/300], Step [19300/27733], Loss: 3.8031\n",
      "Epoch [5/300], Step [19400/27733], Loss: 4.5202\n",
      "Epoch [5/300], Step [19500/27733], Loss: 4.6456\n",
      "Epoch [5/300], Step [19600/27733], Loss: 4.2733\n",
      "Epoch [5/300], Step [19700/27733], Loss: 3.7896\n",
      "Epoch [5/300], Step [19800/27733], Loss: 4.1995\n",
      "Epoch [5/300], Step [19900/27733], Loss: 4.4550\n",
      "Epoch [5/300], Step [20000/27733], Loss: 3.2235\n",
      "Epoch [5/300], Step [20100/27733], Loss: 4.8883\n",
      "Epoch [5/300], Step [20200/27733], Loss: 4.6656\n",
      "Epoch [5/300], Step [20300/27733], Loss: 4.3090\n",
      "Epoch [5/300], Step [20400/27733], Loss: 4.1716\n",
      "Epoch [5/300], Step [20500/27733], Loss: 3.9535\n",
      "Epoch [5/300], Step [20600/27733], Loss: 3.9016\n",
      "Epoch [5/300], Step [20700/27733], Loss: 4.6359\n",
      "Epoch [5/300], Step [20800/27733], Loss: 3.8690\n",
      "Epoch [5/300], Step [20900/27733], Loss: 3.6534\n",
      "Epoch [5/300], Step [21000/27733], Loss: 4.8936\n",
      "Epoch [5/300], Step [21100/27733], Loss: 3.9619\n",
      "Epoch [5/300], Step [21200/27733], Loss: 4.1055\n",
      "Epoch [5/300], Step [21300/27733], Loss: 3.9720\n",
      "Epoch [5/300], Step [21400/27733], Loss: 4.8945\n",
      "Epoch [5/300], Step [21500/27733], Loss: 3.3152\n",
      "Epoch [5/300], Step [21600/27733], Loss: 4.0441\n",
      "Epoch [5/300], Step [21700/27733], Loss: 3.9144\n",
      "Epoch [5/300], Step [21800/27733], Loss: 3.9471\n",
      "Epoch [5/300], Step [21900/27733], Loss: 4.5844\n",
      "Epoch [5/300], Step [22000/27733], Loss: 4.0061\n",
      "Epoch [5/300], Step [22100/27733], Loss: 3.6716\n",
      "Epoch [5/300], Step [22200/27733], Loss: 3.4543\n",
      "Epoch [5/300], Step [22300/27733], Loss: 4.3635\n",
      "Epoch [5/300], Step [22400/27733], Loss: 4.0136\n",
      "Epoch [5/300], Step [22500/27733], Loss: 3.5172\n",
      "Epoch [5/300], Step [22600/27733], Loss: 5.4447\n",
      "Epoch [5/300], Step [22700/27733], Loss: 4.0382\n",
      "Epoch [5/300], Step [22800/27733], Loss: 3.8257\n",
      "Epoch [5/300], Step [22900/27733], Loss: 4.7186\n",
      "Epoch [5/300], Step [23000/27733], Loss: 3.9839\n",
      "Epoch [5/300], Step [23100/27733], Loss: 4.0966\n",
      "Epoch [5/300], Step [23200/27733], Loss: 4.2639\n",
      "Epoch [5/300], Step [23300/27733], Loss: 3.3678\n",
      "Epoch [5/300], Step [23400/27733], Loss: 4.6854\n",
      "Epoch [5/300], Step [23500/27733], Loss: 5.5531\n",
      "Epoch [5/300], Step [23600/27733], Loss: 4.4128\n",
      "Epoch [5/300], Step [23700/27733], Loss: 3.6774\n",
      "Epoch [5/300], Step [23800/27733], Loss: 4.8459\n",
      "Epoch [5/300], Step [23900/27733], Loss: 4.6233\n",
      "Epoch [5/300], Step [24000/27733], Loss: 4.2571\n",
      "Epoch [5/300], Step [24100/27733], Loss: 4.7350\n",
      "Epoch [5/300], Step [24200/27733], Loss: 4.3473\n",
      "Epoch [5/300], Step [24300/27733], Loss: 4.6041\n",
      "Epoch [5/300], Step [24400/27733], Loss: 4.2809\n",
      "Epoch [5/300], Step [24500/27733], Loss: 4.2310\n",
      "Epoch [5/300], Step [24600/27733], Loss: 3.1387\n",
      "Epoch [5/300], Step [24700/27733], Loss: 5.0510\n",
      "Epoch [5/300], Step [24800/27733], Loss: 5.3385\n",
      "Epoch [5/300], Step [24900/27733], Loss: 5.8847\n",
      "Epoch [5/300], Step [25000/27733], Loss: 3.6567\n",
      "Epoch [5/300], Step [25100/27733], Loss: 4.4805\n",
      "Epoch [5/300], Step [25200/27733], Loss: 3.8591\n",
      "Epoch [5/300], Step [25300/27733], Loss: 4.7197\n",
      "Epoch [5/300], Step [25400/27733], Loss: 4.2329\n",
      "Epoch [5/300], Step [25500/27733], Loss: 4.2197\n",
      "Epoch [5/300], Step [25600/27733], Loss: 3.6965\n",
      "Epoch [5/300], Step [25700/27733], Loss: 4.6675\n",
      "Epoch [5/300], Step [25800/27733], Loss: 3.7132\n",
      "Epoch [5/300], Step [25900/27733], Loss: 4.1712\n",
      "Epoch [5/300], Step [26000/27733], Loss: 4.0457\n",
      "Epoch [5/300], Step [26100/27733], Loss: 4.9373\n",
      "Epoch [5/300], Step [26200/27733], Loss: 4.3177\n",
      "Epoch [5/300], Step [26300/27733], Loss: 4.9092\n",
      "Epoch [5/300], Step [26400/27733], Loss: 4.5532\n",
      "Epoch [5/300], Step [26500/27733], Loss: 4.0994\n",
      "Epoch [5/300], Step [26600/27733], Loss: 5.3465\n",
      "Epoch [5/300], Step [26700/27733], Loss: 4.4710\n",
      "Epoch [5/300], Step [26800/27733], Loss: 5.0829\n",
      "Epoch [5/300], Step [26900/27733], Loss: 4.2694\n",
      "Epoch [5/300], Step [27000/27733], Loss: 3.5360\n",
      "Epoch [5/300], Step [27100/27733], Loss: 4.3224\n",
      "Epoch [5/300], Step [27200/27733], Loss: 2.4505\n",
      "Epoch [5/300], Step [27300/27733], Loss: 4.1209\n",
      "Epoch [5/300], Step [27400/27733], Loss: 4.6387\n",
      "Epoch [5/300], Step [27500/27733], Loss: 5.8614\n",
      "Epoch [5/300], Step [27600/27733], Loss: 4.1697\n",
      "Epoch [5/300], Step [27700/27733], Loss: 5.3562\n",
      "Epoch [6/300], Step [100/27733], Loss: 4.1582\n",
      "Epoch [6/300], Step [200/27733], Loss: 3.7676\n",
      "Epoch [6/300], Step [300/27733], Loss: 4.3210\n",
      "Epoch [6/300], Step [400/27733], Loss: 3.9978\n",
      "Epoch [6/300], Step [500/27733], Loss: 3.7750\n",
      "Epoch [6/300], Step [600/27733], Loss: 3.4197\n",
      "Epoch [6/300], Step [700/27733], Loss: 4.0455\n",
      "Epoch [6/300], Step [800/27733], Loss: 3.6523\n",
      "Epoch [6/300], Step [900/27733], Loss: 3.8347\n",
      "Epoch [6/300], Step [1000/27733], Loss: 3.8403\n",
      "Epoch [6/300], Step [1100/27733], Loss: 3.4976\n",
      "Epoch [6/300], Step [1200/27733], Loss: 4.0385\n",
      "Epoch [6/300], Step [1300/27733], Loss: 3.6125\n",
      "Epoch [6/300], Step [1400/27733], Loss: 3.8708\n",
      "Epoch [6/300], Step [1500/27733], Loss: 3.9385\n",
      "Epoch [6/300], Step [1600/27733], Loss: 3.0568\n",
      "Epoch [6/300], Step [1700/27733], Loss: 3.6393\n",
      "Epoch [6/300], Step [1800/27733], Loss: 4.0053\n",
      "Epoch [6/300], Step [1900/27733], Loss: 3.6237\n",
      "Epoch [6/300], Step [2000/27733], Loss: 3.5045\n",
      "Epoch [6/300], Step [2100/27733], Loss: 4.1729\n",
      "Epoch [6/300], Step [2200/27733], Loss: 3.5808\n",
      "Epoch [6/300], Step [2300/27733], Loss: 3.9293\n",
      "Epoch [6/300], Step [2400/27733], Loss: 4.7357\n",
      "Epoch [6/300], Step [2500/27733], Loss: 3.4738\n",
      "Epoch [6/300], Step [2600/27733], Loss: 4.0020\n",
      "Epoch [6/300], Step [2700/27733], Loss: 3.6049\n",
      "Epoch [6/300], Step [2800/27733], Loss: 3.1432\n",
      "Epoch [6/300], Step [2900/27733], Loss: 3.6443\n",
      "Epoch [6/300], Step [3000/27733], Loss: 4.5884\n",
      "Epoch [6/300], Step [3100/27733], Loss: 4.3243\n",
      "Epoch [6/300], Step [3200/27733], Loss: 4.0431\n",
      "Epoch [6/300], Step [3300/27733], Loss: 3.3899\n",
      "Epoch [6/300], Step [3400/27733], Loss: 3.0842\n",
      "Epoch [6/300], Step [3500/27733], Loss: 4.1170\n",
      "Epoch [6/300], Step [3600/27733], Loss: 3.5577\n",
      "Epoch [6/300], Step [3700/27733], Loss: 4.5575\n",
      "Epoch [6/300], Step [3800/27733], Loss: 4.0713\n",
      "Epoch [6/300], Step [3900/27733], Loss: 3.8631\n",
      "Epoch [6/300], Step [4000/27733], Loss: 4.3491\n",
      "Epoch [6/300], Step [4100/27733], Loss: 4.6305\n",
      "Epoch [6/300], Step [4200/27733], Loss: 3.7394\n",
      "Epoch [6/300], Step [4300/27733], Loss: 4.8405\n",
      "Epoch [6/300], Step [4400/27733], Loss: 4.4425\n",
      "Epoch [6/300], Step [4500/27733], Loss: 3.5772\n",
      "Epoch [6/300], Step [4600/27733], Loss: 4.4065\n",
      "Epoch [6/300], Step [4700/27733], Loss: 3.8655\n",
      "Epoch [6/300], Step [4800/27733], Loss: 3.4127\n",
      "Epoch [6/300], Step [4900/27733], Loss: 3.7624\n",
      "Epoch [6/300], Step [5000/27733], Loss: 4.2723\n",
      "Epoch [6/300], Step [5100/27733], Loss: 4.0444\n",
      "Epoch [6/300], Step [5200/27733], Loss: 3.4882\n",
      "Epoch [6/300], Step [5300/27733], Loss: 3.7747\n",
      "Epoch [6/300], Step [5400/27733], Loss: 4.5352\n",
      "Epoch [6/300], Step [5500/27733], Loss: 3.9211\n",
      "Epoch [6/300], Step [5600/27733], Loss: 3.7277\n",
      "Epoch [6/300], Step [5700/27733], Loss: 3.6411\n",
      "Epoch [6/300], Step [5800/27733], Loss: 3.8503\n",
      "Epoch [6/300], Step [5900/27733], Loss: 3.3778\n",
      "Epoch [6/300], Step [6000/27733], Loss: 3.1019\n",
      "Epoch [6/300], Step [6100/27733], Loss: 4.5803\n",
      "Epoch [6/300], Step [6200/27733], Loss: 3.2289\n",
      "Epoch [6/300], Step [6300/27733], Loss: 3.9385\n",
      "Epoch [6/300], Step [6400/27733], Loss: 3.1571\n",
      "Epoch [6/300], Step [6500/27733], Loss: 4.4237\n",
      "Epoch [6/300], Step [6600/27733], Loss: 4.0598\n",
      "Epoch [6/300], Step [6700/27733], Loss: 4.0614\n",
      "Epoch [6/300], Step [6800/27733], Loss: 4.1358\n",
      "Epoch [6/300], Step [6900/27733], Loss: 3.6943\n",
      "Epoch [6/300], Step [7000/27733], Loss: 4.4693\n",
      "Epoch [6/300], Step [7100/27733], Loss: 4.4448\n",
      "Epoch [6/300], Step [7200/27733], Loss: 3.0062\n",
      "Epoch [6/300], Step [7300/27733], Loss: 3.1780\n",
      "Epoch [6/300], Step [7400/27733], Loss: 4.9139\n",
      "Epoch [6/300], Step [7500/27733], Loss: 4.0132\n",
      "Epoch [6/300], Step [7600/27733], Loss: 3.6303\n",
      "Epoch [6/300], Step [7700/27733], Loss: 3.1614\n",
      "Epoch [6/300], Step [7800/27733], Loss: 3.6326\n",
      "Epoch [6/300], Step [7900/27733], Loss: 3.8030\n",
      "Epoch [6/300], Step [8000/27733], Loss: 3.6243\n",
      "Epoch [6/300], Step [8100/27733], Loss: 4.5304\n",
      "Epoch [6/300], Step [8200/27733], Loss: 3.3695\n",
      "Epoch [6/300], Step [8300/27733], Loss: 4.2757\n",
      "Epoch [6/300], Step [8400/27733], Loss: 3.0453\n",
      "Epoch [6/300], Step [8500/27733], Loss: 4.3555\n",
      "Epoch [6/300], Step [8600/27733], Loss: 4.8778\n",
      "Epoch [6/300], Step [8700/27733], Loss: 3.8920\n",
      "Epoch [6/300], Step [8800/27733], Loss: 4.2912\n",
      "Epoch [6/300], Step [8900/27733], Loss: 4.0964\n",
      "Epoch [6/300], Step [9000/27733], Loss: 4.9715\n",
      "Epoch [6/300], Step [9100/27733], Loss: 4.1310\n",
      "Epoch [6/300], Step [9200/27733], Loss: 5.0690\n",
      "Epoch [6/300], Step [9300/27733], Loss: 3.9971\n",
      "Epoch [6/300], Step [9400/27733], Loss: 3.8247\n",
      "Epoch [6/300], Step [9500/27733], Loss: 3.0319\n",
      "Epoch [6/300], Step [9600/27733], Loss: 4.1818\n",
      "Epoch [6/300], Step [9700/27733], Loss: 3.2715\n",
      "Epoch [6/300], Step [9800/27733], Loss: 3.8158\n",
      "Epoch [6/300], Step [9900/27733], Loss: 3.8873\n",
      "Epoch [6/300], Step [10000/27733], Loss: 3.9656\n",
      "Epoch [6/300], Step [10100/27733], Loss: 3.9470\n",
      "Epoch [6/300], Step [10200/27733], Loss: 4.2619\n",
      "Epoch [6/300], Step [10300/27733], Loss: 3.2967\n",
      "Epoch [6/300], Step [10400/27733], Loss: 3.6780\n",
      "Epoch [6/300], Step [10500/27733], Loss: 3.9868\n",
      "Epoch [6/300], Step [10600/27733], Loss: 5.4811\n",
      "Epoch [6/300], Step [10700/27733], Loss: 3.7385\n",
      "Epoch [6/300], Step [10800/27733], Loss: 3.2018\n",
      "Epoch [6/300], Step [10900/27733], Loss: 4.0659\n",
      "Epoch [6/300], Step [11000/27733], Loss: 4.3989\n",
      "Epoch [6/300], Step [11100/27733], Loss: 3.8933\n",
      "Epoch [6/300], Step [11200/27733], Loss: 4.4478\n",
      "Epoch [6/300], Step [11300/27733], Loss: 3.9425\n",
      "Epoch [6/300], Step [11400/27733], Loss: 3.4041\n",
      "Epoch [6/300], Step [11500/27733], Loss: 3.8555\n",
      "Epoch [6/300], Step [11600/27733], Loss: 3.9353\n",
      "Epoch [6/300], Step [11700/27733], Loss: 3.5642\n",
      "Epoch [6/300], Step [11800/27733], Loss: 4.7941\n",
      "Epoch [6/300], Step [11900/27733], Loss: 4.4379\n",
      "Epoch [6/300], Step [12000/27733], Loss: 4.7668\n",
      "Epoch [6/300], Step [12100/27733], Loss: 3.3655\n",
      "Epoch [6/300], Step [12200/27733], Loss: 4.7963\n",
      "Epoch [6/300], Step [12300/27733], Loss: 4.0838\n",
      "Epoch [6/300], Step [12400/27733], Loss: 3.3371\n",
      "Epoch [6/300], Step [12500/27733], Loss: 4.9580\n",
      "Epoch [6/300], Step [12600/27733], Loss: 4.1546\n",
      "Epoch [6/300], Step [12700/27733], Loss: 4.0977\n",
      "Epoch [6/300], Step [12800/27733], Loss: 3.4096\n",
      "Epoch [6/300], Step [12900/27733], Loss: 5.0333\n",
      "Epoch [6/300], Step [13000/27733], Loss: 4.2504\n",
      "Epoch [6/300], Step [13100/27733], Loss: 3.8766\n",
      "Epoch [6/300], Step [13200/27733], Loss: 3.7776\n",
      "Epoch [6/300], Step [13300/27733], Loss: 3.8183\n",
      "Epoch [6/300], Step [13400/27733], Loss: 4.6617\n",
      "Epoch [6/300], Step [13500/27733], Loss: 4.1387\n",
      "Epoch [6/300], Step [13600/27733], Loss: 4.6469\n",
      "Epoch [6/300], Step [13700/27733], Loss: 3.2708\n",
      "Epoch [6/300], Step [13800/27733], Loss: 3.4204\n",
      "Epoch [6/300], Step [13900/27733], Loss: 4.2659\n",
      "Epoch [6/300], Step [14000/27733], Loss: 2.6516\n",
      "Epoch [6/300], Step [14100/27733], Loss: 3.9643\n",
      "Epoch [6/300], Step [14200/27733], Loss: 4.3249\n",
      "Epoch [6/300], Step [14300/27733], Loss: 4.6118\n",
      "Epoch [6/300], Step [14400/27733], Loss: 3.9402\n",
      "Epoch [6/300], Step [14500/27733], Loss: 4.0696\n",
      "Epoch [6/300], Step [14600/27733], Loss: 4.1756\n",
      "Epoch [6/300], Step [14700/27733], Loss: 4.0365\n",
      "Epoch [6/300], Step [14800/27733], Loss: 4.4455\n",
      "Epoch [6/300], Step [14900/27733], Loss: 5.0339\n",
      "Epoch [6/300], Step [15000/27733], Loss: 4.0643\n",
      "Epoch [6/300], Step [15100/27733], Loss: 4.0874\n",
      "Epoch [6/300], Step [15200/27733], Loss: 4.0466\n",
      "Epoch [6/300], Step [15300/27733], Loss: 4.1442\n",
      "Epoch [6/300], Step [15400/27733], Loss: 4.4546\n",
      "Epoch [6/300], Step [15500/27733], Loss: 3.9643\n",
      "Epoch [6/300], Step [15600/27733], Loss: 4.2913\n",
      "Epoch [6/300], Step [15700/27733], Loss: 5.0831\n",
      "Epoch [6/300], Step [15800/27733], Loss: 3.2133\n",
      "Epoch [6/300], Step [15900/27733], Loss: 4.8499\n",
      "Epoch [6/300], Step [16000/27733], Loss: 3.6804\n",
      "Epoch [6/300], Step [16100/27733], Loss: 3.5005\n",
      "Epoch [6/300], Step [16200/27733], Loss: 4.5203\n",
      "Epoch [6/300], Step [16300/27733], Loss: 4.0159\n",
      "Epoch [6/300], Step [16400/27733], Loss: 3.9390\n",
      "Epoch [6/300], Step [16500/27733], Loss: 3.8423\n",
      "Epoch [6/300], Step [16600/27733], Loss: 4.4775\n",
      "Epoch [6/300], Step [16700/27733], Loss: 4.3358\n",
      "Epoch [6/300], Step [16800/27733], Loss: 4.2785\n",
      "Epoch [6/300], Step [16900/27733], Loss: 4.5546\n",
      "Epoch [6/300], Step [17000/27733], Loss: 4.3677\n",
      "Epoch [6/300], Step [17100/27733], Loss: 4.5133\n",
      "Epoch [6/300], Step [17200/27733], Loss: 3.5680\n",
      "Epoch [6/300], Step [17300/27733], Loss: 4.1795\n",
      "Epoch [6/300], Step [17400/27733], Loss: 4.2323\n",
      "Epoch [6/300], Step [17500/27733], Loss: 5.3817\n",
      "Epoch [6/300], Step [17600/27733], Loss: 3.5041\n",
      "Epoch [6/300], Step [17700/27733], Loss: 3.8813\n",
      "Epoch [6/300], Step [17800/27733], Loss: 3.4247\n",
      "Epoch [6/300], Step [17900/27733], Loss: 3.9629\n",
      "Epoch [6/300], Step [18000/27733], Loss: 4.2311\n",
      "Epoch [6/300], Step [18100/27733], Loss: 3.1479\n",
      "Epoch [6/300], Step [18200/27733], Loss: 4.1397\n",
      "Epoch [6/300], Step [18300/27733], Loss: 3.3442\n",
      "Epoch [6/300], Step [18400/27733], Loss: 3.8683\n",
      "Epoch [6/300], Step [18500/27733], Loss: 4.8041\n",
      "Epoch [6/300], Step [18600/27733], Loss: 5.1797\n",
      "Epoch [6/300], Step [18700/27733], Loss: 3.4547\n",
      "Epoch [6/300], Step [18800/27733], Loss: 4.6097\n",
      "Epoch [6/300], Step [18900/27733], Loss: 4.0850\n",
      "Epoch [6/300], Step [19000/27733], Loss: 4.2302\n",
      "Epoch [6/300], Step [19100/27733], Loss: 4.6064\n",
      "Epoch [6/300], Step [19200/27733], Loss: 4.2165\n",
      "Epoch [6/300], Step [19300/27733], Loss: 3.8429\n",
      "Epoch [6/300], Step [19400/27733], Loss: 4.5007\n",
      "Epoch [6/300], Step [19500/27733], Loss: 4.8500\n",
      "Epoch [6/300], Step [19600/27733], Loss: 4.1181\n",
      "Epoch [6/300], Step [19700/27733], Loss: 3.6432\n",
      "Epoch [6/300], Step [19800/27733], Loss: 4.1156\n",
      "Epoch [6/300], Step [19900/27733], Loss: 4.4837\n",
      "Epoch [6/300], Step [20000/27733], Loss: 4.2097\n",
      "Epoch [6/300], Step [20100/27733], Loss: 4.5047\n",
      "Epoch [6/300], Step [20200/27733], Loss: 4.0961\n",
      "Epoch [6/300], Step [20300/27733], Loss: 4.7300\n",
      "Epoch [6/300], Step [20400/27733], Loss: 4.0847\n",
      "Epoch [6/300], Step [20500/27733], Loss: 4.5831\n",
      "Epoch [6/300], Step [20600/27733], Loss: 3.6113\n",
      "Epoch [6/300], Step [20700/27733], Loss: 4.4193\n",
      "Epoch [6/300], Step [20800/27733], Loss: 4.2750\n",
      "Epoch [6/300], Step [20900/27733], Loss: 4.1834\n",
      "Epoch [6/300], Step [21000/27733], Loss: 3.4741\n",
      "Epoch [6/300], Step [21100/27733], Loss: 4.0034\n",
      "Epoch [6/300], Step [21200/27733], Loss: 4.3703\n",
      "Epoch [6/300], Step [21300/27733], Loss: 4.0378\n",
      "Epoch [6/300], Step [21400/27733], Loss: 3.9721\n",
      "Epoch [6/300], Step [21500/27733], Loss: 4.2101\n",
      "Epoch [6/300], Step [21600/27733], Loss: 4.9452\n",
      "Epoch [6/300], Step [21700/27733], Loss: 4.1062\n",
      "Epoch [6/300], Step [21800/27733], Loss: 4.7622\n",
      "Epoch [6/300], Step [21900/27733], Loss: 4.3435\n",
      "Epoch [6/300], Step [22000/27733], Loss: 3.9619\n",
      "Epoch [6/300], Step [22100/27733], Loss: 3.9570\n",
      "Epoch [6/300], Step [22200/27733], Loss: 4.6819\n",
      "Epoch [6/300], Step [22300/27733], Loss: 4.3243\n",
      "Epoch [6/300], Step [22400/27733], Loss: 4.0334\n",
      "Epoch [6/300], Step [22500/27733], Loss: 3.9014\n",
      "Epoch [6/300], Step [22600/27733], Loss: 3.7112\n",
      "Epoch [6/300], Step [22700/27733], Loss: 3.8646\n",
      "Epoch [6/300], Step [22800/27733], Loss: 4.0203\n",
      "Epoch [6/300], Step [22900/27733], Loss: 4.4297\n",
      "Epoch [6/300], Step [23000/27733], Loss: 4.3348\n",
      "Epoch [6/300], Step [23100/27733], Loss: 3.9170\n",
      "Epoch [6/300], Step [23200/27733], Loss: 3.3915\n",
      "Epoch [6/300], Step [23300/27733], Loss: 3.3295\n",
      "Epoch [6/300], Step [23400/27733], Loss: 3.7165\n",
      "Epoch [6/300], Step [23500/27733], Loss: 4.0992\n",
      "Epoch [6/300], Step [23600/27733], Loss: 3.0162\n",
      "Epoch [6/300], Step [23700/27733], Loss: 4.6222\n",
      "Epoch [6/300], Step [23800/27733], Loss: 4.1077\n",
      "Epoch [6/300], Step [23900/27733], Loss: 4.2447\n",
      "Epoch [6/300], Step [24000/27733], Loss: 4.9398\n",
      "Epoch [6/300], Step [24100/27733], Loss: 4.5488\n",
      "Epoch [6/300], Step [24200/27733], Loss: 3.1168\n",
      "Epoch [6/300], Step [24300/27733], Loss: 4.1030\n",
      "Epoch [6/300], Step [24400/27733], Loss: 4.2947\n",
      "Epoch [6/300], Step [24500/27733], Loss: 4.3919\n",
      "Epoch [6/300], Step [24600/27733], Loss: 3.5551\n",
      "Epoch [6/300], Step [24700/27733], Loss: 4.0884\n",
      "Epoch [6/300], Step [24800/27733], Loss: 5.1584\n",
      "Epoch [6/300], Step [24900/27733], Loss: 4.0578\n",
      "Epoch [6/300], Step [25000/27733], Loss: 4.3760\n",
      "Epoch [6/300], Step [25100/27733], Loss: 4.7301\n",
      "Epoch [6/300], Step [25200/27733], Loss: 4.7011\n",
      "Epoch [6/300], Step [25300/27733], Loss: 3.6968\n",
      "Epoch [6/300], Step [25400/27733], Loss: 4.2062\n",
      "Epoch [6/300], Step [25500/27733], Loss: 4.8780\n",
      "Epoch [6/300], Step [25600/27733], Loss: 3.8263\n",
      "Epoch [6/300], Step [25700/27733], Loss: 3.4803\n",
      "Epoch [6/300], Step [25800/27733], Loss: 4.1093\n",
      "Epoch [6/300], Step [25900/27733], Loss: 4.6971\n",
      "Epoch [6/300], Step [26000/27733], Loss: 3.8016\n",
      "Epoch [6/300], Step [26100/27733], Loss: 3.0986\n",
      "Epoch [6/300], Step [26200/27733], Loss: 4.5076\n",
      "Epoch [6/300], Step [26300/27733], Loss: 3.4724\n",
      "Epoch [6/300], Step [26400/27733], Loss: 3.5126\n",
      "Epoch [6/300], Step [26500/27733], Loss: 4.6037\n",
      "Epoch [6/300], Step [26600/27733], Loss: 4.1421\n",
      "Epoch [6/300], Step [26700/27733], Loss: 3.8404\n",
      "Epoch [6/300], Step [26800/27733], Loss: 4.2888\n",
      "Epoch [6/300], Step [26900/27733], Loss: 3.6295\n",
      "Epoch [6/300], Step [27000/27733], Loss: 4.6020\n",
      "Epoch [6/300], Step [27100/27733], Loss: 4.7334\n",
      "Epoch [6/300], Step [27200/27733], Loss: 4.1117\n",
      "Epoch [6/300], Step [27300/27733], Loss: 3.4203\n",
      "Epoch [6/300], Step [27400/27733], Loss: 4.3644\n",
      "Epoch [6/300], Step [27500/27733], Loss: 4.2713\n",
      "Epoch [6/300], Step [27600/27733], Loss: 4.4111\n",
      "Epoch [6/300], Step [27700/27733], Loss: 2.9069\n",
      "Epoch [7/300], Step [100/27733], Loss: 3.1164\n",
      "Epoch [7/300], Step [200/27733], Loss: 4.0307\n",
      "Epoch [7/300], Step [300/27733], Loss: 3.6907\n",
      "Epoch [7/300], Step [400/27733], Loss: 5.3536\n",
      "Epoch [7/300], Step [500/27733], Loss: 2.9631\n",
      "Epoch [7/300], Step [600/27733], Loss: 4.0827\n",
      "Epoch [7/300], Step [700/27733], Loss: 3.7263\n",
      "Epoch [7/300], Step [800/27733], Loss: 2.8893\n",
      "Epoch [7/300], Step [900/27733], Loss: 3.7853\n",
      "Epoch [7/300], Step [1000/27733], Loss: 3.7405\n",
      "Epoch [7/300], Step [1100/27733], Loss: 3.4005\n",
      "Epoch [7/300], Step [1200/27733], Loss: 4.5645\n",
      "Epoch [7/300], Step [1300/27733], Loss: 3.3779\n",
      "Epoch [7/300], Step [1400/27733], Loss: 2.7333\n",
      "Epoch [7/300], Step [1500/27733], Loss: 3.3535\n",
      "Epoch [7/300], Step [1600/27733], Loss: 4.1931\n",
      "Epoch [7/300], Step [1700/27733], Loss: 3.9695\n",
      "Epoch [7/300], Step [1800/27733], Loss: 4.9058\n",
      "Epoch [7/300], Step [1900/27733], Loss: 3.6902\n",
      "Epoch [7/300], Step [2000/27733], Loss: 3.0012\n",
      "Epoch [7/300], Step [2100/27733], Loss: 3.9313\n",
      "Epoch [7/300], Step [2200/27733], Loss: 3.4367\n",
      "Epoch [7/300], Step [2300/27733], Loss: 3.2647\n",
      "Epoch [7/300], Step [2400/27733], Loss: 3.9684\n",
      "Epoch [7/300], Step [2500/27733], Loss: 4.5915\n",
      "Epoch [7/300], Step [2600/27733], Loss: 4.7108\n",
      "Epoch [7/300], Step [2700/27733], Loss: 3.5328\n",
      "Epoch [7/300], Step [2800/27733], Loss: 4.2753\n",
      "Epoch [7/300], Step [2900/27733], Loss: 3.4610\n",
      "Epoch [7/300], Step [3000/27733], Loss: 3.0357\n",
      "Epoch [7/300], Step [3100/27733], Loss: 3.2733\n",
      "Epoch [7/300], Step [3200/27733], Loss: 3.9836\n",
      "Epoch [7/300], Step [3300/27733], Loss: 3.0626\n",
      "Epoch [7/300], Step [3400/27733], Loss: 3.0455\n",
      "Epoch [7/300], Step [3500/27733], Loss: 3.3950\n",
      "Epoch [7/300], Step [3600/27733], Loss: 2.7455\n",
      "Epoch [7/300], Step [3700/27733], Loss: 3.7727\n",
      "Epoch [7/300], Step [3800/27733], Loss: 3.6775\n",
      "Epoch [7/300], Step [3900/27733], Loss: 4.7666\n",
      "Epoch [7/300], Step [4000/27733], Loss: 3.8389\n",
      "Epoch [7/300], Step [4100/27733], Loss: 3.0540\n",
      "Epoch [7/300], Step [4200/27733], Loss: 3.0779\n",
      "Epoch [7/300], Step [4300/27733], Loss: 3.5089\n",
      "Epoch [7/300], Step [4400/27733], Loss: 3.2076\n",
      "Epoch [7/300], Step [4500/27733], Loss: 3.1314\n",
      "Epoch [7/300], Step [4600/27733], Loss: 4.8825\n",
      "Epoch [7/300], Step [4700/27733], Loss: 3.2706\n",
      "Epoch [7/300], Step [4800/27733], Loss: 3.4322\n",
      "Epoch [7/300], Step [4900/27733], Loss: 3.4406\n",
      "Epoch [7/300], Step [5000/27733], Loss: 4.0253\n",
      "Epoch [7/300], Step [5100/27733], Loss: 3.2274\n",
      "Epoch [7/300], Step [5200/27733], Loss: 3.0370\n",
      "Epoch [7/300], Step [5300/27733], Loss: 4.1512\n",
      "Epoch [7/300], Step [5400/27733], Loss: 3.5993\n",
      "Epoch [7/300], Step [5500/27733], Loss: 4.8452\n",
      "Epoch [7/300], Step [5600/27733], Loss: 4.3975\n",
      "Epoch [7/300], Step [5700/27733], Loss: 3.4178\n",
      "Epoch [7/300], Step [5800/27733], Loss: 4.0439\n",
      "Epoch [7/300], Step [5900/27733], Loss: 4.0173\n",
      "Epoch [7/300], Step [6000/27733], Loss: 2.9836\n",
      "Epoch [7/300], Step [6100/27733], Loss: 3.1466\n",
      "Epoch [7/300], Step [6200/27733], Loss: 3.7637\n",
      "Epoch [7/300], Step [6300/27733], Loss: 4.7022\n",
      "Epoch [7/300], Step [6400/27733], Loss: 4.7017\n",
      "Epoch [7/300], Step [6500/27733], Loss: 3.9713\n",
      "Epoch [7/300], Step [6600/27733], Loss: 3.8839\n",
      "Epoch [7/300], Step [6700/27733], Loss: 3.5153\n",
      "Epoch [7/300], Step [6800/27733], Loss: 3.8525\n",
      "Epoch [7/300], Step [6900/27733], Loss: 4.1442\n",
      "Epoch [7/300], Step [7000/27733], Loss: 4.0365\n",
      "Epoch [7/300], Step [7100/27733], Loss: 3.6827\n",
      "Epoch [7/300], Step [7200/27733], Loss: 3.7093\n",
      "Epoch [7/300], Step [7300/27733], Loss: 3.8786\n",
      "Epoch [7/300], Step [7400/27733], Loss: 3.5193\n",
      "Epoch [7/300], Step [7500/27733], Loss: 3.5435\n",
      "Epoch [7/300], Step [7600/27733], Loss: 3.9644\n",
      "Epoch [7/300], Step [7700/27733], Loss: 3.6642\n",
      "Epoch [7/300], Step [7800/27733], Loss: 3.5126\n",
      "Epoch [7/300], Step [7900/27733], Loss: 3.6795\n",
      "Epoch [7/300], Step [8000/27733], Loss: 4.4053\n",
      "Epoch [7/300], Step [8100/27733], Loss: 4.0430\n",
      "Epoch [7/300], Step [8200/27733], Loss: 3.9146\n",
      "Epoch [7/300], Step [8300/27733], Loss: 3.7964\n",
      "Epoch [7/300], Step [8400/27733], Loss: 3.8735\n",
      "Epoch [7/300], Step [8500/27733], Loss: 3.5451\n",
      "Epoch [7/300], Step [8600/27733], Loss: 3.2030\n",
      "Epoch [7/300], Step [8700/27733], Loss: 3.2850\n",
      "Epoch [7/300], Step [8800/27733], Loss: 3.5454\n",
      "Epoch [7/300], Step [8900/27733], Loss: 3.9211\n",
      "Epoch [7/300], Step [9000/27733], Loss: 3.6658\n",
      "Epoch [7/300], Step [9100/27733], Loss: 5.0186\n",
      "Epoch [7/300], Step [9200/27733], Loss: 3.6618\n",
      "Epoch [7/300], Step [9300/27733], Loss: 3.4403\n",
      "Epoch [7/300], Step [9400/27733], Loss: 3.9003\n",
      "Epoch [7/300], Step [9500/27733], Loss: 3.0700\n",
      "Epoch [7/300], Step [9600/27733], Loss: 3.7393\n",
      "Epoch [7/300], Step [9700/27733], Loss: 3.5949\n",
      "Epoch [7/300], Step [9800/27733], Loss: 3.6754\n",
      "Epoch [7/300], Step [9900/27733], Loss: 4.0712\n",
      "Epoch [7/300], Step [10000/27733], Loss: 3.6870\n",
      "Epoch [7/300], Step [10100/27733], Loss: 3.6843\n",
      "Epoch [7/300], Step [10200/27733], Loss: 3.6486\n",
      "Epoch [7/300], Step [10300/27733], Loss: 3.9702\n",
      "Epoch [7/300], Step [10400/27733], Loss: 3.8542\n",
      "Epoch [7/300], Step [10500/27733], Loss: 3.9161\n",
      "Epoch [7/300], Step [10600/27733], Loss: 4.2648\n",
      "Epoch [7/300], Step [10700/27733], Loss: 3.7513\n",
      "Epoch [7/300], Step [10800/27733], Loss: 4.6392\n",
      "Epoch [7/300], Step [10900/27733], Loss: 3.9305\n",
      "Epoch [7/300], Step [11000/27733], Loss: 4.1439\n",
      "Epoch [7/300], Step [11100/27733], Loss: 3.4403\n",
      "Epoch [7/300], Step [11200/27733], Loss: 4.1182\n",
      "Epoch [7/300], Step [11300/27733], Loss: 4.2623\n",
      "Epoch [7/300], Step [11400/27733], Loss: 4.0630\n",
      "Epoch [7/300], Step [11500/27733], Loss: 3.0275\n",
      "Epoch [7/300], Step [11600/27733], Loss: 3.7021\n",
      "Epoch [7/300], Step [11700/27733], Loss: 4.1540\n",
      "Epoch [7/300], Step [11800/27733], Loss: 3.9100\n",
      "Epoch [7/300], Step [11900/27733], Loss: 3.6866\n",
      "Epoch [7/300], Step [12000/27733], Loss: 3.8256\n",
      "Epoch [7/300], Step [12100/27733], Loss: 4.8274\n",
      "Epoch [7/300], Step [12200/27733], Loss: 3.8983\n",
      "Epoch [7/300], Step [12300/27733], Loss: 4.3741\n",
      "Epoch [7/300], Step [12400/27733], Loss: 4.1936\n",
      "Epoch [7/300], Step [12500/27733], Loss: 4.0905\n",
      "Epoch [7/300], Step [12600/27733], Loss: 4.2543\n",
      "Epoch [7/300], Step [12700/27733], Loss: 3.6216\n",
      "Epoch [7/300], Step [12800/27733], Loss: 3.5258\n",
      "Epoch [7/300], Step [12900/27733], Loss: 3.5751\n",
      "Epoch [7/300], Step [13000/27733], Loss: 4.5538\n",
      "Epoch [7/300], Step [13100/27733], Loss: 3.7267\n",
      "Epoch [7/300], Step [13200/27733], Loss: 4.7900\n",
      "Epoch [7/300], Step [13300/27733], Loss: 4.1823\n",
      "Epoch [7/300], Step [13400/27733], Loss: 3.4490\n",
      "Epoch [7/300], Step [13500/27733], Loss: 2.9728\n",
      "Epoch [7/300], Step [13600/27733], Loss: 3.7572\n",
      "Epoch [7/300], Step [13700/27733], Loss: 3.8468\n",
      "Epoch [7/300], Step [13800/27733], Loss: 3.3242\n",
      "Epoch [7/300], Step [13900/27733], Loss: 3.6197\n",
      "Epoch [7/300], Step [14000/27733], Loss: 3.6000\n",
      "Epoch [7/300], Step [14100/27733], Loss: 3.7164\n",
      "Epoch [7/300], Step [14200/27733], Loss: 3.1256\n",
      "Epoch [7/300], Step [14300/27733], Loss: 4.1855\n",
      "Epoch [7/300], Step [14400/27733], Loss: 2.6734\n",
      "Epoch [7/300], Step [14500/27733], Loss: 3.9617\n",
      "Epoch [7/300], Step [14600/27733], Loss: 4.0964\n",
      "Epoch [7/300], Step [14700/27733], Loss: 3.3967\n",
      "Epoch [7/300], Step [14800/27733], Loss: 3.2252\n",
      "Epoch [7/300], Step [14900/27733], Loss: 4.8093\n",
      "Epoch [7/300], Step [15000/27733], Loss: 4.2367\n",
      "Epoch [7/300], Step [15100/27733], Loss: 4.3017\n",
      "Epoch [7/300], Step [15200/27733], Loss: 3.7518\n",
      "Epoch [7/300], Step [15300/27733], Loss: 3.6316\n",
      "Epoch [7/300], Step [15400/27733], Loss: 4.2084\n",
      "Epoch [7/300], Step [15500/27733], Loss: 4.1160\n",
      "Epoch [7/300], Step [15600/27733], Loss: 3.6455\n",
      "Epoch [7/300], Step [15700/27733], Loss: 3.3063\n",
      "Epoch [7/300], Step [15800/27733], Loss: 3.0908\n",
      "Epoch [7/300], Step [15900/27733], Loss: 4.3787\n",
      "Epoch [7/300], Step [16000/27733], Loss: 3.9661\n",
      "Epoch [7/300], Step [16100/27733], Loss: 4.0093\n",
      "Epoch [7/300], Step [16200/27733], Loss: 3.7067\n",
      "Epoch [7/300], Step [16300/27733], Loss: 3.5388\n",
      "Epoch [7/300], Step [16400/27733], Loss: 3.5777\n",
      "Epoch [7/300], Step [16500/27733], Loss: 4.1510\n",
      "Epoch [7/300], Step [16600/27733], Loss: 2.9076\n",
      "Epoch [7/300], Step [16700/27733], Loss: 3.8614\n",
      "Epoch [7/300], Step [16800/27733], Loss: 3.5592\n",
      "Epoch [7/300], Step [16900/27733], Loss: 3.2156\n",
      "Epoch [7/300], Step [17000/27733], Loss: 4.4638\n",
      "Epoch [7/300], Step [17100/27733], Loss: 4.4830\n",
      "Epoch [7/300], Step [17200/27733], Loss: 4.5198\n",
      "Epoch [7/300], Step [17300/27733], Loss: 4.8445\n",
      "Epoch [7/300], Step [17400/27733], Loss: 3.4791\n",
      "Epoch [7/300], Step [17500/27733], Loss: 3.7561\n",
      "Epoch [7/300], Step [17600/27733], Loss: 4.0712\n",
      "Epoch [7/300], Step [17700/27733], Loss: 3.6586\n",
      "Epoch [7/300], Step [17800/27733], Loss: 3.4744\n",
      "Epoch [7/300], Step [17900/27733], Loss: 4.1136\n",
      "Epoch [7/300], Step [18000/27733], Loss: 4.2652\n",
      "Epoch [7/300], Step [18100/27733], Loss: 4.3130\n",
      "Epoch [7/300], Step [18200/27733], Loss: 4.5311\n",
      "Epoch [7/300], Step [18300/27733], Loss: 3.3287\n",
      "Epoch [7/300], Step [18400/27733], Loss: 3.6302\n",
      "Epoch [7/300], Step [18500/27733], Loss: 3.9263\n",
      "Epoch [7/300], Step [18600/27733], Loss: 3.4377\n",
      "Epoch [7/300], Step [18700/27733], Loss: 3.9429\n",
      "Epoch [7/300], Step [18800/27733], Loss: 2.8968\n",
      "Epoch [7/300], Step [18900/27733], Loss: 3.4660\n",
      "Epoch [7/300], Step [19000/27733], Loss: 5.0573\n",
      "Epoch [7/300], Step [19100/27733], Loss: 4.4242\n",
      "Epoch [7/300], Step [19200/27733], Loss: 3.9855\n",
      "Epoch [7/300], Step [19300/27733], Loss: 3.9546\n",
      "Epoch [7/300], Step [19400/27733], Loss: 3.5921\n",
      "Epoch [7/300], Step [19500/27733], Loss: 4.1709\n",
      "Epoch [7/300], Step [19600/27733], Loss: 4.3414\n",
      "Epoch [7/300], Step [19700/27733], Loss: 3.2047\n",
      "Epoch [7/300], Step [19800/27733], Loss: 4.4273\n",
      "Epoch [7/300], Step [19900/27733], Loss: 4.2984\n",
      "Epoch [7/300], Step [20000/27733], Loss: 4.1301\n",
      "Epoch [7/300], Step [20100/27733], Loss: 4.1501\n",
      "Epoch [7/300], Step [20200/27733], Loss: 3.6670\n",
      "Epoch [7/300], Step [20300/27733], Loss: 4.3263\n",
      "Epoch [7/300], Step [20400/27733], Loss: 4.0973\n",
      "Epoch [7/300], Step [20500/27733], Loss: 4.4965\n",
      "Epoch [7/300], Step [20600/27733], Loss: 3.8064\n",
      "Epoch [7/300], Step [20700/27733], Loss: 4.7186\n",
      "Epoch [7/300], Step [20800/27733], Loss: 4.7374\n",
      "Epoch [7/300], Step [20900/27733], Loss: 4.8285\n",
      "Epoch [7/300], Step [21000/27733], Loss: 3.9859\n",
      "Epoch [7/300], Step [21100/27733], Loss: 3.9969\n",
      "Epoch [7/300], Step [21200/27733], Loss: 4.6474\n",
      "Epoch [7/300], Step [21300/27733], Loss: 3.8504\n",
      "Epoch [7/300], Step [21400/27733], Loss: 3.8681\n",
      "Epoch [7/300], Step [21500/27733], Loss: 4.5955\n",
      "Epoch [7/300], Step [21600/27733], Loss: 4.4489\n",
      "Epoch [7/300], Step [21700/27733], Loss: 4.1935\n",
      "Epoch [7/300], Step [21800/27733], Loss: 3.1637\n",
      "Epoch [7/300], Step [21900/27733], Loss: 4.4304\n",
      "Epoch [7/300], Step [22000/27733], Loss: 3.4865\n",
      "Epoch [7/300], Step [22100/27733], Loss: 3.9554\n",
      "Epoch [7/300], Step [22200/27733], Loss: 4.2063\n",
      "Epoch [7/300], Step [22300/27733], Loss: 4.5469\n",
      "Epoch [7/300], Step [22400/27733], Loss: 3.1717\n",
      "Epoch [7/300], Step [22500/27733], Loss: 4.0718\n",
      "Epoch [7/300], Step [22600/27733], Loss: 4.0175\n",
      "Epoch [7/300], Step [22700/27733], Loss: 4.3279\n",
      "Epoch [7/300], Step [22800/27733], Loss: 3.5592\n",
      "Epoch [7/300], Step [22900/27733], Loss: 4.0240\n",
      "Epoch [7/300], Step [23000/27733], Loss: 3.6550\n",
      "Epoch [7/300], Step [23100/27733], Loss: 4.0767\n",
      "Epoch [7/300], Step [23200/27733], Loss: 4.2192\n",
      "Epoch [7/300], Step [23300/27733], Loss: 3.5329\n",
      "Epoch [7/300], Step [23400/27733], Loss: 3.6760\n",
      "Epoch [7/300], Step [23500/27733], Loss: 4.7130\n",
      "Epoch [7/300], Step [23600/27733], Loss: 3.7576\n",
      "Epoch [7/300], Step [23700/27733], Loss: 3.3349\n",
      "Epoch [7/300], Step [23800/27733], Loss: 4.3999\n",
      "Epoch [7/300], Step [23900/27733], Loss: 4.5359\n",
      "Epoch [7/300], Step [24000/27733], Loss: 4.0137\n",
      "Epoch [7/300], Step [24100/27733], Loss: 3.7249\n",
      "Epoch [7/300], Step [24200/27733], Loss: 3.5645\n",
      "Epoch [7/300], Step [24300/27733], Loss: 4.1773\n",
      "Epoch [7/300], Step [24400/27733], Loss: 5.3557\n",
      "Epoch [7/300], Step [24500/27733], Loss: 2.9594\n",
      "Epoch [7/300], Step [24600/27733], Loss: 4.1591\n",
      "Epoch [7/300], Step [24700/27733], Loss: 4.1172\n",
      "Epoch [7/300], Step [24800/27733], Loss: 5.1329\n",
      "Epoch [7/300], Step [24900/27733], Loss: 3.6987\n",
      "Epoch [7/300], Step [25000/27733], Loss: 4.0181\n",
      "Epoch [7/300], Step [25100/27733], Loss: 4.7720\n",
      "Epoch [7/300], Step [25200/27733], Loss: 3.7398\n",
      "Epoch [7/300], Step [25300/27733], Loss: 3.2692\n",
      "Epoch [7/300], Step [25400/27733], Loss: 4.0835\n",
      "Epoch [7/300], Step [25500/27733], Loss: 3.3889\n",
      "Epoch [7/300], Step [25600/27733], Loss: 3.8854\n",
      "Epoch [7/300], Step [25700/27733], Loss: 3.3072\n",
      "Epoch [7/300], Step [25800/27733], Loss: 3.4910\n",
      "Epoch [7/300], Step [25900/27733], Loss: 4.7680\n",
      "Epoch [7/300], Step [26000/27733], Loss: 4.1907\n",
      "Epoch [7/300], Step [26100/27733], Loss: 4.2739\n",
      "Epoch [7/300], Step [26200/27733], Loss: 3.9662\n",
      "Epoch [7/300], Step [26300/27733], Loss: 3.5543\n",
      "Epoch [7/300], Step [26400/27733], Loss: 3.5641\n",
      "Epoch [7/300], Step [26500/27733], Loss: 4.0666\n",
      "Epoch [7/300], Step [26600/27733], Loss: 3.3119\n",
      "Epoch [7/300], Step [26700/27733], Loss: 3.7224\n",
      "Epoch [7/300], Step [26800/27733], Loss: 3.7369\n",
      "Epoch [7/300], Step [26900/27733], Loss: 4.8831\n",
      "Epoch [7/300], Step [27000/27733], Loss: 3.4907\n",
      "Epoch [7/300], Step [27100/27733], Loss: 4.0141\n",
      "Epoch [7/300], Step [27200/27733], Loss: 4.8586\n",
      "Epoch [7/300], Step [27300/27733], Loss: 3.8553\n",
      "Epoch [7/300], Step [27400/27733], Loss: 4.3931\n",
      "Epoch [7/300], Step [27500/27733], Loss: 4.4582\n",
      "Epoch [7/300], Step [27600/27733], Loss: 5.3609\n",
      "Epoch [7/300], Step [27700/27733], Loss: 5.0035\n",
      "Epoch [8/300], Step [100/27733], Loss: 3.5535\n",
      "Epoch [8/300], Step [200/27733], Loss: 4.2724\n",
      "Epoch [8/300], Step [300/27733], Loss: 3.9920\n",
      "Epoch [8/300], Step [400/27733], Loss: 3.1770\n",
      "Epoch [8/300], Step [500/27733], Loss: 4.2576\n",
      "Epoch [8/300], Step [600/27733], Loss: 3.0106\n",
      "Epoch [8/300], Step [700/27733], Loss: 2.9148\n",
      "Epoch [8/300], Step [800/27733], Loss: 3.4621\n",
      "Epoch [8/300], Step [900/27733], Loss: 3.0660\n",
      "Epoch [8/300], Step [1000/27733], Loss: 3.4494\n",
      "Epoch [8/300], Step [1100/27733], Loss: 3.4821\n",
      "Epoch [8/300], Step [1200/27733], Loss: 2.8887\n",
      "Epoch [8/300], Step [1300/27733], Loss: 3.0759\n",
      "Epoch [8/300], Step [1400/27733], Loss: 3.0024\n",
      "Epoch [8/300], Step [1500/27733], Loss: 3.2379\n",
      "Epoch [8/300], Step [1600/27733], Loss: 2.2301\n",
      "Epoch [8/300], Step [1700/27733], Loss: 3.5704\n",
      "Epoch [8/300], Step [1800/27733], Loss: 3.9745\n",
      "Epoch [8/300], Step [1900/27733], Loss: 3.3442\n",
      "Epoch [8/300], Step [2000/27733], Loss: 3.2086\n",
      "Epoch [8/300], Step [2100/27733], Loss: 3.1168\n",
      "Epoch [8/300], Step [2200/27733], Loss: 2.9741\n",
      "Epoch [8/300], Step [2300/27733], Loss: 3.7917\n",
      "Epoch [8/300], Step [2400/27733], Loss: 4.0703\n",
      "Epoch [8/300], Step [2500/27733], Loss: 2.6728\n",
      "Epoch [8/300], Step [2600/27733], Loss: 3.9705\n",
      "Epoch [8/300], Step [2700/27733], Loss: 3.6256\n",
      "Epoch [8/300], Step [2800/27733], Loss: 3.5395\n",
      "Epoch [8/300], Step [2900/27733], Loss: 3.6891\n",
      "Epoch [8/300], Step [3000/27733], Loss: 3.3417\n",
      "Epoch [8/300], Step [3100/27733], Loss: 3.3660\n",
      "Epoch [8/300], Step [3200/27733], Loss: 3.1515\n",
      "Epoch [8/300], Step [3300/27733], Loss: 4.1117\n",
      "Epoch [8/300], Step [3400/27733], Loss: 3.7760\n",
      "Epoch [8/300], Step [3500/27733], Loss: 2.8399\n",
      "Epoch [8/300], Step [3600/27733], Loss: 3.2008\n",
      "Epoch [8/300], Step [3700/27733], Loss: 3.7991\n",
      "Epoch [8/300], Step [3800/27733], Loss: 3.3781\n",
      "Epoch [8/300], Step [3900/27733], Loss: 3.3088\n",
      "Epoch [8/300], Step [4000/27733], Loss: 3.8651\n",
      "Epoch [8/300], Step [4100/27733], Loss: 3.0254\n",
      "Epoch [8/300], Step [4200/27733], Loss: 4.0521\n",
      "Epoch [8/300], Step [4300/27733], Loss: 3.7844\n",
      "Epoch [8/300], Step [4400/27733], Loss: 3.7685\n",
      "Epoch [8/300], Step [4500/27733], Loss: 3.0850\n",
      "Epoch [8/300], Step [4600/27733], Loss: 4.0261\n",
      "Epoch [8/300], Step [4700/27733], Loss: 3.7446\n",
      "Epoch [8/300], Step [4800/27733], Loss: 3.2542\n",
      "Epoch [8/300], Step [4900/27733], Loss: 3.0005\n",
      "Epoch [8/300], Step [5000/27733], Loss: 2.8972\n",
      "Epoch [8/300], Step [5100/27733], Loss: 3.7131\n",
      "Epoch [8/300], Step [5200/27733], Loss: 3.3191\n",
      "Epoch [8/300], Step [5300/27733], Loss: 3.3932\n",
      "Epoch [8/300], Step [5400/27733], Loss: 3.6388\n",
      "Epoch [8/300], Step [5500/27733], Loss: 3.5945\n",
      "Epoch [8/300], Step [5600/27733], Loss: 3.3582\n",
      "Epoch [8/300], Step [5700/27733], Loss: 3.3146\n",
      "Epoch [8/300], Step [5800/27733], Loss: 4.4553\n",
      "Epoch [8/300], Step [5900/27733], Loss: 3.2996\n",
      "Epoch [8/300], Step [6000/27733], Loss: 3.6968\n",
      "Epoch [8/300], Step [6100/27733], Loss: 3.2428\n",
      "Epoch [8/300], Step [6200/27733], Loss: 3.2855\n",
      "Epoch [8/300], Step [6300/27733], Loss: 4.4104\n",
      "Epoch [8/300], Step [6400/27733], Loss: 4.3411\n",
      "Epoch [8/300], Step [6500/27733], Loss: 3.7731\n",
      "Epoch [8/300], Step [6600/27733], Loss: 4.0992\n",
      "Epoch [8/300], Step [6700/27733], Loss: 3.6477\n",
      "Epoch [8/300], Step [6800/27733], Loss: 3.0475\n",
      "Epoch [8/300], Step [6900/27733], Loss: 4.2001\n",
      "Epoch [8/300], Step [7000/27733], Loss: 3.4096\n",
      "Epoch [8/300], Step [7100/27733], Loss: 4.3283\n",
      "Epoch [8/300], Step [7200/27733], Loss: 3.6491\n",
      "Epoch [8/300], Step [7300/27733], Loss: 3.1203\n",
      "Epoch [8/300], Step [7400/27733], Loss: 3.6902\n",
      "Epoch [8/300], Step [7500/27733], Loss: 3.3928\n",
      "Epoch [8/300], Step [7600/27733], Loss: 3.2938\n",
      "Epoch [8/300], Step [7700/27733], Loss: 3.5647\n",
      "Epoch [8/300], Step [7800/27733], Loss: 2.7900\n",
      "Epoch [8/300], Step [7900/27733], Loss: 3.4652\n",
      "Epoch [8/300], Step [8000/27733], Loss: 4.3535\n",
      "Epoch [8/300], Step [8100/27733], Loss: 4.1512\n",
      "Epoch [8/300], Step [8200/27733], Loss: 3.5793\n",
      "Epoch [8/300], Step [8300/27733], Loss: 4.6214\n",
      "Epoch [8/300], Step [8400/27733], Loss: 3.9546\n",
      "Epoch [8/300], Step [8500/27733], Loss: 4.5457\n",
      "Epoch [8/300], Step [8600/27733], Loss: 3.8697\n",
      "Epoch [8/300], Step [8700/27733], Loss: 2.8131\n",
      "Epoch [8/300], Step [8800/27733], Loss: 4.0583\n",
      "Epoch [8/300], Step [8900/27733], Loss: 3.3693\n",
      "Epoch [8/300], Step [9000/27733], Loss: 3.3252\n",
      "Epoch [8/300], Step [9100/27733], Loss: 3.4679\n",
      "Epoch [8/300], Step [9200/27733], Loss: 3.2401\n",
      "Epoch [8/300], Step [9300/27733], Loss: 3.9448\n",
      "Epoch [8/300], Step [9400/27733], Loss: 3.8500\n",
      "Epoch [8/300], Step [9500/27733], Loss: 4.2282\n",
      "Epoch [8/300], Step [9600/27733], Loss: 3.4905\n",
      "Epoch [8/300], Step [9700/27733], Loss: 3.6325\n",
      "Epoch [8/300], Step [9800/27733], Loss: 3.8916\n",
      "Epoch [8/300], Step [9900/27733], Loss: 3.1918\n",
      "Epoch [8/300], Step [10000/27733], Loss: 3.5082\n",
      "Epoch [8/300], Step [10100/27733], Loss: 4.1490\n",
      "Epoch [8/300], Step [10200/27733], Loss: 3.9094\n",
      "Epoch [8/300], Step [10300/27733], Loss: 4.2172\n",
      "Epoch [8/300], Step [10400/27733], Loss: 3.5956\n",
      "Epoch [8/300], Step [10500/27733], Loss: 3.0287\n",
      "Epoch [8/300], Step [10600/27733], Loss: 3.5373\n",
      "Epoch [8/300], Step [10700/27733], Loss: 3.7910\n",
      "Epoch [8/300], Step [10800/27733], Loss: 4.0269\n",
      "Epoch [8/300], Step [10900/27733], Loss: 3.8774\n",
      "Epoch [8/300], Step [11000/27733], Loss: 2.9340\n",
      "Epoch [8/300], Step [11100/27733], Loss: 4.1811\n",
      "Epoch [8/300], Step [11200/27733], Loss: 3.4807\n",
      "Epoch [8/300], Step [11300/27733], Loss: 4.3727\n",
      "Epoch [8/300], Step [11400/27733], Loss: 3.9936\n",
      "Epoch [8/300], Step [11500/27733], Loss: 4.0818\n",
      "Epoch [8/300], Step [11600/27733], Loss: 4.5312\n",
      "Epoch [8/300], Step [11700/27733], Loss: 4.0185\n",
      "Epoch [8/300], Step [11800/27733], Loss: 3.7390\n",
      "Epoch [8/300], Step [11900/27733], Loss: 3.9174\n",
      "Epoch [8/300], Step [12000/27733], Loss: 5.2885\n",
      "Epoch [8/300], Step [12100/27733], Loss: 3.1122\n",
      "Epoch [8/300], Step [12200/27733], Loss: 3.6156\n",
      "Epoch [8/300], Step [12300/27733], Loss: 4.4571\n",
      "Epoch [8/300], Step [12400/27733], Loss: 4.5998\n",
      "Epoch [8/300], Step [12500/27733], Loss: 4.5217\n",
      "Epoch [8/300], Step [12600/27733], Loss: 3.4992\n",
      "Epoch [8/300], Step [12700/27733], Loss: 4.2893\n",
      "Epoch [8/300], Step [12800/27733], Loss: 3.9070\n",
      "Epoch [8/300], Step [12900/27733], Loss: 3.1892\n",
      "Epoch [8/300], Step [13000/27733], Loss: 3.6922\n",
      "Epoch [8/300], Step [13100/27733], Loss: 4.3569\n",
      "Epoch [8/300], Step [13200/27733], Loss: 3.5922\n",
      "Epoch [8/300], Step [13300/27733], Loss: 3.5480\n",
      "Epoch [8/300], Step [13400/27733], Loss: 3.5514\n",
      "Epoch [8/300], Step [13500/27733], Loss: 3.4723\n",
      "Epoch [8/300], Step [13600/27733], Loss: 3.3830\n",
      "Epoch [8/300], Step [13700/27733], Loss: 4.0323\n",
      "Epoch [8/300], Step [13800/27733], Loss: 3.9486\n",
      "Epoch [8/300], Step [13900/27733], Loss: 3.8960\n",
      "Epoch [8/300], Step [14000/27733], Loss: 3.1760\n",
      "Epoch [8/300], Step [14100/27733], Loss: 3.7275\n",
      "Epoch [8/300], Step [14200/27733], Loss: 3.1594\n",
      "Epoch [8/300], Step [14300/27733], Loss: 3.6779\n",
      "Epoch [8/300], Step [14400/27733], Loss: 3.1931\n",
      "Epoch [8/300], Step [14500/27733], Loss: 2.9232\n",
      "Epoch [8/300], Step [14600/27733], Loss: 3.4997\n",
      "Epoch [8/300], Step [14700/27733], Loss: 3.8556\n",
      "Epoch [8/300], Step [14800/27733], Loss: 3.6997\n",
      "Epoch [8/300], Step [14900/27733], Loss: 3.4098\n",
      "Epoch [8/300], Step [15000/27733], Loss: 3.8208\n",
      "Epoch [8/300], Step [15100/27733], Loss: 2.7889\n",
      "Epoch [8/300], Step [15200/27733], Loss: 3.6550\n",
      "Epoch [8/300], Step [15300/27733], Loss: 3.4192\n",
      "Epoch [8/300], Step [15400/27733], Loss: 3.8775\n",
      "Epoch [8/300], Step [15500/27733], Loss: 3.7935\n",
      "Epoch [8/300], Step [15600/27733], Loss: 2.7963\n",
      "Epoch [8/300], Step [15700/27733], Loss: 4.4580\n",
      "Epoch [8/300], Step [15800/27733], Loss: 4.0668\n",
      "Epoch [8/300], Step [15900/27733], Loss: 3.5376\n",
      "Epoch [8/300], Step [16000/27733], Loss: 4.7271\n",
      "Epoch [8/300], Step [16100/27733], Loss: 4.3590\n",
      "Epoch [8/300], Step [16200/27733], Loss: 4.2569\n",
      "Epoch [8/300], Step [16300/27733], Loss: 3.6946\n",
      "Epoch [8/300], Step [16400/27733], Loss: 4.0370\n",
      "Epoch [8/300], Step [16500/27733], Loss: 4.0679\n",
      "Epoch [8/300], Step [16600/27733], Loss: 5.0737\n",
      "Epoch [8/300], Step [16700/27733], Loss: 3.0302\n",
      "Epoch [8/300], Step [16800/27733], Loss: 2.5933\n",
      "Epoch [8/300], Step [16900/27733], Loss: 4.5370\n",
      "Epoch [8/300], Step [17000/27733], Loss: 4.3331\n",
      "Epoch [8/300], Step [17100/27733], Loss: 3.3914\n",
      "Epoch [8/300], Step [17200/27733], Loss: 3.6413\n",
      "Epoch [8/300], Step [17300/27733], Loss: 3.4167\n",
      "Epoch [8/300], Step [17400/27733], Loss: 4.0450\n",
      "Epoch [8/300], Step [17500/27733], Loss: 3.3172\n",
      "Epoch [8/300], Step [17600/27733], Loss: 4.2459\n",
      "Epoch [8/300], Step [17700/27733], Loss: 3.6061\n",
      "Epoch [8/300], Step [17800/27733], Loss: 4.1900\n",
      "Epoch [8/300], Step [17900/27733], Loss: 3.5139\n",
      "Epoch [8/300], Step [18000/27733], Loss: 4.4485\n",
      "Epoch [8/300], Step [18100/27733], Loss: 3.2151\n",
      "Epoch [8/300], Step [18200/27733], Loss: 4.3767\n",
      "Epoch [8/300], Step [18300/27733], Loss: 3.4774\n",
      "Epoch [8/300], Step [18400/27733], Loss: 3.7059\n",
      "Epoch [8/300], Step [18500/27733], Loss: 4.2951\n",
      "Epoch [8/300], Step [18600/27733], Loss: 3.4951\n",
      "Epoch [8/300], Step [18700/27733], Loss: 3.5174\n",
      "Epoch [8/300], Step [18800/27733], Loss: 2.9133\n",
      "Epoch [8/300], Step [18900/27733], Loss: 3.6796\n",
      "Epoch [8/300], Step [19000/27733], Loss: 3.3408\n",
      "Epoch [8/300], Step [19100/27733], Loss: 3.6714\n",
      "Epoch [8/300], Step [19200/27733], Loss: 4.5413\n",
      "Epoch [8/300], Step [19300/27733], Loss: 4.1451\n",
      "Epoch [8/300], Step [19400/27733], Loss: 3.4578\n",
      "Epoch [8/300], Step [19500/27733], Loss: 4.1236\n",
      "Epoch [8/300], Step [19600/27733], Loss: 4.0439\n",
      "Epoch [8/300], Step [19700/27733], Loss: 4.6338\n",
      "Epoch [8/300], Step [19800/27733], Loss: 4.6844\n",
      "Epoch [8/300], Step [19900/27733], Loss: 4.0295\n",
      "Epoch [8/300], Step [20000/27733], Loss: 3.9731\n",
      "Epoch [8/300], Step [20100/27733], Loss: 3.7161\n",
      "Epoch [8/300], Step [20200/27733], Loss: 4.0956\n",
      "Epoch [8/300], Step [20300/27733], Loss: 3.8623\n",
      "Epoch [8/300], Step [20400/27733], Loss: 3.6369\n",
      "Epoch [8/300], Step [20500/27733], Loss: 3.8080\n",
      "Epoch [8/300], Step [20600/27733], Loss: 3.8541\n",
      "Epoch [8/300], Step [20700/27733], Loss: 4.1717\n",
      "Epoch [8/300], Step [20800/27733], Loss: 4.3745\n",
      "Epoch [8/300], Step [20900/27733], Loss: 4.2817\n",
      "Epoch [8/300], Step [21000/27733], Loss: 3.2677\n",
      "Epoch [8/300], Step [21100/27733], Loss: 3.6360\n",
      "Epoch [8/300], Step [21200/27733], Loss: 3.9851\n",
      "Epoch [8/300], Step [21300/27733], Loss: 5.0933\n",
      "Epoch [8/300], Step [21400/27733], Loss: 4.0656\n",
      "Epoch [8/300], Step [21500/27733], Loss: 4.0204\n",
      "Epoch [8/300], Step [21600/27733], Loss: 3.0424\n",
      "Epoch [8/300], Step [21700/27733], Loss: 3.1372\n",
      "Epoch [8/300], Step [21800/27733], Loss: 3.5594\n",
      "Epoch [8/300], Step [21900/27733], Loss: 3.5233\n",
      "Epoch [8/300], Step [22000/27733], Loss: 3.0871\n",
      "Epoch [8/300], Step [22100/27733], Loss: 4.3142\n",
      "Epoch [8/300], Step [22200/27733], Loss: 3.4205\n",
      "Epoch [8/300], Step [22300/27733], Loss: 3.8000\n",
      "Epoch [8/300], Step [22400/27733], Loss: 3.6355\n",
      "Epoch [8/300], Step [22500/27733], Loss: 3.2397\n",
      "Epoch [8/300], Step [22600/27733], Loss: 3.7989\n",
      "Epoch [8/300], Step [22700/27733], Loss: 4.0404\n",
      "Epoch [8/300], Step [22800/27733], Loss: 3.9773\n",
      "Epoch [8/300], Step [22900/27733], Loss: 3.8984\n",
      "Epoch [8/300], Step [23000/27733], Loss: 3.6879\n",
      "Epoch [8/300], Step [23100/27733], Loss: 3.7526\n",
      "Epoch [8/300], Step [23200/27733], Loss: 2.6413\n",
      "Epoch [8/300], Step [23300/27733], Loss: 3.8490\n",
      "Epoch [8/300], Step [23400/27733], Loss: 4.4990\n",
      "Epoch [8/300], Step [23500/27733], Loss: 4.1628\n",
      "Epoch [8/300], Step [23600/27733], Loss: 3.9256\n",
      "Epoch [8/300], Step [23700/27733], Loss: 3.0208\n",
      "Epoch [8/300], Step [23800/27733], Loss: 4.1648\n",
      "Epoch [8/300], Step [23900/27733], Loss: 2.7084\n",
      "Epoch [8/300], Step [24000/27733], Loss: 4.4539\n",
      "Epoch [8/300], Step [24100/27733], Loss: 3.8213\n",
      "Epoch [8/300], Step [24200/27733], Loss: 3.5499\n",
      "Epoch [8/300], Step [24300/27733], Loss: 3.6054\n",
      "Epoch [8/300], Step [24400/27733], Loss: 4.3423\n",
      "Epoch [8/300], Step [24500/27733], Loss: 5.0272\n",
      "Epoch [8/300], Step [24600/27733], Loss: 4.3617\n",
      "Epoch [8/300], Step [24700/27733], Loss: 4.5667\n",
      "Epoch [8/300], Step [24800/27733], Loss: 4.6072\n",
      "Epoch [8/300], Step [24900/27733], Loss: 4.3014\n",
      "Epoch [8/300], Step [25000/27733], Loss: 4.7424\n",
      "Epoch [8/300], Step [25100/27733], Loss: 4.0262\n",
      "Epoch [8/300], Step [25200/27733], Loss: 3.3154\n",
      "Epoch [8/300], Step [25300/27733], Loss: 3.5491\n",
      "Epoch [8/300], Step [25400/27733], Loss: 3.5171\n",
      "Epoch [8/300], Step [25500/27733], Loss: 4.2587\n",
      "Epoch [8/300], Step [25600/27733], Loss: 3.6802\n",
      "Epoch [8/300], Step [25700/27733], Loss: 2.6236\n",
      "Epoch [8/300], Step [25800/27733], Loss: 4.5606\n",
      "Epoch [8/300], Step [25900/27733], Loss: 4.0582\n",
      "Epoch [8/300], Step [26000/27733], Loss: 4.2719\n",
      "Epoch [8/300], Step [26100/27733], Loss: 3.9755\n",
      "Epoch [8/300], Step [26200/27733], Loss: 4.0832\n",
      "Epoch [8/300], Step [26300/27733], Loss: 4.0775\n",
      "Epoch [8/300], Step [26400/27733], Loss: 3.5516\n",
      "Epoch [8/300], Step [26500/27733], Loss: 3.1783\n",
      "Epoch [8/300], Step [26600/27733], Loss: 3.7473\n",
      "Epoch [8/300], Step [26700/27733], Loss: 3.9334\n",
      "Epoch [8/300], Step [26800/27733], Loss: 3.9286\n",
      "Epoch [8/300], Step [26900/27733], Loss: 3.6301\n",
      "Epoch [8/300], Step [27000/27733], Loss: 3.6728\n",
      "Epoch [8/300], Step [27100/27733], Loss: 4.6434\n",
      "Epoch [8/300], Step [27200/27733], Loss: 3.3740\n",
      "Epoch [8/300], Step [27300/27733], Loss: 3.4567\n",
      "Epoch [8/300], Step [27400/27733], Loss: 4.4324\n",
      "Epoch [8/300], Step [27500/27733], Loss: 3.6296\n",
      "Epoch [8/300], Step [27600/27733], Loss: 3.6540\n",
      "Epoch [8/300], Step [27700/27733], Loss: 3.6677\n",
      "Epoch [9/300], Step [100/27733], Loss: 3.0346\n",
      "Epoch [9/300], Step [200/27733], Loss: 2.8977\n",
      "Epoch [9/300], Step [300/27733], Loss: 3.8846\n",
      "Epoch [9/300], Step [400/27733], Loss: 3.9547\n",
      "Epoch [9/300], Step [500/27733], Loss: 3.1126\n",
      "Epoch [9/300], Step [600/27733], Loss: 2.4141\n",
      "Epoch [9/300], Step [700/27733], Loss: 3.6714\n",
      "Epoch [9/300], Step [800/27733], Loss: 3.1228\n",
      "Epoch [9/300], Step [900/27733], Loss: 3.3160\n",
      "Epoch [9/300], Step [1000/27733], Loss: 4.2548\n",
      "Epoch [9/300], Step [1100/27733], Loss: 2.8562\n",
      "Epoch [9/300], Step [1200/27733], Loss: 4.0861\n",
      "Epoch [9/300], Step [1300/27733], Loss: 3.8974\n",
      "Epoch [9/300], Step [1400/27733], Loss: 3.4088\n",
      "Epoch [9/300], Step [1500/27733], Loss: 3.6005\n",
      "Epoch [9/300], Step [1600/27733], Loss: 2.6534\n",
      "Epoch [9/300], Step [1700/27733], Loss: 3.7645\n",
      "Epoch [9/300], Step [1800/27733], Loss: 2.7873\n",
      "Epoch [9/300], Step [1900/27733], Loss: 4.7741\n",
      "Epoch [9/300], Step [2000/27733], Loss: 4.3106\n",
      "Epoch [9/300], Step [2100/27733], Loss: 3.8026\n",
      "Epoch [9/300], Step [2200/27733], Loss: 3.7134\n",
      "Epoch [9/300], Step [2300/27733], Loss: 3.3067\n",
      "Epoch [9/300], Step [2400/27733], Loss: 3.5949\n",
      "Epoch [9/300], Step [2500/27733], Loss: 2.6922\n",
      "Epoch [9/300], Step [2600/27733], Loss: 3.0039\n",
      "Epoch [9/300], Step [2700/27733], Loss: 4.2490\n",
      "Epoch [9/300], Step [2800/27733], Loss: 3.4100\n",
      "Epoch [9/300], Step [2900/27733], Loss: 3.0407\n",
      "Epoch [9/300], Step [3000/27733], Loss: 3.7415\n",
      "Epoch [9/300], Step [3100/27733], Loss: 2.9908\n",
      "Epoch [9/300], Step [3200/27733], Loss: 3.0411\n",
      "Epoch [9/300], Step [3300/27733], Loss: 2.8732\n",
      "Epoch [9/300], Step [3400/27733], Loss: 3.8327\n",
      "Epoch [9/300], Step [3500/27733], Loss: 3.7605\n",
      "Epoch [9/300], Step [3600/27733], Loss: 2.1563\n",
      "Epoch [9/300], Step [3700/27733], Loss: 3.5205\n",
      "Epoch [9/300], Step [3800/27733], Loss: 3.7576\n",
      "Epoch [9/300], Step [3900/27733], Loss: 2.9947\n",
      "Epoch [9/300], Step [4000/27733], Loss: 3.2937\n",
      "Epoch [9/300], Step [4100/27733], Loss: 3.4195\n",
      "Epoch [9/300], Step [4200/27733], Loss: 3.0029\n",
      "Epoch [9/300], Step [4300/27733], Loss: 3.5252\n",
      "Epoch [9/300], Step [4400/27733], Loss: 3.9037\n",
      "Epoch [9/300], Step [4500/27733], Loss: 3.5568\n",
      "Epoch [9/300], Step [4600/27733], Loss: 3.7704\n",
      "Epoch [9/300], Step [4700/27733], Loss: 3.2251\n",
      "Epoch [9/300], Step [4800/27733], Loss: 3.2317\n",
      "Epoch [9/300], Step [4900/27733], Loss: 3.5888\n",
      "Epoch [9/300], Step [5000/27733], Loss: 3.9785\n",
      "Epoch [9/300], Step [5100/27733], Loss: 3.5821\n",
      "Epoch [9/300], Step [5200/27733], Loss: 2.7106\n",
      "Epoch [9/300], Step [5300/27733], Loss: 3.2253\n",
      "Epoch [9/300], Step [5400/27733], Loss: 3.3694\n",
      "Epoch [9/300], Step [5500/27733], Loss: 3.5873\n",
      "Epoch [9/300], Step [5600/27733], Loss: 3.2580\n",
      "Epoch [9/300], Step [5700/27733], Loss: 3.3529\n",
      "Epoch [9/300], Step [5800/27733], Loss: 3.0206\n",
      "Epoch [9/300], Step [5900/27733], Loss: 3.2714\n",
      "Epoch [9/300], Step [6000/27733], Loss: 4.5592\n",
      "Epoch [9/300], Step [6100/27733], Loss: 2.7607\n",
      "Epoch [9/300], Step [6200/27733], Loss: 2.6717\n",
      "Epoch [9/300], Step [6300/27733], Loss: 3.7690\n",
      "Epoch [9/300], Step [6400/27733], Loss: 2.6746\n",
      "Epoch [9/300], Step [6500/27733], Loss: 3.4129\n",
      "Epoch [9/300], Step [6600/27733], Loss: 3.2628\n",
      "Epoch [9/300], Step [6700/27733], Loss: 4.1602\n",
      "Epoch [9/300], Step [6800/27733], Loss: 2.6754\n",
      "Epoch [9/300], Step [6900/27733], Loss: 3.0006\n",
      "Epoch [9/300], Step [7000/27733], Loss: 2.9491\n",
      "Epoch [9/300], Step [7100/27733], Loss: 4.2537\n",
      "Epoch [9/300], Step [7200/27733], Loss: 3.9923\n",
      "Epoch [9/300], Step [7300/27733], Loss: 3.2847\n",
      "Epoch [9/300], Step [7400/27733], Loss: 3.8137\n",
      "Epoch [9/300], Step [7500/27733], Loss: 4.2810\n",
      "Epoch [9/300], Step [7600/27733], Loss: 2.7516\n",
      "Epoch [9/300], Step [7700/27733], Loss: 3.0056\n",
      "Epoch [9/300], Step [7800/27733], Loss: 3.6874\n",
      "Epoch [9/300], Step [7900/27733], Loss: 3.5875\n",
      "Epoch [9/300], Step [8000/27733], Loss: 3.2357\n",
      "Epoch [9/300], Step [8100/27733], Loss: 4.1622\n",
      "Epoch [9/300], Step [8200/27733], Loss: 3.8883\n",
      "Epoch [9/300], Step [8300/27733], Loss: 3.8856\n",
      "Epoch [9/300], Step [8400/27733], Loss: 4.3647\n",
      "Epoch [9/300], Step [8500/27733], Loss: 3.8119\n",
      "Epoch [9/300], Step [8600/27733], Loss: 4.0757\n",
      "Epoch [9/300], Step [8700/27733], Loss: 3.0104\n",
      "Epoch [9/300], Step [8800/27733], Loss: 2.9859\n",
      "Epoch [9/300], Step [8900/27733], Loss: 3.3722\n",
      "Epoch [9/300], Step [9000/27733], Loss: 3.2825\n",
      "Epoch [9/300], Step [9100/27733], Loss: 2.9574\n",
      "Epoch [9/300], Step [9200/27733], Loss: 4.4170\n",
      "Epoch [9/300], Step [9300/27733], Loss: 4.4095\n",
      "Epoch [9/300], Step [9400/27733], Loss: 3.5389\n",
      "Epoch [9/300], Step [9500/27733], Loss: 4.0547\n",
      "Epoch [9/300], Step [9600/27733], Loss: 3.2503\n",
      "Epoch [9/300], Step [9700/27733], Loss: 3.9342\n",
      "Epoch [9/300], Step [9800/27733], Loss: 3.5345\n",
      "Epoch [9/300], Step [9900/27733], Loss: 3.3379\n",
      "Epoch [9/300], Step [10000/27733], Loss: 3.5179\n",
      "Epoch [9/300], Step [10100/27733], Loss: 3.2522\n",
      "Epoch [9/300], Step [10200/27733], Loss: 3.6325\n",
      "Epoch [9/300], Step [10300/27733], Loss: 3.6147\n",
      "Epoch [9/300], Step [10400/27733], Loss: 3.1958\n",
      "Epoch [9/300], Step [10500/27733], Loss: 3.5083\n",
      "Epoch [9/300], Step [10600/27733], Loss: 3.2621\n",
      "Epoch [9/300], Step [10700/27733], Loss: 4.3891\n",
      "Epoch [9/300], Step [10800/27733], Loss: 3.6407\n",
      "Epoch [9/300], Step [10900/27733], Loss: 3.9470\n",
      "Epoch [9/300], Step [11000/27733], Loss: 3.3890\n",
      "Epoch [9/300], Step [11100/27733], Loss: 3.9379\n",
      "Epoch [9/300], Step [11200/27733], Loss: 3.9830\n",
      "Epoch [9/300], Step [11300/27733], Loss: 3.7778\n",
      "Epoch [9/300], Step [11400/27733], Loss: 3.0962\n",
      "Epoch [9/300], Step [11500/27733], Loss: 3.0129\n",
      "Epoch [9/300], Step [11600/27733], Loss: 3.1413\n",
      "Epoch [9/300], Step [11700/27733], Loss: 3.4289\n",
      "Epoch [9/300], Step [11800/27733], Loss: 3.5195\n",
      "Epoch [9/300], Step [11900/27733], Loss: 4.1089\n",
      "Epoch [9/300], Step [12000/27733], Loss: 4.3676\n",
      "Epoch [9/300], Step [12100/27733], Loss: 3.9192\n",
      "Epoch [9/300], Step [12200/27733], Loss: 3.2612\n",
      "Epoch [9/300], Step [12300/27733], Loss: 3.5828\n",
      "Epoch [9/300], Step [12400/27733], Loss: 3.7861\n",
      "Epoch [9/300], Step [12500/27733], Loss: 3.9307\n",
      "Epoch [9/300], Step [12600/27733], Loss: 3.1377\n",
      "Epoch [9/300], Step [12700/27733], Loss: 3.6222\n",
      "Epoch [9/300], Step [12800/27733], Loss: 3.4168\n",
      "Epoch [9/300], Step [12900/27733], Loss: 4.1715\n",
      "Epoch [9/300], Step [13000/27733], Loss: 3.5231\n",
      "Epoch [9/300], Step [13100/27733], Loss: 3.4382\n",
      "Epoch [9/300], Step [13200/27733], Loss: 4.4884\n",
      "Epoch [9/300], Step [13300/27733], Loss: 4.6124\n",
      "Epoch [9/300], Step [13400/27733], Loss: 4.3775\n",
      "Epoch [9/300], Step [13500/27733], Loss: 3.5965\n",
      "Epoch [9/300], Step [13600/27733], Loss: 4.2260\n",
      "Epoch [9/300], Step [13700/27733], Loss: 3.4045\n",
      "Epoch [9/300], Step [13800/27733], Loss: 4.1230\n",
      "Epoch [9/300], Step [13900/27733], Loss: 2.5960\n",
      "Epoch [9/300], Step [14000/27733], Loss: 4.2380\n",
      "Epoch [9/300], Step [14100/27733], Loss: 4.1106\n",
      "Epoch [9/300], Step [14200/27733], Loss: 3.6520\n",
      "Epoch [9/300], Step [14300/27733], Loss: 3.4773\n",
      "Epoch [9/300], Step [14400/27733], Loss: 3.4138\n",
      "Epoch [9/300], Step [14500/27733], Loss: 4.5231\n",
      "Epoch [9/300], Step [14600/27733], Loss: 3.5668\n",
      "Epoch [9/300], Step [14700/27733], Loss: 4.3533\n",
      "Epoch [9/300], Step [14800/27733], Loss: 3.9468\n",
      "Epoch [9/300], Step [14900/27733], Loss: 3.6023\n",
      "Epoch [9/300], Step [15000/27733], Loss: 3.5432\n",
      "Epoch [9/300], Step [15100/27733], Loss: 2.7836\n",
      "Epoch [9/300], Step [15200/27733], Loss: 3.5510\n",
      "Epoch [9/300], Step [15300/27733], Loss: 3.3616\n",
      "Epoch [9/300], Step [15400/27733], Loss: 3.9054\n",
      "Epoch [9/300], Step [15500/27733], Loss: 4.1583\n",
      "Epoch [9/300], Step [15600/27733], Loss: 3.1905\n",
      "Epoch [9/300], Step [15700/27733], Loss: 3.7295\n",
      "Epoch [9/300], Step [15800/27733], Loss: 2.8121\n",
      "Epoch [9/300], Step [15900/27733], Loss: 3.9110\n",
      "Epoch [9/300], Step [16000/27733], Loss: 3.9015\n",
      "Epoch [9/300], Step [16100/27733], Loss: 3.2837\n",
      "Epoch [9/300], Step [16200/27733], Loss: 2.7222\n",
      "Epoch [9/300], Step [16300/27733], Loss: 2.3851\n",
      "Epoch [9/300], Step [16400/27733], Loss: 3.7504\n",
      "Epoch [9/300], Step [16500/27733], Loss: 4.3735\n",
      "Epoch [9/300], Step [16600/27733], Loss: 4.0121\n",
      "Epoch [9/300], Step [16700/27733], Loss: 4.4664\n",
      "Epoch [9/300], Step [16800/27733], Loss: 3.9386\n",
      "Epoch [9/300], Step [16900/27733], Loss: 3.0724\n",
      "Epoch [9/300], Step [17000/27733], Loss: 3.3169\n",
      "Epoch [9/300], Step [17100/27733], Loss: 4.1137\n",
      "Epoch [9/300], Step [17200/27733], Loss: 3.6070\n",
      "Epoch [9/300], Step [17300/27733], Loss: 3.8814\n",
      "Epoch [9/300], Step [17400/27733], Loss: 3.6871\n",
      "Epoch [9/300], Step [17500/27733], Loss: 4.2670\n",
      "Epoch [9/300], Step [17600/27733], Loss: 3.4344\n",
      "Epoch [9/300], Step [17700/27733], Loss: 4.3650\n",
      "Epoch [9/300], Step [17800/27733], Loss: 3.2486\n",
      "Epoch [9/300], Step [17900/27733], Loss: 4.4019\n",
      "Epoch [9/300], Step [18000/27733], Loss: 3.9850\n",
      "Epoch [9/300], Step [18100/27733], Loss: 3.8815\n",
      "Epoch [9/300], Step [18200/27733], Loss: 3.4283\n",
      "Epoch [9/300], Step [18300/27733], Loss: 3.5365\n",
      "Epoch [9/300], Step [18400/27733], Loss: 4.5186\n",
      "Epoch [9/300], Step [18500/27733], Loss: 3.4805\n",
      "Epoch [9/300], Step [18600/27733], Loss: 3.9498\n",
      "Epoch [9/300], Step [18700/27733], Loss: 3.3895\n",
      "Epoch [9/300], Step [18800/27733], Loss: 3.8765\n",
      "Epoch [9/300], Step [18900/27733], Loss: 4.6006\n",
      "Epoch [9/300], Step [19000/27733], Loss: 4.0775\n",
      "Epoch [9/300], Step [19100/27733], Loss: 4.3832\n",
      "Epoch [9/300], Step [19200/27733], Loss: 3.9134\n",
      "Epoch [9/300], Step [19300/27733], Loss: 4.0211\n",
      "Epoch [9/300], Step [19400/27733], Loss: 4.3056\n",
      "Epoch [9/300], Step [19500/27733], Loss: 2.7137\n",
      "Epoch [9/300], Step [19600/27733], Loss: 4.5183\n",
      "Epoch [9/300], Step [19700/27733], Loss: 3.3939\n",
      "Epoch [9/300], Step [19800/27733], Loss: 4.4707\n",
      "Epoch [9/300], Step [19900/27733], Loss: 4.2345\n",
      "Epoch [9/300], Step [20000/27733], Loss: 2.9387\n",
      "Epoch [9/300], Step [20100/27733], Loss: 3.7274\n",
      "Epoch [9/300], Step [20200/27733], Loss: 4.0055\n",
      "Epoch [9/300], Step [20300/27733], Loss: 4.5851\n",
      "Epoch [9/300], Step [20400/27733], Loss: 3.9384\n",
      "Epoch [9/300], Step [20500/27733], Loss: 4.2758\n",
      "Epoch [9/300], Step [20600/27733], Loss: 4.6108\n",
      "Epoch [9/300], Step [20700/27733], Loss: 4.1092\n",
      "Epoch [9/300], Step [20800/27733], Loss: 2.9582\n",
      "Epoch [9/300], Step [20900/27733], Loss: 3.8805\n",
      "Epoch [9/300], Step [21000/27733], Loss: 3.2759\n",
      "Epoch [9/300], Step [21100/27733], Loss: 4.1550\n",
      "Epoch [9/300], Step [21200/27733], Loss: 3.4811\n",
      "Epoch [9/300], Step [21300/27733], Loss: 3.7186\n",
      "Epoch [9/300], Step [21400/27733], Loss: 3.3684\n",
      "Epoch [9/300], Step [21500/27733], Loss: 3.5230\n",
      "Epoch [9/300], Step [21600/27733], Loss: 3.9140\n",
      "Epoch [9/300], Step [21700/27733], Loss: 4.1587\n",
      "Epoch [9/300], Step [21800/27733], Loss: 4.0498\n",
      "Epoch [9/300], Step [21900/27733], Loss: 4.1210\n",
      "Epoch [9/300], Step [22000/27733], Loss: 2.5585\n",
      "Epoch [9/300], Step [22100/27733], Loss: 4.5158\n",
      "Epoch [9/300], Step [22200/27733], Loss: 3.8538\n",
      "Epoch [9/300], Step [22300/27733], Loss: 3.9375\n",
      "Epoch [9/300], Step [22400/27733], Loss: 3.6545\n",
      "Epoch [9/300], Step [22500/27733], Loss: 3.4566\n",
      "Epoch [9/300], Step [22600/27733], Loss: 4.1886\n",
      "Epoch [9/300], Step [22700/27733], Loss: 3.5493\n",
      "Epoch [9/300], Step [22800/27733], Loss: 3.4139\n",
      "Epoch [9/300], Step [22900/27733], Loss: 4.5171\n",
      "Epoch [9/300], Step [23000/27733], Loss: 4.3090\n",
      "Epoch [9/300], Step [23100/27733], Loss: 3.8314\n",
      "Epoch [9/300], Step [23200/27733], Loss: 4.5494\n",
      "Epoch [9/300], Step [23300/27733], Loss: 3.6029\n",
      "Epoch [9/300], Step [23400/27733], Loss: 4.0890\n",
      "Epoch [9/300], Step [23500/27733], Loss: 4.2983\n",
      "Epoch [9/300], Step [23600/27733], Loss: 3.3812\n",
      "Epoch [9/300], Step [23700/27733], Loss: 4.0829\n",
      "Epoch [9/300], Step [23800/27733], Loss: 2.8575\n",
      "Epoch [9/300], Step [23900/27733], Loss: 4.1041\n",
      "Epoch [9/300], Step [24000/27733], Loss: 4.3993\n",
      "Epoch [9/300], Step [24100/27733], Loss: 3.9207\n",
      "Epoch [9/300], Step [24200/27733], Loss: 4.0384\n",
      "Epoch [9/300], Step [24300/27733], Loss: 3.6634\n",
      "Epoch [9/300], Step [24400/27733], Loss: 4.9952\n",
      "Epoch [9/300], Step [24500/27733], Loss: 3.9769\n",
      "Epoch [9/300], Step [24600/27733], Loss: 3.3858\n",
      "Epoch [9/300], Step [24700/27733], Loss: 3.8439\n",
      "Epoch [9/300], Step [24800/27733], Loss: 4.0581\n",
      "Epoch [9/300], Step [24900/27733], Loss: 4.0777\n",
      "Epoch [9/300], Step [25000/27733], Loss: 4.0829\n",
      "Epoch [9/300], Step [25100/27733], Loss: 4.2799\n",
      "Epoch [9/300], Step [25200/27733], Loss: 4.0383\n",
      "Epoch [9/300], Step [25300/27733], Loss: 3.8348\n",
      "Epoch [9/300], Step [25400/27733], Loss: 4.0876\n",
      "Epoch [9/300], Step [25500/27733], Loss: 3.4117\n",
      "Epoch [9/300], Step [25600/27733], Loss: 3.5719\n",
      "Epoch [9/300], Step [25700/27733], Loss: 3.7965\n",
      "Epoch [9/300], Step [25800/27733], Loss: 3.6189\n",
      "Epoch [9/300], Step [25900/27733], Loss: 3.5379\n",
      "Epoch [9/300], Step [26000/27733], Loss: 3.8657\n",
      "Epoch [9/300], Step [26100/27733], Loss: 4.0007\n",
      "Epoch [9/300], Step [26200/27733], Loss: 3.6061\n",
      "Epoch [9/300], Step [26300/27733], Loss: 3.2333\n",
      "Epoch [9/300], Step [26400/27733], Loss: 3.6567\n",
      "Epoch [9/300], Step [26500/27733], Loss: 4.6845\n",
      "Epoch [9/300], Step [26600/27733], Loss: 3.6948\n",
      "Epoch [9/300], Step [26700/27733], Loss: 3.8168\n",
      "Epoch [9/300], Step [26800/27733], Loss: 4.0345\n",
      "Epoch [9/300], Step [26900/27733], Loss: 3.6747\n",
      "Epoch [9/300], Step [27000/27733], Loss: 4.2303\n",
      "Epoch [9/300], Step [27100/27733], Loss: 3.0572\n",
      "Epoch [9/300], Step [27200/27733], Loss: 3.7847\n",
      "Epoch [9/300], Step [27300/27733], Loss: 3.9726\n",
      "Epoch [9/300], Step [27400/27733], Loss: 4.0568\n",
      "Epoch [9/300], Step [27500/27733], Loss: 4.4236\n",
      "Epoch [9/300], Step [27600/27733], Loss: 3.6560\n",
      "Epoch [9/300], Step [27700/27733], Loss: 3.5171\n",
      "Epoch [10/300], Step [100/27733], Loss: 2.9812\n",
      "Epoch [10/300], Step [200/27733], Loss: 3.2733\n",
      "Epoch [10/300], Step [300/27733], Loss: 3.0060\n",
      "Epoch [10/300], Step [400/27733], Loss: 3.3825\n",
      "Epoch [10/300], Step [500/27733], Loss: 2.8533\n",
      "Epoch [10/300], Step [600/27733], Loss: 3.5191\n",
      "Epoch [10/300], Step [700/27733], Loss: 3.8456\n",
      "Epoch [10/300], Step [800/27733], Loss: 3.9042\n",
      "Epoch [10/300], Step [900/27733], Loss: 3.4912\n",
      "Epoch [10/300], Step [1000/27733], Loss: 3.6373\n",
      "Epoch [10/300], Step [1100/27733], Loss: 2.9250\n",
      "Epoch [10/300], Step [1200/27733], Loss: 2.2356\n",
      "Epoch [10/300], Step [1300/27733], Loss: 3.9860\n",
      "Epoch [10/300], Step [1400/27733], Loss: 3.1129\n",
      "Epoch [10/300], Step [1500/27733], Loss: 2.2943\n",
      "Epoch [10/300], Step [1600/27733], Loss: 3.6976\n",
      "Epoch [10/300], Step [1700/27733], Loss: 4.3155\n",
      "Epoch [10/300], Step [1800/27733], Loss: 3.6618\n",
      "Epoch [10/300], Step [1900/27733], Loss: 3.7558\n",
      "Epoch [10/300], Step [2000/27733], Loss: 3.7737\n",
      "Epoch [10/300], Step [2100/27733], Loss: 4.2635\n",
      "Epoch [10/300], Step [2200/27733], Loss: 3.3380\n",
      "Epoch [10/300], Step [2300/27733], Loss: 2.5618\n",
      "Epoch [10/300], Step [2400/27733], Loss: 3.1127\n",
      "Epoch [10/300], Step [2500/27733], Loss: 4.2656\n",
      "Epoch [10/300], Step [2600/27733], Loss: 4.3064\n",
      "Epoch [10/300], Step [2700/27733], Loss: 2.8482\n",
      "Epoch [10/300], Step [2800/27733], Loss: 3.7469\n",
      "Epoch [10/300], Step [2900/27733], Loss: 3.2643\n",
      "Epoch [10/300], Step [3000/27733], Loss: 2.8155\n",
      "Epoch [10/300], Step [3100/27733], Loss: 2.6805\n",
      "Epoch [10/300], Step [3200/27733], Loss: 3.7020\n",
      "Epoch [10/300], Step [3300/27733], Loss: 3.5113\n",
      "Epoch [10/300], Step [3400/27733], Loss: 2.4693\n",
      "Epoch [10/300], Step [3500/27733], Loss: 3.6339\n",
      "Epoch [10/300], Step [3600/27733], Loss: 3.8076\n",
      "Epoch [10/300], Step [3700/27733], Loss: 3.9095\n",
      "Epoch [10/300], Step [3800/27733], Loss: 3.3933\n",
      "Epoch [10/300], Step [3900/27733], Loss: 3.7035\n",
      "Epoch [10/300], Step [4000/27733], Loss: 3.2797\n",
      "Epoch [10/300], Step [4100/27733], Loss: 2.9128\n",
      "Epoch [10/300], Step [4200/27733], Loss: 3.0502\n",
      "Epoch [10/300], Step [4300/27733], Loss: 3.3914\n",
      "Epoch [10/300], Step [4400/27733], Loss: 3.3588\n",
      "Epoch [10/300], Step [4500/27733], Loss: 4.0597\n",
      "Epoch [10/300], Step [4600/27733], Loss: 3.0474\n",
      "Epoch [10/300], Step [4700/27733], Loss: 3.8983\n",
      "Epoch [10/300], Step [4800/27733], Loss: 3.4141\n",
      "Epoch [10/300], Step [4900/27733], Loss: 3.3662\n",
      "Epoch [10/300], Step [5000/27733], Loss: 4.0519\n",
      "Epoch [10/300], Step [5100/27733], Loss: 3.5970\n",
      "Epoch [10/300], Step [5200/27733], Loss: 3.0246\n",
      "Epoch [10/300], Step [5300/27733], Loss: 3.3854\n",
      "Epoch [10/300], Step [5400/27733], Loss: 3.5184\n",
      "Epoch [10/300], Step [5500/27733], Loss: 3.2255\n",
      "Epoch [10/300], Step [5600/27733], Loss: 3.8184\n",
      "Epoch [10/300], Step [5700/27733], Loss: 3.5614\n",
      "Epoch [10/300], Step [5800/27733], Loss: 4.3983\n",
      "Epoch [10/300], Step [5900/27733], Loss: 2.8925\n",
      "Epoch [10/300], Step [6000/27733], Loss: 3.1203\n",
      "Epoch [10/300], Step [6100/27733], Loss: 3.5853\n",
      "Epoch [10/300], Step [6200/27733], Loss: 2.5251\n",
      "Epoch [10/300], Step [6300/27733], Loss: 3.1492\n",
      "Epoch [10/300], Step [6400/27733], Loss: 3.0111\n",
      "Epoch [10/300], Step [6500/27733], Loss: 2.8389\n",
      "Epoch [10/300], Step [6600/27733], Loss: 3.1887\n",
      "Epoch [10/300], Step [6700/27733], Loss: 3.0720\n",
      "Epoch [10/300], Step [6800/27733], Loss: 3.5875\n",
      "Epoch [10/300], Step [6900/27733], Loss: 3.1400\n",
      "Epoch [10/300], Step [7000/27733], Loss: 2.7879\n",
      "Epoch [10/300], Step [7100/27733], Loss: 3.0130\n",
      "Epoch [10/300], Step [7200/27733], Loss: 3.6982\n",
      "Epoch [10/300], Step [7300/27733], Loss: 3.4800\n",
      "Epoch [10/300], Step [7400/27733], Loss: 3.7118\n",
      "Epoch [10/300], Step [7500/27733], Loss: 3.6469\n",
      "Epoch [10/300], Step [7600/27733], Loss: 3.8098\n",
      "Epoch [10/300], Step [7700/27733], Loss: 2.3907\n",
      "Epoch [10/300], Step [7800/27733], Loss: 3.3461\n",
      "Epoch [10/300], Step [7900/27733], Loss: 3.3993\n",
      "Epoch [10/300], Step [8000/27733], Loss: 2.8559\n",
      "Epoch [10/300], Step [8100/27733], Loss: 3.9563\n",
      "Epoch [10/300], Step [8200/27733], Loss: 3.2428\n",
      "Epoch [10/300], Step [8300/27733], Loss: 4.2637\n",
      "Epoch [10/300], Step [8400/27733], Loss: 4.3546\n",
      "Epoch [10/300], Step [8500/27733], Loss: 3.0619\n",
      "Epoch [10/300], Step [8600/27733], Loss: 2.9168\n",
      "Epoch [10/300], Step [8700/27733], Loss: 4.2013\n",
      "Epoch [10/300], Step [8800/27733], Loss: 3.5412\n",
      "Epoch [10/300], Step [8900/27733], Loss: 3.1958\n",
      "Epoch [10/300], Step [9000/27733], Loss: 2.9701\n",
      "Epoch [10/300], Step [9100/27733], Loss: 3.3147\n",
      "Epoch [10/300], Step [9200/27733], Loss: 3.1606\n",
      "Epoch [10/300], Step [9300/27733], Loss: 3.7808\n",
      "Epoch [10/300], Step [9400/27733], Loss: 2.8981\n",
      "Epoch [10/300], Step [9500/27733], Loss: 3.9670\n",
      "Epoch [10/300], Step [9600/27733], Loss: 3.6914\n",
      "Epoch [10/300], Step [9700/27733], Loss: 3.5316\n",
      "Epoch [10/300], Step [9800/27733], Loss: 4.1507\n",
      "Epoch [10/300], Step [9900/27733], Loss: 3.6053\n",
      "Epoch [10/300], Step [10000/27733], Loss: 2.9973\n",
      "Epoch [10/300], Step [10100/27733], Loss: 3.6486\n",
      "Epoch [10/300], Step [10200/27733], Loss: 4.2112\n",
      "Epoch [10/300], Step [10300/27733], Loss: 3.9321\n",
      "Epoch [10/300], Step [10400/27733], Loss: 3.1168\n",
      "Epoch [10/300], Step [10500/27733], Loss: 3.8613\n",
      "Epoch [10/300], Step [10600/27733], Loss: 4.1107\n",
      "Epoch [10/300], Step [10700/27733], Loss: 3.9378\n",
      "Epoch [10/300], Step [10800/27733], Loss: 4.1445\n",
      "Epoch [10/300], Step [10900/27733], Loss: 3.2204\n",
      "Epoch [10/300], Step [11000/27733], Loss: 3.6831\n",
      "Epoch [10/300], Step [11100/27733], Loss: 3.4726\n",
      "Epoch [10/300], Step [11200/27733], Loss: 3.2904\n",
      "Epoch [10/300], Step [11300/27733], Loss: 3.6522\n",
      "Epoch [10/300], Step [11400/27733], Loss: 3.4906\n",
      "Epoch [10/300], Step [11500/27733], Loss: 2.8670\n",
      "Epoch [10/300], Step [11600/27733], Loss: 3.0324\n",
      "Epoch [10/300], Step [11700/27733], Loss: 3.1661\n",
      "Epoch [10/300], Step [11800/27733], Loss: 3.2420\n",
      "Epoch [10/300], Step [11900/27733], Loss: 3.0344\n",
      "Epoch [10/300], Step [12000/27733], Loss: 3.1133\n",
      "Epoch [10/300], Step [12100/27733], Loss: 3.2901\n",
      "Epoch [10/300], Step [12200/27733], Loss: 3.5845\n",
      "Epoch [10/300], Step [12300/27733], Loss: 3.0236\n",
      "Epoch [10/300], Step [12400/27733], Loss: 3.6256\n",
      "Epoch [10/300], Step [12500/27733], Loss: 3.4764\n",
      "Epoch [10/300], Step [12600/27733], Loss: 3.1358\n",
      "Epoch [10/300], Step [12700/27733], Loss: 3.6829\n",
      "Epoch [10/300], Step [12800/27733], Loss: 3.5475\n",
      "Epoch [10/300], Step [12900/27733], Loss: 3.3082\n",
      "Epoch [10/300], Step [13000/27733], Loss: 3.3193\n",
      "Epoch [10/300], Step [13100/27733], Loss: 3.1706\n",
      "Epoch [10/300], Step [13200/27733], Loss: 3.1949\n",
      "Epoch [10/300], Step [13300/27733], Loss: 3.0643\n",
      "Epoch [10/300], Step [13400/27733], Loss: 4.1178\n",
      "Epoch [10/300], Step [13500/27733], Loss: 4.9804\n",
      "Epoch [10/300], Step [13600/27733], Loss: 3.1510\n",
      "Epoch [10/300], Step [13700/27733], Loss: 4.3748\n",
      "Epoch [10/300], Step [13800/27733], Loss: 3.9313\n",
      "Epoch [10/300], Step [13900/27733], Loss: 3.4162\n",
      "Epoch [10/300], Step [14000/27733], Loss: 3.7223\n",
      "Epoch [10/300], Step [14100/27733], Loss: 2.6252\n",
      "Epoch [10/300], Step [14200/27733], Loss: 4.5672\n",
      "Epoch [10/300], Step [14300/27733], Loss: 3.0226\n",
      "Epoch [10/300], Step [14400/27733], Loss: 3.5168\n",
      "Epoch [10/300], Step [14500/27733], Loss: 3.5771\n",
      "Epoch [10/300], Step [14600/27733], Loss: 3.3190\n",
      "Epoch [10/300], Step [14700/27733], Loss: 3.5716\n",
      "Epoch [10/300], Step [14800/27733], Loss: 3.0742\n",
      "Epoch [10/300], Step [14900/27733], Loss: 3.4209\n",
      "Epoch [10/300], Step [15000/27733], Loss: 4.0723\n",
      "Epoch [10/300], Step [15100/27733], Loss: 3.6073\n",
      "Epoch [10/300], Step [15200/27733], Loss: 2.9976\n",
      "Epoch [10/300], Step [15300/27733], Loss: 5.0027\n",
      "Epoch [10/300], Step [15400/27733], Loss: 4.1279\n",
      "Epoch [10/300], Step [15500/27733], Loss: 3.9833\n",
      "Epoch [10/300], Step [15600/27733], Loss: 3.0979\n",
      "Epoch [10/300], Step [15700/27733], Loss: 3.9518\n",
      "Epoch [10/300], Step [15800/27733], Loss: 3.9882\n",
      "Epoch [10/300], Step [15900/27733], Loss: 3.3421\n",
      "Epoch [10/300], Step [16000/27733], Loss: 2.7941\n",
      "Epoch [10/300], Step [16100/27733], Loss: 3.2234\n",
      "Epoch [10/300], Step [16200/27733], Loss: 3.9485\n",
      "Epoch [10/300], Step [16300/27733], Loss: 3.9872\n",
      "Epoch [10/300], Step [16400/27733], Loss: 3.1972\n",
      "Epoch [10/300], Step [16500/27733], Loss: 3.6865\n",
      "Epoch [10/300], Step [16600/27733], Loss: 4.0684\n",
      "Epoch [10/300], Step [16700/27733], Loss: 3.6369\n",
      "Epoch [10/300], Step [16800/27733], Loss: 2.9323\n",
      "Epoch [10/300], Step [16900/27733], Loss: 3.7397\n",
      "Epoch [10/300], Step [17000/27733], Loss: 3.3109\n",
      "Epoch [10/300], Step [17100/27733], Loss: 3.0408\n",
      "Epoch [10/300], Step [17200/27733], Loss: 4.0384\n",
      "Epoch [10/300], Step [17300/27733], Loss: 3.2804\n",
      "Epoch [10/300], Step [17400/27733], Loss: 3.5739\n",
      "Epoch [10/300], Step [17500/27733], Loss: 3.7454\n",
      "Epoch [10/300], Step [17600/27733], Loss: 3.4431\n",
      "Epoch [10/300], Step [17700/27733], Loss: 4.2612\n",
      "Epoch [10/300], Step [17800/27733], Loss: 4.3812\n",
      "Epoch [10/300], Step [17900/27733], Loss: 3.2699\n",
      "Epoch [10/300], Step [18000/27733], Loss: 3.2224\n",
      "Epoch [10/300], Step [18100/27733], Loss: 3.3177\n",
      "Epoch [10/300], Step [18200/27733], Loss: 4.3103\n",
      "Epoch [10/300], Step [18300/27733], Loss: 3.9242\n",
      "Epoch [10/300], Step [18400/27733], Loss: 2.9740\n",
      "Epoch [10/300], Step [18500/27733], Loss: 4.3451\n",
      "Epoch [10/300], Step [18600/27733], Loss: 3.4284\n",
      "Epoch [10/300], Step [18700/27733], Loss: 3.6874\n",
      "Epoch [10/300], Step [18800/27733], Loss: 3.4055\n",
      "Epoch [10/300], Step [18900/27733], Loss: 3.5978\n",
      "Epoch [10/300], Step [19000/27733], Loss: 4.4629\n",
      "Epoch [10/300], Step [19100/27733], Loss: 3.7155\n",
      "Epoch [10/300], Step [19200/27733], Loss: 3.4672\n",
      "Epoch [10/300], Step [19300/27733], Loss: 5.1184\n",
      "Epoch [10/300], Step [19400/27733], Loss: 4.0924\n",
      "Epoch [10/300], Step [19500/27733], Loss: 2.7953\n",
      "Epoch [10/300], Step [19600/27733], Loss: 3.3518\n",
      "Epoch [10/300], Step [19700/27733], Loss: 3.8854\n",
      "Epoch [10/300], Step [19800/27733], Loss: 3.3963\n",
      "Epoch [10/300], Step [19900/27733], Loss: 3.5037\n",
      "Epoch [10/300], Step [20000/27733], Loss: 3.5199\n",
      "Epoch [10/300], Step [20100/27733], Loss: 3.0481\n",
      "Epoch [10/300], Step [20200/27733], Loss: 3.1375\n",
      "Epoch [10/300], Step [20300/27733], Loss: 3.8219\n",
      "Epoch [10/300], Step [20400/27733], Loss: 3.3897\n",
      "Epoch [10/300], Step [20500/27733], Loss: 3.9656\n",
      "Epoch [10/300], Step [20600/27733], Loss: 4.1359\n",
      "Epoch [10/300], Step [20700/27733], Loss: 4.6101\n",
      "Epoch [10/300], Step [20800/27733], Loss: 3.4413\n",
      "Epoch [10/300], Step [20900/27733], Loss: 3.9209\n",
      "Epoch [10/300], Step [21000/27733], Loss: 3.0890\n",
      "Epoch [10/300], Step [21100/27733], Loss: 4.0051\n",
      "Epoch [10/300], Step [21200/27733], Loss: 4.0338\n",
      "Epoch [10/300], Step [21300/27733], Loss: 4.0253\n",
      "Epoch [10/300], Step [21400/27733], Loss: 4.3957\n",
      "Epoch [10/300], Step [21500/27733], Loss: 3.0160\n",
      "Epoch [10/300], Step [21600/27733], Loss: 3.8705\n",
      "Epoch [10/300], Step [21700/27733], Loss: 3.5570\n",
      "Epoch [10/300], Step [21800/27733], Loss: 3.5259\n",
      "Epoch [10/300], Step [21900/27733], Loss: 3.3145\n",
      "Epoch [10/300], Step [22000/27733], Loss: 3.6178\n",
      "Epoch [10/300], Step [22100/27733], Loss: 4.7289\n",
      "Epoch [10/300], Step [22200/27733], Loss: 3.5105\n",
      "Epoch [10/300], Step [22300/27733], Loss: 3.6059\n",
      "Epoch [10/300], Step [22400/27733], Loss: 3.2770\n",
      "Epoch [10/300], Step [22500/27733], Loss: 3.7137\n",
      "Epoch [10/300], Step [22600/27733], Loss: 3.8192\n",
      "Epoch [10/300], Step [22700/27733], Loss: 3.9325\n",
      "Epoch [10/300], Step [22800/27733], Loss: 2.9950\n",
      "Epoch [10/300], Step [22900/27733], Loss: 3.9615\n",
      "Epoch [10/300], Step [23000/27733], Loss: 3.6183\n",
      "Epoch [10/300], Step [23100/27733], Loss: 2.9386\n",
      "Epoch [10/300], Step [23200/27733], Loss: 3.8646\n",
      "Epoch [10/300], Step [23300/27733], Loss: 4.1454\n",
      "Epoch [10/300], Step [23400/27733], Loss: 4.0439\n",
      "Epoch [10/300], Step [23500/27733], Loss: 3.4658\n",
      "Epoch [10/300], Step [23600/27733], Loss: 3.0457\n",
      "Epoch [10/300], Step [23700/27733], Loss: 3.0276\n",
      "Epoch [10/300], Step [23800/27733], Loss: 4.0862\n",
      "Epoch [10/300], Step [23900/27733], Loss: 3.0422\n",
      "Epoch [10/300], Step [24000/27733], Loss: 4.3111\n",
      "Epoch [10/300], Step [24100/27733], Loss: 3.0882\n",
      "Epoch [10/300], Step [24200/27733], Loss: 4.3694\n",
      "Epoch [10/300], Step [24300/27733], Loss: 4.1094\n",
      "Epoch [10/300], Step [24400/27733], Loss: 3.6434\n",
      "Epoch [10/300], Step [24500/27733], Loss: 2.9424\n",
      "Epoch [10/300], Step [24600/27733], Loss: 4.5701\n",
      "Epoch [10/300], Step [24700/27733], Loss: 4.5818\n",
      "Epoch [10/300], Step [24800/27733], Loss: 3.7770\n",
      "Epoch [10/300], Step [24900/27733], Loss: 3.7476\n",
      "Epoch [10/300], Step [25000/27733], Loss: 3.6851\n",
      "Epoch [10/300], Step [25100/27733], Loss: 4.0579\n",
      "Epoch [10/300], Step [25200/27733], Loss: 3.6977\n",
      "Epoch [10/300], Step [25300/27733], Loss: 4.1953\n",
      "Epoch [10/300], Step [25400/27733], Loss: 3.7065\n",
      "Epoch [10/300], Step [25500/27733], Loss: 4.3463\n",
      "Epoch [10/300], Step [25600/27733], Loss: 4.3044\n",
      "Epoch [10/300], Step [25700/27733], Loss: 3.9116\n",
      "Epoch [10/300], Step [25800/27733], Loss: 3.0855\n",
      "Epoch [10/300], Step [25900/27733], Loss: 2.7883\n",
      "Epoch [10/300], Step [26000/27733], Loss: 3.2128\n",
      "Epoch [10/300], Step [26100/27733], Loss: 4.1690\n",
      "Epoch [10/300], Step [26200/27733], Loss: 4.0033\n",
      "Epoch [10/300], Step [26300/27733], Loss: 4.1476\n",
      "Epoch [10/300], Step [26400/27733], Loss: 4.7561\n",
      "Epoch [10/300], Step [26500/27733], Loss: 3.9364\n",
      "Epoch [10/300], Step [26600/27733], Loss: 4.2880\n",
      "Epoch [10/300], Step [26700/27733], Loss: 3.1836\n",
      "Epoch [10/300], Step [26800/27733], Loss: 3.8961\n",
      "Epoch [10/300], Step [26900/27733], Loss: 3.0434\n",
      "Epoch [10/300], Step [27000/27733], Loss: 3.2363\n",
      "Epoch [10/300], Step [27100/27733], Loss: 3.6571\n",
      "Epoch [10/300], Step [27200/27733], Loss: 2.8774\n",
      "Epoch [10/300], Step [27300/27733], Loss: 3.5275\n",
      "Epoch [10/300], Step [27400/27733], Loss: 4.4024\n",
      "Epoch [10/300], Step [27500/27733], Loss: 4.0283\n",
      "Epoch [10/300], Step [27600/27733], Loss: 3.2536\n",
      "Epoch [10/300], Step [27700/27733], Loss: 4.2132\n",
      "Epoch [11/300], Step [100/27733], Loss: 3.2886\n",
      "Epoch [11/300], Step [200/27733], Loss: 2.7934\n",
      "Epoch [11/300], Step [300/27733], Loss: 3.5273\n",
      "Epoch [11/300], Step [400/27733], Loss: 3.3842\n",
      "Epoch [11/300], Step [500/27733], Loss: 3.0934\n",
      "Epoch [11/300], Step [600/27733], Loss: 3.0057\n",
      "Epoch [11/300], Step [700/27733], Loss: 2.8372\n",
      "Epoch [11/300], Step [800/27733], Loss: 3.0994\n",
      "Epoch [11/300], Step [900/27733], Loss: 3.0773\n",
      "Epoch [11/300], Step [1000/27733], Loss: 2.9456\n",
      "Epoch [11/300], Step [1100/27733], Loss: 3.0945\n",
      "Epoch [11/300], Step [1200/27733], Loss: 2.7443\n",
      "Epoch [11/300], Step [1300/27733], Loss: 3.3209\n",
      "Epoch [11/300], Step [1400/27733], Loss: 3.1694\n",
      "Epoch [11/300], Step [1500/27733], Loss: 1.9537\n",
      "Epoch [11/300], Step [1600/27733], Loss: 2.6949\n",
      "Epoch [11/300], Step [1700/27733], Loss: 3.2186\n",
      "Epoch [11/300], Step [1800/27733], Loss: 3.1917\n",
      "Epoch [11/300], Step [1900/27733], Loss: 3.1990\n",
      "Epoch [11/300], Step [2000/27733], Loss: 3.1354\n",
      "Epoch [11/300], Step [2100/27733], Loss: 3.1347\n",
      "Epoch [11/300], Step [2200/27733], Loss: 3.2477\n",
      "Epoch [11/300], Step [2300/27733], Loss: 2.9132\n",
      "Epoch [11/300], Step [2400/27733], Loss: 3.0169\n",
      "Epoch [11/300], Step [2500/27733], Loss: 3.5411\n",
      "Epoch [11/300], Step [2600/27733], Loss: 2.9337\n",
      "Epoch [11/300], Step [2700/27733], Loss: 2.0894\n",
      "Epoch [11/300], Step [2800/27733], Loss: 2.9842\n",
      "Epoch [11/300], Step [2900/27733], Loss: 3.4416\n",
      "Epoch [11/300], Step [3000/27733], Loss: 3.2269\n",
      "Epoch [11/300], Step [3100/27733], Loss: 2.4021\n",
      "Epoch [11/300], Step [3200/27733], Loss: 2.6892\n",
      "Epoch [11/300], Step [3300/27733], Loss: 3.9209\n",
      "Epoch [11/300], Step [3400/27733], Loss: 3.2867\n",
      "Epoch [11/300], Step [3500/27733], Loss: 3.5695\n",
      "Epoch [11/300], Step [3600/27733], Loss: 3.4678\n",
      "Epoch [11/300], Step [3700/27733], Loss: 3.3064\n",
      "Epoch [11/300], Step [3800/27733], Loss: 3.4574\n",
      "Epoch [11/300], Step [3900/27733], Loss: 3.1458\n",
      "Epoch [11/300], Step [4000/27733], Loss: 3.4618\n",
      "Epoch [11/300], Step [4100/27733], Loss: 3.7886\n",
      "Epoch [11/300], Step [4200/27733], Loss: 3.8559\n",
      "Epoch [11/300], Step [4300/27733], Loss: 3.3869\n",
      "Epoch [11/300], Step [4400/27733], Loss: 4.1258\n",
      "Epoch [11/300], Step [4500/27733], Loss: 2.2463\n",
      "Epoch [11/300], Step [4600/27733], Loss: 2.8160\n",
      "Epoch [11/300], Step [4700/27733], Loss: 2.8919\n",
      "Epoch [11/300], Step [4800/27733], Loss: 2.6894\n",
      "Epoch [11/300], Step [4900/27733], Loss: 3.2762\n",
      "Epoch [11/300], Step [5000/27733], Loss: 2.6480\n",
      "Epoch [11/300], Step [5100/27733], Loss: 4.0721\n",
      "Epoch [11/300], Step [5200/27733], Loss: 3.2356\n",
      "Epoch [11/300], Step [5300/27733], Loss: 3.8177\n",
      "Epoch [11/300], Step [5400/27733], Loss: 3.2991\n",
      "Epoch [11/300], Step [5500/27733], Loss: 3.7269\n",
      "Epoch [11/300], Step [5600/27733], Loss: 3.4061\n",
      "Epoch [11/300], Step [5700/27733], Loss: 3.1816\n",
      "Epoch [11/300], Step [5800/27733], Loss: 3.3741\n",
      "Epoch [11/300], Step [5900/27733], Loss: 3.0657\n",
      "Epoch [11/300], Step [6000/27733], Loss: 4.2408\n",
      "Epoch [11/300], Step [6100/27733], Loss: 3.3558\n",
      "Epoch [11/300], Step [6200/27733], Loss: 3.9266\n",
      "Epoch [11/300], Step [6300/27733], Loss: 3.0514\n",
      "Epoch [11/300], Step [6400/27733], Loss: 3.9157\n",
      "Epoch [11/300], Step [6500/27733], Loss: 3.2662\n",
      "Epoch [11/300], Step [6600/27733], Loss: 3.7785\n",
      "Epoch [11/300], Step [6700/27733], Loss: 2.6367\n",
      "Epoch [11/300], Step [6800/27733], Loss: 3.3739\n",
      "Epoch [11/300], Step [6900/27733], Loss: 3.7458\n",
      "Epoch [11/300], Step [7000/27733], Loss: 3.3611\n",
      "Epoch [11/300], Step [7100/27733], Loss: 3.7880\n",
      "Epoch [11/300], Step [7200/27733], Loss: 4.0248\n",
      "Epoch [11/300], Step [7300/27733], Loss: 2.6390\n",
      "Epoch [11/300], Step [7400/27733], Loss: 2.9452\n",
      "Epoch [11/300], Step [7500/27733], Loss: 3.6148\n",
      "Epoch [11/300], Step [7600/27733], Loss: 3.6646\n",
      "Epoch [11/300], Step [7700/27733], Loss: 4.1127\n",
      "Epoch [11/300], Step [7800/27733], Loss: 3.4367\n",
      "Epoch [11/300], Step [7900/27733], Loss: 4.1513\n",
      "Epoch [11/300], Step [8000/27733], Loss: 3.1355\n",
      "Epoch [11/300], Step [8100/27733], Loss: 2.7497\n",
      "Epoch [11/300], Step [8200/27733], Loss: 3.1379\n",
      "Epoch [11/300], Step [8300/27733], Loss: 4.9968\n",
      "Epoch [11/300], Step [8400/27733], Loss: 3.6842\n",
      "Epoch [11/300], Step [8500/27733], Loss: 2.5687\n",
      "Epoch [11/300], Step [8600/27733], Loss: 3.2295\n",
      "Epoch [11/300], Step [8700/27733], Loss: 4.0418\n",
      "Epoch [11/300], Step [8800/27733], Loss: 3.5764\n",
      "Epoch [11/300], Step [8900/27733], Loss: 4.6472\n",
      "Epoch [11/300], Step [9000/27733], Loss: 3.7286\n",
      "Epoch [11/300], Step [9100/27733], Loss: 3.0034\n",
      "Epoch [11/300], Step [9200/27733], Loss: 2.8360\n",
      "Epoch [11/300], Step [9300/27733], Loss: 3.2922\n",
      "Epoch [11/300], Step [9400/27733], Loss: 3.5986\n",
      "Epoch [11/300], Step [9500/27733], Loss: 2.9038\n",
      "Epoch [11/300], Step [9600/27733], Loss: 3.5879\n",
      "Epoch [11/300], Step [9700/27733], Loss: 3.9344\n",
      "Epoch [11/300], Step [9800/27733], Loss: 3.2513\n",
      "Epoch [11/300], Step [9900/27733], Loss: 3.8413\n",
      "Epoch [11/300], Step [10000/27733], Loss: 3.1265\n",
      "Epoch [11/300], Step [10100/27733], Loss: 3.6433\n",
      "Epoch [11/300], Step [10200/27733], Loss: 3.0182\n",
      "Epoch [11/300], Step [10300/27733], Loss: 2.9287\n",
      "Epoch [11/300], Step [10400/27733], Loss: 3.6724\n",
      "Epoch [11/300], Step [10500/27733], Loss: 3.0640\n",
      "Epoch [11/300], Step [10600/27733], Loss: 2.5852\n",
      "Epoch [11/300], Step [10700/27733], Loss: 3.6094\n",
      "Epoch [11/300], Step [10800/27733], Loss: 3.4953\n",
      "Epoch [11/300], Step [10900/27733], Loss: 3.5333\n",
      "Epoch [11/300], Step [11000/27733], Loss: 3.5336\n",
      "Epoch [11/300], Step [11100/27733], Loss: 3.1920\n",
      "Epoch [11/300], Step [11200/27733], Loss: 3.3918\n",
      "Epoch [11/300], Step [11300/27733], Loss: 2.5454\n",
      "Epoch [11/300], Step [11400/27733], Loss: 2.9306\n",
      "Epoch [11/300], Step [11500/27733], Loss: 2.7436\n",
      "Epoch [11/300], Step [11600/27733], Loss: 4.0991\n",
      "Epoch [11/300], Step [11700/27733], Loss: 2.9651\n",
      "Epoch [11/300], Step [11800/27733], Loss: 3.5513\n",
      "Epoch [11/300], Step [11900/27733], Loss: 3.2550\n",
      "Epoch [11/300], Step [12000/27733], Loss: 3.9943\n",
      "Epoch [11/300], Step [12100/27733], Loss: 3.2985\n",
      "Epoch [11/300], Step [12200/27733], Loss: 4.0893\n",
      "Epoch [11/300], Step [12300/27733], Loss: 4.5569\n",
      "Epoch [11/300], Step [12400/27733], Loss: 3.8412\n",
      "Epoch [11/300], Step [12500/27733], Loss: 4.4460\n",
      "Epoch [11/300], Step [12600/27733], Loss: 3.6649\n",
      "Epoch [11/300], Step [12700/27733], Loss: 3.6670\n",
      "Epoch [11/300], Step [12800/27733], Loss: 3.8834\n",
      "Epoch [11/300], Step [12900/27733], Loss: 3.4699\n",
      "Epoch [11/300], Step [13000/27733], Loss: 4.3255\n",
      "Epoch [11/300], Step [13100/27733], Loss: 1.9694\n",
      "Epoch [11/300], Step [13200/27733], Loss: 2.7099\n",
      "Epoch [11/300], Step [13300/27733], Loss: 3.6217\n",
      "Epoch [11/300], Step [13400/27733], Loss: 4.1450\n",
      "Epoch [11/300], Step [13500/27733], Loss: 2.8393\n",
      "Epoch [11/300], Step [13600/27733], Loss: 3.6275\n",
      "Epoch [11/300], Step [13700/27733], Loss: 3.0572\n",
      "Epoch [11/300], Step [13800/27733], Loss: 3.6493\n",
      "Epoch [11/300], Step [13900/27733], Loss: 2.9818\n",
      "Epoch [11/300], Step [14000/27733], Loss: 3.1172\n",
      "Epoch [11/300], Step [14100/27733], Loss: 3.1347\n",
      "Epoch [11/300], Step [14200/27733], Loss: 4.0898\n",
      "Epoch [11/300], Step [14300/27733], Loss: 2.6548\n",
      "Epoch [11/300], Step [14400/27733], Loss: 3.4416\n",
      "Epoch [11/300], Step [14500/27733], Loss: 2.8488\n",
      "Epoch [11/300], Step [14600/27733], Loss: 3.5081\n",
      "Epoch [11/300], Step [14700/27733], Loss: 3.7398\n",
      "Epoch [11/300], Step [14800/27733], Loss: 3.7190\n",
      "Epoch [11/300], Step [14900/27733], Loss: 3.5253\n",
      "Epoch [11/300], Step [15000/27733], Loss: 3.2370\n",
      "Epoch [11/300], Step [15100/27733], Loss: 3.8014\n",
      "Epoch [11/300], Step [15200/27733], Loss: 3.6962\n",
      "Epoch [11/300], Step [15300/27733], Loss: 4.1717\n",
      "Epoch [11/300], Step [15400/27733], Loss: 2.9206\n",
      "Epoch [11/300], Step [15500/27733], Loss: 2.6672\n",
      "Epoch [11/300], Step [15600/27733], Loss: 4.0170\n",
      "Epoch [11/300], Step [15700/27733], Loss: 2.7744\n",
      "Epoch [11/300], Step [15800/27733], Loss: 3.3341\n",
      "Epoch [11/300], Step [15900/27733], Loss: 2.5633\n",
      "Epoch [11/300], Step [16000/27733], Loss: 3.7596\n",
      "Epoch [11/300], Step [16100/27733], Loss: 4.5267\n",
      "Epoch [11/300], Step [16200/27733], Loss: 3.4470\n",
      "Epoch [11/300], Step [16300/27733], Loss: 3.4581\n",
      "Epoch [11/300], Step [16400/27733], Loss: 3.8129\n",
      "Epoch [11/300], Step [16500/27733], Loss: 3.2180\n",
      "Epoch [11/300], Step [16600/27733], Loss: 2.8811\n",
      "Epoch [11/300], Step [16700/27733], Loss: 3.8355\n",
      "Epoch [11/300], Step [16800/27733], Loss: 3.9638\n",
      "Epoch [11/300], Step [16900/27733], Loss: 2.9348\n",
      "Epoch [11/300], Step [17000/27733], Loss: 4.5126\n",
      "Epoch [11/300], Step [17100/27733], Loss: 3.6108\n",
      "Epoch [11/300], Step [17200/27733], Loss: 3.9604\n",
      "Epoch [11/300], Step [17300/27733], Loss: 4.1967\n",
      "Epoch [11/300], Step [17400/27733], Loss: 3.7295\n",
      "Epoch [11/300], Step [17500/27733], Loss: 2.6821\n",
      "Epoch [11/300], Step [17600/27733], Loss: 4.2150\n",
      "Epoch [11/300], Step [17700/27733], Loss: 3.3625\n",
      "Epoch [11/300], Step [17800/27733], Loss: 4.1816\n",
      "Epoch [11/300], Step [17900/27733], Loss: 4.4620\n",
      "Epoch [11/300], Step [18000/27733], Loss: 3.7902\n",
      "Epoch [11/300], Step [18100/27733], Loss: 4.2722\n",
      "Epoch [11/300], Step [18200/27733], Loss: 3.8139\n",
      "Epoch [11/300], Step [18300/27733], Loss: 4.0541\n",
      "Epoch [11/300], Step [18400/27733], Loss: 3.1809\n",
      "Epoch [11/300], Step [18500/27733], Loss: 2.5730\n",
      "Epoch [11/300], Step [18600/27733], Loss: 3.0041\n",
      "Epoch [11/300], Step [18700/27733], Loss: 3.5555\n",
      "Epoch [11/300], Step [18800/27733], Loss: 3.3956\n",
      "Epoch [11/300], Step [18900/27733], Loss: 2.6923\n",
      "Epoch [11/300], Step [19000/27733], Loss: 3.5932\n",
      "Epoch [11/300], Step [19100/27733], Loss: 3.0814\n",
      "Epoch [11/300], Step [19200/27733], Loss: 4.1553\n",
      "Epoch [11/300], Step [19300/27733], Loss: 3.3210\n",
      "Epoch [11/300], Step [19400/27733], Loss: 2.9771\n",
      "Epoch [11/300], Step [19500/27733], Loss: 3.6967\n",
      "Epoch [11/300], Step [19600/27733], Loss: 4.7811\n",
      "Epoch [11/300], Step [19700/27733], Loss: 2.6661\n",
      "Epoch [11/300], Step [19800/27733], Loss: 3.6940\n",
      "Epoch [11/300], Step [19900/27733], Loss: 3.9327\n",
      "Epoch [11/300], Step [20000/27733], Loss: 3.2542\n",
      "Epoch [11/300], Step [20100/27733], Loss: 3.6447\n",
      "Epoch [11/300], Step [20200/27733], Loss: 3.5578\n",
      "Epoch [11/300], Step [20300/27733], Loss: 3.9387\n",
      "Epoch [11/300], Step [20400/27733], Loss: 4.0464\n",
      "Epoch [11/300], Step [20500/27733], Loss: 3.5803\n",
      "Epoch [11/300], Step [20600/27733], Loss: 3.8625\n",
      "Epoch [11/300], Step [20700/27733], Loss: 2.6789\n",
      "Epoch [11/300], Step [20800/27733], Loss: 3.8644\n",
      "Epoch [11/300], Step [20900/27733], Loss: 3.2289\n",
      "Epoch [11/300], Step [21000/27733], Loss: 4.9512\n",
      "Epoch [11/300], Step [21100/27733], Loss: 3.3255\n",
      "Epoch [11/300], Step [21200/27733], Loss: 4.3625\n",
      "Epoch [11/300], Step [21300/27733], Loss: 5.2566\n",
      "Epoch [11/300], Step [21400/27733], Loss: 3.8177\n",
      "Epoch [11/300], Step [21500/27733], Loss: 3.0116\n",
      "Epoch [11/300], Step [21600/27733], Loss: 3.9627\n",
      "Epoch [11/300], Step [21700/27733], Loss: 3.0519\n",
      "Epoch [11/300], Step [21800/27733], Loss: 4.5683\n",
      "Epoch [11/300], Step [21900/27733], Loss: 3.7385\n",
      "Epoch [11/300], Step [22000/27733], Loss: 3.3948\n",
      "Epoch [11/300], Step [22100/27733], Loss: 3.3609\n",
      "Epoch [11/300], Step [22200/27733], Loss: 3.9473\n",
      "Epoch [11/300], Step [22300/27733], Loss: 2.7829\n",
      "Epoch [11/300], Step [22400/27733], Loss: 3.1571\n",
      "Epoch [11/300], Step [22500/27733], Loss: 3.4496\n",
      "Epoch [11/300], Step [22600/27733], Loss: 4.2709\n",
      "Epoch [11/300], Step [22700/27733], Loss: 4.5622\n",
      "Epoch [11/300], Step [22800/27733], Loss: 2.8950\n",
      "Epoch [11/300], Step [22900/27733], Loss: 4.1111\n",
      "Epoch [11/300], Step [23000/27733], Loss: 2.9824\n",
      "Epoch [11/300], Step [23100/27733], Loss: 3.6711\n",
      "Epoch [11/300], Step [23200/27733], Loss: 4.7192\n",
      "Epoch [11/300], Step [23300/27733], Loss: 4.5604\n",
      "Epoch [11/300], Step [23400/27733], Loss: 3.7006\n",
      "Epoch [11/300], Step [23500/27733], Loss: 3.5259\n",
      "Epoch [11/300], Step [23600/27733], Loss: 3.8532\n",
      "Epoch [11/300], Step [23700/27733], Loss: 3.3082\n",
      "Epoch [11/300], Step [23800/27733], Loss: 3.3609\n",
      "Epoch [11/300], Step [23900/27733], Loss: 4.6197\n",
      "Epoch [11/300], Step [24000/27733], Loss: 3.9627\n",
      "Epoch [11/300], Step [24100/27733], Loss: 2.8382\n",
      "Epoch [11/300], Step [24200/27733], Loss: 3.2755\n",
      "Epoch [11/300], Step [24300/27733], Loss: 3.1171\n",
      "Epoch [11/300], Step [24400/27733], Loss: 3.3633\n",
      "Epoch [11/300], Step [24500/27733], Loss: 4.4416\n",
      "Epoch [11/300], Step [24600/27733], Loss: 4.1051\n",
      "Epoch [11/300], Step [24700/27733], Loss: 3.2529\n",
      "Epoch [11/300], Step [24800/27733], Loss: 3.2944\n",
      "Epoch [11/300], Step [24900/27733], Loss: 3.3295\n",
      "Epoch [11/300], Step [25000/27733], Loss: 3.6675\n",
      "Epoch [11/300], Step [25100/27733], Loss: 3.9189\n",
      "Epoch [11/300], Step [25200/27733], Loss: 4.2212\n",
      "Epoch [11/300], Step [25300/27733], Loss: 4.1621\n",
      "Epoch [11/300], Step [25400/27733], Loss: 3.7624\n",
      "Epoch [11/300], Step [25500/27733], Loss: 4.0384\n",
      "Epoch [11/300], Step [25600/27733], Loss: 4.2294\n",
      "Epoch [11/300], Step [25700/27733], Loss: 3.2813\n",
      "Epoch [11/300], Step [25800/27733], Loss: 3.6373\n",
      "Epoch [11/300], Step [25900/27733], Loss: 3.7040\n",
      "Epoch [11/300], Step [26000/27733], Loss: 4.4176\n",
      "Epoch [11/300], Step [26100/27733], Loss: 4.2919\n",
      "Epoch [11/300], Step [26200/27733], Loss: 3.9505\n",
      "Epoch [11/300], Step [26300/27733], Loss: 4.0947\n",
      "Epoch [11/300], Step [26400/27733], Loss: 3.5495\n",
      "Epoch [11/300], Step [26500/27733], Loss: 2.5793\n",
      "Epoch [11/300], Step [26600/27733], Loss: 3.6312\n",
      "Epoch [11/300], Step [26700/27733], Loss: 3.3194\n",
      "Epoch [11/300], Step [26800/27733], Loss: 3.6616\n",
      "Epoch [11/300], Step [26900/27733], Loss: 3.5123\n",
      "Epoch [11/300], Step [27000/27733], Loss: 3.7121\n",
      "Epoch [11/300], Step [27100/27733], Loss: 3.7529\n",
      "Epoch [11/300], Step [27200/27733], Loss: 4.2672\n",
      "Epoch [11/300], Step [27300/27733], Loss: 3.3471\n",
      "Epoch [11/300], Step [27400/27733], Loss: 3.3548\n",
      "Epoch [11/300], Step [27500/27733], Loss: 2.6659\n",
      "Epoch [11/300], Step [27600/27733], Loss: 4.2881\n",
      "Epoch [11/300], Step [27700/27733], Loss: 3.3220\n",
      "Epoch [12/300], Step [100/27733], Loss: 3.3937\n",
      "Epoch [12/300], Step [200/27733], Loss: 2.9694\n",
      "Epoch [12/300], Step [300/27733], Loss: 2.8606\n",
      "Epoch [12/300], Step [400/27733], Loss: 3.0542\n",
      "Epoch [12/300], Step [500/27733], Loss: 3.1611\n",
      "Epoch [12/300], Step [600/27733], Loss: 3.7194\n",
      "Epoch [12/300], Step [700/27733], Loss: 3.0495\n",
      "Epoch [12/300], Step [800/27733], Loss: 2.9664\n",
      "Epoch [12/300], Step [900/27733], Loss: 3.8002\n",
      "Epoch [12/300], Step [1000/27733], Loss: 3.5308\n",
      "Epoch [12/300], Step [1100/27733], Loss: 3.5494\n",
      "Epoch [12/300], Step [1200/27733], Loss: 3.4206\n",
      "Epoch [12/300], Step [1300/27733], Loss: 2.8101\n",
      "Epoch [12/300], Step [1400/27733], Loss: 3.2087\n",
      "Epoch [12/300], Step [1500/27733], Loss: 3.0509\n",
      "Epoch [12/300], Step [1600/27733], Loss: 2.7768\n",
      "Epoch [12/300], Step [1700/27733], Loss: 3.3482\n",
      "Epoch [12/300], Step [1800/27733], Loss: 3.4418\n",
      "Epoch [12/300], Step [1900/27733], Loss: 3.3747\n",
      "Epoch [12/300], Step [2000/27733], Loss: 3.5352\n",
      "Epoch [12/300], Step [2100/27733], Loss: 3.3635\n",
      "Epoch [12/300], Step [2200/27733], Loss: 3.5485\n",
      "Epoch [12/300], Step [2300/27733], Loss: 3.7143\n",
      "Epoch [12/300], Step [2400/27733], Loss: 3.2913\n",
      "Epoch [12/300], Step [2500/27733], Loss: 2.5114\n",
      "Epoch [12/300], Step [2600/27733], Loss: 3.5979\n",
      "Epoch [12/300], Step [2700/27733], Loss: 3.3861\n",
      "Epoch [12/300], Step [2800/27733], Loss: 3.4577\n",
      "Epoch [12/300], Step [2900/27733], Loss: 3.5997\n",
      "Epoch [12/300], Step [3000/27733], Loss: 2.9536\n",
      "Epoch [12/300], Step [3100/27733], Loss: 2.1163\n",
      "Epoch [12/300], Step [3200/27733], Loss: 3.4535\n",
      "Epoch [12/300], Step [3300/27733], Loss: 3.3453\n",
      "Epoch [12/300], Step [3400/27733], Loss: 3.5794\n",
      "Epoch [12/300], Step [3500/27733], Loss: 3.5065\n",
      "Epoch [12/300], Step [3600/27733], Loss: 3.1400\n",
      "Epoch [12/300], Step [3700/27733], Loss: 2.5140\n",
      "Epoch [12/300], Step [3800/27733], Loss: 3.0469\n",
      "Epoch [12/300], Step [3900/27733], Loss: 3.5429\n",
      "Epoch [12/300], Step [4000/27733], Loss: 3.0450\n",
      "Epoch [12/300], Step [4100/27733], Loss: 3.4048\n",
      "Epoch [12/300], Step [4200/27733], Loss: 3.0174\n",
      "Epoch [12/300], Step [4300/27733], Loss: 3.7081\n",
      "Epoch [12/300], Step [4400/27733], Loss: 3.9698\n",
      "Epoch [12/300], Step [4500/27733], Loss: 3.4916\n",
      "Epoch [12/300], Step [4600/27733], Loss: 3.7749\n",
      "Epoch [12/300], Step [4700/27733], Loss: 3.3055\n",
      "Epoch [12/300], Step [4800/27733], Loss: 3.1110\n",
      "Epoch [12/300], Step [4900/27733], Loss: 3.3122\n",
      "Epoch [12/300], Step [5000/27733], Loss: 3.8077\n",
      "Epoch [12/300], Step [5100/27733], Loss: 2.6325\n",
      "Epoch [12/300], Step [5200/27733], Loss: 3.4568\n",
      "Epoch [12/300], Step [5300/27733], Loss: 3.0586\n",
      "Epoch [12/300], Step [5400/27733], Loss: 3.8159\n",
      "Epoch [12/300], Step [5500/27733], Loss: 2.9511\n",
      "Epoch [12/300], Step [5600/27733], Loss: 3.0988\n",
      "Epoch [12/300], Step [5700/27733], Loss: 2.8435\n",
      "Epoch [12/300], Step [5800/27733], Loss: 4.3016\n",
      "Epoch [12/300], Step [5900/27733], Loss: 2.9262\n",
      "Epoch [12/300], Step [6000/27733], Loss: 2.4508\n",
      "Epoch [12/300], Step [6100/27733], Loss: 3.2988\n",
      "Epoch [12/300], Step [6200/27733], Loss: 3.6295\n",
      "Epoch [12/300], Step [6300/27733], Loss: 2.3769\n",
      "Epoch [12/300], Step [6400/27733], Loss: 2.8564\n",
      "Epoch [12/300], Step [6500/27733], Loss: 2.8386\n",
      "Epoch [12/300], Step [6600/27733], Loss: 2.5368\n",
      "Epoch [12/300], Step [6700/27733], Loss: 4.0439\n",
      "Epoch [12/300], Step [6800/27733], Loss: 3.4427\n",
      "Epoch [12/300], Step [6900/27733], Loss: 4.2166\n",
      "Epoch [12/300], Step [7000/27733], Loss: 3.7499\n",
      "Epoch [12/300], Step [7100/27733], Loss: 2.7600\n",
      "Epoch [12/300], Step [7200/27733], Loss: 3.1877\n",
      "Epoch [12/300], Step [7300/27733], Loss: 3.6465\n",
      "Epoch [12/300], Step [7400/27733], Loss: 2.6550\n",
      "Epoch [12/300], Step [7500/27733], Loss: 3.2734\n",
      "Epoch [12/300], Step [7600/27733], Loss: 2.4555\n",
      "Epoch [12/300], Step [7700/27733], Loss: 3.1665\n",
      "Epoch [12/300], Step [7800/27733], Loss: 4.2635\n",
      "Epoch [12/300], Step [7900/27733], Loss: 3.9586\n",
      "Epoch [12/300], Step [8000/27733], Loss: 2.6789\n",
      "Epoch [12/300], Step [8100/27733], Loss: 2.7035\n",
      "Epoch [12/300], Step [8200/27733], Loss: 3.5948\n",
      "Epoch [12/300], Step [8300/27733], Loss: 3.0628\n",
      "Epoch [12/300], Step [8400/27733], Loss: 2.4961\n",
      "Epoch [12/300], Step [8500/27733], Loss: 3.7901\n",
      "Epoch [12/300], Step [8600/27733], Loss: 3.0186\n",
      "Epoch [12/300], Step [8700/27733], Loss: 3.5655\n",
      "Epoch [12/300], Step [8800/27733], Loss: 2.6944\n",
      "Epoch [12/300], Step [8900/27733], Loss: 2.5882\n",
      "Epoch [12/300], Step [9000/27733], Loss: 2.7677\n",
      "Epoch [12/300], Step [9100/27733], Loss: 3.9442\n",
      "Epoch [12/300], Step [9200/27733], Loss: 3.0751\n",
      "Epoch [12/300], Step [9300/27733], Loss: 2.9020\n",
      "Epoch [12/300], Step [9400/27733], Loss: 3.7237\n",
      "Epoch [12/300], Step [9500/27733], Loss: 3.1499\n",
      "Epoch [12/300], Step [9600/27733], Loss: 3.2271\n",
      "Epoch [12/300], Step [9700/27733], Loss: 2.5891\n",
      "Epoch [12/300], Step [9800/27733], Loss: 3.5289\n",
      "Epoch [12/300], Step [9900/27733], Loss: 3.6545\n",
      "Epoch [12/300], Step [10000/27733], Loss: 3.1780\n",
      "Epoch [12/300], Step [10100/27733], Loss: 3.6676\n",
      "Epoch [12/300], Step [10200/27733], Loss: 3.1259\n",
      "Epoch [12/300], Step [10300/27733], Loss: 3.4551\n",
      "Epoch [12/300], Step [10400/27733], Loss: 3.8261\n",
      "Epoch [12/300], Step [10500/27733], Loss: 2.9146\n",
      "Epoch [12/300], Step [10600/27733], Loss: 2.6693\n",
      "Epoch [12/300], Step [10700/27733], Loss: 2.9769\n",
      "Epoch [12/300], Step [10800/27733], Loss: 3.1806\n",
      "Epoch [12/300], Step [10900/27733], Loss: 3.5279\n",
      "Epoch [12/300], Step [11000/27733], Loss: 3.5282\n",
      "Epoch [12/300], Step [11100/27733], Loss: 2.8072\n",
      "Epoch [12/300], Step [11200/27733], Loss: 3.0395\n",
      "Epoch [12/300], Step [11300/27733], Loss: 3.0787\n",
      "Epoch [12/300], Step [11400/27733], Loss: 3.7447\n",
      "Epoch [12/300], Step [11500/27733], Loss: 3.3152\n",
      "Epoch [12/300], Step [11600/27733], Loss: 3.7194\n",
      "Epoch [12/300], Step [11700/27733], Loss: 3.3728\n",
      "Epoch [12/300], Step [11800/27733], Loss: 3.8472\n",
      "Epoch [12/300], Step [11900/27733], Loss: 3.1134\n",
      "Epoch [12/300], Step [12000/27733], Loss: 2.6873\n",
      "Epoch [12/300], Step [12100/27733], Loss: 3.2371\n",
      "Epoch [12/300], Step [12200/27733], Loss: 4.3548\n",
      "Epoch [12/300], Step [12300/27733], Loss: 3.8770\n",
      "Epoch [12/300], Step [12400/27733], Loss: 4.0606\n",
      "Epoch [12/300], Step [12500/27733], Loss: 2.9086\n",
      "Epoch [12/300], Step [12600/27733], Loss: 3.0803\n",
      "Epoch [12/300], Step [12700/27733], Loss: 2.6733\n",
      "Epoch [12/300], Step [12800/27733], Loss: 3.2949\n",
      "Epoch [12/300], Step [12900/27733], Loss: 3.9742\n",
      "Epoch [12/300], Step [13000/27733], Loss: 3.9545\n",
      "Epoch [12/300], Step [13100/27733], Loss: 2.8260\n",
      "Epoch [12/300], Step [13200/27733], Loss: 4.5228\n",
      "Epoch [12/300], Step [13300/27733], Loss: 3.3349\n",
      "Epoch [12/300], Step [13400/27733], Loss: 3.6331\n",
      "Epoch [12/300], Step [13500/27733], Loss: 3.4287\n",
      "Epoch [12/300], Step [13600/27733], Loss: 2.7025\n",
      "Epoch [12/300], Step [13700/27733], Loss: 3.6168\n",
      "Epoch [12/300], Step [13800/27733], Loss: 3.4213\n",
      "Epoch [12/300], Step [13900/27733], Loss: 3.3480\n",
      "Epoch [12/300], Step [14000/27733], Loss: 2.7572\n",
      "Epoch [12/300], Step [14100/27733], Loss: 3.3607\n",
      "Epoch [12/300], Step [14200/27733], Loss: 3.4354\n",
      "Epoch [12/300], Step [14300/27733], Loss: 3.7397\n",
      "Epoch [12/300], Step [14400/27733], Loss: 2.9599\n",
      "Epoch [12/300], Step [14500/27733], Loss: 3.9525\n",
      "Epoch [12/300], Step [14600/27733], Loss: 3.2097\n",
      "Epoch [12/300], Step [14700/27733], Loss: 3.5216\n",
      "Epoch [12/300], Step [14800/27733], Loss: 3.4676\n",
      "Epoch [12/300], Step [14900/27733], Loss: 3.3547\n",
      "Epoch [12/300], Step [15000/27733], Loss: 4.1246\n",
      "Epoch [12/300], Step [15100/27733], Loss: 3.5927\n",
      "Epoch [12/300], Step [15200/27733], Loss: 3.8830\n",
      "Epoch [12/300], Step [15300/27733], Loss: 4.1557\n",
      "Epoch [12/300], Step [15400/27733], Loss: 4.2735\n",
      "Epoch [12/300], Step [15500/27733], Loss: 3.8164\n",
      "Epoch [12/300], Step [15600/27733], Loss: 3.1041\n",
      "Epoch [12/300], Step [15700/27733], Loss: 3.7703\n",
      "Epoch [12/300], Step [15800/27733], Loss: 3.8104\n",
      "Epoch [12/300], Step [15900/27733], Loss: 3.2992\n",
      "Epoch [12/300], Step [16000/27733], Loss: 4.1835\n",
      "Epoch [12/300], Step [16100/27733], Loss: 3.1577\n",
      "Epoch [12/300], Step [16200/27733], Loss: 4.0687\n",
      "Epoch [12/300], Step [16300/27733], Loss: 2.8650\n",
      "Epoch [12/300], Step [16400/27733], Loss: 3.2576\n",
      "Epoch [12/300], Step [16500/27733], Loss: 3.0531\n",
      "Epoch [12/300], Step [16600/27733], Loss: 3.8223\n",
      "Epoch [12/300], Step [16700/27733], Loss: 4.4909\n",
      "Epoch [12/300], Step [16800/27733], Loss: 3.1948\n",
      "Epoch [12/300], Step [16900/27733], Loss: 2.8551\n",
      "Epoch [12/300], Step [17000/27733], Loss: 3.8612\n",
      "Epoch [12/300], Step [17100/27733], Loss: 3.2886\n",
      "Epoch [12/300], Step [17200/27733], Loss: 2.6945\n",
      "Epoch [12/300], Step [17300/27733], Loss: 3.8274\n",
      "Epoch [12/300], Step [17400/27733], Loss: 3.7454\n",
      "Epoch [12/300], Step [17500/27733], Loss: 3.5435\n",
      "Epoch [12/300], Step [17600/27733], Loss: 3.5140\n",
      "Epoch [12/300], Step [17700/27733], Loss: 3.2204\n",
      "Epoch [12/300], Step [17800/27733], Loss: 3.3818\n",
      "Epoch [12/300], Step [17900/27733], Loss: 3.8353\n",
      "Epoch [12/300], Step [18000/27733], Loss: 3.1794\n",
      "Epoch [12/300], Step [18100/27733], Loss: 3.5646\n",
      "Epoch [12/300], Step [18200/27733], Loss: 3.5649\n",
      "Epoch [12/300], Step [18300/27733], Loss: 4.2070\n",
      "Epoch [12/300], Step [18400/27733], Loss: 3.8596\n",
      "Epoch [12/300], Step [18500/27733], Loss: 3.9183\n",
      "Epoch [12/300], Step [18600/27733], Loss: 2.7698\n",
      "Epoch [12/300], Step [18700/27733], Loss: 3.9006\n",
      "Epoch [12/300], Step [18800/27733], Loss: 3.9941\n",
      "Epoch [12/300], Step [18900/27733], Loss: 3.8932\n",
      "Epoch [12/300], Step [19000/27733], Loss: 3.9666\n",
      "Epoch [12/300], Step [19100/27733], Loss: 3.4043\n",
      "Epoch [12/300], Step [19200/27733], Loss: 3.8568\n",
      "Epoch [12/300], Step [19300/27733], Loss: 3.5483\n",
      "Epoch [12/300], Step [19400/27733], Loss: 3.1562\n",
      "Epoch [12/300], Step [19500/27733], Loss: 3.9734\n",
      "Epoch [12/300], Step [19600/27733], Loss: 3.2592\n",
      "Epoch [12/300], Step [19700/27733], Loss: 3.9384\n",
      "Epoch [12/300], Step [19800/27733], Loss: 3.4565\n",
      "Epoch [12/300], Step [19900/27733], Loss: 3.2024\n",
      "Epoch [12/300], Step [20000/27733], Loss: 3.1542\n",
      "Epoch [12/300], Step [20100/27733], Loss: 3.1745\n",
      "Epoch [12/300], Step [20200/27733], Loss: 2.9922\n",
      "Epoch [12/300], Step [20300/27733], Loss: 3.5243\n",
      "Epoch [12/300], Step [20400/27733], Loss: 3.0647\n",
      "Epoch [12/300], Step [20500/27733], Loss: 4.1592\n",
      "Epoch [12/300], Step [20600/27733], Loss: 4.3176\n",
      "Epoch [12/300], Step [20700/27733], Loss: 3.7282\n",
      "Epoch [12/300], Step [20800/27733], Loss: 4.1168\n",
      "Epoch [12/300], Step [20900/27733], Loss: 2.9713\n",
      "Epoch [12/300], Step [21000/27733], Loss: 3.5238\n",
      "Epoch [12/300], Step [21100/27733], Loss: 3.3802\n",
      "Epoch [12/300], Step [21200/27733], Loss: 3.7208\n",
      "Epoch [12/300], Step [21300/27733], Loss: 3.4894\n",
      "Epoch [12/300], Step [21400/27733], Loss: 3.2817\n",
      "Epoch [12/300], Step [21500/27733], Loss: 3.7161\n",
      "Epoch [12/300], Step [21600/27733], Loss: 3.8201\n",
      "Epoch [12/300], Step [21700/27733], Loss: 3.8330\n",
      "Epoch [12/300], Step [21800/27733], Loss: 3.5525\n",
      "Epoch [12/300], Step [21900/27733], Loss: 3.1743\n",
      "Epoch [12/300], Step [22000/27733], Loss: 3.9032\n",
      "Epoch [12/300], Step [22100/27733], Loss: 3.3792\n",
      "Epoch [12/300], Step [22200/27733], Loss: 4.2649\n",
      "Epoch [12/300], Step [22300/27733], Loss: 3.6777\n",
      "Epoch [12/300], Step [22400/27733], Loss: 3.8733\n",
      "Epoch [12/300], Step [22500/27733], Loss: 2.9530\n",
      "Epoch [12/300], Step [22600/27733], Loss: 4.4674\n",
      "Epoch [12/300], Step [22700/27733], Loss: 3.0598\n",
      "Epoch [12/300], Step [22800/27733], Loss: 3.9738\n",
      "Epoch [12/300], Step [22900/27733], Loss: 2.5041\n",
      "Epoch [12/300], Step [23000/27733], Loss: 3.5595\n",
      "Epoch [12/300], Step [23100/27733], Loss: 3.7972\n",
      "Epoch [12/300], Step [23200/27733], Loss: 2.2307\n",
      "Epoch [12/300], Step [23300/27733], Loss: 2.8355\n",
      "Epoch [12/300], Step [23400/27733], Loss: 3.2376\n",
      "Epoch [12/300], Step [23500/27733], Loss: 4.2959\n",
      "Epoch [12/300], Step [23600/27733], Loss: 3.1916\n",
      "Epoch [12/300], Step [23700/27733], Loss: 3.3382\n",
      "Epoch [12/300], Step [23800/27733], Loss: 3.0542\n",
      "Epoch [12/300], Step [23900/27733], Loss: 2.8339\n",
      "Epoch [12/300], Step [24000/27733], Loss: 2.9457\n",
      "Epoch [12/300], Step [24100/27733], Loss: 2.6730\n",
      "Epoch [12/300], Step [24200/27733], Loss: 2.9778\n",
      "Epoch [12/300], Step [24300/27733], Loss: 3.6883\n",
      "Epoch [12/300], Step [24400/27733], Loss: 4.6233\n",
      "Epoch [12/300], Step [24500/27733], Loss: 3.4204\n",
      "Epoch [12/300], Step [24600/27733], Loss: 3.7775\n",
      "Epoch [12/300], Step [24700/27733], Loss: 2.9419\n",
      "Epoch [12/300], Step [24800/27733], Loss: 3.4947\n",
      "Epoch [12/300], Step [24900/27733], Loss: 4.1353\n",
      "Epoch [12/300], Step [25000/27733], Loss: 4.5574\n",
      "Epoch [12/300], Step [25100/27733], Loss: 3.3279\n",
      "Epoch [12/300], Step [25200/27733], Loss: 3.1571\n",
      "Epoch [12/300], Step [25300/27733], Loss: 3.2531\n",
      "Epoch [12/300], Step [25400/27733], Loss: 4.5723\n",
      "Epoch [12/300], Step [25500/27733], Loss: 3.8005\n",
      "Epoch [12/300], Step [25600/27733], Loss: 4.0049\n",
      "Epoch [12/300], Step [25700/27733], Loss: 3.0177\n",
      "Epoch [12/300], Step [25800/27733], Loss: 4.1560\n",
      "Epoch [12/300], Step [25900/27733], Loss: 3.3789\n",
      "Epoch [12/300], Step [26000/27733], Loss: 3.1917\n",
      "Epoch [12/300], Step [26100/27733], Loss: 4.4938\n",
      "Epoch [12/300], Step [26200/27733], Loss: 3.9861\n",
      "Epoch [12/300], Step [26300/27733], Loss: 3.5051\n",
      "Epoch [12/300], Step [26400/27733], Loss: 3.8519\n",
      "Epoch [12/300], Step [26500/27733], Loss: 5.6002\n",
      "Epoch [12/300], Step [26600/27733], Loss: 3.2402\n",
      "Epoch [12/300], Step [26700/27733], Loss: 3.5562\n",
      "Epoch [12/300], Step [26800/27733], Loss: 2.9369\n",
      "Epoch [12/300], Step [26900/27733], Loss: 3.8876\n",
      "Epoch [12/300], Step [27000/27733], Loss: 2.9060\n",
      "Epoch [12/300], Step [27100/27733], Loss: 3.2838\n",
      "Epoch [12/300], Step [27200/27733], Loss: 3.9166\n",
      "Epoch [12/300], Step [27300/27733], Loss: 4.2230\n",
      "Epoch [12/300], Step [27400/27733], Loss: 4.6196\n",
      "Epoch [12/300], Step [27500/27733], Loss: 4.3957\n",
      "Epoch [12/300], Step [27600/27733], Loss: 4.6168\n",
      "Epoch [12/300], Step [27700/27733], Loss: 3.6492\n",
      "Epoch [13/300], Step [100/27733], Loss: 3.5921\n",
      "Epoch [13/300], Step [200/27733], Loss: 2.7522\n",
      "Epoch [13/300], Step [300/27733], Loss: 2.8131\n",
      "Epoch [13/300], Step [400/27733], Loss: 2.9288\n",
      "Epoch [13/300], Step [500/27733], Loss: 3.3036\n",
      "Epoch [13/300], Step [600/27733], Loss: 2.3943\n",
      "Epoch [13/300], Step [700/27733], Loss: 2.8470\n",
      "Epoch [13/300], Step [800/27733], Loss: 3.2316\n",
      "Epoch [13/300], Step [900/27733], Loss: 3.1840\n",
      "Epoch [13/300], Step [1000/27733], Loss: 3.0245\n",
      "Epoch [13/300], Step [1100/27733], Loss: 3.9032\n",
      "Epoch [13/300], Step [1200/27733], Loss: 3.1114\n",
      "Epoch [13/300], Step [1300/27733], Loss: 3.4167\n",
      "Epoch [13/300], Step [1400/27733], Loss: 3.2838\n",
      "Epoch [13/300], Step [1500/27733], Loss: 3.6580\n",
      "Epoch [13/300], Step [1600/27733], Loss: 2.8346\n",
      "Epoch [13/300], Step [1700/27733], Loss: 3.3586\n",
      "Epoch [13/300], Step [1800/27733], Loss: 2.8133\n",
      "Epoch [13/300], Step [1900/27733], Loss: 3.3653\n",
      "Epoch [13/300], Step [2000/27733], Loss: 2.5308\n",
      "Epoch [13/300], Step [2100/27733], Loss: 2.4696\n",
      "Epoch [13/300], Step [2200/27733], Loss: 3.6487\n",
      "Epoch [13/300], Step [2300/27733], Loss: 2.9825\n",
      "Epoch [13/300], Step [2400/27733], Loss: 2.9523\n",
      "Epoch [13/300], Step [2500/27733], Loss: 3.5977\n",
      "Epoch [13/300], Step [2600/27733], Loss: 3.4625\n",
      "Epoch [13/300], Step [2700/27733], Loss: 3.5712\n",
      "Epoch [13/300], Step [2800/27733], Loss: 3.4907\n",
      "Epoch [13/300], Step [2900/27733], Loss: 2.6544\n",
      "Epoch [13/300], Step [3000/27733], Loss: 2.9200\n",
      "Epoch [13/300], Step [3100/27733], Loss: 2.9311\n",
      "Epoch [13/300], Step [3200/27733], Loss: 3.0063\n",
      "Epoch [13/300], Step [3300/27733], Loss: 3.3062\n",
      "Epoch [13/300], Step [3400/27733], Loss: 4.4175\n",
      "Epoch [13/300], Step [3500/27733], Loss: 2.8365\n",
      "Epoch [13/300], Step [3600/27733], Loss: 3.6618\n",
      "Epoch [13/300], Step [3700/27733], Loss: 3.2102\n",
      "Epoch [13/300], Step [3800/27733], Loss: 3.2750\n",
      "Epoch [13/300], Step [3900/27733], Loss: 3.1336\n",
      "Epoch [13/300], Step [4000/27733], Loss: 3.4169\n",
      "Epoch [13/300], Step [4100/27733], Loss: 2.9469\n",
      "Epoch [13/300], Step [4200/27733], Loss: 2.8698\n",
      "Epoch [13/300], Step [4300/27733], Loss: 3.5412\n",
      "Epoch [13/300], Step [4400/27733], Loss: 2.7070\n",
      "Epoch [13/300], Step [4500/27733], Loss: 3.8883\n",
      "Epoch [13/300], Step [4600/27733], Loss: 3.4193\n",
      "Epoch [13/300], Step [4700/27733], Loss: 2.7641\n",
      "Epoch [13/300], Step [4800/27733], Loss: 3.7717\n",
      "Epoch [13/300], Step [4900/27733], Loss: 3.5504\n",
      "Epoch [13/300], Step [5000/27733], Loss: 2.9531\n",
      "Epoch [13/300], Step [5100/27733], Loss: 3.1809\n",
      "Epoch [13/300], Step [5200/27733], Loss: 3.4232\n",
      "Epoch [13/300], Step [5300/27733], Loss: 3.0848\n",
      "Epoch [13/300], Step [5400/27733], Loss: 3.3016\n",
      "Epoch [13/300], Step [5500/27733], Loss: 2.7459\n",
      "Epoch [13/300], Step [5600/27733], Loss: 3.8767\n",
      "Epoch [13/300], Step [5700/27733], Loss: 3.1854\n",
      "Epoch [13/300], Step [5800/27733], Loss: 3.4035\n",
      "Epoch [13/300], Step [5900/27733], Loss: 2.9621\n",
      "Epoch [13/300], Step [6000/27733], Loss: 3.8417\n",
      "Epoch [13/300], Step [6100/27733], Loss: 3.6844\n",
      "Epoch [13/300], Step [6200/27733], Loss: 3.1648\n",
      "Epoch [13/300], Step [6300/27733], Loss: 3.3769\n",
      "Epoch [13/300], Step [6400/27733], Loss: 3.5102\n",
      "Epoch [13/300], Step [6500/27733], Loss: 3.7713\n",
      "Epoch [13/300], Step [6600/27733], Loss: 3.9896\n",
      "Epoch [13/300], Step [6700/27733], Loss: 3.3624\n",
      "Epoch [13/300], Step [6800/27733], Loss: 3.4278\n",
      "Epoch [13/300], Step [6900/27733], Loss: 3.5424\n",
      "Epoch [13/300], Step [7000/27733], Loss: 3.7629\n",
      "Epoch [13/300], Step [7100/27733], Loss: 2.8766\n",
      "Epoch [13/300], Step [7200/27733], Loss: 3.0293\n",
      "Epoch [13/300], Step [7300/27733], Loss: 3.7675\n",
      "Epoch [13/300], Step [7400/27733], Loss: 3.4634\n",
      "Epoch [13/300], Step [7500/27733], Loss: 3.2644\n",
      "Epoch [13/300], Step [7600/27733], Loss: 2.8382\n",
      "Epoch [13/300], Step [7700/27733], Loss: 3.2773\n",
      "Epoch [13/300], Step [7800/27733], Loss: 3.0347\n",
      "Epoch [13/300], Step [7900/27733], Loss: 2.5606\n",
      "Epoch [13/300], Step [8000/27733], Loss: 2.7797\n",
      "Epoch [13/300], Step [8100/27733], Loss: 2.4187\n",
      "Epoch [13/300], Step [8200/27733], Loss: 3.4259\n",
      "Epoch [13/300], Step [8300/27733], Loss: 4.4004\n",
      "Epoch [13/300], Step [8400/27733], Loss: 3.4915\n",
      "Epoch [13/300], Step [8500/27733], Loss: 3.6385\n",
      "Epoch [13/300], Step [8600/27733], Loss: 3.6223\n",
      "Epoch [13/300], Step [8700/27733], Loss: 2.3939\n",
      "Epoch [13/300], Step [8800/27733], Loss: 3.8311\n",
      "Epoch [13/300], Step [8900/27733], Loss: 3.1751\n",
      "Epoch [13/300], Step [9000/27733], Loss: 3.9511\n",
      "Epoch [13/300], Step [9100/27733], Loss: 4.0838\n",
      "Epoch [13/300], Step [9200/27733], Loss: 2.8379\n",
      "Epoch [13/300], Step [9300/27733], Loss: 3.2801\n",
      "Epoch [13/300], Step [9400/27733], Loss: 3.2437\n",
      "Epoch [13/300], Step [9500/27733], Loss: 2.9540\n",
      "Epoch [13/300], Step [9600/27733], Loss: 3.1333\n",
      "Epoch [13/300], Step [9700/27733], Loss: 3.1185\n",
      "Epoch [13/300], Step [9800/27733], Loss: 3.4315\n",
      "Epoch [13/300], Step [9900/27733], Loss: 3.4553\n",
      "Epoch [13/300], Step [10000/27733], Loss: 3.1343\n",
      "Epoch [13/300], Step [10100/27733], Loss: 2.9350\n",
      "Epoch [13/300], Step [10200/27733], Loss: 3.2276\n",
      "Epoch [13/300], Step [10300/27733], Loss: 2.6581\n",
      "Epoch [13/300], Step [10400/27733], Loss: 2.9233\n",
      "Epoch [13/300], Step [10500/27733], Loss: 3.5667\n",
      "Epoch [13/300], Step [10600/27733], Loss: 2.8399\n",
      "Epoch [13/300], Step [10700/27733], Loss: 4.6236\n",
      "Epoch [13/300], Step [10800/27733], Loss: 3.0990\n",
      "Epoch [13/300], Step [10900/27733], Loss: 2.9633\n",
      "Epoch [13/300], Step [11000/27733], Loss: 4.0852\n",
      "Epoch [13/300], Step [11100/27733], Loss: 3.4640\n",
      "Epoch [13/300], Step [11200/27733], Loss: 3.6396\n",
      "Epoch [13/300], Step [11300/27733], Loss: 3.9037\n",
      "Epoch [13/300], Step [11400/27733], Loss: 3.9103\n",
      "Epoch [13/300], Step [11500/27733], Loss: 3.8802\n",
      "Epoch [13/300], Step [11600/27733], Loss: 3.1829\n",
      "Epoch [13/300], Step [11700/27733], Loss: 3.0857\n",
      "Epoch [13/300], Step [11800/27733], Loss: 3.2600\n",
      "Epoch [13/300], Step [11900/27733], Loss: 2.0907\n",
      "Epoch [13/300], Step [12000/27733], Loss: 2.6168\n",
      "Epoch [13/300], Step [12100/27733], Loss: 3.2301\n",
      "Epoch [13/300], Step [12200/27733], Loss: 3.3095\n",
      "Epoch [13/300], Step [12300/27733], Loss: 3.6987\n",
      "Epoch [13/300], Step [12400/27733], Loss: 3.2499\n",
      "Epoch [13/300], Step [12500/27733], Loss: 2.9139\n",
      "Epoch [13/300], Step [12600/27733], Loss: 2.8005\n",
      "Epoch [13/300], Step [12700/27733], Loss: 3.6092\n",
      "Epoch [13/300], Step [12800/27733], Loss: 4.0085\n",
      "Epoch [13/300], Step [12900/27733], Loss: 3.1626\n",
      "Epoch [13/300], Step [13000/27733], Loss: 2.6993\n",
      "Epoch [13/300], Step [13100/27733], Loss: 3.3126\n",
      "Epoch [13/300], Step [13200/27733], Loss: 3.4748\n",
      "Epoch [13/300], Step [13300/27733], Loss: 2.9339\n",
      "Epoch [13/300], Step [13400/27733], Loss: 3.0923\n",
      "Epoch [13/300], Step [13500/27733], Loss: 3.3046\n",
      "Epoch [13/300], Step [13600/27733], Loss: 4.5362\n",
      "Epoch [13/300], Step [13700/27733], Loss: 2.8829\n",
      "Epoch [13/300], Step [13800/27733], Loss: 3.4297\n",
      "Epoch [13/300], Step [13900/27733], Loss: 2.5944\n",
      "Epoch [13/300], Step [14000/27733], Loss: 4.1103\n",
      "Epoch [13/300], Step [14100/27733], Loss: 2.9068\n",
      "Epoch [13/300], Step [14200/27733], Loss: 3.7051\n",
      "Epoch [13/300], Step [14300/27733], Loss: 3.9446\n",
      "Epoch [13/300], Step [14400/27733], Loss: 3.7979\n",
      "Epoch [13/300], Step [14500/27733], Loss: 3.0697\n",
      "Epoch [13/300], Step [14600/27733], Loss: 4.4716\n",
      "Epoch [13/300], Step [14700/27733], Loss: 3.8680\n",
      "Epoch [13/300], Step [14800/27733], Loss: 3.8259\n",
      "Epoch [13/300], Step [14900/27733], Loss: 3.8328\n",
      "Epoch [13/300], Step [15000/27733], Loss: 3.3879\n",
      "Epoch [13/300], Step [15100/27733], Loss: 2.7398\n",
      "Epoch [13/300], Step [15200/27733], Loss: 4.4360\n",
      "Epoch [13/300], Step [15300/27733], Loss: 3.3901\n",
      "Epoch [13/300], Step [15400/27733], Loss: 3.8886\n",
      "Epoch [13/300], Step [15500/27733], Loss: 3.5699\n",
      "Epoch [13/300], Step [15600/27733], Loss: 3.4520\n",
      "Epoch [13/300], Step [15700/27733], Loss: 4.4527\n",
      "Epoch [13/300], Step [15800/27733], Loss: 3.3137\n",
      "Epoch [13/300], Step [15900/27733], Loss: 3.5521\n",
      "Epoch [13/300], Step [16000/27733], Loss: 3.1842\n",
      "Epoch [13/300], Step [16100/27733], Loss: 3.2801\n",
      "Epoch [13/300], Step [16200/27733], Loss: 2.6380\n",
      "Epoch [13/300], Step [16300/27733], Loss: 3.9283\n",
      "Epoch [13/300], Step [16400/27733], Loss: 3.0177\n",
      "Epoch [13/300], Step [16500/27733], Loss: 4.0607\n",
      "Epoch [13/300], Step [16600/27733], Loss: 3.2253\n",
      "Epoch [13/300], Step [16700/27733], Loss: 4.0701\n",
      "Epoch [13/300], Step [16800/27733], Loss: 3.1414\n",
      "Epoch [13/300], Step [16900/27733], Loss: 3.2313\n",
      "Epoch [13/300], Step [17000/27733], Loss: 3.7420\n",
      "Epoch [13/300], Step [17100/27733], Loss: 2.6838\n",
      "Epoch [13/300], Step [17200/27733], Loss: 2.7196\n",
      "Epoch [13/300], Step [17300/27733], Loss: 3.9471\n",
      "Epoch [13/300], Step [17400/27733], Loss: 4.1556\n",
      "Epoch [13/300], Step [17500/27733], Loss: 3.6299\n",
      "Epoch [13/300], Step [17600/27733], Loss: 3.7100\n",
      "Epoch [13/300], Step [17700/27733], Loss: 3.8819\n",
      "Epoch [13/300], Step [17800/27733], Loss: 3.4486\n",
      "Epoch [13/300], Step [17900/27733], Loss: 3.0467\n",
      "Epoch [13/300], Step [18000/27733], Loss: 3.9463\n",
      "Epoch [13/300], Step [18100/27733], Loss: 3.6796\n",
      "Epoch [13/300], Step [18200/27733], Loss: 3.7317\n",
      "Epoch [13/300], Step [18300/27733], Loss: 2.0430\n",
      "Epoch [13/300], Step [18400/27733], Loss: 3.3520\n",
      "Epoch [13/300], Step [18500/27733], Loss: 3.4288\n",
      "Epoch [13/300], Step [18600/27733], Loss: 3.3993\n",
      "Epoch [13/300], Step [18700/27733], Loss: 3.7677\n",
      "Epoch [13/300], Step [18800/27733], Loss: 3.6314\n",
      "Epoch [13/300], Step [18900/27733], Loss: 2.9961\n",
      "Epoch [13/300], Step [19000/27733], Loss: 2.8110\n",
      "Epoch [13/300], Step [19100/27733], Loss: 3.7466\n",
      "Epoch [13/300], Step [19200/27733], Loss: 3.4745\n",
      "Epoch [13/300], Step [19300/27733], Loss: 2.7176\n",
      "Epoch [13/300], Step [19400/27733], Loss: 4.0605\n",
      "Epoch [13/300], Step [19500/27733], Loss: 3.8984\n",
      "Epoch [13/300], Step [19600/27733], Loss: 3.7460\n",
      "Epoch [13/300], Step [19700/27733], Loss: 4.1262\n",
      "Epoch [13/300], Step [19800/27733], Loss: 3.5892\n",
      "Epoch [13/300], Step [19900/27733], Loss: 3.0277\n",
      "Epoch [13/300], Step [20000/27733], Loss: 2.3730\n",
      "Epoch [13/300], Step [20100/27733], Loss: 3.7106\n",
      "Epoch [13/300], Step [20200/27733], Loss: 3.8487\n",
      "Epoch [13/300], Step [20300/27733], Loss: 3.4707\n",
      "Epoch [13/300], Step [20400/27733], Loss: 3.2511\n",
      "Epoch [13/300], Step [20500/27733], Loss: 3.5939\n",
      "Epoch [13/300], Step [20600/27733], Loss: 3.8297\n",
      "Epoch [13/300], Step [20700/27733], Loss: 2.9683\n",
      "Epoch [13/300], Step [20800/27733], Loss: 3.5729\n",
      "Epoch [13/300], Step [20900/27733], Loss: 2.9283\n",
      "Epoch [13/300], Step [21000/27733], Loss: 3.7256\n",
      "Epoch [13/300], Step [21100/27733], Loss: 3.3508\n",
      "Epoch [13/300], Step [21200/27733], Loss: 3.6833\n",
      "Epoch [13/300], Step [21300/27733], Loss: 4.6181\n",
      "Epoch [13/300], Step [21400/27733], Loss: 3.5079\n",
      "Epoch [13/300], Step [21500/27733], Loss: 3.0446\n",
      "Epoch [13/300], Step [21600/27733], Loss: 3.5014\n",
      "Epoch [13/300], Step [21700/27733], Loss: 3.7435\n",
      "Epoch [13/300], Step [21800/27733], Loss: 3.5495\n",
      "Epoch [13/300], Step [21900/27733], Loss: 3.3366\n",
      "Epoch [13/300], Step [22000/27733], Loss: 3.0775\n",
      "Epoch [13/300], Step [22100/27733], Loss: 3.8368\n",
      "Epoch [13/300], Step [22200/27733], Loss: 4.1499\n",
      "Epoch [13/300], Step [22300/27733], Loss: 3.0110\n",
      "Epoch [13/300], Step [22400/27733], Loss: 4.0077\n",
      "Epoch [13/300], Step [22500/27733], Loss: 4.0341\n",
      "Epoch [13/300], Step [22600/27733], Loss: 3.4550\n",
      "Epoch [13/300], Step [22700/27733], Loss: 2.9383\n",
      "Epoch [13/300], Step [22800/27733], Loss: 3.1341\n",
      "Epoch [13/300], Step [22900/27733], Loss: 3.6062\n",
      "Epoch [13/300], Step [23000/27733], Loss: 3.2245\n",
      "Epoch [13/300], Step [23100/27733], Loss: 3.1656\n",
      "Epoch [13/300], Step [23200/27733], Loss: 3.4816\n",
      "Epoch [13/300], Step [23300/27733], Loss: 4.5233\n",
      "Epoch [13/300], Step [23400/27733], Loss: 3.3971\n",
      "Epoch [13/300], Step [23500/27733], Loss: 3.6374\n",
      "Epoch [13/300], Step [23600/27733], Loss: 3.1359\n",
      "Epoch [13/300], Step [23700/27733], Loss: 3.8290\n",
      "Epoch [13/300], Step [23800/27733], Loss: 3.4685\n",
      "Epoch [13/300], Step [23900/27733], Loss: 4.2274\n",
      "Epoch [13/300], Step [24000/27733], Loss: 4.1973\n",
      "Epoch [13/300], Step [24100/27733], Loss: 3.4761\n",
      "Epoch [13/300], Step [24200/27733], Loss: 4.1184\n",
      "Epoch [13/300], Step [24300/27733], Loss: 3.6669\n",
      "Epoch [13/300], Step [24400/27733], Loss: 2.7100\n",
      "Epoch [13/300], Step [24500/27733], Loss: 4.0181\n",
      "Epoch [13/300], Step [24600/27733], Loss: 3.3820\n",
      "Epoch [13/300], Step [24700/27733], Loss: 3.6020\n",
      "Epoch [13/300], Step [24800/27733], Loss: 3.6173\n",
      "Epoch [13/300], Step [24900/27733], Loss: 3.3586\n",
      "Epoch [13/300], Step [25000/27733], Loss: 4.1218\n",
      "Epoch [13/300], Step [25100/27733], Loss: 4.6334\n",
      "Epoch [13/300], Step [25200/27733], Loss: 3.3086\n",
      "Epoch [13/300], Step [25300/27733], Loss: 3.0911\n",
      "Epoch [13/300], Step [25400/27733], Loss: 4.2502\n",
      "Epoch [13/300], Step [25500/27733], Loss: 3.3006\n",
      "Epoch [13/300], Step [25600/27733], Loss: 3.4788\n",
      "Epoch [13/300], Step [25700/27733], Loss: 4.0134\n",
      "Epoch [13/300], Step [25800/27733], Loss: 4.1376\n",
      "Epoch [13/300], Step [25900/27733], Loss: 2.6506\n",
      "Epoch [13/300], Step [26000/27733], Loss: 4.7406\n",
      "Epoch [13/300], Step [26100/27733], Loss: 2.8560\n",
      "Epoch [13/300], Step [26200/27733], Loss: 3.4886\n",
      "Epoch [13/300], Step [26300/27733], Loss: 3.4325\n",
      "Epoch [13/300], Step [26400/27733], Loss: 4.1973\n",
      "Epoch [13/300], Step [26500/27733], Loss: 3.7286\n",
      "Epoch [13/300], Step [26600/27733], Loss: 3.7810\n",
      "Epoch [13/300], Step [26700/27733], Loss: 3.7281\n",
      "Epoch [13/300], Step [26800/27733], Loss: 3.8868\n",
      "Epoch [13/300], Step [26900/27733], Loss: 3.1990\n",
      "Epoch [13/300], Step [27000/27733], Loss: 4.2420\n",
      "Epoch [13/300], Step [27100/27733], Loss: 3.0100\n",
      "Epoch [13/300], Step [27200/27733], Loss: 2.6543\n",
      "Epoch [13/300], Step [27300/27733], Loss: 3.1201\n",
      "Epoch [13/300], Step [27400/27733], Loss: 3.3545\n",
      "Epoch [13/300], Step [27500/27733], Loss: 3.4899\n",
      "Epoch [13/300], Step [27600/27733], Loss: 4.3642\n",
      "Epoch [13/300], Step [27700/27733], Loss: 3.5362\n",
      "Epoch [14/300], Step [100/27733], Loss: 2.5649\n",
      "Epoch [14/300], Step [200/27733], Loss: 2.4526\n",
      "Epoch [14/300], Step [300/27733], Loss: 3.4346\n",
      "Epoch [14/300], Step [400/27733], Loss: 3.0639\n",
      "Epoch [14/300], Step [500/27733], Loss: 3.1215\n",
      "Epoch [14/300], Step [600/27733], Loss: 3.6466\n",
      "Epoch [14/300], Step [700/27733], Loss: 2.7252\n",
      "Epoch [14/300], Step [800/27733], Loss: 3.4926\n",
      "Epoch [14/300], Step [900/27733], Loss: 3.3008\n",
      "Epoch [14/300], Step [1000/27733], Loss: 3.3908\n",
      "Epoch [14/300], Step [1100/27733], Loss: 2.9481\n",
      "Epoch [14/300], Step [1200/27733], Loss: 3.0504\n",
      "Epoch [14/300], Step [1300/27733], Loss: 3.2375\n",
      "Epoch [14/300], Step [1400/27733], Loss: 4.5480\n",
      "Epoch [14/300], Step [1500/27733], Loss: 2.7675\n",
      "Epoch [14/300], Step [1600/27733], Loss: 3.2263\n",
      "Epoch [14/300], Step [1700/27733], Loss: 3.2415\n",
      "Epoch [14/300], Step [1800/27733], Loss: 3.0305\n",
      "Epoch [14/300], Step [1900/27733], Loss: 2.5463\n",
      "Epoch [14/300], Step [2000/27733], Loss: 2.8350\n",
      "Epoch [14/300], Step [2100/27733], Loss: 3.4408\n",
      "Epoch [14/300], Step [2200/27733], Loss: 2.7416\n",
      "Epoch [14/300], Step [2300/27733], Loss: 3.2297\n",
      "Epoch [14/300], Step [2400/27733], Loss: 3.1092\n",
      "Epoch [14/300], Step [2500/27733], Loss: 3.6209\n",
      "Epoch [14/300], Step [2600/27733], Loss: 3.1171\n",
      "Epoch [14/300], Step [2700/27733], Loss: 2.8507\n",
      "Epoch [14/300], Step [2800/27733], Loss: 3.5431\n",
      "Epoch [14/300], Step [2900/27733], Loss: 2.9593\n",
      "Epoch [14/300], Step [3000/27733], Loss: 3.4382\n",
      "Epoch [14/300], Step [3100/27733], Loss: 3.2656\n",
      "Epoch [14/300], Step [3200/27733], Loss: 2.7337\n",
      "Epoch [14/300], Step [3300/27733], Loss: 2.5079\n",
      "Epoch [14/300], Step [3400/27733], Loss: 3.4414\n",
      "Epoch [14/300], Step [3500/27733], Loss: 3.1844\n",
      "Epoch [14/300], Step [3600/27733], Loss: 2.6936\n",
      "Epoch [14/300], Step [3700/27733], Loss: 3.7352\n",
      "Epoch [14/300], Step [3800/27733], Loss: 2.6729\n",
      "Epoch [14/300], Step [3900/27733], Loss: 2.9099\n",
      "Epoch [14/300], Step [4000/27733], Loss: 3.4533\n",
      "Epoch [14/300], Step [4100/27733], Loss: 2.3798\n",
      "Epoch [14/300], Step [4200/27733], Loss: 2.3578\n",
      "Epoch [14/300], Step [4300/27733], Loss: 3.0519\n",
      "Epoch [14/300], Step [4400/27733], Loss: 3.0920\n",
      "Epoch [14/300], Step [4500/27733], Loss: 3.0476\n",
      "Epoch [14/300], Step [4600/27733], Loss: 2.9625\n",
      "Epoch [14/300], Step [4700/27733], Loss: 2.5830\n",
      "Epoch [14/300], Step [4800/27733], Loss: 3.7487\n",
      "Epoch [14/300], Step [4900/27733], Loss: 3.1429\n",
      "Epoch [14/300], Step [5000/27733], Loss: 3.3452\n",
      "Epoch [14/300], Step [5100/27733], Loss: 3.5769\n",
      "Epoch [14/300], Step [5200/27733], Loss: 3.0965\n",
      "Epoch [14/300], Step [5300/27733], Loss: 3.3630\n",
      "Epoch [14/300], Step [5400/27733], Loss: 2.8172\n",
      "Epoch [14/300], Step [5500/27733], Loss: 2.5669\n",
      "Epoch [14/300], Step [5600/27733], Loss: 3.0082\n",
      "Epoch [14/300], Step [5700/27733], Loss: 3.0498\n",
      "Epoch [14/300], Step [5800/27733], Loss: 3.3019\n",
      "Epoch [14/300], Step [5900/27733], Loss: 3.4315\n",
      "Epoch [14/300], Step [6000/27733], Loss: 2.6635\n",
      "Epoch [14/300], Step [6100/27733], Loss: 3.3417\n",
      "Epoch [14/300], Step [6200/27733], Loss: 3.1550\n",
      "Epoch [14/300], Step [6300/27733], Loss: 2.6370\n",
      "Epoch [14/300], Step [6400/27733], Loss: 3.8898\n",
      "Epoch [14/300], Step [6500/27733], Loss: 3.1945\n",
      "Epoch [14/300], Step [6600/27733], Loss: 2.4869\n",
      "Epoch [14/300], Step [6700/27733], Loss: 3.2937\n",
      "Epoch [14/300], Step [6800/27733], Loss: 2.7116\n",
      "Epoch [14/300], Step [6900/27733], Loss: 3.2245\n",
      "Epoch [14/300], Step [7000/27733], Loss: 2.8358\n",
      "Epoch [14/300], Step [7100/27733], Loss: 3.3523\n",
      "Epoch [14/300], Step [7200/27733], Loss: 3.1497\n",
      "Epoch [14/300], Step [7300/27733], Loss: 2.5921\n",
      "Epoch [14/300], Step [7400/27733], Loss: 2.8961\n",
      "Epoch [14/300], Step [7500/27733], Loss: 3.2598\n",
      "Epoch [14/300], Step [7600/27733], Loss: 3.8507\n",
      "Epoch [14/300], Step [7700/27733], Loss: 2.8711\n",
      "Epoch [14/300], Step [7800/27733], Loss: 3.1934\n",
      "Epoch [14/300], Step [7900/27733], Loss: 3.2585\n",
      "Epoch [14/300], Step [8000/27733], Loss: 2.9178\n",
      "Epoch [14/300], Step [8100/27733], Loss: 3.4784\n",
      "Epoch [14/300], Step [8200/27733], Loss: 3.2262\n",
      "Epoch [14/300], Step [8300/27733], Loss: 3.2013\n",
      "Epoch [14/300], Step [8400/27733], Loss: 3.2955\n",
      "Epoch [14/300], Step [8500/27733], Loss: 3.2138\n",
      "Epoch [14/300], Step [8600/27733], Loss: 2.5983\n",
      "Epoch [14/300], Step [8700/27733], Loss: 2.9563\n",
      "Epoch [14/300], Step [8800/27733], Loss: 3.6278\n",
      "Epoch [14/300], Step [8900/27733], Loss: 3.1827\n",
      "Epoch [14/300], Step [9000/27733], Loss: 3.1605\n",
      "Epoch [14/300], Step [9100/27733], Loss: 2.2239\n",
      "Epoch [14/300], Step [9200/27733], Loss: 3.7722\n",
      "Epoch [14/300], Step [9300/27733], Loss: 2.6317\n",
      "Epoch [14/300], Step [9400/27733], Loss: 2.9252\n",
      "Epoch [14/300], Step [9500/27733], Loss: 3.6443\n",
      "Epoch [14/300], Step [9600/27733], Loss: 3.4772\n",
      "Epoch [14/300], Step [9700/27733], Loss: 3.6128\n",
      "Epoch [14/300], Step [9800/27733], Loss: 3.2867\n",
      "Epoch [14/300], Step [9900/27733], Loss: 2.5488\n",
      "Epoch [14/300], Step [10000/27733], Loss: 2.8697\n",
      "Epoch [14/300], Step [10100/27733], Loss: 3.2303\n",
      "Epoch [14/300], Step [10200/27733], Loss: 2.8834\n",
      "Epoch [14/300], Step [10300/27733], Loss: 2.8615\n",
      "Epoch [14/300], Step [10400/27733], Loss: 3.8253\n",
      "Epoch [14/300], Step [10500/27733], Loss: 3.2527\n",
      "Epoch [14/300], Step [10600/27733], Loss: 3.6707\n",
      "Epoch [14/300], Step [10700/27733], Loss: 3.7148\n",
      "Epoch [14/300], Step [10800/27733], Loss: 2.6291\n",
      "Epoch [14/300], Step [10900/27733], Loss: 3.0167\n",
      "Epoch [14/300], Step [11000/27733], Loss: 3.2607\n",
      "Epoch [14/300], Step [11100/27733], Loss: 3.1697\n",
      "Epoch [14/300], Step [11200/27733], Loss: 3.4143\n",
      "Epoch [14/300], Step [11300/27733], Loss: 2.8944\n",
      "Epoch [14/300], Step [11400/27733], Loss: 3.7861\n",
      "Epoch [14/300], Step [11500/27733], Loss: 3.5282\n",
      "Epoch [14/300], Step [11600/27733], Loss: 3.1472\n",
      "Epoch [14/300], Step [11700/27733], Loss: 3.3961\n",
      "Epoch [14/300], Step [11800/27733], Loss: 3.2104\n",
      "Epoch [14/300], Step [11900/27733], Loss: 2.6755\n",
      "Epoch [14/300], Step [12000/27733], Loss: 2.7485\n",
      "Epoch [14/300], Step [12100/27733], Loss: 3.3711\n",
      "Epoch [14/300], Step [12200/27733], Loss: 4.0448\n",
      "Epoch [14/300], Step [12300/27733], Loss: 4.0346\n",
      "Epoch [14/300], Step [12400/27733], Loss: 3.1990\n",
      "Epoch [14/300], Step [12500/27733], Loss: 3.6015\n",
      "Epoch [14/300], Step [12600/27733], Loss: 2.9096\n",
      "Epoch [14/300], Step [12700/27733], Loss: 3.2770\n",
      "Epoch [14/300], Step [12800/27733], Loss: 3.2047\n",
      "Epoch [14/300], Step [12900/27733], Loss: 3.9807\n",
      "Epoch [14/300], Step [13000/27733], Loss: 4.0239\n",
      "Epoch [14/300], Step [13100/27733], Loss: 3.7373\n",
      "Epoch [14/300], Step [13200/27733], Loss: 3.9237\n",
      "Epoch [14/300], Step [13300/27733], Loss: 3.6485\n",
      "Epoch [14/300], Step [13400/27733], Loss: 4.0025\n",
      "Epoch [14/300], Step [13500/27733], Loss: 2.7615\n",
      "Epoch [14/300], Step [13600/27733], Loss: 2.4998\n",
      "Epoch [14/300], Step [13700/27733], Loss: 2.7718\n",
      "Epoch [14/300], Step [13800/27733], Loss: 2.4603\n",
      "Epoch [14/300], Step [13900/27733], Loss: 3.6163\n",
      "Epoch [14/300], Step [14000/27733], Loss: 3.8163\n",
      "Epoch [14/300], Step [14100/27733], Loss: 3.6186\n",
      "Epoch [14/300], Step [14200/27733], Loss: 3.3646\n",
      "Epoch [14/300], Step [14300/27733], Loss: 4.3091\n",
      "Epoch [14/300], Step [14400/27733], Loss: 2.9846\n",
      "Epoch [14/300], Step [14500/27733], Loss: 4.0218\n",
      "Epoch [14/300], Step [14600/27733], Loss: 3.7092\n",
      "Epoch [14/300], Step [14700/27733], Loss: 3.9559\n",
      "Epoch [14/300], Step [14800/27733], Loss: 4.1671\n",
      "Epoch [14/300], Step [14900/27733], Loss: 3.1688\n",
      "Epoch [14/300], Step [15000/27733], Loss: 2.8275\n",
      "Epoch [14/300], Step [15100/27733], Loss: 3.1720\n",
      "Epoch [14/300], Step [15200/27733], Loss: 2.5752\n",
      "Epoch [14/300], Step [15300/27733], Loss: 3.4479\n",
      "Epoch [14/300], Step [15400/27733], Loss: 3.0182\n",
      "Epoch [14/300], Step [15500/27733], Loss: 3.3497\n",
      "Epoch [14/300], Step [15600/27733], Loss: 3.1178\n",
      "Epoch [14/300], Step [15700/27733], Loss: 3.7570\n",
      "Epoch [14/300], Step [15800/27733], Loss: 4.0634\n",
      "Epoch [14/300], Step [15900/27733], Loss: 2.6186\n",
      "Epoch [14/300], Step [16000/27733], Loss: 3.4617\n",
      "Epoch [14/300], Step [16100/27733], Loss: 3.7127\n",
      "Epoch [14/300], Step [16200/27733], Loss: 3.3286\n",
      "Epoch [14/300], Step [16300/27733], Loss: 3.7703\n",
      "Epoch [14/300], Step [16400/27733], Loss: 3.5176\n",
      "Epoch [14/300], Step [16500/27733], Loss: 3.2957\n",
      "Epoch [14/300], Step [16600/27733], Loss: 3.4976\n",
      "Epoch [14/300], Step [16700/27733], Loss: 3.3316\n",
      "Epoch [14/300], Step [16800/27733], Loss: 3.4093\n",
      "Epoch [14/300], Step [16900/27733], Loss: 2.9129\n",
      "Epoch [14/300], Step [17000/27733], Loss: 4.2190\n",
      "Epoch [14/300], Step [17100/27733], Loss: 3.8969\n",
      "Epoch [14/300], Step [17200/27733], Loss: 3.3714\n",
      "Epoch [14/300], Step [17300/27733], Loss: 3.1237\n",
      "Epoch [14/300], Step [17400/27733], Loss: 2.8861\n",
      "Epoch [14/300], Step [17500/27733], Loss: 3.5849\n",
      "Epoch [14/300], Step [17600/27733], Loss: 3.2397\n",
      "Epoch [14/300], Step [17700/27733], Loss: 4.7112\n",
      "Epoch [14/300], Step [17800/27733], Loss: 3.0138\n",
      "Epoch [14/300], Step [17900/27733], Loss: 3.2841\n",
      "Epoch [14/300], Step [18000/27733], Loss: 2.9501\n",
      "Epoch [14/300], Step [18100/27733], Loss: 3.5375\n",
      "Epoch [14/300], Step [18200/27733], Loss: 3.7922\n",
      "Epoch [14/300], Step [18300/27733], Loss: 3.8247\n",
      "Epoch [14/300], Step [18400/27733], Loss: 2.7038\n",
      "Epoch [14/300], Step [18500/27733], Loss: 2.9067\n",
      "Epoch [14/300], Step [18600/27733], Loss: 3.3261\n",
      "Epoch [14/300], Step [18700/27733], Loss: 3.5946\n",
      "Epoch [14/300], Step [18800/27733], Loss: 3.6057\n",
      "Epoch [14/300], Step [18900/27733], Loss: 2.7103\n",
      "Epoch [14/300], Step [19000/27733], Loss: 2.5803\n",
      "Epoch [14/300], Step [19100/27733], Loss: 3.0385\n",
      "Epoch [14/300], Step [19200/27733], Loss: 2.2212\n",
      "Epoch [14/300], Step [19300/27733], Loss: 2.8279\n",
      "Epoch [14/300], Step [19400/27733], Loss: 3.7854\n",
      "Epoch [14/300], Step [19500/27733], Loss: 3.9931\n",
      "Epoch [14/300], Step [19600/27733], Loss: 3.9920\n",
      "Epoch [14/300], Step [19700/27733], Loss: 4.4418\n",
      "Epoch [14/300], Step [19800/27733], Loss: 3.6148\n",
      "Epoch [14/300], Step [19900/27733], Loss: 4.1669\n",
      "Epoch [14/300], Step [20000/27733], Loss: 4.1840\n",
      "Epoch [14/300], Step [20100/27733], Loss: 4.0740\n",
      "Epoch [14/300], Step [20200/27733], Loss: 3.0227\n",
      "Epoch [14/300], Step [20300/27733], Loss: 3.3148\n",
      "Epoch [14/300], Step [20400/27733], Loss: 3.8822\n",
      "Epoch [14/300], Step [20500/27733], Loss: 2.5627\n",
      "Epoch [14/300], Step [20600/27733], Loss: 3.4854\n",
      "Epoch [14/300], Step [20700/27733], Loss: 3.1532\n",
      "Epoch [14/300], Step [20800/27733], Loss: 3.9493\n",
      "Epoch [14/300], Step [20900/27733], Loss: 3.4874\n",
      "Epoch [14/300], Step [21000/27733], Loss: 3.3983\n",
      "Epoch [14/300], Step [21100/27733], Loss: 3.6417\n",
      "Epoch [14/300], Step [21200/27733], Loss: 2.8380\n",
      "Epoch [14/300], Step [21300/27733], Loss: 3.1050\n",
      "Epoch [14/300], Step [21400/27733], Loss: 3.9428\n",
      "Epoch [14/300], Step [21500/27733], Loss: 3.1724\n",
      "Epoch [14/300], Step [21600/27733], Loss: 3.4932\n",
      "Epoch [14/300], Step [21700/27733], Loss: 3.9437\n",
      "Epoch [14/300], Step [21800/27733], Loss: 3.0120\n",
      "Epoch [14/300], Step [21900/27733], Loss: 3.1574\n",
      "Epoch [14/300], Step [22000/27733], Loss: 4.1328\n",
      "Epoch [14/300], Step [22100/27733], Loss: 2.7090\n",
      "Epoch [14/300], Step [22200/27733], Loss: 3.4861\n",
      "Epoch [14/300], Step [22300/27733], Loss: 3.7115\n",
      "Epoch [14/300], Step [22400/27733], Loss: 2.8675\n",
      "Epoch [14/300], Step [22500/27733], Loss: 4.0621\n",
      "Epoch [14/300], Step [22600/27733], Loss: 3.5163\n",
      "Epoch [14/300], Step [22700/27733], Loss: 3.3482\n",
      "Epoch [14/300], Step [22800/27733], Loss: 3.7059\n",
      "Epoch [14/300], Step [22900/27733], Loss: 3.9468\n",
      "Epoch [14/300], Step [23000/27733], Loss: 3.7066\n",
      "Epoch [14/300], Step [23100/27733], Loss: 3.1906\n",
      "Epoch [14/300], Step [23200/27733], Loss: 3.6608\n",
      "Epoch [14/300], Step [23300/27733], Loss: 3.7028\n",
      "Epoch [14/300], Step [23400/27733], Loss: 3.7818\n",
      "Epoch [14/300], Step [23500/27733], Loss: 2.8375\n",
      "Epoch [14/300], Step [23600/27733], Loss: 3.8897\n",
      "Epoch [14/300], Step [23700/27733], Loss: 3.2196\n",
      "Epoch [14/300], Step [23800/27733], Loss: 2.9207\n",
      "Epoch [14/300], Step [23900/27733], Loss: 3.4561\n",
      "Epoch [14/300], Step [24000/27733], Loss: 3.4042\n",
      "Epoch [14/300], Step [24100/27733], Loss: 3.7907\n",
      "Epoch [14/300], Step [24200/27733], Loss: 4.0785\n",
      "Epoch [14/300], Step [24300/27733], Loss: 3.3409\n",
      "Epoch [14/300], Step [24400/27733], Loss: 2.8130\n",
      "Epoch [14/300], Step [24500/27733], Loss: 3.7977\n",
      "Epoch [14/300], Step [24600/27733], Loss: 3.3460\n",
      "Epoch [14/300], Step [24700/27733], Loss: 3.8422\n",
      "Epoch [14/300], Step [24800/27733], Loss: 2.5813\n",
      "Epoch [14/300], Step [24900/27733], Loss: 3.5938\n",
      "Epoch [14/300], Step [25000/27733], Loss: 3.5628\n",
      "Epoch [14/300], Step [25100/27733], Loss: 3.3877\n",
      "Epoch [14/300], Step [25200/27733], Loss: 3.1086\n",
      "Epoch [14/300], Step [25300/27733], Loss: 2.7567\n",
      "Epoch [14/300], Step [25400/27733], Loss: 3.7455\n",
      "Epoch [14/300], Step [25500/27733], Loss: 3.5207\n",
      "Epoch [14/300], Step [25600/27733], Loss: 3.3341\n",
      "Epoch [14/300], Step [25700/27733], Loss: 3.9560\n",
      "Epoch [14/300], Step [25800/27733], Loss: 4.2607\n",
      "Epoch [14/300], Step [25900/27733], Loss: 3.4687\n",
      "Epoch [14/300], Step [26000/27733], Loss: 4.0241\n",
      "Epoch [14/300], Step [26100/27733], Loss: 3.4782\n",
      "Epoch [14/300], Step [26200/27733], Loss: 2.7533\n",
      "Epoch [14/300], Step [26300/27733], Loss: 3.8925\n",
      "Epoch [14/300], Step [26400/27733], Loss: 3.2836\n",
      "Epoch [14/300], Step [26500/27733], Loss: 3.7959\n",
      "Epoch [14/300], Step [26600/27733], Loss: 3.7942\n",
      "Epoch [14/300], Step [26700/27733], Loss: 4.3655\n",
      "Epoch [14/300], Step [26800/27733], Loss: 2.7272\n",
      "Epoch [14/300], Step [26900/27733], Loss: 3.9095\n",
      "Epoch [14/300], Step [27000/27733], Loss: 4.5003\n",
      "Epoch [14/300], Step [27100/27733], Loss: 3.2430\n",
      "Epoch [14/300], Step [27200/27733], Loss: 3.1066\n",
      "Epoch [14/300], Step [27300/27733], Loss: 3.9518\n",
      "Epoch [14/300], Step [27400/27733], Loss: 3.4012\n",
      "Epoch [14/300], Step [27500/27733], Loss: 3.2980\n",
      "Epoch [14/300], Step [27600/27733], Loss: 2.7481\n",
      "Epoch [14/300], Step [27700/27733], Loss: 3.2636\n",
      "Epoch [15/300], Step [100/27733], Loss: 2.6046\n",
      "Epoch [15/300], Step [200/27733], Loss: 3.2006\n",
      "Epoch [15/300], Step [300/27733], Loss: 3.2407\n",
      "Epoch [15/300], Step [400/27733], Loss: 2.7869\n",
      "Epoch [15/300], Step [500/27733], Loss: 2.7033\n",
      "Epoch [15/300], Step [600/27733], Loss: 2.6105\n",
      "Epoch [15/300], Step [700/27733], Loss: 2.8655\n",
      "Epoch [15/300], Step [800/27733], Loss: 2.7600\n",
      "Epoch [15/300], Step [900/27733], Loss: 3.8690\n",
      "Epoch [15/300], Step [1000/27733], Loss: 2.7166\n",
      "Epoch [15/300], Step [1100/27733], Loss: 3.3460\n",
      "Epoch [15/300], Step [1200/27733], Loss: 2.4539\n",
      "Epoch [15/300], Step [1300/27733], Loss: 2.8074\n",
      "Epoch [15/300], Step [1400/27733], Loss: 2.8431\n",
      "Epoch [15/300], Step [1500/27733], Loss: 3.2111\n",
      "Epoch [15/300], Step [1600/27733], Loss: 2.6721\n",
      "Epoch [15/300], Step [1700/27733], Loss: 2.9865\n",
      "Epoch [15/300], Step [1800/27733], Loss: 3.4748\n",
      "Epoch [15/300], Step [1900/27733], Loss: 2.7727\n",
      "Epoch [15/300], Step [2000/27733], Loss: 2.7902\n",
      "Epoch [15/300], Step [2100/27733], Loss: 3.0174\n",
      "Epoch [15/300], Step [2200/27733], Loss: 1.9138\n",
      "Epoch [15/300], Step [2300/27733], Loss: 2.5068\n",
      "Epoch [15/300], Step [2400/27733], Loss: 3.2057\n",
      "Epoch [15/300], Step [2500/27733], Loss: 3.0748\n",
      "Epoch [15/300], Step [2600/27733], Loss: 2.8928\n",
      "Epoch [15/300], Step [2700/27733], Loss: 2.5125\n",
      "Epoch [15/300], Step [2800/27733], Loss: 2.7643\n",
      "Epoch [15/300], Step [2900/27733], Loss: 2.5377\n",
      "Epoch [15/300], Step [3000/27733], Loss: 3.5846\n",
      "Epoch [15/300], Step [3100/27733], Loss: 2.9546\n",
      "Epoch [15/300], Step [3200/27733], Loss: 3.4050\n",
      "Epoch [15/300], Step [3300/27733], Loss: 3.6154\n",
      "Epoch [15/300], Step [3400/27733], Loss: 2.7757\n",
      "Epoch [15/300], Step [3500/27733], Loss: 3.0680\n",
      "Epoch [15/300], Step [3600/27733], Loss: 2.4450\n",
      "Epoch [15/300], Step [3700/27733], Loss: 3.5107\n",
      "Epoch [15/300], Step [3800/27733], Loss: 2.6125\n",
      "Epoch [15/300], Step [3900/27733], Loss: 3.3244\n",
      "Epoch [15/300], Step [4000/27733], Loss: 3.0716\n",
      "Epoch [15/300], Step [4100/27733], Loss: 2.9428\n",
      "Epoch [15/300], Step [4200/27733], Loss: 3.0867\n",
      "Epoch [15/300], Step [4300/27733], Loss: 3.7258\n",
      "Epoch [15/300], Step [4400/27733], Loss: 2.6746\n",
      "Epoch [15/300], Step [4500/27733], Loss: 2.9521\n",
      "Epoch [15/300], Step [4600/27733], Loss: 2.8308\n",
      "Epoch [15/300], Step [4700/27733], Loss: 3.6888\n",
      "Epoch [15/300], Step [4800/27733], Loss: 3.2502\n",
      "Epoch [15/300], Step [4900/27733], Loss: 3.1316\n",
      "Epoch [15/300], Step [5000/27733], Loss: 3.6882\n",
      "Epoch [15/300], Step [5100/27733], Loss: 2.4492\n",
      "Epoch [15/300], Step [5200/27733], Loss: 3.8043\n",
      "Epoch [15/300], Step [5300/27733], Loss: 3.0333\n",
      "Epoch [15/300], Step [5400/27733], Loss: 3.1342\n",
      "Epoch [15/300], Step [5500/27733], Loss: 3.0156\n",
      "Epoch [15/300], Step [5600/27733], Loss: 2.6449\n",
      "Epoch [15/300], Step [5700/27733], Loss: 3.4763\n",
      "Epoch [15/300], Step [5800/27733], Loss: 2.3494\n",
      "Epoch [15/300], Step [5900/27733], Loss: 3.4209\n",
      "Epoch [15/300], Step [6000/27733], Loss: 3.7279\n",
      "Epoch [15/300], Step [6100/27733], Loss: 4.0376\n",
      "Epoch [15/300], Step [6200/27733], Loss: 3.4132\n",
      "Epoch [15/300], Step [6300/27733], Loss: 2.8785\n",
      "Epoch [15/300], Step [6400/27733], Loss: 2.9030\n",
      "Epoch [15/300], Step [6500/27733], Loss: 3.4534\n",
      "Epoch [15/300], Step [6600/27733], Loss: 2.5323\n",
      "Epoch [15/300], Step [6700/27733], Loss: 3.6023\n",
      "Epoch [15/300], Step [6800/27733], Loss: 3.5964\n",
      "Epoch [15/300], Step [6900/27733], Loss: 2.4470\n",
      "Epoch [15/300], Step [7000/27733], Loss: 3.1131\n",
      "Epoch [15/300], Step [7100/27733], Loss: 3.2187\n",
      "Epoch [15/300], Step [7200/27733], Loss: 3.0753\n",
      "Epoch [15/300], Step [7300/27733], Loss: 2.9581\n",
      "Epoch [15/300], Step [7400/27733], Loss: 3.5576\n",
      "Epoch [15/300], Step [7500/27733], Loss: 3.8617\n",
      "Epoch [15/300], Step [7600/27733], Loss: 3.0837\n",
      "Epoch [15/300], Step [7700/27733], Loss: 2.9077\n",
      "Epoch [15/300], Step [7800/27733], Loss: 2.8479\n",
      "Epoch [15/300], Step [7900/27733], Loss: 3.7063\n",
      "Epoch [15/300], Step [8000/27733], Loss: 2.6485\n",
      "Epoch [15/300], Step [8100/27733], Loss: 2.4224\n",
      "Epoch [15/300], Step [8200/27733], Loss: 3.7224\n",
      "Epoch [15/300], Step [8300/27733], Loss: 3.2347\n",
      "Epoch [15/300], Step [8400/27733], Loss: 2.8724\n",
      "Epoch [15/300], Step [8500/27733], Loss: 3.2592\n",
      "Epoch [15/300], Step [8600/27733], Loss: 2.8400\n",
      "Epoch [15/300], Step [8700/27733], Loss: 2.8807\n",
      "Epoch [15/300], Step [8800/27733], Loss: 1.9857\n",
      "Epoch [15/300], Step [8900/27733], Loss: 3.6589\n",
      "Epoch [15/300], Step [9000/27733], Loss: 3.1181\n",
      "Epoch [15/300], Step [9100/27733], Loss: 3.1114\n",
      "Epoch [15/300], Step [9200/27733], Loss: 3.4810\n",
      "Epoch [15/300], Step [9300/27733], Loss: 3.0347\n",
      "Epoch [15/300], Step [9400/27733], Loss: 4.4122\n",
      "Epoch [15/300], Step [9500/27733], Loss: 4.0484\n",
      "Epoch [15/300], Step [9600/27733], Loss: 2.8265\n",
      "Epoch [15/300], Step [9700/27733], Loss: 3.8109\n",
      "Epoch [15/300], Step [9800/27733], Loss: 3.1765\n",
      "Epoch [15/300], Step [9900/27733], Loss: 3.0356\n",
      "Epoch [15/300], Step [10000/27733], Loss: 2.8729\n",
      "Epoch [15/300], Step [10100/27733], Loss: 3.6723\n",
      "Epoch [15/300], Step [10200/27733], Loss: 3.6832\n",
      "Epoch [15/300], Step [10300/27733], Loss: 4.1301\n",
      "Epoch [15/300], Step [10400/27733], Loss: 2.8046\n",
      "Epoch [15/300], Step [10500/27733], Loss: 3.0604\n",
      "Epoch [15/300], Step [10600/27733], Loss: 3.3804\n",
      "Epoch [15/300], Step [10700/27733], Loss: 3.1831\n",
      "Epoch [15/300], Step [10800/27733], Loss: 3.1595\n",
      "Epoch [15/300], Step [10900/27733], Loss: 3.4564\n",
      "Epoch [15/300], Step [11000/27733], Loss: 2.7370\n",
      "Epoch [15/300], Step [11100/27733], Loss: 3.5352\n",
      "Epoch [15/300], Step [11200/27733], Loss: 3.5520\n",
      "Epoch [15/300], Step [11300/27733], Loss: 2.8714\n",
      "Epoch [15/300], Step [11400/27733], Loss: 3.2965\n",
      "Epoch [15/300], Step [11500/27733], Loss: 2.7752\n",
      "Epoch [15/300], Step [11600/27733], Loss: 3.1257\n",
      "Epoch [15/300], Step [11700/27733], Loss: 3.8924\n",
      "Epoch [15/300], Step [11800/27733], Loss: 4.0124\n",
      "Epoch [15/300], Step [11900/27733], Loss: 2.6154\n",
      "Epoch [15/300], Step [12000/27733], Loss: 2.2986\n",
      "Epoch [15/300], Step [12100/27733], Loss: 3.3387\n",
      "Epoch [15/300], Step [12200/27733], Loss: 2.7505\n",
      "Epoch [15/300], Step [12300/27733], Loss: 3.4518\n",
      "Epoch [15/300], Step [12400/27733], Loss: 2.6360\n",
      "Epoch [15/300], Step [12500/27733], Loss: 3.6920\n",
      "Epoch [15/300], Step [12600/27733], Loss: 2.8415\n",
      "Epoch [15/300], Step [12700/27733], Loss: 2.8435\n",
      "Epoch [15/300], Step [12800/27733], Loss: 3.7254\n",
      "Epoch [15/300], Step [12900/27733], Loss: 2.7682\n",
      "Epoch [15/300], Step [13000/27733], Loss: 2.4374\n",
      "Epoch [15/300], Step [13100/27733], Loss: 2.5768\n",
      "Epoch [15/300], Step [13200/27733], Loss: 2.7380\n",
      "Epoch [15/300], Step [13300/27733], Loss: 2.3285\n",
      "Epoch [15/300], Step [13400/27733], Loss: 3.4079\n",
      "Epoch [15/300], Step [13500/27733], Loss: 3.8200\n",
      "Epoch [15/300], Step [13600/27733], Loss: 3.8891\n",
      "Epoch [15/300], Step [13700/27733], Loss: 2.8530\n",
      "Epoch [15/300], Step [13800/27733], Loss: 2.7064\n",
      "Epoch [15/300], Step [13900/27733], Loss: 3.4798\n",
      "Epoch [15/300], Step [14000/27733], Loss: 3.0683\n",
      "Epoch [15/300], Step [14100/27733], Loss: 3.6412\n",
      "Epoch [15/300], Step [14200/27733], Loss: 2.6614\n",
      "Epoch [15/300], Step [14300/27733], Loss: 3.1671\n",
      "Epoch [15/300], Step [14400/27733], Loss: 3.4293\n",
      "Epoch [15/300], Step [14500/27733], Loss: 2.9803\n",
      "Epoch [15/300], Step [14600/27733], Loss: 3.0009\n",
      "Epoch [15/300], Step [14700/27733], Loss: 3.0382\n",
      "Epoch [15/300], Step [14800/27733], Loss: 4.0552\n",
      "Epoch [15/300], Step [14900/27733], Loss: 3.1396\n",
      "Epoch [15/300], Step [15000/27733], Loss: 3.3804\n",
      "Epoch [15/300], Step [15100/27733], Loss: 3.2148\n",
      "Epoch [15/300], Step [15200/27733], Loss: 3.1321\n",
      "Epoch [15/300], Step [15300/27733], Loss: 3.5489\n",
      "Epoch [15/300], Step [15400/27733], Loss: 3.7372\n",
      "Epoch [15/300], Step [15500/27733], Loss: 3.0045\n",
      "Epoch [15/300], Step [15600/27733], Loss: 3.6975\n",
      "Epoch [15/300], Step [15700/27733], Loss: 3.1108\n",
      "Epoch [15/300], Step [15800/27733], Loss: 3.4205\n",
      "Epoch [15/300], Step [15900/27733], Loss: 2.3235\n",
      "Epoch [15/300], Step [16000/27733], Loss: 2.5059\n",
      "Epoch [15/300], Step [16100/27733], Loss: 3.4222\n",
      "Epoch [15/300], Step [16200/27733], Loss: 3.8284\n",
      "Epoch [15/300], Step [16300/27733], Loss: 3.6489\n",
      "Epoch [15/300], Step [16400/27733], Loss: 2.8954\n",
      "Epoch [15/300], Step [16500/27733], Loss: 3.2433\n",
      "Epoch [15/300], Step [16600/27733], Loss: 3.4039\n",
      "Epoch [15/300], Step [16700/27733], Loss: 3.0953\n",
      "Epoch [15/300], Step [16800/27733], Loss: 3.5652\n",
      "Epoch [15/300], Step [16900/27733], Loss: 2.9499\n",
      "Epoch [15/300], Step [17000/27733], Loss: 3.2341\n",
      "Epoch [15/300], Step [17100/27733], Loss: 3.5284\n",
      "Epoch [15/300], Step [17200/27733], Loss: 3.8121\n",
      "Epoch [15/300], Step [17300/27733], Loss: 3.9705\n",
      "Epoch [15/300], Step [17400/27733], Loss: 3.8162\n",
      "Epoch [15/300], Step [17500/27733], Loss: 3.3357\n",
      "Epoch [15/300], Step [17600/27733], Loss: 3.2136\n",
      "Epoch [15/300], Step [17700/27733], Loss: 2.9270\n",
      "Epoch [15/300], Step [17800/27733], Loss: 2.8990\n",
      "Epoch [15/300], Step [17900/27733], Loss: 2.9306\n",
      "Epoch [15/300], Step [18000/27733], Loss: 2.7290\n",
      "Epoch [15/300], Step [18100/27733], Loss: 2.9908\n",
      "Epoch [15/300], Step [18200/27733], Loss: 2.9534\n",
      "Epoch [15/300], Step [18300/27733], Loss: 3.4722\n",
      "Epoch [15/300], Step [18400/27733], Loss: 3.9217\n",
      "Epoch [15/300], Step [18500/27733], Loss: 3.9847\n",
      "Epoch [15/300], Step [18600/27733], Loss: 3.3151\n",
      "Epoch [15/300], Step [18700/27733], Loss: 3.5743\n",
      "Epoch [15/300], Step [18800/27733], Loss: 2.2870\n",
      "Epoch [15/300], Step [18900/27733], Loss: 3.1482\n",
      "Epoch [15/300], Step [19000/27733], Loss: 3.7714\n",
      "Epoch [15/300], Step [19100/27733], Loss: 4.0362\n",
      "Epoch [15/300], Step [19200/27733], Loss: 3.6190\n",
      "Epoch [15/300], Step [19300/27733], Loss: 2.5549\n",
      "Epoch [15/300], Step [19400/27733], Loss: 3.1931\n",
      "Epoch [15/300], Step [19500/27733], Loss: 3.4015\n",
      "Epoch [15/300], Step [19600/27733], Loss: 3.9381\n",
      "Epoch [15/300], Step [19700/27733], Loss: 3.0626\n",
      "Epoch [15/300], Step [19800/27733], Loss: 3.5596\n",
      "Epoch [15/300], Step [19900/27733], Loss: 2.4479\n",
      "Epoch [15/300], Step [20000/27733], Loss: 3.2115\n",
      "Epoch [15/300], Step [20100/27733], Loss: 4.0906\n",
      "Epoch [15/300], Step [20200/27733], Loss: 2.8787\n",
      "Epoch [15/300], Step [20300/27733], Loss: 4.5498\n",
      "Epoch [15/300], Step [20400/27733], Loss: 2.5114\n",
      "Epoch [15/300], Step [20500/27733], Loss: 3.4020\n",
      "Epoch [15/300], Step [20600/27733], Loss: 4.1343\n",
      "Epoch [15/300], Step [20700/27733], Loss: 3.3055\n",
      "Epoch [15/300], Step [20800/27733], Loss: 3.2631\n",
      "Epoch [15/300], Step [20900/27733], Loss: 3.3532\n",
      "Epoch [15/300], Step [21000/27733], Loss: 3.7999\n",
      "Epoch [15/300], Step [21100/27733], Loss: 3.1206\n",
      "Epoch [15/300], Step [21200/27733], Loss: 3.7166\n",
      "Epoch [15/300], Step [21300/27733], Loss: 3.5057\n",
      "Epoch [15/300], Step [21400/27733], Loss: 4.4455\n",
      "Epoch [15/300], Step [21500/27733], Loss: 3.5607\n",
      "Epoch [15/300], Step [21600/27733], Loss: 3.6365\n",
      "Epoch [15/300], Step [21700/27733], Loss: 3.7068\n",
      "Epoch [15/300], Step [21800/27733], Loss: 2.5002\n",
      "Epoch [15/300], Step [21900/27733], Loss: 2.3016\n",
      "Epoch [15/300], Step [22000/27733], Loss: 2.9648\n",
      "Epoch [15/300], Step [22100/27733], Loss: 4.0836\n",
      "Epoch [15/300], Step [22200/27733], Loss: 3.7364\n",
      "Epoch [15/300], Step [22300/27733], Loss: 4.5415\n",
      "Epoch [15/300], Step [22400/27733], Loss: 3.2481\n",
      "Epoch [15/300], Step [22500/27733], Loss: 3.2540\n",
      "Epoch [15/300], Step [22600/27733], Loss: 3.7837\n",
      "Epoch [15/300], Step [22700/27733], Loss: 3.1855\n",
      "Epoch [15/300], Step [22800/27733], Loss: 3.7908\n",
      "Epoch [15/300], Step [22900/27733], Loss: 2.7900\n",
      "Epoch [15/300], Step [23000/27733], Loss: 3.7541\n",
      "Epoch [15/300], Step [23100/27733], Loss: 3.7121\n",
      "Epoch [15/300], Step [23200/27733], Loss: 3.2486\n",
      "Epoch [15/300], Step [23300/27733], Loss: 3.7420\n",
      "Epoch [15/300], Step [23400/27733], Loss: 2.6579\n",
      "Epoch [15/300], Step [23500/27733], Loss: 3.6195\n",
      "Epoch [15/300], Step [23600/27733], Loss: 2.9501\n",
      "Epoch [15/300], Step [23700/27733], Loss: 3.2697\n",
      "Epoch [15/300], Step [23800/27733], Loss: 4.3283\n",
      "Epoch [15/300], Step [23900/27733], Loss: 2.9623\n",
      "Epoch [15/300], Step [24000/27733], Loss: 3.1480\n",
      "Epoch [15/300], Step [24100/27733], Loss: 3.3282\n",
      "Epoch [15/300], Step [24200/27733], Loss: 3.1866\n",
      "Epoch [15/300], Step [24300/27733], Loss: 4.1677\n",
      "Epoch [15/300], Step [24400/27733], Loss: 3.4961\n",
      "Epoch [15/300], Step [24500/27733], Loss: 4.0567\n",
      "Epoch [15/300], Step [24600/27733], Loss: 3.9291\n",
      "Epoch [15/300], Step [24700/27733], Loss: 3.6849\n",
      "Epoch [15/300], Step [24800/27733], Loss: 2.7999\n",
      "Epoch [15/300], Step [24900/27733], Loss: 3.6996\n",
      "Epoch [15/300], Step [25000/27733], Loss: 3.2554\n",
      "Epoch [15/300], Step [25100/27733], Loss: 2.9585\n",
      "Epoch [15/300], Step [25200/27733], Loss: 3.7070\n",
      "Epoch [15/300], Step [25300/27733], Loss: 4.0511\n",
      "Epoch [15/300], Step [25400/27733], Loss: 2.5760\n",
      "Epoch [15/300], Step [25500/27733], Loss: 3.1115\n",
      "Epoch [15/300], Step [25600/27733], Loss: 4.0239\n",
      "Epoch [15/300], Step [25700/27733], Loss: 4.0502\n",
      "Epoch [15/300], Step [25800/27733], Loss: 3.4113\n",
      "Epoch [15/300], Step [25900/27733], Loss: 3.5319\n",
      "Epoch [15/300], Step [26000/27733], Loss: 3.8768\n",
      "Epoch [15/300], Step [26100/27733], Loss: 3.2438\n",
      "Epoch [15/300], Step [26200/27733], Loss: 3.4307\n",
      "Epoch [15/300], Step [26300/27733], Loss: 3.0075\n",
      "Epoch [15/300], Step [26400/27733], Loss: 3.6222\n",
      "Epoch [15/300], Step [26500/27733], Loss: 3.5879\n",
      "Epoch [15/300], Step [26600/27733], Loss: 3.3234\n",
      "Epoch [15/300], Step [26700/27733], Loss: 2.8521\n",
      "Epoch [15/300], Step [26800/27733], Loss: 3.4580\n",
      "Epoch [15/300], Step [26900/27733], Loss: 3.7186\n",
      "Epoch [15/300], Step [27000/27733], Loss: 3.4577\n",
      "Epoch [15/300], Step [27100/27733], Loss: 3.8123\n",
      "Epoch [15/300], Step [27200/27733], Loss: 3.1397\n",
      "Epoch [15/300], Step [27300/27733], Loss: 4.1074\n",
      "Epoch [15/300], Step [27400/27733], Loss: 4.2034\n",
      "Epoch [15/300], Step [27500/27733], Loss: 3.0908\n",
      "Epoch [15/300], Step [27600/27733], Loss: 4.7025\n",
      "Epoch [15/300], Step [27700/27733], Loss: 4.4978\n",
      "Epoch [16/300], Step [100/27733], Loss: 3.0754\n",
      "Epoch [16/300], Step [200/27733], Loss: 2.9222\n",
      "Epoch [16/300], Step [300/27733], Loss: 2.1947\n",
      "Epoch [16/300], Step [400/27733], Loss: 2.9221\n",
      "Epoch [16/300], Step [500/27733], Loss: 3.3718\n",
      "Epoch [16/300], Step [600/27733], Loss: 3.2681\n",
      "Epoch [16/300], Step [700/27733], Loss: 2.4825\n",
      "Epoch [16/300], Step [800/27733], Loss: 3.1039\n",
      "Epoch [16/300], Step [900/27733], Loss: 2.9078\n",
      "Epoch [16/300], Step [1000/27733], Loss: 3.3739\n",
      "Epoch [16/300], Step [1100/27733], Loss: 2.7640\n",
      "Epoch [16/300], Step [1200/27733], Loss: 3.6694\n",
      "Epoch [16/300], Step [1300/27733], Loss: 2.4159\n",
      "Epoch [16/300], Step [1400/27733], Loss: 2.6793\n",
      "Epoch [16/300], Step [1500/27733], Loss: 2.7172\n",
      "Epoch [16/300], Step [1600/27733], Loss: 3.4671\n",
      "Epoch [16/300], Step [1700/27733], Loss: 3.0461\n",
      "Epoch [16/300], Step [1800/27733], Loss: 2.7709\n",
      "Epoch [16/300], Step [1900/27733], Loss: 3.3337\n",
      "Epoch [16/300], Step [2000/27733], Loss: 3.2694\n",
      "Epoch [16/300], Step [2100/27733], Loss: 3.3686\n",
      "Epoch [16/300], Step [2200/27733], Loss: 2.7450\n",
      "Epoch [16/300], Step [2300/27733], Loss: 3.3328\n",
      "Epoch [16/300], Step [2400/27733], Loss: 3.1374\n",
      "Epoch [16/300], Step [2500/27733], Loss: 2.9245\n",
      "Epoch [16/300], Step [2600/27733], Loss: 3.2680\n",
      "Epoch [16/300], Step [2700/27733], Loss: 3.4457\n",
      "Epoch [16/300], Step [2800/27733], Loss: 2.1918\n",
      "Epoch [16/300], Step [2900/27733], Loss: 3.6406\n",
      "Epoch [16/300], Step [3000/27733], Loss: 3.9425\n",
      "Epoch [16/300], Step [3100/27733], Loss: 2.6556\n",
      "Epoch [16/300], Step [3200/27733], Loss: 2.5338\n",
      "Epoch [16/300], Step [3300/27733], Loss: 2.7724\n",
      "Epoch [16/300], Step [3400/27733], Loss: 3.8508\n",
      "Epoch [16/300], Step [3500/27733], Loss: 2.7454\n",
      "Epoch [16/300], Step [3600/27733], Loss: 2.6590\n",
      "Epoch [16/300], Step [3700/27733], Loss: 2.0780\n",
      "Epoch [16/300], Step [3800/27733], Loss: 3.4127\n",
      "Epoch [16/300], Step [3900/27733], Loss: 3.3035\n",
      "Epoch [16/300], Step [4000/27733], Loss: 2.8757\n",
      "Epoch [16/300], Step [4100/27733], Loss: 3.5686\n",
      "Epoch [16/300], Step [4200/27733], Loss: 2.1196\n",
      "Epoch [16/300], Step [4300/27733], Loss: 2.7656\n",
      "Epoch [16/300], Step [4400/27733], Loss: 2.4176\n",
      "Epoch [16/300], Step [4500/27733], Loss: 2.5271\n",
      "Epoch [16/300], Step [4600/27733], Loss: 2.6127\n",
      "Epoch [16/300], Step [4700/27733], Loss: 2.9145\n",
      "Epoch [16/300], Step [4800/27733], Loss: 3.1015\n",
      "Epoch [16/300], Step [4900/27733], Loss: 3.5678\n",
      "Epoch [16/300], Step [5000/27733], Loss: 3.4683\n",
      "Epoch [16/300], Step [5100/27733], Loss: 3.3239\n",
      "Epoch [16/300], Step [5200/27733], Loss: 2.8906\n",
      "Epoch [16/300], Step [5300/27733], Loss: 4.1828\n",
      "Epoch [16/300], Step [5400/27733], Loss: 2.8130\n",
      "Epoch [16/300], Step [5500/27733], Loss: 3.0480\n",
      "Epoch [16/300], Step [5600/27733], Loss: 3.0307\n",
      "Epoch [16/300], Step [5700/27733], Loss: 3.3385\n",
      "Epoch [16/300], Step [5800/27733], Loss: 3.4466\n",
      "Epoch [16/300], Step [5900/27733], Loss: 2.1210\n",
      "Epoch [16/300], Step [6000/27733], Loss: 2.6018\n",
      "Epoch [16/300], Step [6100/27733], Loss: 3.1519\n",
      "Epoch [16/300], Step [6200/27733], Loss: 3.0832\n",
      "Epoch [16/300], Step [6300/27733], Loss: 2.4932\n",
      "Epoch [16/300], Step [6400/27733], Loss: 3.5257\n",
      "Epoch [16/300], Step [6500/27733], Loss: 3.0670\n",
      "Epoch [16/300], Step [6600/27733], Loss: 2.9845\n",
      "Epoch [16/300], Step [6700/27733], Loss: 2.8722\n",
      "Epoch [16/300], Step [6800/27733], Loss: 3.1961\n",
      "Epoch [16/300], Step [6900/27733], Loss: 3.8414\n",
      "Epoch [16/300], Step [7000/27733], Loss: 3.2809\n",
      "Epoch [16/300], Step [7100/27733], Loss: 3.8192\n",
      "Epoch [16/300], Step [7200/27733], Loss: 2.4748\n",
      "Epoch [16/300], Step [7300/27733], Loss: 3.8346\n",
      "Epoch [16/300], Step [7400/27733], Loss: 3.3779\n",
      "Epoch [16/300], Step [7500/27733], Loss: 2.6095\n",
      "Epoch [16/300], Step [7600/27733], Loss: 3.4798\n",
      "Epoch [16/300], Step [7700/27733], Loss: 3.7400\n",
      "Epoch [16/300], Step [7800/27733], Loss: 3.6422\n",
      "Epoch [16/300], Step [7900/27733], Loss: 3.0271\n",
      "Epoch [16/300], Step [8000/27733], Loss: 3.2994\n",
      "Epoch [16/300], Step [8100/27733], Loss: 3.2923\n",
      "Epoch [16/300], Step [8200/27733], Loss: 2.8694\n",
      "Epoch [16/300], Step [8300/27733], Loss: 2.9672\n",
      "Epoch [16/300], Step [8400/27733], Loss: 2.1825\n",
      "Epoch [16/300], Step [8500/27733], Loss: 4.0477\n",
      "Epoch [16/300], Step [8600/27733], Loss: 2.8508\n",
      "Epoch [16/300], Step [8700/27733], Loss: 2.9706\n",
      "Epoch [16/300], Step [8800/27733], Loss: 3.1151\n",
      "Epoch [16/300], Step [8900/27733], Loss: 4.0989\n",
      "Epoch [16/300], Step [9000/27733], Loss: 3.4314\n",
      "Epoch [16/300], Step [9100/27733], Loss: 3.6976\n",
      "Epoch [16/300], Step [9200/27733], Loss: 3.5593\n",
      "Epoch [16/300], Step [9300/27733], Loss: 3.8262\n",
      "Epoch [16/300], Step [9400/27733], Loss: 3.7644\n",
      "Epoch [16/300], Step [9500/27733], Loss: 3.9424\n",
      "Epoch [16/300], Step [9600/27733], Loss: 3.6628\n",
      "Epoch [16/300], Step [9700/27733], Loss: 3.2823\n",
      "Epoch [16/300], Step [9800/27733], Loss: 3.4323\n",
      "Epoch [16/300], Step [9900/27733], Loss: 3.2096\n",
      "Epoch [16/300], Step [10000/27733], Loss: 3.3672\n",
      "Epoch [16/300], Step [10100/27733], Loss: 3.1215\n",
      "Epoch [16/300], Step [10200/27733], Loss: 3.0228\n",
      "Epoch [16/300], Step [10300/27733], Loss: 2.2838\n",
      "Epoch [16/300], Step [10400/27733], Loss: 3.1827\n",
      "Epoch [16/300], Step [10500/27733], Loss: 3.6668\n",
      "Epoch [16/300], Step [10600/27733], Loss: 3.1281\n",
      "Epoch [16/300], Step [10700/27733], Loss: 2.7847\n",
      "Epoch [16/300], Step [10800/27733], Loss: 2.9228\n",
      "Epoch [16/300], Step [10900/27733], Loss: 2.8285\n",
      "Epoch [16/300], Step [11000/27733], Loss: 2.9228\n",
      "Epoch [16/300], Step [11100/27733], Loss: 3.3816\n",
      "Epoch [16/300], Step [11200/27733], Loss: 2.7443\n",
      "Epoch [16/300], Step [11300/27733], Loss: 3.1442\n",
      "Epoch [16/300], Step [11400/27733], Loss: 3.1607\n",
      "Epoch [16/300], Step [11500/27733], Loss: 3.7056\n",
      "Epoch [16/300], Step [11600/27733], Loss: 3.4209\n",
      "Epoch [16/300], Step [11700/27733], Loss: 2.9088\n",
      "Epoch [16/300], Step [11800/27733], Loss: 2.9463\n",
      "Epoch [16/300], Step [11900/27733], Loss: 2.7826\n",
      "Epoch [16/300], Step [12000/27733], Loss: 3.8810\n",
      "Epoch [16/300], Step [12100/27733], Loss: 3.5204\n",
      "Epoch [16/300], Step [12200/27733], Loss: 2.5088\n",
      "Epoch [16/300], Step [12300/27733], Loss: 4.2748\n",
      "Epoch [16/300], Step [12400/27733], Loss: 4.0534\n",
      "Epoch [16/300], Step [12500/27733], Loss: 4.3012\n",
      "Epoch [16/300], Step [12600/27733], Loss: 3.4606\n",
      "Epoch [16/300], Step [12700/27733], Loss: 3.2378\n",
      "Epoch [16/300], Step [12800/27733], Loss: 3.0404\n",
      "Epoch [16/300], Step [12900/27733], Loss: 3.6684\n",
      "Epoch [16/300], Step [13000/27733], Loss: 2.5617\n",
      "Epoch [16/300], Step [13100/27733], Loss: 3.6870\n",
      "Epoch [16/300], Step [13200/27733], Loss: 3.2396\n",
      "Epoch [16/300], Step [13300/27733], Loss: 3.7682\n",
      "Epoch [16/300], Step [13400/27733], Loss: 3.1222\n",
      "Epoch [16/300], Step [13500/27733], Loss: 3.2335\n",
      "Epoch [16/300], Step [13600/27733], Loss: 2.9846\n",
      "Epoch [16/300], Step [13700/27733], Loss: 2.7556\n",
      "Epoch [16/300], Step [13800/27733], Loss: 3.0436\n",
      "Epoch [16/300], Step [13900/27733], Loss: 3.4785\n",
      "Epoch [16/300], Step [14000/27733], Loss: 2.6510\n",
      "Epoch [16/300], Step [14100/27733], Loss: 3.1138\n",
      "Epoch [16/300], Step [14200/27733], Loss: 3.1379\n",
      "Epoch [16/300], Step [14300/27733], Loss: 3.3911\n",
      "Epoch [16/300], Step [14400/27733], Loss: 2.6920\n",
      "Epoch [16/300], Step [14500/27733], Loss: 2.5599\n",
      "Epoch [16/300], Step [14600/27733], Loss: 3.4440\n",
      "Epoch [16/300], Step [14700/27733], Loss: 3.5576\n",
      "Epoch [16/300], Step [14800/27733], Loss: 3.9890\n",
      "Epoch [16/300], Step [14900/27733], Loss: 3.6227\n",
      "Epoch [16/300], Step [15000/27733], Loss: 3.8031\n",
      "Epoch [16/300], Step [15100/27733], Loss: 3.6824\n",
      "Epoch [16/300], Step [15200/27733], Loss: 3.9301\n",
      "Epoch [16/300], Step [15300/27733], Loss: 3.6383\n",
      "Epoch [16/300], Step [15400/27733], Loss: 2.6702\n",
      "Epoch [16/300], Step [15500/27733], Loss: 3.5735\n",
      "Epoch [16/300], Step [15600/27733], Loss: 3.8988\n",
      "Epoch [16/300], Step [15700/27733], Loss: 3.0711\n",
      "Epoch [16/300], Step [15800/27733], Loss: 3.4000\n",
      "Epoch [16/300], Step [15900/27733], Loss: 3.4092\n",
      "Epoch [16/300], Step [16000/27733], Loss: 3.3473\n",
      "Epoch [16/300], Step [16100/27733], Loss: 2.1643\n",
      "Epoch [16/300], Step [16200/27733], Loss: 2.6741\n",
      "Epoch [16/300], Step [16300/27733], Loss: 4.1426\n",
      "Epoch [16/300], Step [16400/27733], Loss: 3.2649\n",
      "Epoch [16/300], Step [16500/27733], Loss: 3.1820\n",
      "Epoch [16/300], Step [16600/27733], Loss: 2.7462\n",
      "Epoch [16/300], Step [16700/27733], Loss: 2.8543\n",
      "Epoch [16/300], Step [16800/27733], Loss: 2.8025\n",
      "Epoch [16/300], Step [16900/27733], Loss: 4.0096\n",
      "Epoch [16/300], Step [17000/27733], Loss: 3.5590\n",
      "Epoch [16/300], Step [17100/27733], Loss: 3.1579\n",
      "Epoch [16/300], Step [17200/27733], Loss: 3.3465\n",
      "Epoch [16/300], Step [17300/27733], Loss: 3.4906\n",
      "Epoch [16/300], Step [17400/27733], Loss: 3.0904\n",
      "Epoch [16/300], Step [17500/27733], Loss: 2.6385\n",
      "Epoch [16/300], Step [17600/27733], Loss: 3.4565\n",
      "Epoch [16/300], Step [17700/27733], Loss: 2.9367\n",
      "Epoch [16/300], Step [17800/27733], Loss: 3.4156\n",
      "Epoch [16/300], Step [17900/27733], Loss: 3.0810\n",
      "Epoch [16/300], Step [18000/27733], Loss: 3.8038\n",
      "Epoch [16/300], Step [18100/27733], Loss: 2.8838\n",
      "Epoch [16/300], Step [18200/27733], Loss: 3.1689\n",
      "Epoch [16/300], Step [18300/27733], Loss: 3.2088\n",
      "Epoch [16/300], Step [18400/27733], Loss: 2.3566\n",
      "Epoch [16/300], Step [18500/27733], Loss: 2.5345\n",
      "Epoch [16/300], Step [18600/27733], Loss: 4.0274\n",
      "Epoch [16/300], Step [18700/27733], Loss: 3.4285\n",
      "Epoch [16/300], Step [18800/27733], Loss: 3.1659\n",
      "Epoch [16/300], Step [18900/27733], Loss: 2.8472\n",
      "Epoch [16/300], Step [19000/27733], Loss: 3.0083\n",
      "Epoch [16/300], Step [19100/27733], Loss: 3.3170\n",
      "Epoch [16/300], Step [19200/27733], Loss: 2.9248\n",
      "Epoch [16/300], Step [19300/27733], Loss: 3.4089\n",
      "Epoch [16/300], Step [19400/27733], Loss: 3.7423\n",
      "Epoch [16/300], Step [19500/27733], Loss: 2.5524\n",
      "Epoch [16/300], Step [19600/27733], Loss: 3.2412\n",
      "Epoch [16/300], Step [19700/27733], Loss: 3.0095\n",
      "Epoch [16/300], Step [19800/27733], Loss: 3.1392\n",
      "Epoch [16/300], Step [19900/27733], Loss: 3.0473\n",
      "Epoch [16/300], Step [20000/27733], Loss: 3.3007\n",
      "Epoch [16/300], Step [20100/27733], Loss: 2.7886\n",
      "Epoch [16/300], Step [20200/27733], Loss: 3.1072\n",
      "Epoch [16/300], Step [20300/27733], Loss: 3.0902\n",
      "Epoch [16/300], Step [20400/27733], Loss: 4.2138\n",
      "Epoch [16/300], Step [20500/27733], Loss: 3.2209\n",
      "Epoch [16/300], Step [20600/27733], Loss: 3.0694\n",
      "Epoch [16/300], Step [20700/27733], Loss: 3.6924\n",
      "Epoch [16/300], Step [20800/27733], Loss: 2.1955\n",
      "Epoch [16/300], Step [20900/27733], Loss: 2.8942\n",
      "Epoch [16/300], Step [21000/27733], Loss: 4.0515\n",
      "Epoch [16/300], Step [21100/27733], Loss: 3.8261\n",
      "Epoch [16/300], Step [21200/27733], Loss: 3.9030\n",
      "Epoch [16/300], Step [21300/27733], Loss: 4.4908\n",
      "Epoch [16/300], Step [21400/27733], Loss: 3.1207\n",
      "Epoch [16/300], Step [21500/27733], Loss: 3.1373\n",
      "Epoch [16/300], Step [21600/27733], Loss: 3.1495\n",
      "Epoch [16/300], Step [21700/27733], Loss: 2.7747\n",
      "Epoch [16/300], Step [21800/27733], Loss: 4.2734\n",
      "Epoch [16/300], Step [21900/27733], Loss: 3.0983\n",
      "Epoch [16/300], Step [22000/27733], Loss: 3.3595\n",
      "Epoch [16/300], Step [22100/27733], Loss: 2.6760\n",
      "Epoch [16/300], Step [22200/27733], Loss: 2.9550\n",
      "Epoch [16/300], Step [22300/27733], Loss: 3.6300\n",
      "Epoch [16/300], Step [22400/27733], Loss: 4.0272\n",
      "Epoch [16/300], Step [22500/27733], Loss: 3.1643\n",
      "Epoch [16/300], Step [22600/27733], Loss: 3.6480\n",
      "Epoch [16/300], Step [22700/27733], Loss: 3.9342\n",
      "Epoch [16/300], Step [22800/27733], Loss: 3.2562\n",
      "Epoch [16/300], Step [22900/27733], Loss: 3.4434\n",
      "Epoch [16/300], Step [23000/27733], Loss: 3.5980\n",
      "Epoch [16/300], Step [23100/27733], Loss: 3.4084\n",
      "Epoch [16/300], Step [23200/27733], Loss: 3.5617\n",
      "Epoch [16/300], Step [23300/27733], Loss: 3.1279\n",
      "Epoch [16/300], Step [23400/27733], Loss: 2.9315\n",
      "Epoch [16/300], Step [23500/27733], Loss: 3.3338\n",
      "Epoch [16/300], Step [23600/27733], Loss: 2.9454\n",
      "Epoch [16/300], Step [23700/27733], Loss: 3.8749\n",
      "Epoch [16/300], Step [23800/27733], Loss: 3.4622\n",
      "Epoch [16/300], Step [23900/27733], Loss: 2.9369\n",
      "Epoch [16/300], Step [24000/27733], Loss: 3.9119\n",
      "Epoch [16/300], Step [24100/27733], Loss: 3.6559\n",
      "Epoch [16/300], Step [24200/27733], Loss: 3.5733\n",
      "Epoch [16/300], Step [24300/27733], Loss: 2.7521\n",
      "Epoch [16/300], Step [24400/27733], Loss: 2.5461\n",
      "Epoch [16/300], Step [24500/27733], Loss: 3.4664\n",
      "Epoch [16/300], Step [24600/27733], Loss: 3.0394\n",
      "Epoch [16/300], Step [24700/27733], Loss: 3.0720\n",
      "Epoch [16/300], Step [24800/27733], Loss: 2.7684\n",
      "Epoch [16/300], Step [24900/27733], Loss: 3.1636\n",
      "Epoch [16/300], Step [25000/27733], Loss: 3.8370\n",
      "Epoch [16/300], Step [25100/27733], Loss: 4.0754\n",
      "Epoch [16/300], Step [25200/27733], Loss: 2.6104\n",
      "Epoch [16/300], Step [25300/27733], Loss: 3.0136\n",
      "Epoch [16/300], Step [25400/27733], Loss: 3.0027\n",
      "Epoch [16/300], Step [25500/27733], Loss: 4.3566\n",
      "Epoch [16/300], Step [25600/27733], Loss: 3.1604\n",
      "Epoch [16/300], Step [25700/27733], Loss: 4.3955\n",
      "Epoch [16/300], Step [25800/27733], Loss: 2.7247\n",
      "Epoch [16/300], Step [25900/27733], Loss: 3.3957\n",
      "Epoch [16/300], Step [26000/27733], Loss: 4.1663\n",
      "Epoch [16/300], Step [26100/27733], Loss: 3.5596\n",
      "Epoch [16/300], Step [26200/27733], Loss: 4.1224\n",
      "Epoch [16/300], Step [26300/27733], Loss: 3.4085\n",
      "Epoch [16/300], Step [26400/27733], Loss: 3.2971\n",
      "Epoch [16/300], Step [26500/27733], Loss: 3.2420\n",
      "Epoch [16/300], Step [26600/27733], Loss: 2.7932\n",
      "Epoch [16/300], Step [26700/27733], Loss: 3.7898\n",
      "Epoch [16/300], Step [26800/27733], Loss: 3.4738\n",
      "Epoch [16/300], Step [26900/27733], Loss: 4.0378\n",
      "Epoch [16/300], Step [27000/27733], Loss: 2.7794\n",
      "Epoch [16/300], Step [27100/27733], Loss: 3.2376\n",
      "Epoch [16/300], Step [27200/27733], Loss: 3.1423\n",
      "Epoch [16/300], Step [27300/27733], Loss: 3.1313\n",
      "Epoch [16/300], Step [27400/27733], Loss: 3.9390\n",
      "Epoch [16/300], Step [27500/27733], Loss: 3.9762\n",
      "Epoch [16/300], Step [27600/27733], Loss: 2.4203\n",
      "Epoch [16/300], Step [27700/27733], Loss: 4.3575\n",
      "Epoch [17/300], Step [100/27733], Loss: 3.4888\n",
      "Epoch [17/300], Step [200/27733], Loss: 2.3559\n",
      "Epoch [17/300], Step [300/27733], Loss: 2.8667\n",
      "Epoch [17/300], Step [400/27733], Loss: 2.5477\n",
      "Epoch [17/300], Step [500/27733], Loss: 2.7179\n",
      "Epoch [17/300], Step [600/27733], Loss: 2.5842\n",
      "Epoch [17/300], Step [700/27733], Loss: 2.8351\n",
      "Epoch [17/300], Step [800/27733], Loss: 2.6830\n",
      "Epoch [17/300], Step [900/27733], Loss: 2.2024\n",
      "Epoch [17/300], Step [1000/27733], Loss: 2.5595\n",
      "Epoch [17/300], Step [1100/27733], Loss: 2.7067\n",
      "Epoch [17/300], Step [1200/27733], Loss: 3.1940\n",
      "Epoch [17/300], Step [1300/27733], Loss: 2.5683\n",
      "Epoch [17/300], Step [1400/27733], Loss: 2.2723\n",
      "Epoch [17/300], Step [1500/27733], Loss: 3.4965\n",
      "Epoch [17/300], Step [1600/27733], Loss: 3.3673\n",
      "Epoch [17/300], Step [1700/27733], Loss: 2.1723\n",
      "Epoch [17/300], Step [1800/27733], Loss: 3.1342\n",
      "Epoch [17/300], Step [1900/27733], Loss: 3.3859\n",
      "Epoch [17/300], Step [2000/27733], Loss: 3.3293\n",
      "Epoch [17/300], Step [2100/27733], Loss: 3.3648\n",
      "Epoch [17/300], Step [2200/27733], Loss: 2.5662\n",
      "Epoch [17/300], Step [2300/27733], Loss: 2.8821\n",
      "Epoch [17/300], Step [2400/27733], Loss: 2.6324\n",
      "Epoch [17/300], Step [2500/27733], Loss: 3.7435\n",
      "Epoch [17/300], Step [2600/27733], Loss: 2.6680\n",
      "Epoch [17/300], Step [2700/27733], Loss: 2.8198\n",
      "Epoch [17/300], Step [2800/27733], Loss: 3.0605\n",
      "Epoch [17/300], Step [2900/27733], Loss: 2.8479\n",
      "Epoch [17/300], Step [3000/27733], Loss: 2.8018\n",
      "Epoch [17/300], Step [3100/27733], Loss: 2.9926\n",
      "Epoch [17/300], Step [3200/27733], Loss: 3.1657\n",
      "Epoch [17/300], Step [3300/27733], Loss: 2.8849\n",
      "Epoch [17/300], Step [3400/27733], Loss: 2.3591\n",
      "Epoch [17/300], Step [3500/27733], Loss: 2.6159\n",
      "Epoch [17/300], Step [3600/27733], Loss: 3.0105\n",
      "Epoch [17/300], Step [3700/27733], Loss: 2.6479\n",
      "Epoch [17/300], Step [3800/27733], Loss: 2.7675\n",
      "Epoch [17/300], Step [3900/27733], Loss: 2.4974\n",
      "Epoch [17/300], Step [4000/27733], Loss: 2.9405\n",
      "Epoch [17/300], Step [4100/27733], Loss: 2.7910\n",
      "Epoch [17/300], Step [4200/27733], Loss: 2.6259\n",
      "Epoch [17/300], Step [4300/27733], Loss: 3.6636\n",
      "Epoch [17/300], Step [4400/27733], Loss: 2.3455\n",
      "Epoch [17/300], Step [4500/27733], Loss: 3.4184\n",
      "Epoch [17/300], Step [4600/27733], Loss: 3.3250\n",
      "Epoch [17/300], Step [4700/27733], Loss: 2.7804\n",
      "Epoch [17/300], Step [4800/27733], Loss: 2.9028\n",
      "Epoch [17/300], Step [4900/27733], Loss: 2.9601\n",
      "Epoch [17/300], Step [5000/27733], Loss: 3.6401\n",
      "Epoch [17/300], Step [5100/27733], Loss: 2.5955\n",
      "Epoch [17/300], Step [5200/27733], Loss: 2.5542\n",
      "Epoch [17/300], Step [5300/27733], Loss: 2.9836\n",
      "Epoch [17/300], Step [5400/27733], Loss: 3.4609\n",
      "Epoch [17/300], Step [5500/27733], Loss: 2.1439\n",
      "Epoch [17/300], Step [5600/27733], Loss: 3.0991\n",
      "Epoch [17/300], Step [5700/27733], Loss: 3.3896\n",
      "Epoch [17/300], Step [5800/27733], Loss: 3.4233\n",
      "Epoch [17/300], Step [5900/27733], Loss: 3.2019\n",
      "Epoch [17/300], Step [6000/27733], Loss: 2.0716\n",
      "Epoch [17/300], Step [6100/27733], Loss: 3.1446\n",
      "Epoch [17/300], Step [6200/27733], Loss: 3.2881\n",
      "Epoch [17/300], Step [6300/27733], Loss: 2.8245\n",
      "Epoch [17/300], Step [6400/27733], Loss: 3.0987\n",
      "Epoch [17/300], Step [6500/27733], Loss: 3.0768\n",
      "Epoch [17/300], Step [6600/27733], Loss: 2.9920\n",
      "Epoch [17/300], Step [6700/27733], Loss: 3.6323\n",
      "Epoch [17/300], Step [6800/27733], Loss: 4.0205\n",
      "Epoch [17/300], Step [6900/27733], Loss: 2.9343\n",
      "Epoch [17/300], Step [7000/27733], Loss: 3.2064\n",
      "Epoch [17/300], Step [7100/27733], Loss: 3.4656\n",
      "Epoch [17/300], Step [7200/27733], Loss: 2.8444\n",
      "Epoch [17/300], Step [7300/27733], Loss: 3.7996\n",
      "Epoch [17/300], Step [7400/27733], Loss: 2.4470\n",
      "Epoch [17/300], Step [7500/27733], Loss: 3.4029\n",
      "Epoch [17/300], Step [7600/27733], Loss: 2.6351\n",
      "Epoch [17/300], Step [7700/27733], Loss: 3.5989\n",
      "Epoch [17/300], Step [7800/27733], Loss: 2.7498\n",
      "Epoch [17/300], Step [7900/27733], Loss: 3.4295\n",
      "Epoch [17/300], Step [8000/27733], Loss: 2.8284\n",
      "Epoch [17/300], Step [8100/27733], Loss: 2.8315\n",
      "Epoch [17/300], Step [8200/27733], Loss: 2.4858\n",
      "Epoch [17/300], Step [8300/27733], Loss: 2.8920\n",
      "Epoch [17/300], Step [8400/27733], Loss: 3.0197\n",
      "Epoch [17/300], Step [8500/27733], Loss: 2.1595\n",
      "Epoch [17/300], Step [8600/27733], Loss: 2.2679\n",
      "Epoch [17/300], Step [8700/27733], Loss: 3.5596\n",
      "Epoch [17/300], Step [8800/27733], Loss: 2.8899\n",
      "Epoch [17/300], Step [8900/27733], Loss: 2.8515\n",
      "Epoch [17/300], Step [9000/27733], Loss: 3.9156\n",
      "Epoch [17/300], Step [9100/27733], Loss: 2.9238\n",
      "Epoch [17/300], Step [9200/27733], Loss: 3.0106\n",
      "Epoch [17/300], Step [9300/27733], Loss: 3.5858\n",
      "Epoch [17/300], Step [9400/27733], Loss: 3.0005\n",
      "Epoch [17/300], Step [9500/27733], Loss: 3.0580\n",
      "Epoch [17/300], Step [9600/27733], Loss: 2.8246\n",
      "Epoch [17/300], Step [9700/27733], Loss: 3.2457\n",
      "Epoch [17/300], Step [9800/27733], Loss: 2.8293\n",
      "Epoch [17/300], Step [9900/27733], Loss: 3.2297\n",
      "Epoch [17/300], Step [10000/27733], Loss: 2.6746\n",
      "Epoch [17/300], Step [10100/27733], Loss: 2.7305\n",
      "Epoch [17/300], Step [10200/27733], Loss: 2.8125\n",
      "Epoch [17/300], Step [10300/27733], Loss: 2.2275\n",
      "Epoch [17/300], Step [10400/27733], Loss: 3.3269\n",
      "Epoch [17/300], Step [10500/27733], Loss: 3.2588\n",
      "Epoch [17/300], Step [10600/27733], Loss: 3.6206\n",
      "Epoch [17/300], Step [10700/27733], Loss: 2.4060\n",
      "Epoch [17/300], Step [10800/27733], Loss: 3.7883\n",
      "Epoch [17/300], Step [10900/27733], Loss: 3.0864\n",
      "Epoch [17/300], Step [11000/27733], Loss: 2.8776\n",
      "Epoch [17/300], Step [11100/27733], Loss: 3.1944\n",
      "Epoch [17/300], Step [11200/27733], Loss: 2.7081\n",
      "Epoch [17/300], Step [11300/27733], Loss: 3.0077\n",
      "Epoch [17/300], Step [11400/27733], Loss: 2.8559\n",
      "Epoch [17/300], Step [11500/27733], Loss: 3.2545\n",
      "Epoch [17/300], Step [11600/27733], Loss: 2.4094\n",
      "Epoch [17/300], Step [11700/27733], Loss: 3.1829\n",
      "Epoch [17/300], Step [11800/27733], Loss: 3.4933\n",
      "Epoch [17/300], Step [11900/27733], Loss: 3.0381\n",
      "Epoch [17/300], Step [12000/27733], Loss: 3.3302\n",
      "Epoch [17/300], Step [12100/27733], Loss: 3.2590\n",
      "Epoch [17/300], Step [12200/27733], Loss: 3.4141\n",
      "Epoch [17/300], Step [12300/27733], Loss: 3.9583\n",
      "Epoch [17/300], Step [12400/27733], Loss: 2.7530\n",
      "Epoch [17/300], Step [12500/27733], Loss: 3.5102\n",
      "Epoch [17/300], Step [12600/27733], Loss: 3.1749\n",
      "Epoch [17/300], Step [12700/27733], Loss: 2.7364\n",
      "Epoch [17/300], Step [12800/27733], Loss: 3.2925\n",
      "Epoch [17/300], Step [12900/27733], Loss: 3.6313\n",
      "Epoch [17/300], Step [13000/27733], Loss: 3.9952\n",
      "Epoch [17/300], Step [13100/27733], Loss: 3.2993\n",
      "Epoch [17/300], Step [13200/27733], Loss: 3.1392\n",
      "Epoch [17/300], Step [13300/27733], Loss: 2.7933\n",
      "Epoch [17/300], Step [13400/27733], Loss: 2.8163\n",
      "Epoch [17/300], Step [13500/27733], Loss: 2.9245\n",
      "Epoch [17/300], Step [13600/27733], Loss: 2.9283\n",
      "Epoch [17/300], Step [13700/27733], Loss: 3.5269\n",
      "Epoch [17/300], Step [13800/27733], Loss: 3.3797\n",
      "Epoch [17/300], Step [13900/27733], Loss: 3.1766\n",
      "Epoch [17/300], Step [14000/27733], Loss: 2.9781\n",
      "Epoch [17/300], Step [14100/27733], Loss: 3.3390\n",
      "Epoch [17/300], Step [14200/27733], Loss: 2.7269\n",
      "Epoch [17/300], Step [14300/27733], Loss: 2.8780\n",
      "Epoch [17/300], Step [14400/27733], Loss: 3.6555\n",
      "Epoch [17/300], Step [14500/27733], Loss: 3.6002\n",
      "Epoch [17/300], Step [14600/27733], Loss: 3.8491\n",
      "Epoch [17/300], Step [14700/27733], Loss: 2.5470\n",
      "Epoch [17/300], Step [14800/27733], Loss: 2.7371\n",
      "Epoch [17/300], Step [14900/27733], Loss: 3.9470\n",
      "Epoch [17/300], Step [15000/27733], Loss: 3.2852\n",
      "Epoch [17/300], Step [15100/27733], Loss: 2.6540\n",
      "Epoch [17/300], Step [15200/27733], Loss: 2.4647\n",
      "Epoch [17/300], Step [15300/27733], Loss: 2.3712\n",
      "Epoch [17/300], Step [15400/27733], Loss: 3.5975\n",
      "Epoch [17/300], Step [15500/27733], Loss: 3.0549\n",
      "Epoch [17/300], Step [15600/27733], Loss: 3.8436\n",
      "Epoch [17/300], Step [15700/27733], Loss: 2.6728\n",
      "Epoch [17/300], Step [15800/27733], Loss: 3.8213\n",
      "Epoch [17/300], Step [15900/27733], Loss: 3.3896\n",
      "Epoch [17/300], Step [16000/27733], Loss: 4.0039\n",
      "Epoch [17/300], Step [16100/27733], Loss: 3.6682\n",
      "Epoch [17/300], Step [16200/27733], Loss: 3.8432\n",
      "Epoch [17/300], Step [16300/27733], Loss: 3.0504\n",
      "Epoch [17/300], Step [16400/27733], Loss: 3.1066\n",
      "Epoch [17/300], Step [16500/27733], Loss: 3.3509\n",
      "Epoch [17/300], Step [16600/27733], Loss: 3.8871\n",
      "Epoch [17/300], Step [16700/27733], Loss: 2.4204\n",
      "Epoch [17/300], Step [16800/27733], Loss: 3.4417\n",
      "Epoch [17/300], Step [16900/27733], Loss: 3.7652\n",
      "Epoch [17/300], Step [17000/27733], Loss: 3.4532\n",
      "Epoch [17/300], Step [17100/27733], Loss: 3.8904\n",
      "Epoch [17/300], Step [17200/27733], Loss: 3.6875\n",
      "Epoch [17/300], Step [17300/27733], Loss: 3.4731\n",
      "Epoch [17/300], Step [17400/27733], Loss: 4.1906\n",
      "Epoch [17/300], Step [17500/27733], Loss: 2.4281\n",
      "Epoch [17/300], Step [17600/27733], Loss: 4.1744\n",
      "Epoch [17/300], Step [17700/27733], Loss: 3.8821\n",
      "Epoch [17/300], Step [17800/27733], Loss: 2.4451\n",
      "Epoch [17/300], Step [17900/27733], Loss: 3.5883\n",
      "Epoch [17/300], Step [18000/27733], Loss: 3.5615\n",
      "Epoch [17/300], Step [18100/27733], Loss: 3.0316\n",
      "Epoch [17/300], Step [18200/27733], Loss: 2.9844\n",
      "Epoch [17/300], Step [18300/27733], Loss: 3.0411\n",
      "Epoch [17/300], Step [18400/27733], Loss: 2.9878\n",
      "Epoch [17/300], Step [18500/27733], Loss: 2.8654\n",
      "Epoch [17/300], Step [18600/27733], Loss: 2.5158\n",
      "Epoch [17/300], Step [18700/27733], Loss: 3.3487\n",
      "Epoch [17/300], Step [18800/27733], Loss: 3.3992\n",
      "Epoch [17/300], Step [18900/27733], Loss: 3.5164\n",
      "Epoch [17/300], Step [19000/27733], Loss: 3.0097\n",
      "Epoch [17/300], Step [19100/27733], Loss: 3.8645\n",
      "Epoch [17/300], Step [19200/27733], Loss: 2.8943\n",
      "Epoch [17/300], Step [19300/27733], Loss: 3.3533\n",
      "Epoch [17/300], Step [19400/27733], Loss: 3.4949\n",
      "Epoch [17/300], Step [19500/27733], Loss: 2.8739\n",
      "Epoch [17/300], Step [19600/27733], Loss: 4.0137\n",
      "Epoch [17/300], Step [19700/27733], Loss: 3.2913\n",
      "Epoch [17/300], Step [19800/27733], Loss: 3.5296\n",
      "Epoch [17/300], Step [19900/27733], Loss: 3.2533\n",
      "Epoch [17/300], Step [20000/27733], Loss: 3.1193\n",
      "Epoch [17/300], Step [20100/27733], Loss: 3.5703\n",
      "Epoch [17/300], Step [20200/27733], Loss: 2.8677\n",
      "Epoch [17/300], Step [20300/27733], Loss: 3.2686\n",
      "Epoch [17/300], Step [20400/27733], Loss: 3.5862\n",
      "Epoch [17/300], Step [20500/27733], Loss: 2.9738\n",
      "Epoch [17/300], Step [20600/27733], Loss: 2.8806\n",
      "Epoch [17/300], Step [20700/27733], Loss: 3.7998\n",
      "Epoch [17/300], Step [20800/27733], Loss: 3.3499\n",
      "Epoch [17/300], Step [20900/27733], Loss: 3.4000\n",
      "Epoch [17/300], Step [21000/27733], Loss: 2.4217\n",
      "Epoch [17/300], Step [21100/27733], Loss: 2.8800\n",
      "Epoch [17/300], Step [21200/27733], Loss: 3.5060\n",
      "Epoch [17/300], Step [21300/27733], Loss: 3.4364\n",
      "Epoch [17/300], Step [21400/27733], Loss: 3.3435\n",
      "Epoch [17/300], Step [21500/27733], Loss: 3.3632\n",
      "Epoch [17/300], Step [21600/27733], Loss: 4.1362\n",
      "Epoch [17/300], Step [21700/27733], Loss: 2.8818\n",
      "Epoch [17/300], Step [21800/27733], Loss: 3.0742\n",
      "Epoch [17/300], Step [21900/27733], Loss: 2.8657\n",
      "Epoch [17/300], Step [22000/27733], Loss: 3.6129\n",
      "Epoch [17/300], Step [22100/27733], Loss: 3.4205\n",
      "Epoch [17/300], Step [22200/27733], Loss: 3.2834\n",
      "Epoch [17/300], Step [22300/27733], Loss: 2.6532\n",
      "Epoch [17/300], Step [22400/27733], Loss: 3.1691\n",
      "Epoch [17/300], Step [22500/27733], Loss: 4.4794\n",
      "Epoch [17/300], Step [22600/27733], Loss: 3.2225\n",
      "Epoch [17/300], Step [22700/27733], Loss: 3.5089\n",
      "Epoch [17/300], Step [22800/27733], Loss: 3.3518\n",
      "Epoch [17/300], Step [22900/27733], Loss: 2.7422\n",
      "Epoch [17/300], Step [23000/27733], Loss: 3.4459\n",
      "Epoch [17/300], Step [23100/27733], Loss: 2.9547\n",
      "Epoch [17/300], Step [23200/27733], Loss: 3.5448\n",
      "Epoch [17/300], Step [23300/27733], Loss: 4.1483\n",
      "Epoch [17/300], Step [23400/27733], Loss: 3.1245\n",
      "Epoch [17/300], Step [23500/27733], Loss: 3.2463\n",
      "Epoch [17/300], Step [23600/27733], Loss: 4.1517\n",
      "Epoch [17/300], Step [23700/27733], Loss: 2.8541\n",
      "Epoch [17/300], Step [23800/27733], Loss: 3.0640\n",
      "Epoch [17/300], Step [23900/27733], Loss: 2.4030\n",
      "Epoch [17/300], Step [24000/27733], Loss: 2.7194\n",
      "Epoch [17/300], Step [24100/27733], Loss: 3.6201\n",
      "Epoch [17/300], Step [24200/27733], Loss: 3.9850\n",
      "Epoch [17/300], Step [24300/27733], Loss: 3.7384\n",
      "Epoch [17/300], Step [24400/27733], Loss: 2.5267\n",
      "Epoch [17/300], Step [24500/27733], Loss: 3.2005\n",
      "Epoch [17/300], Step [24600/27733], Loss: 3.1018\n",
      "Epoch [17/300], Step [24700/27733], Loss: 3.0667\n",
      "Epoch [17/300], Step [24800/27733], Loss: 3.8009\n",
      "Epoch [17/300], Step [24900/27733], Loss: 2.1258\n",
      "Epoch [17/300], Step [25000/27733], Loss: 3.1865\n",
      "Epoch [17/300], Step [25100/27733], Loss: 3.8619\n",
      "Epoch [17/300], Step [25200/27733], Loss: 2.4624\n",
      "Epoch [17/300], Step [25300/27733], Loss: 3.4870\n",
      "Epoch [17/300], Step [25400/27733], Loss: 3.2213\n",
      "Epoch [17/300], Step [25500/27733], Loss: 2.6379\n",
      "Epoch [17/300], Step [25600/27733], Loss: 2.9382\n",
      "Epoch [17/300], Step [25700/27733], Loss: 2.9705\n",
      "Epoch [17/300], Step [25800/27733], Loss: 2.6045\n",
      "Epoch [17/300], Step [25900/27733], Loss: 2.9262\n",
      "Epoch [17/300], Step [26000/27733], Loss: 4.0076\n",
      "Epoch [17/300], Step [26100/27733], Loss: 3.6959\n",
      "Epoch [17/300], Step [26200/27733], Loss: 4.2472\n",
      "Epoch [17/300], Step [26300/27733], Loss: 3.8988\n",
      "Epoch [17/300], Step [26400/27733], Loss: 2.9755\n",
      "Epoch [17/300], Step [26500/27733], Loss: 3.9257\n",
      "Epoch [17/300], Step [26600/27733], Loss: 3.3666\n",
      "Epoch [17/300], Step [26700/27733], Loss: 3.7231\n",
      "Epoch [17/300], Step [26800/27733], Loss: 3.1891\n",
      "Epoch [17/300], Step [26900/27733], Loss: 3.0398\n",
      "Epoch [17/300], Step [27000/27733], Loss: 4.2751\n",
      "Epoch [17/300], Step [27100/27733], Loss: 2.7912\n",
      "Epoch [17/300], Step [27200/27733], Loss: 4.3768\n",
      "Epoch [17/300], Step [27300/27733], Loss: 3.4453\n",
      "Epoch [17/300], Step [27400/27733], Loss: 3.3372\n",
      "Epoch [17/300], Step [27500/27733], Loss: 3.2208\n",
      "Epoch [17/300], Step [27600/27733], Loss: 4.1182\n",
      "Epoch [17/300], Step [27700/27733], Loss: 4.4870\n",
      "Epoch [18/300], Step [100/27733], Loss: 2.1287\n",
      "Epoch [18/300], Step [200/27733], Loss: 2.6331\n",
      "Epoch [18/300], Step [300/27733], Loss: 3.3752\n",
      "Epoch [18/300], Step [400/27733], Loss: 2.9270\n",
      "Epoch [18/300], Step [500/27733], Loss: 2.6990\n",
      "Epoch [18/300], Step [600/27733], Loss: 2.7975\n",
      "Epoch [18/300], Step [700/27733], Loss: 2.9598\n",
      "Epoch [18/300], Step [800/27733], Loss: 2.6533\n",
      "Epoch [18/300], Step [900/27733], Loss: 1.8843\n",
      "Epoch [18/300], Step [1000/27733], Loss: 3.3697\n",
      "Epoch [18/300], Step [1100/27733], Loss: 3.2754\n",
      "Epoch [18/300], Step [1200/27733], Loss: 2.5996\n",
      "Epoch [18/300], Step [1300/27733], Loss: 2.6642\n",
      "Epoch [18/300], Step [1400/27733], Loss: 3.2722\n",
      "Epoch [18/300], Step [1500/27733], Loss: 3.0842\n",
      "Epoch [18/300], Step [1600/27733], Loss: 3.2515\n",
      "Epoch [18/300], Step [1700/27733], Loss: 3.7571\n",
      "Epoch [18/300], Step [1800/27733], Loss: 2.8843\n",
      "Epoch [18/300], Step [1900/27733], Loss: 2.8767\n",
      "Epoch [18/300], Step [2000/27733], Loss: 3.3154\n",
      "Epoch [18/300], Step [2100/27733], Loss: 3.1372\n",
      "Epoch [18/300], Step [2200/27733], Loss: 3.8711\n",
      "Epoch [18/300], Step [2300/27733], Loss: 2.9816\n",
      "Epoch [18/300], Step [2400/27733], Loss: 2.6118\n",
      "Epoch [18/300], Step [2500/27733], Loss: 3.0439\n",
      "Epoch [18/300], Step [2600/27733], Loss: 2.8072\n",
      "Epoch [18/300], Step [2700/27733], Loss: 2.2868\n",
      "Epoch [18/300], Step [2800/27733], Loss: 3.0648\n",
      "Epoch [18/300], Step [2900/27733], Loss: 2.5082\n",
      "Epoch [18/300], Step [3000/27733], Loss: 2.9280\n",
      "Epoch [18/300], Step [3100/27733], Loss: 3.2066\n",
      "Epoch [18/300], Step [3200/27733], Loss: 2.6066\n",
      "Epoch [18/300], Step [3300/27733], Loss: 2.8953\n",
      "Epoch [18/300], Step [3400/27733], Loss: 2.5684\n",
      "Epoch [18/300], Step [3500/27733], Loss: 3.3565\n",
      "Epoch [18/300], Step [3600/27733], Loss: 2.6717\n",
      "Epoch [18/300], Step [3700/27733], Loss: 3.0679\n",
      "Epoch [18/300], Step [3800/27733], Loss: 3.3726\n",
      "Epoch [18/300], Step [3900/27733], Loss: 2.8610\n",
      "Epoch [18/300], Step [4000/27733], Loss: 2.6538\n",
      "Epoch [18/300], Step [4100/27733], Loss: 3.2044\n",
      "Epoch [18/300], Step [4200/27733], Loss: 2.7874\n",
      "Epoch [18/300], Step [4300/27733], Loss: 2.3178\n",
      "Epoch [18/300], Step [4400/27733], Loss: 3.1091\n",
      "Epoch [18/300], Step [4500/27733], Loss: 4.2355\n",
      "Epoch [18/300], Step [4600/27733], Loss: 3.5749\n",
      "Epoch [18/300], Step [4700/27733], Loss: 2.1330\n",
      "Epoch [18/300], Step [4800/27733], Loss: 3.5415\n",
      "Epoch [18/300], Step [4900/27733], Loss: 2.2642\n",
      "Epoch [18/300], Step [5000/27733], Loss: 2.5577\n",
      "Epoch [18/300], Step [5100/27733], Loss: 3.0426\n",
      "Epoch [18/300], Step [5200/27733], Loss: 2.4986\n",
      "Epoch [18/300], Step [5300/27733], Loss: 2.9751\n",
      "Epoch [18/300], Step [5400/27733], Loss: 3.0174\n",
      "Epoch [18/300], Step [5500/27733], Loss: 2.8258\n",
      "Epoch [18/300], Step [5600/27733], Loss: 3.2807\n",
      "Epoch [18/300], Step [5700/27733], Loss: 3.7302\n",
      "Epoch [18/300], Step [5800/27733], Loss: 3.0079\n",
      "Epoch [18/300], Step [5900/27733], Loss: 2.3940\n",
      "Epoch [18/300], Step [6000/27733], Loss: 2.9371\n",
      "Epoch [18/300], Step [6100/27733], Loss: 4.1036\n",
      "Epoch [18/300], Step [6200/27733], Loss: 2.9838\n",
      "Epoch [18/300], Step [6300/27733], Loss: 3.0083\n",
      "Epoch [18/300], Step [6400/27733], Loss: 3.3488\n",
      "Epoch [18/300], Step [6500/27733], Loss: 3.0525\n",
      "Epoch [18/300], Step [6600/27733], Loss: 2.8741\n",
      "Epoch [18/300], Step [6700/27733], Loss: 2.9336\n",
      "Epoch [18/300], Step [6800/27733], Loss: 3.3176\n",
      "Epoch [18/300], Step [6900/27733], Loss: 3.4027\n",
      "Epoch [18/300], Step [7000/27733], Loss: 2.3829\n",
      "Epoch [18/300], Step [7100/27733], Loss: 3.0236\n",
      "Epoch [18/300], Step [7200/27733], Loss: 2.6146\n",
      "Epoch [18/300], Step [7300/27733], Loss: 3.3074\n",
      "Epoch [18/300], Step [7400/27733], Loss: 3.7471\n",
      "Epoch [18/300], Step [7500/27733], Loss: 2.6789\n",
      "Epoch [18/300], Step [7600/27733], Loss: 2.9326\n",
      "Epoch [18/300], Step [7700/27733], Loss: 2.8397\n",
      "Epoch [18/300], Step [7800/27733], Loss: 2.9601\n",
      "Epoch [18/300], Step [7900/27733], Loss: 3.2407\n",
      "Epoch [18/300], Step [8000/27733], Loss: 2.7514\n",
      "Epoch [18/300], Step [8100/27733], Loss: 3.0673\n",
      "Epoch [18/300], Step [8200/27733], Loss: 2.4482\n",
      "Epoch [18/300], Step [8300/27733], Loss: 2.6280\n",
      "Epoch [18/300], Step [8400/27733], Loss: 2.8689\n",
      "Epoch [18/300], Step [8500/27733], Loss: 3.1392\n",
      "Epoch [18/300], Step [8600/27733], Loss: 3.4647\n",
      "Epoch [18/300], Step [8700/27733], Loss: 1.8014\n",
      "Epoch [18/300], Step [8800/27733], Loss: 2.3961\n",
      "Epoch [18/300], Step [8900/27733], Loss: 2.6104\n",
      "Epoch [18/300], Step [9000/27733], Loss: 3.2601\n",
      "Epoch [18/300], Step [9100/27733], Loss: 2.4746\n",
      "Epoch [18/300], Step [9200/27733], Loss: 3.1551\n",
      "Epoch [18/300], Step [9300/27733], Loss: 2.4769\n",
      "Epoch [18/300], Step [9400/27733], Loss: 2.7082\n",
      "Epoch [18/300], Step [9500/27733], Loss: 2.1751\n",
      "Epoch [18/300], Step [9600/27733], Loss: 2.9799\n",
      "Epoch [18/300], Step [9700/27733], Loss: 2.5819\n",
      "Epoch [18/300], Step [9800/27733], Loss: 3.0227\n",
      "Epoch [18/300], Step [9900/27733], Loss: 2.5054\n",
      "Epoch [18/300], Step [10000/27733], Loss: 3.6454\n",
      "Epoch [18/300], Step [10100/27733], Loss: 2.3194\n",
      "Epoch [18/300], Step [10200/27733], Loss: 2.7720\n",
      "Epoch [18/300], Step [10300/27733], Loss: 4.1125\n",
      "Epoch [18/300], Step [10400/27733], Loss: 2.9728\n",
      "Epoch [18/300], Step [10500/27733], Loss: 3.0264\n",
      "Epoch [18/300], Step [10600/27733], Loss: 2.3587\n",
      "Epoch [18/300], Step [10700/27733], Loss: 2.5831\n",
      "Epoch [18/300], Step [10800/27733], Loss: 3.9989\n",
      "Epoch [18/300], Step [10900/27733], Loss: 3.1164\n",
      "Epoch [18/300], Step [11000/27733], Loss: 3.4991\n",
      "Epoch [18/300], Step [11100/27733], Loss: 3.6169\n",
      "Epoch [18/300], Step [11200/27733], Loss: 3.3396\n",
      "Epoch [18/300], Step [11300/27733], Loss: 3.0299\n",
      "Epoch [18/300], Step [11400/27733], Loss: 3.7783\n",
      "Epoch [18/300], Step [11500/27733], Loss: 2.6013\n",
      "Epoch [18/300], Step [11600/27733], Loss: 3.4290\n",
      "Epoch [18/300], Step [11700/27733], Loss: 3.7641\n",
      "Epoch [18/300], Step [11800/27733], Loss: 3.0483\n",
      "Epoch [18/300], Step [11900/27733], Loss: 3.0055\n",
      "Epoch [18/300], Step [12000/27733], Loss: 2.7443\n",
      "Epoch [18/300], Step [12100/27733], Loss: 2.3792\n",
      "Epoch [18/300], Step [12200/27733], Loss: 2.7522\n",
      "Epoch [18/300], Step [12300/27733], Loss: 2.3585\n",
      "Epoch [18/300], Step [12400/27733], Loss: 3.1832\n",
      "Epoch [18/300], Step [12500/27733], Loss: 3.3372\n",
      "Epoch [18/300], Step [12600/27733], Loss: 2.8986\n",
      "Epoch [18/300], Step [12700/27733], Loss: 2.7525\n",
      "Epoch [18/300], Step [12800/27733], Loss: 3.7335\n",
      "Epoch [18/300], Step [12900/27733], Loss: 2.2665\n",
      "Epoch [18/300], Step [13000/27733], Loss: 3.6925\n",
      "Epoch [18/300], Step [13100/27733], Loss: 2.6374\n",
      "Epoch [18/300], Step [13200/27733], Loss: 2.8864\n",
      "Epoch [18/300], Step [13300/27733], Loss: 2.4473\n",
      "Epoch [18/300], Step [13400/27733], Loss: 3.0615\n",
      "Epoch [18/300], Step [13500/27733], Loss: 2.6617\n",
      "Epoch [18/300], Step [13600/27733], Loss: 3.1116\n",
      "Epoch [18/300], Step [13700/27733], Loss: 3.1453\n",
      "Epoch [18/300], Step [13800/27733], Loss: 3.7816\n",
      "Epoch [18/300], Step [13900/27733], Loss: 3.6854\n",
      "Epoch [18/300], Step [14000/27733], Loss: 3.3237\n",
      "Epoch [18/300], Step [14100/27733], Loss: 3.6293\n",
      "Epoch [18/300], Step [14200/27733], Loss: 3.7833\n",
      "Epoch [18/300], Step [14300/27733], Loss: 2.6416\n",
      "Epoch [18/300], Step [14400/27733], Loss: 2.9780\n",
      "Epoch [18/300], Step [14500/27733], Loss: 4.0532\n",
      "Epoch [18/300], Step [14600/27733], Loss: 2.6362\n",
      "Epoch [18/300], Step [14700/27733], Loss: 2.9552\n",
      "Epoch [18/300], Step [14800/27733], Loss: 3.2130\n",
      "Epoch [18/300], Step [14900/27733], Loss: 3.2102\n",
      "Epoch [18/300], Step [15000/27733], Loss: 2.7460\n",
      "Epoch [18/300], Step [15100/27733], Loss: 3.1195\n",
      "Epoch [18/300], Step [15200/27733], Loss: 3.2390\n",
      "Epoch [18/300], Step [15300/27733], Loss: 2.2340\n",
      "Epoch [18/300], Step [15400/27733], Loss: 2.9510\n",
      "Epoch [18/300], Step [15500/27733], Loss: 2.8643\n",
      "Epoch [18/300], Step [15600/27733], Loss: 2.9660\n",
      "Epoch [18/300], Step [15700/27733], Loss: 4.5421\n",
      "Epoch [18/300], Step [15800/27733], Loss: 2.8652\n",
      "Epoch [18/300], Step [15900/27733], Loss: 3.7291\n",
      "Epoch [18/300], Step [16000/27733], Loss: 4.3412\n",
      "Epoch [18/300], Step [16100/27733], Loss: 3.9029\n",
      "Epoch [18/300], Step [16200/27733], Loss: 3.3523\n",
      "Epoch [18/300], Step [16300/27733], Loss: 3.6153\n",
      "Epoch [18/300], Step [16400/27733], Loss: 3.4673\n",
      "Epoch [18/300], Step [16500/27733], Loss: 3.6356\n",
      "Epoch [18/300], Step [16600/27733], Loss: 3.5947\n",
      "Epoch [18/300], Step [16700/27733], Loss: 2.9261\n",
      "Epoch [18/300], Step [16800/27733], Loss: 2.6702\n",
      "Epoch [18/300], Step [16900/27733], Loss: 3.3742\n",
      "Epoch [18/300], Step [17000/27733], Loss: 3.4526\n",
      "Epoch [18/300], Step [17100/27733], Loss: 2.6541\n",
      "Epoch [18/300], Step [17200/27733], Loss: 3.2816\n",
      "Epoch [18/300], Step [17300/27733], Loss: 2.9554\n",
      "Epoch [18/300], Step [17400/27733], Loss: 2.7349\n",
      "Epoch [18/300], Step [17500/27733], Loss: 3.7181\n",
      "Epoch [18/300], Step [17600/27733], Loss: 4.2786\n",
      "Epoch [18/300], Step [17700/27733], Loss: 2.8171\n",
      "Epoch [18/300], Step [17800/27733], Loss: 2.8487\n",
      "Epoch [18/300], Step [17900/27733], Loss: 3.6662\n",
      "Epoch [18/300], Step [18000/27733], Loss: 3.7092\n",
      "Epoch [18/300], Step [18100/27733], Loss: 3.0123\n",
      "Epoch [18/300], Step [18200/27733], Loss: 2.6220\n",
      "Epoch [18/300], Step [18300/27733], Loss: 3.6642\n",
      "Epoch [18/300], Step [18400/27733], Loss: 3.0489\n",
      "Epoch [18/300], Step [18500/27733], Loss: 3.4269\n",
      "Epoch [18/300], Step [18600/27733], Loss: 3.4099\n",
      "Epoch [18/300], Step [18700/27733], Loss: 3.0001\n",
      "Epoch [18/300], Step [18800/27733], Loss: 3.1198\n",
      "Epoch [18/300], Step [18900/27733], Loss: 2.7411\n",
      "Epoch [18/300], Step [19000/27733], Loss: 3.8897\n",
      "Epoch [18/300], Step [19100/27733], Loss: 3.8248\n",
      "Epoch [18/300], Step [19200/27733], Loss: 3.0805\n",
      "Epoch [18/300], Step [19300/27733], Loss: 1.7025\n",
      "Epoch [18/300], Step [19400/27733], Loss: 4.3950\n",
      "Epoch [18/300], Step [19500/27733], Loss: 2.5620\n",
      "Epoch [18/300], Step [19600/27733], Loss: 3.7057\n",
      "Epoch [18/300], Step [19700/27733], Loss: 3.5747\n",
      "Epoch [18/300], Step [19800/27733], Loss: 3.3807\n",
      "Epoch [18/300], Step [19900/27733], Loss: 3.4498\n",
      "Epoch [18/300], Step [20000/27733], Loss: 3.1441\n",
      "Epoch [18/300], Step [20100/27733], Loss: 2.8959\n",
      "Epoch [18/300], Step [20200/27733], Loss: 3.1937\n",
      "Epoch [18/300], Step [20300/27733], Loss: 4.1245\n",
      "Epoch [18/300], Step [20400/27733], Loss: 3.3560\n",
      "Epoch [18/300], Step [20500/27733], Loss: 2.5760\n",
      "Epoch [18/300], Step [20600/27733], Loss: 2.7903\n",
      "Epoch [18/300], Step [20700/27733], Loss: 2.7846\n",
      "Epoch [18/300], Step [20800/27733], Loss: 3.6337\n",
      "Epoch [18/300], Step [20900/27733], Loss: 3.6620\n",
      "Epoch [18/300], Step [21000/27733], Loss: 2.5145\n",
      "Epoch [18/300], Step [21100/27733], Loss: 3.5587\n",
      "Epoch [18/300], Step [21200/27733], Loss: 3.8806\n",
      "Epoch [18/300], Step [21300/27733], Loss: 2.0758\n",
      "Epoch [18/300], Step [21400/27733], Loss: 3.5481\n",
      "Epoch [18/300], Step [21500/27733], Loss: 2.9973\n",
      "Epoch [18/300], Step [21600/27733], Loss: 2.8755\n",
      "Epoch [18/300], Step [21700/27733], Loss: 2.7470\n",
      "Epoch [18/300], Step [21800/27733], Loss: 3.3284\n",
      "Epoch [18/300], Step [21900/27733], Loss: 3.1431\n",
      "Epoch [18/300], Step [22000/27733], Loss: 3.9328\n",
      "Epoch [18/300], Step [22100/27733], Loss: 3.6519\n",
      "Epoch [18/300], Step [22200/27733], Loss: 2.5919\n",
      "Epoch [18/300], Step [22300/27733], Loss: 4.0822\n",
      "Epoch [18/300], Step [22400/27733], Loss: 2.8655\n",
      "Epoch [18/300], Step [22500/27733], Loss: 2.9044\n",
      "Epoch [18/300], Step [22600/27733], Loss: 3.8459\n",
      "Epoch [18/300], Step [22700/27733], Loss: 3.7054\n",
      "Epoch [18/300], Step [22800/27733], Loss: 3.0270\n",
      "Epoch [18/300], Step [22900/27733], Loss: 3.3515\n",
      "Epoch [18/300], Step [23000/27733], Loss: 3.4648\n",
      "Epoch [18/300], Step [23100/27733], Loss: 3.2952\n",
      "Epoch [18/300], Step [23200/27733], Loss: 2.9761\n",
      "Epoch [18/300], Step [23300/27733], Loss: 2.9320\n",
      "Epoch [18/300], Step [23400/27733], Loss: 4.0353\n",
      "Epoch [18/300], Step [23500/27733], Loss: 3.2366\n",
      "Epoch [18/300], Step [23600/27733], Loss: 3.7088\n",
      "Epoch [18/300], Step [23700/27733], Loss: 3.0285\n",
      "Epoch [18/300], Step [23800/27733], Loss: 3.6751\n",
      "Epoch [18/300], Step [23900/27733], Loss: 3.7039\n",
      "Epoch [18/300], Step [24000/27733], Loss: 2.4146\n",
      "Epoch [18/300], Step [24100/27733], Loss: 3.4497\n",
      "Epoch [18/300], Step [24200/27733], Loss: 2.9839\n",
      "Epoch [18/300], Step [24300/27733], Loss: 3.5167\n",
      "Epoch [18/300], Step [24400/27733], Loss: 3.8478\n",
      "Epoch [18/300], Step [24500/27733], Loss: 3.0557\n",
      "Epoch [18/300], Step [24600/27733], Loss: 3.0872\n",
      "Epoch [18/300], Step [24700/27733], Loss: 4.3819\n",
      "Epoch [18/300], Step [24800/27733], Loss: 3.2907\n",
      "Epoch [18/300], Step [24900/27733], Loss: 4.1220\n",
      "Epoch [18/300], Step [25000/27733], Loss: 2.9842\n",
      "Epoch [18/300], Step [25100/27733], Loss: 4.2901\n",
      "Epoch [18/300], Step [25200/27733], Loss: 3.2465\n",
      "Epoch [18/300], Step [25300/27733], Loss: 3.1865\n",
      "Epoch [18/300], Step [25400/27733], Loss: 3.2489\n",
      "Epoch [18/300], Step [25500/27733], Loss: 4.5487\n",
      "Epoch [18/300], Step [25600/27733], Loss: 3.2448\n",
      "Epoch [18/300], Step [25700/27733], Loss: 3.6333\n",
      "Epoch [18/300], Step [25800/27733], Loss: 2.9758\n",
      "Epoch [18/300], Step [25900/27733], Loss: 3.0320\n",
      "Epoch [18/300], Step [26000/27733], Loss: 2.4236\n",
      "Epoch [18/300], Step [26100/27733], Loss: 3.4980\n",
      "Epoch [18/300], Step [26200/27733], Loss: 3.6079\n",
      "Epoch [18/300], Step [26300/27733], Loss: 3.2738\n",
      "Epoch [18/300], Step [26400/27733], Loss: 3.1951\n",
      "Epoch [18/300], Step [26500/27733], Loss: 2.4539\n",
      "Epoch [18/300], Step [26600/27733], Loss: 3.1748\n",
      "Epoch [18/300], Step [26700/27733], Loss: 2.9589\n",
      "Epoch [18/300], Step [26800/27733], Loss: 3.2351\n",
      "Epoch [18/300], Step [26900/27733], Loss: 3.5745\n",
      "Epoch [18/300], Step [27000/27733], Loss: 3.7409\n",
      "Epoch [18/300], Step [27100/27733], Loss: 3.9767\n",
      "Epoch [18/300], Step [27200/27733], Loss: 3.3900\n",
      "Epoch [18/300], Step [27300/27733], Loss: 3.1911\n",
      "Epoch [18/300], Step [27400/27733], Loss: 3.3797\n",
      "Epoch [18/300], Step [27500/27733], Loss: 4.3905\n",
      "Epoch [18/300], Step [27600/27733], Loss: 2.7993\n",
      "Epoch [18/300], Step [27700/27733], Loss: 3.8058\n",
      "Epoch [19/300], Step [100/27733], Loss: 2.8720\n",
      "Epoch [19/300], Step [200/27733], Loss: 3.1105\n",
      "Epoch [19/300], Step [300/27733], Loss: 2.5208\n",
      "Epoch [19/300], Step [400/27733], Loss: 2.9179\n",
      "Epoch [19/300], Step [500/27733], Loss: 3.2081\n",
      "Epoch [19/300], Step [600/27733], Loss: 2.5928\n",
      "Epoch [19/300], Step [700/27733], Loss: 2.5077\n",
      "Epoch [19/300], Step [800/27733], Loss: 2.5765\n",
      "Epoch [19/300], Step [900/27733], Loss: 3.5178\n",
      "Epoch [19/300], Step [1000/27733], Loss: 2.5185\n",
      "Epoch [19/300], Step [1100/27733], Loss: 2.5314\n",
      "Epoch [19/300], Step [1200/27733], Loss: 2.4923\n",
      "Epoch [19/300], Step [1300/27733], Loss: 2.8438\n",
      "Epoch [19/300], Step [1400/27733], Loss: 2.7719\n",
      "Epoch [19/300], Step [1500/27733], Loss: 2.2633\n",
      "Epoch [19/300], Step [1600/27733], Loss: 2.6246\n",
      "Epoch [19/300], Step [1700/27733], Loss: 2.6141\n",
      "Epoch [19/300], Step [1800/27733], Loss: 2.8161\n",
      "Epoch [19/300], Step [1900/27733], Loss: 2.0825\n",
      "Epoch [19/300], Step [2000/27733], Loss: 2.2467\n",
      "Epoch [19/300], Step [2100/27733], Loss: 3.3067\n",
      "Epoch [19/300], Step [2200/27733], Loss: 2.9459\n",
      "Epoch [19/300], Step [2300/27733], Loss: 2.7839\n",
      "Epoch [19/300], Step [2400/27733], Loss: 2.9893\n",
      "Epoch [19/300], Step [2500/27733], Loss: 2.9307\n",
      "Epoch [19/300], Step [2600/27733], Loss: 2.6098\n",
      "Epoch [19/300], Step [2700/27733], Loss: 3.0220\n",
      "Epoch [19/300], Step [2800/27733], Loss: 2.3551\n",
      "Epoch [19/300], Step [2900/27733], Loss: 2.6674\n",
      "Epoch [19/300], Step [3000/27733], Loss: 3.2579\n",
      "Epoch [19/300], Step [3100/27733], Loss: 3.1715\n",
      "Epoch [19/300], Step [3200/27733], Loss: 2.2730\n",
      "Epoch [19/300], Step [3300/27733], Loss: 4.1343\n",
      "Epoch [19/300], Step [3400/27733], Loss: 2.9423\n",
      "Epoch [19/300], Step [3500/27733], Loss: 3.1777\n",
      "Epoch [19/300], Step [3600/27733], Loss: 3.3334\n",
      "Epoch [19/300], Step [3700/27733], Loss: 2.2005\n",
      "Epoch [19/300], Step [3800/27733], Loss: 2.8912\n",
      "Epoch [19/300], Step [3900/27733], Loss: 3.3603\n",
      "Epoch [19/300], Step [4000/27733], Loss: 2.5100\n",
      "Epoch [19/300], Step [4100/27733], Loss: 3.3903\n",
      "Epoch [19/300], Step [4200/27733], Loss: 3.3400\n",
      "Epoch [19/300], Step [4300/27733], Loss: 2.1513\n",
      "Epoch [19/300], Step [4400/27733], Loss: 2.6632\n",
      "Epoch [19/300], Step [4500/27733], Loss: 2.6182\n",
      "Epoch [19/300], Step [4600/27733], Loss: 2.7319\n",
      "Epoch [19/300], Step [4700/27733], Loss: 2.4827\n",
      "Epoch [19/300], Step [4800/27733], Loss: 3.7484\n",
      "Epoch [19/300], Step [4900/27733], Loss: 2.8965\n",
      "Epoch [19/300], Step [5000/27733], Loss: 3.3854\n",
      "Epoch [19/300], Step [5100/27733], Loss: 3.3399\n",
      "Epoch [19/300], Step [5200/27733], Loss: 2.9060\n",
      "Epoch [19/300], Step [5300/27733], Loss: 3.2415\n",
      "Epoch [19/300], Step [5400/27733], Loss: 2.7290\n",
      "Epoch [19/300], Step [5500/27733], Loss: 2.5160\n",
      "Epoch [19/300], Step [5600/27733], Loss: 2.6537\n",
      "Epoch [19/300], Step [5700/27733], Loss: 2.8359\n",
      "Epoch [19/300], Step [5800/27733], Loss: 1.9764\n",
      "Epoch [19/300], Step [5900/27733], Loss: 3.0748\n",
      "Epoch [19/300], Step [6000/27733], Loss: 2.4655\n",
      "Epoch [19/300], Step [6100/27733], Loss: 2.8153\n",
      "Epoch [19/300], Step [6200/27733], Loss: 2.8983\n",
      "Epoch [19/300], Step [6300/27733], Loss: 2.6478\n",
      "Epoch [19/300], Step [6400/27733], Loss: 2.9340\n",
      "Epoch [19/300], Step [6500/27733], Loss: 3.4672\n",
      "Epoch [19/300], Step [6600/27733], Loss: 3.3361\n",
      "Epoch [19/300], Step [6700/27733], Loss: 3.3004\n",
      "Epoch [19/300], Step [6800/27733], Loss: 3.2573\n",
      "Epoch [19/300], Step [6900/27733], Loss: 3.2349\n",
      "Epoch [19/300], Step [7000/27733], Loss: 3.2775\n",
      "Epoch [19/300], Step [7100/27733], Loss: 3.3651\n",
      "Epoch [19/300], Step [7200/27733], Loss: 3.5110\n",
      "Epoch [19/300], Step [7300/27733], Loss: 2.6582\n",
      "Epoch [19/300], Step [7400/27733], Loss: 2.2563\n",
      "Epoch [19/300], Step [7500/27733], Loss: 3.2882\n",
      "Epoch [19/300], Step [7600/27733], Loss: 3.0371\n",
      "Epoch [19/300], Step [7700/27733], Loss: 2.7888\n",
      "Epoch [19/300], Step [7800/27733], Loss: 2.8327\n",
      "Epoch [19/300], Step [7900/27733], Loss: 2.6155\n",
      "Epoch [19/300], Step [8000/27733], Loss: 2.9557\n",
      "Epoch [19/300], Step [8100/27733], Loss: 2.5775\n",
      "Epoch [19/300], Step [8200/27733], Loss: 2.3487\n",
      "Epoch [19/300], Step [8300/27733], Loss: 3.3235\n",
      "Epoch [19/300], Step [8400/27733], Loss: 3.1485\n",
      "Epoch [19/300], Step [8500/27733], Loss: 2.3792\n",
      "Epoch [19/300], Step [8600/27733], Loss: 3.2504\n",
      "Epoch [19/300], Step [8700/27733], Loss: 2.2848\n",
      "Epoch [19/300], Step [8800/27733], Loss: 2.5061\n",
      "Epoch [19/300], Step [8900/27733], Loss: 3.5967\n",
      "Epoch [19/300], Step [9000/27733], Loss: 2.6411\n",
      "Epoch [19/300], Step [9100/27733], Loss: 2.3591\n",
      "Epoch [19/300], Step [9200/27733], Loss: 2.9801\n",
      "Epoch [19/300], Step [9300/27733], Loss: 3.3162\n",
      "Epoch [19/300], Step [9400/27733], Loss: 3.2888\n",
      "Epoch [19/300], Step [9500/27733], Loss: 2.8360\n",
      "Epoch [19/300], Step [9600/27733], Loss: 3.1975\n",
      "Epoch [19/300], Step [9700/27733], Loss: 3.0887\n",
      "Epoch [19/300], Step [9800/27733], Loss: 2.9966\n",
      "Epoch [19/300], Step [9900/27733], Loss: 2.5542\n",
      "Epoch [19/300], Step [10000/27733], Loss: 3.7319\n",
      "Epoch [19/300], Step [10100/27733], Loss: 2.6901\n",
      "Epoch [19/300], Step [10200/27733], Loss: 3.4695\n",
      "Epoch [19/300], Step [10300/27733], Loss: 3.0450\n",
      "Epoch [19/300], Step [10400/27733], Loss: 2.9291\n",
      "Epoch [19/300], Step [10500/27733], Loss: 3.3670\n",
      "Epoch [19/300], Step [10600/27733], Loss: 2.4182\n",
      "Epoch [19/300], Step [10700/27733], Loss: 2.6591\n",
      "Epoch [19/300], Step [10800/27733], Loss: 3.3832\n",
      "Epoch [19/300], Step [10900/27733], Loss: 3.0362\n",
      "Epoch [19/300], Step [11000/27733], Loss: 3.1076\n",
      "Epoch [19/300], Step [11100/27733], Loss: 1.9840\n",
      "Epoch [19/300], Step [11200/27733], Loss: 3.6553\n",
      "Epoch [19/300], Step [11300/27733], Loss: 3.3384\n",
      "Epoch [19/300], Step [11400/27733], Loss: 2.9869\n",
      "Epoch [19/300], Step [11500/27733], Loss: 2.9950\n",
      "Epoch [19/300], Step [11600/27733], Loss: 2.4539\n",
      "Epoch [19/300], Step [11700/27733], Loss: 3.5515\n",
      "Epoch [19/300], Step [11800/27733], Loss: 2.8419\n",
      "Epoch [19/300], Step [11900/27733], Loss: 3.4215\n",
      "Epoch [19/300], Step [12000/27733], Loss: 2.9281\n",
      "Epoch [19/300], Step [12100/27733], Loss: 3.7512\n",
      "Epoch [19/300], Step [12200/27733], Loss: 2.8903\n",
      "Epoch [19/300], Step [12300/27733], Loss: 2.8988\n",
      "Epoch [19/300], Step [12400/27733], Loss: 2.8707\n",
      "Epoch [19/300], Step [12500/27733], Loss: 3.4711\n",
      "Epoch [19/300], Step [12600/27733], Loss: 3.2494\n",
      "Epoch [19/300], Step [12700/27733], Loss: 3.5744\n",
      "Epoch [19/300], Step [12800/27733], Loss: 3.0140\n",
      "Epoch [19/300], Step [12900/27733], Loss: 3.7383\n",
      "Epoch [19/300], Step [13000/27733], Loss: 3.8056\n",
      "Epoch [19/300], Step [13100/27733], Loss: 2.7026\n",
      "Epoch [19/300], Step [13200/27733], Loss: 4.3109\n",
      "Epoch [19/300], Step [13300/27733], Loss: 3.0066\n",
      "Epoch [19/300], Step [13400/27733], Loss: 3.3115\n",
      "Epoch [19/300], Step [13500/27733], Loss: 2.8847\n",
      "Epoch [19/300], Step [13600/27733], Loss: 3.1562\n",
      "Epoch [19/300], Step [13700/27733], Loss: 3.1589\n",
      "Epoch [19/300], Step [13800/27733], Loss: 2.6194\n",
      "Epoch [19/300], Step [13900/27733], Loss: 2.6848\n",
      "Epoch [19/300], Step [14000/27733], Loss: 3.2909\n",
      "Epoch [19/300], Step [14100/27733], Loss: 2.8676\n",
      "Epoch [19/300], Step [14200/27733], Loss: 2.9993\n",
      "Epoch [19/300], Step [14300/27733], Loss: 2.8843\n",
      "Epoch [19/300], Step [14400/27733], Loss: 2.8846\n",
      "Epoch [19/300], Step [14500/27733], Loss: 2.6970\n",
      "Epoch [19/300], Step [14600/27733], Loss: 3.3164\n",
      "Epoch [19/300], Step [14700/27733], Loss: 2.9657\n",
      "Epoch [19/300], Step [14800/27733], Loss: 2.8757\n",
      "Epoch [19/300], Step [14900/27733], Loss: 3.0066\n",
      "Epoch [19/300], Step [15000/27733], Loss: 2.6269\n",
      "Epoch [19/300], Step [15100/27733], Loss: 3.7548\n",
      "Epoch [19/300], Step [15200/27733], Loss: 3.2775\n",
      "Epoch [19/300], Step [15300/27733], Loss: 2.6743\n",
      "Epoch [19/300], Step [15400/27733], Loss: 3.2606\n",
      "Epoch [19/300], Step [15500/27733], Loss: 3.6013\n",
      "Epoch [19/300], Step [15600/27733], Loss: 3.2278\n",
      "Epoch [19/300], Step [15700/27733], Loss: 3.1758\n",
      "Epoch [19/300], Step [15800/27733], Loss: 3.1925\n",
      "Epoch [19/300], Step [15900/27733], Loss: 3.5851\n",
      "Epoch [19/300], Step [16000/27733], Loss: 3.7221\n",
      "Epoch [19/300], Step [16100/27733], Loss: 3.1384\n",
      "Epoch [19/300], Step [16200/27733], Loss: 3.1040\n",
      "Epoch [19/300], Step [16300/27733], Loss: 4.2673\n",
      "Epoch [19/300], Step [16400/27733], Loss: 3.5391\n",
      "Epoch [19/300], Step [16500/27733], Loss: 3.0035\n",
      "Epoch [19/300], Step [16600/27733], Loss: 3.6221\n",
      "Epoch [19/300], Step [16700/27733], Loss: 2.8899\n",
      "Epoch [19/300], Step [16800/27733], Loss: 3.0704\n",
      "Epoch [19/300], Step [16900/27733], Loss: 4.2953\n",
      "Epoch [19/300], Step [17000/27733], Loss: 3.2268\n",
      "Epoch [19/300], Step [17100/27733], Loss: 2.5337\n",
      "Epoch [19/300], Step [17200/27733], Loss: 3.3078\n",
      "Epoch [19/300], Step [17300/27733], Loss: 2.9284\n",
      "Epoch [19/300], Step [17400/27733], Loss: 2.7910\n",
      "Epoch [19/300], Step [17500/27733], Loss: 3.2815\n",
      "Epoch [19/300], Step [17600/27733], Loss: 2.2924\n",
      "Epoch [19/300], Step [17700/27733], Loss: 3.6015\n",
      "Epoch [19/300], Step [17800/27733], Loss: 2.6621\n",
      "Epoch [19/300], Step [17900/27733], Loss: 4.3638\n",
      "Epoch [19/300], Step [18000/27733], Loss: 2.7129\n",
      "Epoch [19/300], Step [18100/27733], Loss: 2.1882\n",
      "Epoch [19/300], Step [18200/27733], Loss: 3.3983\n",
      "Epoch [19/300], Step [18300/27733], Loss: 3.8482\n",
      "Epoch [19/300], Step [18400/27733], Loss: 2.4230\n",
      "Epoch [19/300], Step [18500/27733], Loss: 3.3353\n",
      "Epoch [19/300], Step [18600/27733], Loss: 3.3973\n",
      "Epoch [19/300], Step [18700/27733], Loss: 2.9782\n",
      "Epoch [19/300], Step [18800/27733], Loss: 3.3945\n",
      "Epoch [19/300], Step [18900/27733], Loss: 3.1179\n",
      "Epoch [19/300], Step [19000/27733], Loss: 3.3366\n",
      "Epoch [19/300], Step [19100/27733], Loss: 2.9723\n",
      "Epoch [19/300], Step [19200/27733], Loss: 3.4873\n",
      "Epoch [19/300], Step [19300/27733], Loss: 2.3124\n",
      "Epoch [19/300], Step [19400/27733], Loss: 4.1040\n",
      "Epoch [19/300], Step [19500/27733], Loss: 3.3913\n",
      "Epoch [19/300], Step [19600/27733], Loss: 3.2788\n",
      "Epoch [19/300], Step [19700/27733], Loss: 2.7738\n",
      "Epoch [19/300], Step [19800/27733], Loss: 2.9571\n",
      "Epoch [19/300], Step [19900/27733], Loss: 4.1441\n",
      "Epoch [19/300], Step [20000/27733], Loss: 2.7043\n",
      "Epoch [19/300], Step [20100/27733], Loss: 3.0049\n",
      "Epoch [19/300], Step [20200/27733], Loss: 4.0849\n",
      "Epoch [19/300], Step [20300/27733], Loss: 2.8803\n",
      "Epoch [19/300], Step [20400/27733], Loss: 3.8379\n",
      "Epoch [19/300], Step [20500/27733], Loss: 2.7901\n",
      "Epoch [19/300], Step [20600/27733], Loss: 2.8123\n",
      "Epoch [19/300], Step [20700/27733], Loss: 3.3389\n",
      "Epoch [19/300], Step [20800/27733], Loss: 2.9556\n",
      "Epoch [19/300], Step [20900/27733], Loss: 3.5165\n",
      "Epoch [19/300], Step [21000/27733], Loss: 3.2462\n",
      "Epoch [19/300], Step [21100/27733], Loss: 2.3248\n",
      "Epoch [19/300], Step [21200/27733], Loss: 3.2702\n",
      "Epoch [19/300], Step [21300/27733], Loss: 3.3566\n",
      "Epoch [19/300], Step [21400/27733], Loss: 3.9725\n",
      "Epoch [19/300], Step [21500/27733], Loss: 2.9172\n",
      "Epoch [19/300], Step [21600/27733], Loss: 2.6021\n",
      "Epoch [19/300], Step [21700/27733], Loss: 2.8003\n",
      "Epoch [19/300], Step [21800/27733], Loss: 2.5981\n",
      "Epoch [19/300], Step [21900/27733], Loss: 3.6847\n",
      "Epoch [19/300], Step [22000/27733], Loss: 3.1738\n",
      "Epoch [19/300], Step [22100/27733], Loss: 3.9107\n",
      "Epoch [19/300], Step [22200/27733], Loss: 3.4117\n",
      "Epoch [19/300], Step [22300/27733], Loss: 3.5725\n",
      "Epoch [19/300], Step [22400/27733], Loss: 3.1838\n",
      "Epoch [19/300], Step [22500/27733], Loss: 3.7536\n",
      "Epoch [19/300], Step [22600/27733], Loss: 2.8640\n",
      "Epoch [19/300], Step [22700/27733], Loss: 3.6604\n",
      "Epoch [19/300], Step [22800/27733], Loss: 3.6547\n",
      "Epoch [19/300], Step [22900/27733], Loss: 3.5462\n",
      "Epoch [19/300], Step [23000/27733], Loss: 3.8859\n",
      "Epoch [19/300], Step [23100/27733], Loss: 3.6656\n",
      "Epoch [19/300], Step [23200/27733], Loss: 2.9067\n",
      "Epoch [19/300], Step [23300/27733], Loss: 2.9729\n",
      "Epoch [19/300], Step [23400/27733], Loss: 3.7072\n",
      "Epoch [19/300], Step [23500/27733], Loss: 2.0911\n",
      "Epoch [19/300], Step [23600/27733], Loss: 3.7822\n",
      "Epoch [19/300], Step [23700/27733], Loss: 2.8597\n",
      "Epoch [19/300], Step [23800/27733], Loss: 3.2673\n",
      "Epoch [19/300], Step [23900/27733], Loss: 3.1924\n",
      "Epoch [19/300], Step [24000/27733], Loss: 3.5488\n",
      "Epoch [19/300], Step [24100/27733], Loss: 3.9283\n",
      "Epoch [19/300], Step [24200/27733], Loss: 4.3757\n",
      "Epoch [19/300], Step [24300/27733], Loss: 3.3230\n",
      "Epoch [19/300], Step [24400/27733], Loss: 3.2393\n",
      "Epoch [19/300], Step [24500/27733], Loss: 3.3347\n",
      "Epoch [19/300], Step [24600/27733], Loss: 2.9291\n",
      "Epoch [19/300], Step [24700/27733], Loss: 3.0603\n",
      "Epoch [19/300], Step [24800/27733], Loss: 3.1505\n",
      "Epoch [19/300], Step [24900/27733], Loss: 2.9958\n",
      "Epoch [19/300], Step [25000/27733], Loss: 4.1633\n",
      "Epoch [19/300], Step [25100/27733], Loss: 3.1769\n",
      "Epoch [19/300], Step [25200/27733], Loss: 3.8059\n",
      "Epoch [19/300], Step [25300/27733], Loss: 3.3812\n",
      "Epoch [19/300], Step [25400/27733], Loss: 3.8622\n",
      "Epoch [19/300], Step [25500/27733], Loss: 3.3973\n",
      "Epoch [19/300], Step [25600/27733], Loss: 2.9761\n",
      "Epoch [19/300], Step [25700/27733], Loss: 4.3898\n",
      "Epoch [19/300], Step [25800/27733], Loss: 3.4326\n",
      "Epoch [19/300], Step [25900/27733], Loss: 3.5533\n",
      "Epoch [19/300], Step [26000/27733], Loss: 4.1609\n",
      "Epoch [19/300], Step [26100/27733], Loss: 4.0125\n",
      "Epoch [19/300], Step [26200/27733], Loss: 3.5864\n",
      "Epoch [19/300], Step [26300/27733], Loss: 3.6910\n",
      "Epoch [19/300], Step [26400/27733], Loss: 3.8663\n",
      "Epoch [19/300], Step [26500/27733], Loss: 3.5894\n",
      "Epoch [19/300], Step [26600/27733], Loss: 2.2982\n",
      "Epoch [19/300], Step [26700/27733], Loss: 3.7926\n",
      "Epoch [19/300], Step [26800/27733], Loss: 3.0755\n",
      "Epoch [19/300], Step [26900/27733], Loss: 3.3789\n",
      "Epoch [19/300], Step [27000/27733], Loss: 3.5604\n",
      "Epoch [19/300], Step [27100/27733], Loss: 4.0371\n",
      "Epoch [19/300], Step [27200/27733], Loss: 2.7472\n",
      "Epoch [19/300], Step [27300/27733], Loss: 3.3014\n",
      "Epoch [19/300], Step [27400/27733], Loss: 2.8176\n",
      "Epoch [19/300], Step [27500/27733], Loss: 3.9944\n",
      "Epoch [19/300], Step [27600/27733], Loss: 2.2377\n",
      "Epoch [19/300], Step [27700/27733], Loss: 3.6903\n",
      "Epoch [20/300], Step [100/27733], Loss: 2.4440\n",
      "Epoch [20/300], Step [200/27733], Loss: 2.5209\n",
      "Epoch [20/300], Step [300/27733], Loss: 2.5259\n",
      "Epoch [20/300], Step [400/27733], Loss: 2.8469\n",
      "Epoch [20/300], Step [500/27733], Loss: 3.3975\n",
      "Epoch [20/300], Step [600/27733], Loss: 3.6238\n",
      "Epoch [20/300], Step [700/27733], Loss: 3.2754\n",
      "Epoch [20/300], Step [800/27733], Loss: 2.6087\n",
      "Epoch [20/300], Step [900/27733], Loss: 3.6468\n",
      "Epoch [20/300], Step [1000/27733], Loss: 3.2228\n",
      "Epoch [20/300], Step [1100/27733], Loss: 3.3897\n",
      "Epoch [20/300], Step [1200/27733], Loss: 2.3874\n",
      "Epoch [20/300], Step [1300/27733], Loss: 3.3603\n",
      "Epoch [20/300], Step [1400/27733], Loss: 2.7274\n",
      "Epoch [20/300], Step [1500/27733], Loss: 2.4817\n",
      "Epoch [20/300], Step [1600/27733], Loss: 2.6665\n",
      "Epoch [20/300], Step [1700/27733], Loss: 2.6126\n",
      "Epoch [20/300], Step [1800/27733], Loss: 3.2236\n",
      "Epoch [20/300], Step [1900/27733], Loss: 2.2377\n",
      "Epoch [20/300], Step [2000/27733], Loss: 3.5759\n",
      "Epoch [20/300], Step [2100/27733], Loss: 3.1827\n",
      "Epoch [20/300], Step [2200/27733], Loss: 2.8333\n",
      "Epoch [20/300], Step [2300/27733], Loss: 3.0322\n",
      "Epoch [20/300], Step [2400/27733], Loss: 2.6675\n",
      "Epoch [20/300], Step [2500/27733], Loss: 2.6808\n",
      "Epoch [20/300], Step [2600/27733], Loss: 2.5351\n",
      "Epoch [20/300], Step [2700/27733], Loss: 3.0223\n",
      "Epoch [20/300], Step [2800/27733], Loss: 3.4512\n",
      "Epoch [20/300], Step [2900/27733], Loss: 3.0453\n",
      "Epoch [20/300], Step [3000/27733], Loss: 3.5689\n",
      "Epoch [20/300], Step [3100/27733], Loss: 2.0629\n",
      "Epoch [20/300], Step [3200/27733], Loss: 3.0839\n",
      "Epoch [20/300], Step [3300/27733], Loss: 3.6070\n",
      "Epoch [20/300], Step [3400/27733], Loss: 2.4681\n",
      "Epoch [20/300], Step [3500/27733], Loss: 3.5800\n",
      "Epoch [20/300], Step [3600/27733], Loss: 2.0794\n",
      "Epoch [20/300], Step [3700/27733], Loss: 3.5820\n",
      "Epoch [20/300], Step [3800/27733], Loss: 1.9169\n",
      "Epoch [20/300], Step [3900/27733], Loss: 3.1679\n",
      "Epoch [20/300], Step [4000/27733], Loss: 2.8586\n",
      "Epoch [20/300], Step [4100/27733], Loss: 2.3982\n",
      "Epoch [20/300], Step [4200/27733], Loss: 2.9281\n",
      "Epoch [20/300], Step [4300/27733], Loss: 3.3161\n",
      "Epoch [20/300], Step [4400/27733], Loss: 2.4840\n",
      "Epoch [20/300], Step [4500/27733], Loss: 2.7125\n",
      "Epoch [20/300], Step [4600/27733], Loss: 2.9822\n",
      "Epoch [20/300], Step [4700/27733], Loss: 1.9914\n",
      "Epoch [20/300], Step [4800/27733], Loss: 2.3281\n",
      "Epoch [20/300], Step [4900/27733], Loss: 2.7713\n",
      "Epoch [20/300], Step [5000/27733], Loss: 3.6035\n",
      "Epoch [20/300], Step [5100/27733], Loss: 2.4745\n",
      "Epoch [20/300], Step [5200/27733], Loss: 2.7419\n",
      "Epoch [20/300], Step [5300/27733], Loss: 2.5616\n",
      "Epoch [20/300], Step [5400/27733], Loss: 3.0138\n",
      "Epoch [20/300], Step [5500/27733], Loss: 3.7018\n",
      "Epoch [20/300], Step [5600/27733], Loss: 2.9147\n",
      "Epoch [20/300], Step [5700/27733], Loss: 2.6911\n",
      "Epoch [20/300], Step [5800/27733], Loss: 2.4974\n",
      "Epoch [20/300], Step [5900/27733], Loss: 3.0353\n",
      "Epoch [20/300], Step [6000/27733], Loss: 2.7658\n",
      "Epoch [20/300], Step [6100/27733], Loss: 2.9601\n",
      "Epoch [20/300], Step [6200/27733], Loss: 3.2804\n",
      "Epoch [20/300], Step [6300/27733], Loss: 2.7620\n",
      "Epoch [20/300], Step [6400/27733], Loss: 3.2512\n",
      "Epoch [20/300], Step [6500/27733], Loss: 3.3683\n",
      "Epoch [20/300], Step [6600/27733], Loss: 2.2016\n",
      "Epoch [20/300], Step [6700/27733], Loss: 2.2476\n",
      "Epoch [20/300], Step [6800/27733], Loss: 2.9389\n",
      "Epoch [20/300], Step [6900/27733], Loss: 3.3547\n",
      "Epoch [20/300], Step [7000/27733], Loss: 2.8595\n",
      "Epoch [20/300], Step [7100/27733], Loss: 3.7474\n",
      "Epoch [20/300], Step [7200/27733], Loss: 3.4796\n",
      "Epoch [20/300], Step [7300/27733], Loss: 3.9807\n",
      "Epoch [20/300], Step [7400/27733], Loss: 2.5989\n",
      "Epoch [20/300], Step [7500/27733], Loss: 2.9669\n",
      "Epoch [20/300], Step [7600/27733], Loss: 2.2186\n",
      "Epoch [20/300], Step [7700/27733], Loss: 2.4908\n",
      "Epoch [20/300], Step [7800/27733], Loss: 1.9810\n",
      "Epoch [20/300], Step [7900/27733], Loss: 2.8492\n",
      "Epoch [20/300], Step [8000/27733], Loss: 3.5814\n",
      "Epoch [20/300], Step [8100/27733], Loss: 2.9839\n",
      "Epoch [20/300], Step [8200/27733], Loss: 3.6181\n",
      "Epoch [20/300], Step [8300/27733], Loss: 2.8734\n",
      "Epoch [20/300], Step [8400/27733], Loss: 2.5941\n",
      "Epoch [20/300], Step [8500/27733], Loss: 3.2657\n",
      "Epoch [20/300], Step [8600/27733], Loss: 3.2270\n",
      "Epoch [20/300], Step [8700/27733], Loss: 2.8063\n",
      "Epoch [20/300], Step [8800/27733], Loss: 2.7212\n",
      "Epoch [20/300], Step [8900/27733], Loss: 3.2310\n",
      "Epoch [20/300], Step [9000/27733], Loss: 3.0310\n",
      "Epoch [20/300], Step [9100/27733], Loss: 2.4002\n",
      "Epoch [20/300], Step [9200/27733], Loss: 4.0084\n",
      "Epoch [20/300], Step [9300/27733], Loss: 3.7384\n",
      "Epoch [20/300], Step [9400/27733], Loss: 1.8042\n",
      "Epoch [20/300], Step [9500/27733], Loss: 2.8371\n",
      "Epoch [20/300], Step [9600/27733], Loss: 3.2951\n",
      "Epoch [20/300], Step [9700/27733], Loss: 3.8985\n",
      "Epoch [20/300], Step [9800/27733], Loss: 3.7380\n",
      "Epoch [20/300], Step [9900/27733], Loss: 1.8598\n",
      "Epoch [20/300], Step [10000/27733], Loss: 3.1755\n",
      "Epoch [20/300], Step [10100/27733], Loss: 3.2942\n",
      "Epoch [20/300], Step [10200/27733], Loss: 3.3531\n",
      "Epoch [20/300], Step [10300/27733], Loss: 3.4172\n",
      "Epoch [20/300], Step [10400/27733], Loss: 2.6517\n",
      "Epoch [20/300], Step [10500/27733], Loss: 3.0686\n",
      "Epoch [20/300], Step [10600/27733], Loss: 3.0040\n",
      "Epoch [20/300], Step [10700/27733], Loss: 3.2649\n",
      "Epoch [20/300], Step [10800/27733], Loss: 2.7953\n",
      "Epoch [20/300], Step [10900/27733], Loss: 3.4930\n",
      "Epoch [20/300], Step [11000/27733], Loss: 3.3896\n",
      "Epoch [20/300], Step [11100/27733], Loss: 3.0653\n",
      "Epoch [20/300], Step [11200/27733], Loss: 2.9546\n",
      "Epoch [20/300], Step [11300/27733], Loss: 2.8093\n",
      "Epoch [20/300], Step [11400/27733], Loss: 2.4270\n",
      "Epoch [20/300], Step [11500/27733], Loss: 3.3419\n",
      "Epoch [20/300], Step [11600/27733], Loss: 3.5120\n",
      "Epoch [20/300], Step [11700/27733], Loss: 3.7393\n",
      "Epoch [20/300], Step [11800/27733], Loss: 3.6437\n",
      "Epoch [20/300], Step [11900/27733], Loss: 3.3491\n",
      "Epoch [20/300], Step [12000/27733], Loss: 3.4880\n",
      "Epoch [20/300], Step [12100/27733], Loss: 3.7344\n",
      "Epoch [20/300], Step [12200/27733], Loss: 3.1526\n",
      "Epoch [20/300], Step [12300/27733], Loss: 2.6933\n",
      "Epoch [20/300], Step [12400/27733], Loss: 2.8961\n",
      "Epoch [20/300], Step [12500/27733], Loss: 3.4154\n",
      "Epoch [20/300], Step [12600/27733], Loss: 4.0957\n",
      "Epoch [20/300], Step [12700/27733], Loss: 4.1215\n",
      "Epoch [20/300], Step [12800/27733], Loss: 3.1939\n",
      "Epoch [20/300], Step [12900/27733], Loss: 2.6027\n",
      "Epoch [20/300], Step [13000/27733], Loss: 3.2760\n",
      "Epoch [20/300], Step [13100/27733], Loss: 4.4194\n",
      "Epoch [20/300], Step [13200/27733], Loss: 3.5488\n",
      "Epoch [20/300], Step [13300/27733], Loss: 2.6321\n",
      "Epoch [20/300], Step [13400/27733], Loss: 2.3196\n",
      "Epoch [20/300], Step [13500/27733], Loss: 3.9717\n",
      "Epoch [20/300], Step [13600/27733], Loss: 2.9123\n",
      "Epoch [20/300], Step [13700/27733], Loss: 3.1305\n",
      "Epoch [20/300], Step [13800/27733], Loss: 3.0872\n",
      "Epoch [20/300], Step [13900/27733], Loss: 2.7125\n",
      "Epoch [20/300], Step [14000/27733], Loss: 2.7495\n",
      "Epoch [20/300], Step [14100/27733], Loss: 3.4078\n",
      "Epoch [20/300], Step [14200/27733], Loss: 2.7609\n",
      "Epoch [20/300], Step [14300/27733], Loss: 3.1551\n",
      "Epoch [20/300], Step [14400/27733], Loss: 3.7704\n",
      "Epoch [20/300], Step [14500/27733], Loss: 3.3340\n",
      "Epoch [20/300], Step [14600/27733], Loss: 3.2209\n",
      "Epoch [20/300], Step [14700/27733], Loss: 2.7279\n",
      "Epoch [20/300], Step [14800/27733], Loss: 3.0475\n",
      "Epoch [20/300], Step [14900/27733], Loss: 3.9632\n",
      "Epoch [20/300], Step [15000/27733], Loss: 3.2487\n",
      "Epoch [20/300], Step [15100/27733], Loss: 3.2357\n",
      "Epoch [20/300], Step [15200/27733], Loss: 2.9141\n",
      "Epoch [20/300], Step [15300/27733], Loss: 3.4625\n",
      "Epoch [20/300], Step [15400/27733], Loss: 3.6023\n",
      "Epoch [20/300], Step [15500/27733], Loss: 3.3233\n",
      "Epoch [20/300], Step [15600/27733], Loss: 2.7817\n",
      "Epoch [20/300], Step [15700/27733], Loss: 2.3405\n",
      "Epoch [20/300], Step [15800/27733], Loss: 3.7426\n",
      "Epoch [20/300], Step [15900/27733], Loss: 3.6521\n",
      "Epoch [20/300], Step [16000/27733], Loss: 2.6210\n",
      "Epoch [20/300], Step [16100/27733], Loss: 2.7365\n",
      "Epoch [20/300], Step [16200/27733], Loss: 3.0905\n",
      "Epoch [20/300], Step [16300/27733], Loss: 3.4352\n",
      "Epoch [20/300], Step [16400/27733], Loss: 3.3612\n",
      "Epoch [20/300], Step [16500/27733], Loss: 3.2263\n",
      "Epoch [20/300], Step [16600/27733], Loss: 3.7486\n",
      "Epoch [20/300], Step [16700/27733], Loss: 3.2262\n",
      "Epoch [20/300], Step [16800/27733], Loss: 3.2775\n",
      "Epoch [20/300], Step [16900/27733], Loss: 3.2317\n",
      "Epoch [20/300], Step [17000/27733], Loss: 2.5272\n",
      "Epoch [20/300], Step [17100/27733], Loss: 3.4302\n",
      "Epoch [20/300], Step [17200/27733], Loss: 3.8578\n",
      "Epoch [20/300], Step [17300/27733], Loss: 3.2332\n",
      "Epoch [20/300], Step [17400/27733], Loss: 3.6647\n",
      "Epoch [20/300], Step [17500/27733], Loss: 3.0230\n",
      "Epoch [20/300], Step [17600/27733], Loss: 3.2674\n",
      "Epoch [20/300], Step [17700/27733], Loss: 2.8630\n",
      "Epoch [20/300], Step [17800/27733], Loss: 2.9888\n",
      "Epoch [20/300], Step [17900/27733], Loss: 3.5892\n",
      "Epoch [20/300], Step [18000/27733], Loss: 2.7242\n",
      "Epoch [20/300], Step [18100/27733], Loss: 2.6571\n",
      "Epoch [20/300], Step [18200/27733], Loss: 4.4901\n",
      "Epoch [20/300], Step [18300/27733], Loss: 3.0000\n",
      "Epoch [20/300], Step [18400/27733], Loss: 3.2748\n",
      "Epoch [20/300], Step [18500/27733], Loss: 3.1061\n",
      "Epoch [20/300], Step [18600/27733], Loss: 4.5019\n",
      "Epoch [20/300], Step [18700/27733], Loss: 3.6180\n",
      "Epoch [20/300], Step [18800/27733], Loss: 2.9502\n",
      "Epoch [20/300], Step [18900/27733], Loss: 3.8252\n",
      "Epoch [20/300], Step [19000/27733], Loss: 3.5922\n",
      "Epoch [20/300], Step [19100/27733], Loss: 3.1347\n",
      "Epoch [20/300], Step [19200/27733], Loss: 2.6392\n",
      "Epoch [20/300], Step [19300/27733], Loss: 3.6599\n",
      "Epoch [20/300], Step [19400/27733], Loss: 3.0876\n",
      "Epoch [20/300], Step [19500/27733], Loss: 3.7139\n",
      "Epoch [20/300], Step [19600/27733], Loss: 3.0365\n",
      "Epoch [20/300], Step [19700/27733], Loss: 3.5790\n",
      "Epoch [20/300], Step [19800/27733], Loss: 4.4497\n",
      "Epoch [20/300], Step [19900/27733], Loss: 2.5772\n",
      "Epoch [20/300], Step [20000/27733], Loss: 2.8973\n",
      "Epoch [20/300], Step [20100/27733], Loss: 3.3837\n",
      "Epoch [20/300], Step [20200/27733], Loss: 3.3420\n",
      "Epoch [20/300], Step [20300/27733], Loss: 2.6997\n",
      "Epoch [20/300], Step [20400/27733], Loss: 3.1227\n",
      "Epoch [20/300], Step [20500/27733], Loss: 2.7932\n",
      "Epoch [20/300], Step [20600/27733], Loss: 3.5985\n",
      "Epoch [20/300], Step [20700/27733], Loss: 2.6214\n",
      "Epoch [20/300], Step [20800/27733], Loss: 3.2394\n",
      "Epoch [20/300], Step [20900/27733], Loss: 2.9164\n",
      "Epoch [20/300], Step [21000/27733], Loss: 2.3931\n",
      "Epoch [20/300], Step [21100/27733], Loss: 3.7012\n",
      "Epoch [20/300], Step [21200/27733], Loss: 2.9630\n",
      "Epoch [20/300], Step [21300/27733], Loss: 3.3500\n",
      "Epoch [20/300], Step [21400/27733], Loss: 3.5526\n",
      "Epoch [20/300], Step [21500/27733], Loss: 3.5620\n",
      "Epoch [20/300], Step [21600/27733], Loss: 3.2726\n",
      "Epoch [20/300], Step [21700/27733], Loss: 4.2532\n",
      "Epoch [20/300], Step [21800/27733], Loss: 2.6196\n",
      "Epoch [20/300], Step [21900/27733], Loss: 3.4908\n",
      "Epoch [20/300], Step [22000/27733], Loss: 3.5303\n",
      "Epoch [20/300], Step [22100/27733], Loss: 3.4986\n",
      "Epoch [20/300], Step [22200/27733], Loss: 3.1297\n",
      "Epoch [20/300], Step [22300/27733], Loss: 3.6203\n",
      "Epoch [20/300], Step [22400/27733], Loss: 3.4476\n",
      "Epoch [20/300], Step [22500/27733], Loss: 2.5493\n",
      "Epoch [20/300], Step [22600/27733], Loss: 3.7822\n",
      "Epoch [20/300], Step [22700/27733], Loss: 3.0542\n",
      "Epoch [20/300], Step [22800/27733], Loss: 3.6228\n",
      "Epoch [20/300], Step [22900/27733], Loss: 3.3002\n",
      "Epoch [20/300], Step [23000/27733], Loss: 3.2052\n",
      "Epoch [20/300], Step [23100/27733], Loss: 3.0250\n",
      "Epoch [20/300], Step [23200/27733], Loss: 3.2761\n",
      "Epoch [20/300], Step [23300/27733], Loss: 3.6814\n",
      "Epoch [20/300], Step [23400/27733], Loss: 3.5391\n",
      "Epoch [20/300], Step [23500/27733], Loss: 4.0751\n",
      "Epoch [20/300], Step [23600/27733], Loss: 2.9892\n",
      "Epoch [20/300], Step [23700/27733], Loss: 3.0093\n",
      "Epoch [20/300], Step [23800/27733], Loss: 2.5226\n",
      "Epoch [20/300], Step [23900/27733], Loss: 2.6527\n",
      "Epoch [20/300], Step [24000/27733], Loss: 3.0735\n",
      "Epoch [20/300], Step [24100/27733], Loss: 3.1826\n",
      "Epoch [20/300], Step [24200/27733], Loss: 3.1124\n",
      "Epoch [20/300], Step [24300/27733], Loss: 3.0593\n",
      "Epoch [20/300], Step [24400/27733], Loss: 2.7203\n",
      "Epoch [20/300], Step [24500/27733], Loss: 3.4454\n",
      "Epoch [20/300], Step [24600/27733], Loss: 3.3989\n",
      "Epoch [20/300], Step [24700/27733], Loss: 3.3776\n",
      "Epoch [20/300], Step [24800/27733], Loss: 4.1118\n",
      "Epoch [20/300], Step [24900/27733], Loss: 3.1748\n",
      "Epoch [20/300], Step [25000/27733], Loss: 3.7512\n",
      "Epoch [20/300], Step [25100/27733], Loss: 3.9415\n",
      "Epoch [20/300], Step [25200/27733], Loss: 4.8003\n",
      "Epoch [20/300], Step [25300/27733], Loss: 3.2313\n",
      "Epoch [20/300], Step [25400/27733], Loss: 2.3527\n",
      "Epoch [20/300], Step [25500/27733], Loss: 3.6264\n",
      "Epoch [20/300], Step [25600/27733], Loss: 3.0865\n",
      "Epoch [20/300], Step [25700/27733], Loss: 3.2017\n",
      "Epoch [20/300], Step [25800/27733], Loss: 3.8957\n",
      "Epoch [20/300], Step [25900/27733], Loss: 3.4211\n",
      "Epoch [20/300], Step [26000/27733], Loss: 3.7859\n",
      "Epoch [20/300], Step [26100/27733], Loss: 3.0422\n",
      "Epoch [20/300], Step [26200/27733], Loss: 2.7252\n",
      "Epoch [20/300], Step [26300/27733], Loss: 2.5138\n",
      "Epoch [20/300], Step [26400/27733], Loss: 2.7314\n",
      "Epoch [20/300], Step [26500/27733], Loss: 2.4098\n",
      "Epoch [20/300], Step [26600/27733], Loss: 3.0890\n",
      "Epoch [20/300], Step [26700/27733], Loss: 3.5639\n",
      "Epoch [20/300], Step [26800/27733], Loss: 3.7776\n",
      "Epoch [20/300], Step [26900/27733], Loss: 2.7419\n",
      "Epoch [20/300], Step [27000/27733], Loss: 3.3759\n",
      "Epoch [20/300], Step [27100/27733], Loss: 3.1636\n",
      "Epoch [20/300], Step [27200/27733], Loss: 3.6289\n",
      "Epoch [20/300], Step [27300/27733], Loss: 3.0280\n",
      "Epoch [20/300], Step [27400/27733], Loss: 3.7828\n",
      "Epoch [20/300], Step [27500/27733], Loss: 3.4867\n",
      "Epoch [20/300], Step [27600/27733], Loss: 3.6090\n",
      "Epoch [20/300], Step [27700/27733], Loss: 2.6503\n",
      "Epoch [21/300], Step [100/27733], Loss: 2.4522\n",
      "Epoch [21/300], Step [200/27733], Loss: 2.0377\n",
      "Epoch [21/300], Step [300/27733], Loss: 1.8377\n",
      "Epoch [21/300], Step [400/27733], Loss: 2.7330\n",
      "Epoch [21/300], Step [500/27733], Loss: 3.7959\n",
      "Epoch [21/300], Step [600/27733], Loss: 3.2988\n",
      "Epoch [21/300], Step [700/27733], Loss: 2.2552\n",
      "Epoch [21/300], Step [800/27733], Loss: 2.9001\n",
      "Epoch [21/300], Step [900/27733], Loss: 3.1211\n",
      "Epoch [21/300], Step [1000/27733], Loss: 2.1286\n",
      "Epoch [21/300], Step [1100/27733], Loss: 3.5338\n",
      "Epoch [21/300], Step [1200/27733], Loss: 2.4884\n",
      "Epoch [21/300], Step [1300/27733], Loss: 2.0619\n",
      "Epoch [21/300], Step [1400/27733], Loss: 3.1279\n",
      "Epoch [21/300], Step [1500/27733], Loss: 3.6280\n",
      "Epoch [21/300], Step [1600/27733], Loss: 3.4873\n",
      "Epoch [21/300], Step [1700/27733], Loss: 3.4929\n",
      "Epoch [21/300], Step [1800/27733], Loss: 2.5141\n",
      "Epoch [21/300], Step [1900/27733], Loss: 2.3028\n",
      "Epoch [21/300], Step [2000/27733], Loss: 2.8437\n",
      "Epoch [21/300], Step [2100/27733], Loss: 2.6027\n",
      "Epoch [21/300], Step [2200/27733], Loss: 2.6881\n",
      "Epoch [21/300], Step [2300/27733], Loss: 3.2803\n",
      "Epoch [21/300], Step [2400/27733], Loss: 2.9203\n",
      "Epoch [21/300], Step [2500/27733], Loss: 3.0542\n",
      "Epoch [21/300], Step [2600/27733], Loss: 2.6075\n",
      "Epoch [21/300], Step [2700/27733], Loss: 2.9005\n",
      "Epoch [21/300], Step [2800/27733], Loss: 3.2561\n",
      "Epoch [21/300], Step [2900/27733], Loss: 3.2632\n",
      "Epoch [21/300], Step [3000/27733], Loss: 2.9437\n",
      "Epoch [21/300], Step [3100/27733], Loss: 2.7611\n",
      "Epoch [21/300], Step [3200/27733], Loss: 2.6235\n",
      "Epoch [21/300], Step [3300/27733], Loss: 2.6935\n",
      "Epoch [21/300], Step [3400/27733], Loss: 3.0895\n",
      "Epoch [21/300], Step [3500/27733], Loss: 2.4435\n",
      "Epoch [21/300], Step [3600/27733], Loss: 3.1084\n",
      "Epoch [21/300], Step [3700/27733], Loss: 3.8841\n",
      "Epoch [21/300], Step [3800/27733], Loss: 1.8367\n",
      "Epoch [21/300], Step [3900/27733], Loss: 2.9364\n",
      "Epoch [21/300], Step [4000/27733], Loss: 2.9586\n",
      "Epoch [21/300], Step [4100/27733], Loss: 2.6483\n",
      "Epoch [21/300], Step [4200/27733], Loss: 2.8916\n",
      "Epoch [21/300], Step [4300/27733], Loss: 2.6530\n",
      "Epoch [21/300], Step [4400/27733], Loss: 2.8787\n",
      "Epoch [21/300], Step [4500/27733], Loss: 2.1797\n",
      "Epoch [21/300], Step [4600/27733], Loss: 2.8187\n",
      "Epoch [21/300], Step [4700/27733], Loss: 2.4946\n",
      "Epoch [21/300], Step [4800/27733], Loss: 2.6696\n",
      "Epoch [21/300], Step [4900/27733], Loss: 2.6117\n",
      "Epoch [21/300], Step [5000/27733], Loss: 3.8564\n",
      "Epoch [21/300], Step [5100/27733], Loss: 2.4830\n",
      "Epoch [21/300], Step [5200/27733], Loss: 3.0917\n",
      "Epoch [21/300], Step [5300/27733], Loss: 2.1196\n",
      "Epoch [21/300], Step [5400/27733], Loss: 3.3539\n",
      "Epoch [21/300], Step [5500/27733], Loss: 2.2332\n",
      "Epoch [21/300], Step [5600/27733], Loss: 2.9372\n",
      "Epoch [21/300], Step [5700/27733], Loss: 2.5703\n",
      "Epoch [21/300], Step [5800/27733], Loss: 3.0533\n",
      "Epoch [21/300], Step [5900/27733], Loss: 2.7832\n",
      "Epoch [21/300], Step [6000/27733], Loss: 2.9727\n",
      "Epoch [21/300], Step [6100/27733], Loss: 2.3801\n",
      "Epoch [21/300], Step [6200/27733], Loss: 2.5035\n",
      "Epoch [21/300], Step [6300/27733], Loss: 2.8806\n",
      "Epoch [21/300], Step [6400/27733], Loss: 3.1882\n",
      "Epoch [21/300], Step [6500/27733], Loss: 2.4677\n",
      "Epoch [21/300], Step [6600/27733], Loss: 2.6365\n",
      "Epoch [21/300], Step [6700/27733], Loss: 4.2675\n",
      "Epoch [21/300], Step [6800/27733], Loss: 3.3809\n",
      "Epoch [21/300], Step [6900/27733], Loss: 2.9312\n",
      "Epoch [21/300], Step [7000/27733], Loss: 2.6387\n",
      "Epoch [21/300], Step [7100/27733], Loss: 2.4597\n",
      "Epoch [21/300], Step [7200/27733], Loss: 2.6680\n",
      "Epoch [21/300], Step [7300/27733], Loss: 2.3415\n",
      "Epoch [21/300], Step [7400/27733], Loss: 2.8485\n",
      "Epoch [21/300], Step [7500/27733], Loss: 3.1486\n",
      "Epoch [21/300], Step [7600/27733], Loss: 3.5899\n",
      "Epoch [21/300], Step [7700/27733], Loss: 2.5796\n",
      "Epoch [21/300], Step [7800/27733], Loss: 2.3266\n",
      "Epoch [21/300], Step [7900/27733], Loss: 2.7138\n",
      "Epoch [21/300], Step [8000/27733], Loss: 3.0640\n",
      "Epoch [21/300], Step [8100/27733], Loss: 2.8382\n",
      "Epoch [21/300], Step [8200/27733], Loss: 2.5923\n",
      "Epoch [21/300], Step [8300/27733], Loss: 2.8080\n",
      "Epoch [21/300], Step [8400/27733], Loss: 2.3242\n",
      "Epoch [21/300], Step [8500/27733], Loss: 1.7501\n",
      "Epoch [21/300], Step [8600/27733], Loss: 3.9554\n",
      "Epoch [21/300], Step [8700/27733], Loss: 2.8523\n",
      "Epoch [21/300], Step [8800/27733], Loss: 3.5430\n",
      "Epoch [21/300], Step [8900/27733], Loss: 2.9647\n",
      "Epoch [21/300], Step [9000/27733], Loss: 2.5912\n",
      "Epoch [21/300], Step [9100/27733], Loss: 2.7998\n",
      "Epoch [21/300], Step [9200/27733], Loss: 2.9502\n",
      "Epoch [21/300], Step [9300/27733], Loss: 2.1370\n",
      "Epoch [21/300], Step [9400/27733], Loss: 3.0459\n",
      "Epoch [21/300], Step [9500/27733], Loss: 2.4755\n",
      "Epoch [21/300], Step [9600/27733], Loss: 2.8515\n",
      "Epoch [21/300], Step [9700/27733], Loss: 2.9484\n",
      "Epoch [21/300], Step [9800/27733], Loss: 3.0167\n",
      "Epoch [21/300], Step [9900/27733], Loss: 1.7380\n",
      "Epoch [21/300], Step [10000/27733], Loss: 3.4533\n",
      "Epoch [21/300], Step [10100/27733], Loss: 3.4220\n",
      "Epoch [21/300], Step [10200/27733], Loss: 3.3239\n",
      "Epoch [21/300], Step [10300/27733], Loss: 2.7513\n",
      "Epoch [21/300], Step [10400/27733], Loss: 3.1753\n",
      "Epoch [21/300], Step [10500/27733], Loss: 2.9991\n",
      "Epoch [21/300], Step [10600/27733], Loss: 3.8952\n",
      "Epoch [21/300], Step [10700/27733], Loss: 3.9012\n",
      "Epoch [21/300], Step [10800/27733], Loss: 2.9631\n",
      "Epoch [21/300], Step [10900/27733], Loss: 2.8095\n",
      "Epoch [21/300], Step [11000/27733], Loss: 3.7284\n",
      "Epoch [21/300], Step [11100/27733], Loss: 2.0529\n",
      "Epoch [21/300], Step [11200/27733], Loss: 2.7448\n",
      "Epoch [21/300], Step [11300/27733], Loss: 3.0847\n",
      "Epoch [21/300], Step [11400/27733], Loss: 3.5090\n",
      "Epoch [21/300], Step [11500/27733], Loss: 3.4326\n",
      "Epoch [21/300], Step [11600/27733], Loss: 2.6753\n",
      "Epoch [21/300], Step [11700/27733], Loss: 2.9645\n",
      "Epoch [21/300], Step [11800/27733], Loss: 2.8249\n",
      "Epoch [21/300], Step [11900/27733], Loss: 3.6789\n",
      "Epoch [21/300], Step [12000/27733], Loss: 3.0114\n",
      "Epoch [21/300], Step [12100/27733], Loss: 2.6942\n",
      "Epoch [21/300], Step [12200/27733], Loss: 3.6150\n",
      "Epoch [21/300], Step [12300/27733], Loss: 3.1383\n",
      "Epoch [21/300], Step [12400/27733], Loss: 1.7478\n",
      "Epoch [21/300], Step [12500/27733], Loss: 3.5613\n",
      "Epoch [21/300], Step [12600/27733], Loss: 2.7290\n",
      "Epoch [21/300], Step [12700/27733], Loss: 3.6153\n",
      "Epoch [21/300], Step [12800/27733], Loss: 3.2499\n",
      "Epoch [21/300], Step [12900/27733], Loss: 2.4705\n",
      "Epoch [21/300], Step [13000/27733], Loss: 2.1657\n",
      "Epoch [21/300], Step [13100/27733], Loss: 3.5797\n",
      "Epoch [21/300], Step [13200/27733], Loss: 4.1258\n",
      "Epoch [21/300], Step [13300/27733], Loss: 3.1716\n",
      "Epoch [21/300], Step [13400/27733], Loss: 3.1767\n",
      "Epoch [21/300], Step [13500/27733], Loss: 3.5040\n",
      "Epoch [21/300], Step [13600/27733], Loss: 3.1225\n",
      "Epoch [21/300], Step [13700/27733], Loss: 2.7761\n",
      "Epoch [21/300], Step [13800/27733], Loss: 3.3172\n",
      "Epoch [21/300], Step [13900/27733], Loss: 2.9380\n",
      "Epoch [21/300], Step [14000/27733], Loss: 3.9265\n",
      "Epoch [21/300], Step [14100/27733], Loss: 2.4622\n",
      "Epoch [21/300], Step [14200/27733], Loss: 2.4714\n",
      "Epoch [21/300], Step [14300/27733], Loss: 3.6282\n",
      "Epoch [21/300], Step [14400/27733], Loss: 2.6096\n",
      "Epoch [21/300], Step [14500/27733], Loss: 4.2326\n",
      "Epoch [21/300], Step [14600/27733], Loss: 3.0217\n",
      "Epoch [21/300], Step [14700/27733], Loss: 3.3041\n",
      "Epoch [21/300], Step [14800/27733], Loss: 2.4817\n",
      "Epoch [21/300], Step [14900/27733], Loss: 2.9401\n",
      "Epoch [21/300], Step [15000/27733], Loss: 2.6950\n",
      "Epoch [21/300], Step [15100/27733], Loss: 2.9270\n",
      "Epoch [21/300], Step [15200/27733], Loss: 3.5259\n",
      "Epoch [21/300], Step [15300/27733], Loss: 3.4068\n",
      "Epoch [21/300], Step [15400/27733], Loss: 2.6332\n",
      "Epoch [21/300], Step [15500/27733], Loss: 3.0187\n",
      "Epoch [21/300], Step [15600/27733], Loss: 3.1787\n",
      "Epoch [21/300], Step [15700/27733], Loss: 3.4646\n",
      "Epoch [21/300], Step [15800/27733], Loss: 2.5230\n",
      "Epoch [21/300], Step [15900/27733], Loss: 2.9581\n",
      "Epoch [21/300], Step [16000/27733], Loss: 3.1504\n",
      "Epoch [21/300], Step [16100/27733], Loss: 3.1246\n",
      "Epoch [21/300], Step [16200/27733], Loss: 2.5519\n",
      "Epoch [21/300], Step [16300/27733], Loss: 2.8238\n",
      "Epoch [21/300], Step [16400/27733], Loss: 2.6452\n",
      "Epoch [21/300], Step [16500/27733], Loss: 3.6669\n",
      "Epoch [21/300], Step [16600/27733], Loss: 3.4712\n",
      "Epoch [21/300], Step [16700/27733], Loss: 3.4935\n",
      "Epoch [21/300], Step [16800/27733], Loss: 3.4785\n",
      "Epoch [21/300], Step [16900/27733], Loss: 3.0763\n",
      "Epoch [21/300], Step [17000/27733], Loss: 4.1297\n",
      "Epoch [21/300], Step [17100/27733], Loss: 3.3643\n",
      "Epoch [21/300], Step [17200/27733], Loss: 3.1302\n",
      "Epoch [21/300], Step [17300/27733], Loss: 2.7100\n",
      "Epoch [21/300], Step [17400/27733], Loss: 3.4260\n",
      "Epoch [21/300], Step [17500/27733], Loss: 2.5193\n",
      "Epoch [21/300], Step [17600/27733], Loss: 3.8726\n",
      "Epoch [21/300], Step [17700/27733], Loss: 3.2698\n",
      "Epoch [21/300], Step [17800/27733], Loss: 2.8790\n",
      "Epoch [21/300], Step [17900/27733], Loss: 2.8729\n",
      "Epoch [21/300], Step [18000/27733], Loss: 2.7279\n",
      "Epoch [21/300], Step [18100/27733], Loss: 2.9152\n",
      "Epoch [21/300], Step [18200/27733], Loss: 3.3098\n",
      "Epoch [21/300], Step [18300/27733], Loss: 2.4544\n",
      "Epoch [21/300], Step [18400/27733], Loss: 2.9470\n",
      "Epoch [21/300], Step [18500/27733], Loss: 3.2475\n",
      "Epoch [21/300], Step [18600/27733], Loss: 2.8331\n",
      "Epoch [21/300], Step [18700/27733], Loss: 3.4168\n",
      "Epoch [21/300], Step [18800/27733], Loss: 3.8534\n",
      "Epoch [21/300], Step [18900/27733], Loss: 2.6379\n",
      "Epoch [21/300], Step [19000/27733], Loss: 2.9668\n",
      "Epoch [21/300], Step [19100/27733], Loss: 3.1700\n",
      "Epoch [21/300], Step [19200/27733], Loss: 4.1529\n",
      "Epoch [21/300], Step [19300/27733], Loss: 3.9884\n",
      "Epoch [21/300], Step [19400/27733], Loss: 3.4130\n",
      "Epoch [21/300], Step [19500/27733], Loss: 2.8616\n",
      "Epoch [21/300], Step [19600/27733], Loss: 3.7331\n",
      "Epoch [21/300], Step [19700/27733], Loss: 3.2196\n",
      "Epoch [21/300], Step [19800/27733], Loss: 3.8287\n",
      "Epoch [21/300], Step [19900/27733], Loss: 3.0760\n",
      "Epoch [21/300], Step [20000/27733], Loss: 3.1284\n",
      "Epoch [21/300], Step [20100/27733], Loss: 2.1875\n",
      "Epoch [21/300], Step [20200/27733], Loss: 2.7482\n",
      "Epoch [21/300], Step [20300/27733], Loss: 2.9911\n",
      "Epoch [21/300], Step [20400/27733], Loss: 3.2518\n",
      "Epoch [21/300], Step [20500/27733], Loss: 2.6900\n",
      "Epoch [21/300], Step [20600/27733], Loss: 2.4865\n",
      "Epoch [21/300], Step [20700/27733], Loss: 2.9927\n",
      "Epoch [21/300], Step [20800/27733], Loss: 3.6876\n",
      "Epoch [21/300], Step [20900/27733], Loss: 2.6726\n",
      "Epoch [21/300], Step [21000/27733], Loss: 3.7842\n",
      "Epoch [21/300], Step [21100/27733], Loss: 1.5977\n",
      "Epoch [21/300], Step [21200/27733], Loss: 3.5207\n",
      "Epoch [21/300], Step [21300/27733], Loss: 3.1277\n",
      "Epoch [21/300], Step [21400/27733], Loss: 3.1825\n",
      "Epoch [21/300], Step [21500/27733], Loss: 3.4775\n",
      "Epoch [21/300], Step [21600/27733], Loss: 3.0782\n",
      "Epoch [21/300], Step [21700/27733], Loss: 3.6194\n",
      "Epoch [21/300], Step [21800/27733], Loss: 3.3728\n",
      "Epoch [21/300], Step [21900/27733], Loss: 3.0406\n",
      "Epoch [21/300], Step [22000/27733], Loss: 3.1446\n",
      "Epoch [21/300], Step [22100/27733], Loss: 3.2828\n",
      "Epoch [21/300], Step [22200/27733], Loss: 3.1401\n",
      "Epoch [21/300], Step [22300/27733], Loss: 3.2710\n",
      "Epoch [21/300], Step [22400/27733], Loss: 3.1936\n",
      "Epoch [21/300], Step [22500/27733], Loss: 3.0248\n",
      "Epoch [21/300], Step [22600/27733], Loss: 2.9995\n",
      "Epoch [21/300], Step [22700/27733], Loss: 3.2971\n",
      "Epoch [21/300], Step [22800/27733], Loss: 2.6088\n",
      "Epoch [21/300], Step [22900/27733], Loss: 3.1682\n",
      "Epoch [21/300], Step [23000/27733], Loss: 2.9367\n",
      "Epoch [21/300], Step [23100/27733], Loss: 3.5443\n",
      "Epoch [21/300], Step [23200/27733], Loss: 3.9530\n",
      "Epoch [21/300], Step [23300/27733], Loss: 3.7718\n",
      "Epoch [21/300], Step [23400/27733], Loss: 3.7541\n",
      "Epoch [21/300], Step [23500/27733], Loss: 3.4938\n",
      "Epoch [21/300], Step [23600/27733], Loss: 2.7178\n",
      "Epoch [21/300], Step [23700/27733], Loss: 3.1860\n",
      "Epoch [21/300], Step [23800/27733], Loss: 3.4277\n",
      "Epoch [21/300], Step [23900/27733], Loss: 3.0877\n",
      "Epoch [21/300], Step [24000/27733], Loss: 3.2390\n",
      "Epoch [21/300], Step [24100/27733], Loss: 3.3738\n",
      "Epoch [21/300], Step [24200/27733], Loss: 3.4240\n",
      "Epoch [21/300], Step [24300/27733], Loss: 3.2754\n",
      "Epoch [21/300], Step [24400/27733], Loss: 3.2794\n",
      "Epoch [21/300], Step [24500/27733], Loss: 3.0758\n",
      "Epoch [21/300], Step [24600/27733], Loss: 3.3295\n",
      "Epoch [21/300], Step [24700/27733], Loss: 2.7890\n",
      "Epoch [21/300], Step [24800/27733], Loss: 3.7570\n",
      "Epoch [21/300], Step [24900/27733], Loss: 2.7269\n",
      "Epoch [21/300], Step [25000/27733], Loss: 3.4063\n",
      "Epoch [21/300], Step [25100/27733], Loss: 3.6362\n",
      "Epoch [21/300], Step [25200/27733], Loss: 3.4087\n",
      "Epoch [21/300], Step [25300/27733], Loss: 2.1674\n",
      "Epoch [21/300], Step [25400/27733], Loss: 3.7192\n",
      "Epoch [21/300], Step [25500/27733], Loss: 2.0922\n",
      "Epoch [21/300], Step [25600/27733], Loss: 3.4006\n",
      "Epoch [21/300], Step [25700/27733], Loss: 2.8243\n",
      "Epoch [21/300], Step [25800/27733], Loss: 3.5160\n",
      "Epoch [21/300], Step [25900/27733], Loss: 3.2671\n",
      "Epoch [21/300], Step [26000/27733], Loss: 3.8548\n",
      "Epoch [21/300], Step [26100/27733], Loss: 3.4827\n",
      "Epoch [21/300], Step [26200/27733], Loss: 3.4919\n",
      "Epoch [21/300], Step [26300/27733], Loss: 3.7381\n",
      "Epoch [21/300], Step [26400/27733], Loss: 3.1271\n",
      "Epoch [21/300], Step [26500/27733], Loss: 2.4358\n",
      "Epoch [21/300], Step [26600/27733], Loss: 3.7243\n",
      "Epoch [21/300], Step [26700/27733], Loss: 3.2511\n",
      "Epoch [21/300], Step [26800/27733], Loss: 2.5725\n",
      "Epoch [21/300], Step [26900/27733], Loss: 4.1402\n",
      "Epoch [21/300], Step [27000/27733], Loss: 3.6276\n",
      "Epoch [21/300], Step [27100/27733], Loss: 2.9847\n",
      "Epoch [21/300], Step [27200/27733], Loss: 3.1923\n",
      "Epoch [21/300], Step [27300/27733], Loss: 3.8317\n",
      "Epoch [21/300], Step [27400/27733], Loss: 3.2702\n",
      "Epoch [21/300], Step [27500/27733], Loss: 4.1873\n",
      "Epoch [21/300], Step [27600/27733], Loss: 2.6666\n",
      "Epoch [21/300], Step [27700/27733], Loss: 3.4491\n",
      "Epoch [22/300], Step [100/27733], Loss: 2.5722\n",
      "Epoch [22/300], Step [200/27733], Loss: 2.5368\n",
      "Epoch [22/300], Step [300/27733], Loss: 3.3621\n",
      "Epoch [22/300], Step [400/27733], Loss: 2.5355\n",
      "Epoch [22/300], Step [500/27733], Loss: 2.3195\n",
      "Epoch [22/300], Step [600/27733], Loss: 2.0723\n",
      "Epoch [22/300], Step [700/27733], Loss: 2.5943\n",
      "Epoch [22/300], Step [800/27733], Loss: 2.6187\n",
      "Epoch [22/300], Step [900/27733], Loss: 1.9481\n",
      "Epoch [22/300], Step [1000/27733], Loss: 3.1930\n",
      "Epoch [22/300], Step [1100/27733], Loss: 3.0842\n",
      "Epoch [22/300], Step [1200/27733], Loss: 3.0322\n",
      "Epoch [22/300], Step [1300/27733], Loss: 2.5828\n",
      "Epoch [22/300], Step [1400/27733], Loss: 2.8329\n",
      "Epoch [22/300], Step [1500/27733], Loss: 2.9245\n",
      "Epoch [22/300], Step [1600/27733], Loss: 2.7640\n",
      "Epoch [22/300], Step [1700/27733], Loss: 2.8385\n",
      "Epoch [22/300], Step [1800/27733], Loss: 2.9646\n",
      "Epoch [22/300], Step [1900/27733], Loss: 2.1619\n",
      "Epoch [22/300], Step [2000/27733], Loss: 3.0593\n",
      "Epoch [22/300], Step [2100/27733], Loss: 2.7304\n",
      "Epoch [22/300], Step [2200/27733], Loss: 2.7887\n",
      "Epoch [22/300], Step [2300/27733], Loss: 2.8046\n",
      "Epoch [22/300], Step [2400/27733], Loss: 2.6487\n",
      "Epoch [22/300], Step [2500/27733], Loss: 2.0415\n",
      "Epoch [22/300], Step [2600/27733], Loss: 2.1647\n",
      "Epoch [22/300], Step [2700/27733], Loss: 2.8946\n",
      "Epoch [22/300], Step [2800/27733], Loss: 2.6150\n",
      "Epoch [22/300], Step [2900/27733], Loss: 2.6244\n",
      "Epoch [22/300], Step [3000/27733], Loss: 3.7568\n",
      "Epoch [22/300], Step [3100/27733], Loss: 1.9265\n",
      "Epoch [22/300], Step [3200/27733], Loss: 2.8166\n",
      "Epoch [22/300], Step [3300/27733], Loss: 2.7443\n",
      "Epoch [22/300], Step [3400/27733], Loss: 3.2326\n",
      "Epoch [22/300], Step [3500/27733], Loss: 2.6030\n",
      "Epoch [22/300], Step [3600/27733], Loss: 3.0554\n",
      "Epoch [22/300], Step [3700/27733], Loss: 3.2114\n",
      "Epoch [22/300], Step [3800/27733], Loss: 2.9562\n",
      "Epoch [22/300], Step [3900/27733], Loss: 2.8532\n",
      "Epoch [22/300], Step [4000/27733], Loss: 3.0557\n",
      "Epoch [22/300], Step [4100/27733], Loss: 3.2475\n",
      "Epoch [22/300], Step [4200/27733], Loss: 2.5405\n",
      "Epoch [22/300], Step [4300/27733], Loss: 3.8453\n",
      "Epoch [22/300], Step [4400/27733], Loss: 2.4050\n",
      "Epoch [22/300], Step [4500/27733], Loss: 3.1162\n",
      "Epoch [22/300], Step [4600/27733], Loss: 2.6579\n",
      "Epoch [22/300], Step [4700/27733], Loss: 2.8200\n",
      "Epoch [22/300], Step [4800/27733], Loss: 2.6181\n",
      "Epoch [22/300], Step [4900/27733], Loss: 3.0358\n",
      "Epoch [22/300], Step [5000/27733], Loss: 2.6936\n",
      "Epoch [22/300], Step [5100/27733], Loss: 2.4456\n",
      "Epoch [22/300], Step [5200/27733], Loss: 3.5148\n",
      "Epoch [22/300], Step [5300/27733], Loss: 2.6355\n",
      "Epoch [22/300], Step [5400/27733], Loss: 3.0391\n",
      "Epoch [22/300], Step [5500/27733], Loss: 2.9756\n",
      "Epoch [22/300], Step [5600/27733], Loss: 2.9104\n",
      "Epoch [22/300], Step [5700/27733], Loss: 1.9758\n",
      "Epoch [22/300], Step [5800/27733], Loss: 3.2498\n",
      "Epoch [22/300], Step [5900/27733], Loss: 2.5978\n",
      "Epoch [22/300], Step [6000/27733], Loss: 2.9779\n",
      "Epoch [22/300], Step [6100/27733], Loss: 3.4214\n",
      "Epoch [22/300], Step [6200/27733], Loss: 2.9040\n",
      "Epoch [22/300], Step [6300/27733], Loss: 2.9460\n",
      "Epoch [22/300], Step [6400/27733], Loss: 3.7218\n",
      "Epoch [22/300], Step [6500/27733], Loss: 2.7119\n",
      "Epoch [22/300], Step [6600/27733], Loss: 3.1263\n",
      "Epoch [22/300], Step [6700/27733], Loss: 2.9285\n",
      "Epoch [22/300], Step [6800/27733], Loss: 3.5489\n",
      "Epoch [22/300], Step [6900/27733], Loss: 2.6175\n",
      "Epoch [22/300], Step [7000/27733], Loss: 2.0939\n",
      "Epoch [22/300], Step [7100/27733], Loss: 3.0412\n",
      "Epoch [22/300], Step [7200/27733], Loss: 2.8514\n",
      "Epoch [22/300], Step [7300/27733], Loss: 3.6815\n",
      "Epoch [22/300], Step [7400/27733], Loss: 2.3959\n",
      "Epoch [22/300], Step [7500/27733], Loss: 4.0364\n",
      "Epoch [22/300], Step [7600/27733], Loss: 3.4935\n",
      "Epoch [22/300], Step [7700/27733], Loss: 2.1765\n",
      "Epoch [22/300], Step [7800/27733], Loss: 2.6566\n",
      "Epoch [22/300], Step [7900/27733], Loss: 3.8332\n",
      "Epoch [22/300], Step [8000/27733], Loss: 2.7876\n",
      "Epoch [22/300], Step [8100/27733], Loss: 3.7789\n",
      "Epoch [22/300], Step [8200/27733], Loss: 2.3980\n",
      "Epoch [22/300], Step [8300/27733], Loss: 2.4975\n",
      "Epoch [22/300], Step [8400/27733], Loss: 2.7848\n",
      "Epoch [22/300], Step [8500/27733], Loss: 2.4947\n",
      "Epoch [22/300], Step [8600/27733], Loss: 2.8809\n",
      "Epoch [22/300], Step [8700/27733], Loss: 3.1973\n",
      "Epoch [22/300], Step [8800/27733], Loss: 3.3325\n",
      "Epoch [22/300], Step [8900/27733], Loss: 2.9450\n",
      "Epoch [22/300], Step [9000/27733], Loss: 2.6962\n",
      "Epoch [22/300], Step [9100/27733], Loss: 2.6863\n",
      "Epoch [22/300], Step [9200/27733], Loss: 3.6107\n",
      "Epoch [22/300], Step [9300/27733], Loss: 3.0463\n",
      "Epoch [22/300], Step [9400/27733], Loss: 2.5501\n",
      "Epoch [22/300], Step [9500/27733], Loss: 2.2280\n",
      "Epoch [22/300], Step [9600/27733], Loss: 3.4516\n",
      "Epoch [22/300], Step [9700/27733], Loss: 3.2995\n",
      "Epoch [22/300], Step [9800/27733], Loss: 2.9822\n",
      "Epoch [22/300], Step [9900/27733], Loss: 3.6082\n",
      "Epoch [22/300], Step [10000/27733], Loss: 2.9283\n",
      "Epoch [22/300], Step [10100/27733], Loss: 2.2167\n",
      "Epoch [22/300], Step [10200/27733], Loss: 3.1064\n",
      "Epoch [22/300], Step [10300/27733], Loss: 3.2800\n",
      "Epoch [22/300], Step [10400/27733], Loss: 3.5561\n",
      "Epoch [22/300], Step [10500/27733], Loss: 2.7442\n",
      "Epoch [22/300], Step [10600/27733], Loss: 2.7921\n",
      "Epoch [22/300], Step [10700/27733], Loss: 2.9015\n",
      "Epoch [22/300], Step [10800/27733], Loss: 3.1293\n",
      "Epoch [22/300], Step [10900/27733], Loss: 2.3683\n",
      "Epoch [22/300], Step [11000/27733], Loss: 3.0164\n",
      "Epoch [22/300], Step [11100/27733], Loss: 4.0621\n",
      "Epoch [22/300], Step [11200/27733], Loss: 3.0923\n",
      "Epoch [22/300], Step [11300/27733], Loss: 2.5458\n",
      "Epoch [22/300], Step [11400/27733], Loss: 2.5892\n",
      "Epoch [22/300], Step [11500/27733], Loss: 2.7554\n",
      "Epoch [22/300], Step [11600/27733], Loss: 2.8074\n",
      "Epoch [22/300], Step [11700/27733], Loss: 2.8611\n",
      "Epoch [22/300], Step [11800/27733], Loss: 3.7907\n",
      "Epoch [22/300], Step [11900/27733], Loss: 2.4670\n",
      "Epoch [22/300], Step [12000/27733], Loss: 2.8232\n",
      "Epoch [22/300], Step [12100/27733], Loss: 3.1663\n",
      "Epoch [22/300], Step [12200/27733], Loss: 3.2129\n",
      "Epoch [22/300], Step [12300/27733], Loss: 4.0855\n",
      "Epoch [22/300], Step [12400/27733], Loss: 3.2589\n",
      "Epoch [22/300], Step [12500/27733], Loss: 2.9446\n",
      "Epoch [22/300], Step [12600/27733], Loss: 2.5591\n",
      "Epoch [22/300], Step [12700/27733], Loss: 2.7869\n",
      "Epoch [22/300], Step [12800/27733], Loss: 3.2485\n",
      "Epoch [22/300], Step [12900/27733], Loss: 3.2756\n",
      "Epoch [22/300], Step [13000/27733], Loss: 3.0822\n",
      "Epoch [22/300], Step [13100/27733], Loss: 3.7236\n",
      "Epoch [22/300], Step [13200/27733], Loss: 3.0758\n",
      "Epoch [22/300], Step [13300/27733], Loss: 2.8146\n",
      "Epoch [22/300], Step [13400/27733], Loss: 2.1997\n",
      "Epoch [22/300], Step [13500/27733], Loss: 3.5305\n",
      "Epoch [22/300], Step [13600/27733], Loss: 3.4175\n",
      "Epoch [22/300], Step [13700/27733], Loss: 3.0762\n",
      "Epoch [22/300], Step [13800/27733], Loss: 2.8810\n",
      "Epoch [22/300], Step [13900/27733], Loss: 3.1870\n",
      "Epoch [22/300], Step [14000/27733], Loss: 3.8742\n",
      "Epoch [22/300], Step [14100/27733], Loss: 3.4106\n",
      "Epoch [22/300], Step [14200/27733], Loss: 3.2074\n",
      "Epoch [22/300], Step [14300/27733], Loss: 3.0531\n",
      "Epoch [22/300], Step [14400/27733], Loss: 3.2734\n",
      "Epoch [22/300], Step [14500/27733], Loss: 3.2395\n",
      "Epoch [22/300], Step [14600/27733], Loss: 3.0811\n",
      "Epoch [22/300], Step [14700/27733], Loss: 2.9839\n",
      "Epoch [22/300], Step [14800/27733], Loss: 3.2371\n",
      "Epoch [22/300], Step [14900/27733], Loss: 3.1301\n",
      "Epoch [22/300], Step [15000/27733], Loss: 2.7054\n",
      "Epoch [22/300], Step [15100/27733], Loss: 2.8694\n",
      "Epoch [22/300], Step [15200/27733], Loss: 2.9624\n",
      "Epoch [22/300], Step [15300/27733], Loss: 2.6743\n",
      "Epoch [22/300], Step [15400/27733], Loss: 3.4344\n",
      "Epoch [22/300], Step [15500/27733], Loss: 2.6538\n",
      "Epoch [22/300], Step [15600/27733], Loss: 3.3426\n",
      "Epoch [22/300], Step [15700/27733], Loss: 3.9397\n",
      "Epoch [22/300], Step [15800/27733], Loss: 3.7048\n",
      "Epoch [22/300], Step [15900/27733], Loss: 1.9171\n",
      "Epoch [22/300], Step [16000/27733], Loss: 3.0538\n",
      "Epoch [22/300], Step [16100/27733], Loss: 2.4777\n",
      "Epoch [22/300], Step [16200/27733], Loss: 3.1366\n",
      "Epoch [22/300], Step [16300/27733], Loss: 3.5279\n",
      "Epoch [22/300], Step [16400/27733], Loss: 2.5843\n",
      "Epoch [22/300], Step [16500/27733], Loss: 3.6389\n",
      "Epoch [22/300], Step [16600/27733], Loss: 4.0332\n",
      "Epoch [22/300], Step [16700/27733], Loss: 2.8238\n",
      "Epoch [22/300], Step [16800/27733], Loss: 3.4308\n",
      "Epoch [22/300], Step [16900/27733], Loss: 2.6029\n",
      "Epoch [22/300], Step [17000/27733], Loss: 2.2049\n",
      "Epoch [22/300], Step [17100/27733], Loss: 2.6998\n",
      "Epoch [22/300], Step [17200/27733], Loss: 3.2592\n",
      "Epoch [22/300], Step [17300/27733], Loss: 3.1510\n",
      "Epoch [22/300], Step [17400/27733], Loss: 3.0000\n",
      "Epoch [22/300], Step [17500/27733], Loss: 2.8545\n",
      "Epoch [22/300], Step [17600/27733], Loss: 3.2040\n",
      "Epoch [22/300], Step [17700/27733], Loss: 3.4769\n",
      "Epoch [22/300], Step [17800/27733], Loss: 2.6017\n",
      "Epoch [22/300], Step [17900/27733], Loss: 2.8815\n",
      "Epoch [22/300], Step [18000/27733], Loss: 3.1387\n",
      "Epoch [22/300], Step [18100/27733], Loss: 3.3092\n",
      "Epoch [22/300], Step [18200/27733], Loss: 3.2277\n",
      "Epoch [22/300], Step [18300/27733], Loss: 2.2052\n",
      "Epoch [22/300], Step [18400/27733], Loss: 3.3150\n",
      "Epoch [22/300], Step [18500/27733], Loss: 4.1868\n",
      "Epoch [22/300], Step [18600/27733], Loss: 2.5527\n",
      "Epoch [22/300], Step [18700/27733], Loss: 3.3939\n",
      "Epoch [22/300], Step [18800/27733], Loss: 3.2958\n",
      "Epoch [22/300], Step [18900/27733], Loss: 2.9614\n",
      "Epoch [22/300], Step [19000/27733], Loss: 3.0226\n",
      "Epoch [22/300], Step [19100/27733], Loss: 3.3836\n",
      "Epoch [22/300], Step [19200/27733], Loss: 3.1569\n",
      "Epoch [22/300], Step [19300/27733], Loss: 3.1764\n",
      "Epoch [22/300], Step [19400/27733], Loss: 2.9483\n",
      "Epoch [22/300], Step [19500/27733], Loss: 3.2112\n",
      "Epoch [22/300], Step [19600/27733], Loss: 2.7445\n",
      "Epoch [22/300], Step [19700/27733], Loss: 3.2596\n",
      "Epoch [22/300], Step [19800/27733], Loss: 3.2388\n",
      "Epoch [22/300], Step [19900/27733], Loss: 3.3596\n",
      "Epoch [22/300], Step [20000/27733], Loss: 4.1020\n",
      "Epoch [22/300], Step [20100/27733], Loss: 3.6840\n",
      "Epoch [22/300], Step [20200/27733], Loss: 3.8093\n",
      "Epoch [22/300], Step [20300/27733], Loss: 3.6433\n",
      "Epoch [22/300], Step [20400/27733], Loss: 3.0810\n",
      "Epoch [22/300], Step [20500/27733], Loss: 2.9902\n",
      "Epoch [22/300], Step [20600/27733], Loss: 3.0108\n",
      "Epoch [22/300], Step [20700/27733], Loss: 3.1836\n",
      "Epoch [22/300], Step [20800/27733], Loss: 3.8005\n",
      "Epoch [22/300], Step [20900/27733], Loss: 3.4617\n",
      "Epoch [22/300], Step [21000/27733], Loss: 3.5639\n",
      "Epoch [22/300], Step [21100/27733], Loss: 2.5861\n",
      "Epoch [22/300], Step [21200/27733], Loss: 3.2509\n",
      "Epoch [22/300], Step [21300/27733], Loss: 3.7382\n",
      "Epoch [22/300], Step [21400/27733], Loss: 2.3977\n",
      "Epoch [22/300], Step [21500/27733], Loss: 2.3752\n",
      "Epoch [22/300], Step [21600/27733], Loss: 3.2240\n",
      "Epoch [22/300], Step [21700/27733], Loss: 3.3575\n",
      "Epoch [22/300], Step [21800/27733], Loss: 3.3148\n",
      "Epoch [22/300], Step [21900/27733], Loss: 3.5065\n",
      "Epoch [22/300], Step [22000/27733], Loss: 2.5879\n",
      "Epoch [22/300], Step [22100/27733], Loss: 2.8983\n",
      "Epoch [22/300], Step [22200/27733], Loss: 2.9561\n",
      "Epoch [22/300], Step [22300/27733], Loss: 3.3703\n",
      "Epoch [22/300], Step [22400/27733], Loss: 2.8881\n",
      "Epoch [22/300], Step [22500/27733], Loss: 2.3770\n",
      "Epoch [22/300], Step [22600/27733], Loss: 2.9880\n",
      "Epoch [22/300], Step [22700/27733], Loss: 3.1025\n",
      "Epoch [22/300], Step [22800/27733], Loss: 3.5936\n",
      "Epoch [22/300], Step [22900/27733], Loss: 3.4886\n",
      "Epoch [22/300], Step [23000/27733], Loss: 2.8915\n",
      "Epoch [22/300], Step [23100/27733], Loss: 2.8725\n",
      "Epoch [22/300], Step [23200/27733], Loss: 2.9748\n",
      "Epoch [22/300], Step [23300/27733], Loss: 2.6931\n",
      "Epoch [22/300], Step [23400/27733], Loss: 3.1648\n",
      "Epoch [22/300], Step [23500/27733], Loss: 3.2360\n",
      "Epoch [22/300], Step [23600/27733], Loss: 3.8470\n",
      "Epoch [22/300], Step [23700/27733], Loss: 2.5204\n",
      "Epoch [22/300], Step [23800/27733], Loss: 2.5381\n",
      "Epoch [22/300], Step [23900/27733], Loss: 3.8799\n",
      "Epoch [22/300], Step [24000/27733], Loss: 2.9055\n",
      "Epoch [22/300], Step [24100/27733], Loss: 3.5492\n",
      "Epoch [22/300], Step [24200/27733], Loss: 3.0193\n",
      "Epoch [22/300], Step [24300/27733], Loss: 3.4184\n",
      "Epoch [22/300], Step [24400/27733], Loss: 2.8208\n",
      "Epoch [22/300], Step [24500/27733], Loss: 3.4162\n",
      "Epoch [22/300], Step [24600/27733], Loss: 2.4098\n",
      "Epoch [22/300], Step [24700/27733], Loss: 3.6666\n",
      "Epoch [22/300], Step [24800/27733], Loss: 3.4623\n",
      "Epoch [22/300], Step [24900/27733], Loss: 2.8889\n",
      "Epoch [22/300], Step [25000/27733], Loss: 4.2860\n",
      "Epoch [22/300], Step [25100/27733], Loss: 3.2943\n",
      "Epoch [22/300], Step [25200/27733], Loss: 2.7283\n",
      "Epoch [22/300], Step [25300/27733], Loss: 2.9304\n",
      "Epoch [22/300], Step [25400/27733], Loss: 3.7368\n",
      "Epoch [22/300], Step [25500/27733], Loss: 3.1548\n",
      "Epoch [22/300], Step [25600/27733], Loss: 2.8937\n",
      "Epoch [22/300], Step [25700/27733], Loss: 3.6187\n",
      "Epoch [22/300], Step [25800/27733], Loss: 3.0910\n",
      "Epoch [22/300], Step [25900/27733], Loss: 2.6320\n",
      "Epoch [22/300], Step [26000/27733], Loss: 3.5246\n",
      "Epoch [22/300], Step [26100/27733], Loss: 2.1960\n",
      "Epoch [22/300], Step [26200/27733], Loss: 3.2113\n",
      "Epoch [22/300], Step [26300/27733], Loss: 3.4505\n",
      "Epoch [22/300], Step [26400/27733], Loss: 3.1687\n",
      "Epoch [22/300], Step [26500/27733], Loss: 3.4034\n",
      "Epoch [22/300], Step [26600/27733], Loss: 4.0661\n",
      "Epoch [22/300], Step [26700/27733], Loss: 3.9908\n",
      "Epoch [22/300], Step [26800/27733], Loss: 2.8627\n",
      "Epoch [22/300], Step [26900/27733], Loss: 4.2155\n",
      "Epoch [22/300], Step [27000/27733], Loss: 3.5973\n",
      "Epoch [22/300], Step [27100/27733], Loss: 3.1769\n",
      "Epoch [22/300], Step [27200/27733], Loss: 2.9837\n",
      "Epoch [22/300], Step [27300/27733], Loss: 3.3581\n",
      "Epoch [22/300], Step [27400/27733], Loss: 3.2739\n",
      "Epoch [22/300], Step [27500/27733], Loss: 3.5332\n",
      "Epoch [22/300], Step [27600/27733], Loss: 2.4665\n",
      "Epoch [22/300], Step [27700/27733], Loss: 3.6697\n",
      "Epoch [23/300], Step [100/27733], Loss: 2.9785\n",
      "Epoch [23/300], Step [200/27733], Loss: 2.6775\n",
      "Epoch [23/300], Step [300/27733], Loss: 2.1660\n",
      "Epoch [23/300], Step [400/27733], Loss: 2.5319\n",
      "Epoch [23/300], Step [500/27733], Loss: 2.5028\n",
      "Epoch [23/300], Step [600/27733], Loss: 2.6845\n",
      "Epoch [23/300], Step [700/27733], Loss: 2.3451\n",
      "Epoch [23/300], Step [800/27733], Loss: 2.1662\n",
      "Epoch [23/300], Step [900/27733], Loss: 2.4831\n",
      "Epoch [23/300], Step [1000/27733], Loss: 3.2450\n",
      "Epoch [23/300], Step [1100/27733], Loss: 3.1128\n",
      "Epoch [23/300], Step [1200/27733], Loss: 2.8413\n",
      "Epoch [23/300], Step [1300/27733], Loss: 3.0608\n",
      "Epoch [23/300], Step [1400/27733], Loss: 2.7864\n",
      "Epoch [23/300], Step [1500/27733], Loss: 2.8902\n",
      "Epoch [23/300], Step [1600/27733], Loss: 2.1515\n",
      "Epoch [23/300], Step [1700/27733], Loss: 2.5350\n",
      "Epoch [23/300], Step [1800/27733], Loss: 2.7039\n",
      "Epoch [23/300], Step [1900/27733], Loss: 3.3198\n",
      "Epoch [23/300], Step [2000/27733], Loss: 2.9552\n",
      "Epoch [23/300], Step [2100/27733], Loss: 2.1364\n",
      "Epoch [23/300], Step [2200/27733], Loss: 2.2689\n",
      "Epoch [23/300], Step [2300/27733], Loss: 3.0501\n",
      "Epoch [23/300], Step [2400/27733], Loss: 2.9707\n",
      "Epoch [23/300], Step [2500/27733], Loss: 2.3741\n",
      "Epoch [23/300], Step [2600/27733], Loss: 3.2206\n",
      "Epoch [23/300], Step [2700/27733], Loss: 2.4553\n",
      "Epoch [23/300], Step [2800/27733], Loss: 2.2389\n",
      "Epoch [23/300], Step [2900/27733], Loss: 2.5754\n",
      "Epoch [23/300], Step [3000/27733], Loss: 2.4152\n",
      "Epoch [23/300], Step [3100/27733], Loss: 2.6255\n",
      "Epoch [23/300], Step [3200/27733], Loss: 3.0161\n",
      "Epoch [23/300], Step [3300/27733], Loss: 2.7308\n",
      "Epoch [23/300], Step [3400/27733], Loss: 2.9355\n",
      "Epoch [23/300], Step [3500/27733], Loss: 2.1682\n",
      "Epoch [23/300], Step [3600/27733], Loss: 2.7008\n",
      "Epoch [23/300], Step [3700/27733], Loss: 2.9818\n",
      "Epoch [23/300], Step [3800/27733], Loss: 2.6169\n",
      "Epoch [23/300], Step [3900/27733], Loss: 1.9081\n",
      "Epoch [23/300], Step [4000/27733], Loss: 2.5785\n",
      "Epoch [23/300], Step [4100/27733], Loss: 3.0079\n",
      "Epoch [23/300], Step [4200/27733], Loss: 2.6690\n",
      "Epoch [23/300], Step [4300/27733], Loss: 2.7150\n",
      "Epoch [23/300], Step [4400/27733], Loss: 2.3811\n",
      "Epoch [23/300], Step [4500/27733], Loss: 2.6704\n",
      "Epoch [23/300], Step [4600/27733], Loss: 2.8780\n",
      "Epoch [23/300], Step [4700/27733], Loss: 3.4339\n",
      "Epoch [23/300], Step [4800/27733], Loss: 2.7904\n",
      "Epoch [23/300], Step [4900/27733], Loss: 2.8912\n",
      "Epoch [23/300], Step [5000/27733], Loss: 2.8064\n",
      "Epoch [23/300], Step [5100/27733], Loss: 2.5463\n",
      "Epoch [23/300], Step [5200/27733], Loss: 3.8975\n",
      "Epoch [23/300], Step [5300/27733], Loss: 2.2987\n",
      "Epoch [23/300], Step [5400/27733], Loss: 2.7378\n",
      "Epoch [23/300], Step [5500/27733], Loss: 2.9935\n",
      "Epoch [23/300], Step [5600/27733], Loss: 3.1154\n",
      "Epoch [23/300], Step [5700/27733], Loss: 2.5842\n",
      "Epoch [23/300], Step [5800/27733], Loss: 3.9590\n",
      "Epoch [23/300], Step [5900/27733], Loss: 2.9308\n",
      "Epoch [23/300], Step [6000/27733], Loss: 3.1519\n",
      "Epoch [23/300], Step [6100/27733], Loss: 2.7326\n",
      "Epoch [23/300], Step [6200/27733], Loss: 2.5986\n",
      "Epoch [23/300], Step [6300/27733], Loss: 2.7550\n",
      "Epoch [23/300], Step [6400/27733], Loss: 2.5183\n",
      "Epoch [23/300], Step [6500/27733], Loss: 2.3214\n",
      "Epoch [23/300], Step [6600/27733], Loss: 3.8241\n",
      "Epoch [23/300], Step [6700/27733], Loss: 2.7657\n",
      "Epoch [23/300], Step [6800/27733], Loss: 3.2348\n",
      "Epoch [23/300], Step [6900/27733], Loss: 2.8582\n",
      "Epoch [23/300], Step [7000/27733], Loss: 2.5225\n",
      "Epoch [23/300], Step [7100/27733], Loss: 2.6591\n",
      "Epoch [23/300], Step [7200/27733], Loss: 3.3402\n",
      "Epoch [23/300], Step [7300/27733], Loss: 2.9470\n",
      "Epoch [23/300], Step [7400/27733], Loss: 2.2112\n",
      "Epoch [23/300], Step [7500/27733], Loss: 2.5380\n",
      "Epoch [23/300], Step [7600/27733], Loss: 3.2344\n",
      "Epoch [23/300], Step [7700/27733], Loss: 2.2466\n",
      "Epoch [23/300], Step [7800/27733], Loss: 3.3551\n",
      "Epoch [23/300], Step [7900/27733], Loss: 2.5453\n",
      "Epoch [23/300], Step [8000/27733], Loss: 3.0497\n",
      "Epoch [23/300], Step [8100/27733], Loss: 2.9752\n",
      "Epoch [23/300], Step [8200/27733], Loss: 2.9886\n",
      "Epoch [23/300], Step [8300/27733], Loss: 3.8917\n",
      "Epoch [23/300], Step [8400/27733], Loss: 3.4036\n",
      "Epoch [23/300], Step [8500/27733], Loss: 2.5180\n",
      "Epoch [23/300], Step [8600/27733], Loss: 2.8115\n",
      "Epoch [23/300], Step [8700/27733], Loss: 3.5377\n",
      "Epoch [23/300], Step [8800/27733], Loss: 3.4073\n",
      "Epoch [23/300], Step [8900/27733], Loss: 2.7159\n",
      "Epoch [23/300], Step [9000/27733], Loss: 3.1632\n",
      "Epoch [23/300], Step [9100/27733], Loss: 3.6635\n",
      "Epoch [23/300], Step [9200/27733], Loss: 2.6650\n",
      "Epoch [23/300], Step [9300/27733], Loss: 3.5975\n",
      "Epoch [23/300], Step [9400/27733], Loss: 3.0274\n",
      "Epoch [23/300], Step [9500/27733], Loss: 3.2154\n",
      "Epoch [23/300], Step [9600/27733], Loss: 2.9366\n",
      "Epoch [23/300], Step [9700/27733], Loss: 3.0692\n",
      "Epoch [23/300], Step [9800/27733], Loss: 3.0650\n",
      "Epoch [23/300], Step [9900/27733], Loss: 3.0535\n",
      "Epoch [23/300], Step [10000/27733], Loss: 2.1232\n",
      "Epoch [23/300], Step [10100/27733], Loss: 2.9060\n",
      "Epoch [23/300], Step [10200/27733], Loss: 3.0142\n",
      "Epoch [23/300], Step [10300/27733], Loss: 3.0311\n",
      "Epoch [23/300], Step [10400/27733], Loss: 3.1525\n",
      "Epoch [23/300], Step [10500/27733], Loss: 2.6541\n",
      "Epoch [23/300], Step [10600/27733], Loss: 3.7191\n",
      "Epoch [23/300], Step [10700/27733], Loss: 3.2967\n",
      "Epoch [23/300], Step [10800/27733], Loss: 2.8305\n",
      "Epoch [23/300], Step [10900/27733], Loss: 3.0009\n",
      "Epoch [23/300], Step [11000/27733], Loss: 3.2038\n",
      "Epoch [23/300], Step [11100/27733], Loss: 3.1255\n",
      "Epoch [23/300], Step [11200/27733], Loss: 2.5175\n",
      "Epoch [23/300], Step [11300/27733], Loss: 2.3967\n",
      "Epoch [23/300], Step [11400/27733], Loss: 2.8002\n",
      "Epoch [23/300], Step [11500/27733], Loss: 2.7948\n",
      "Epoch [23/300], Step [11600/27733], Loss: 2.2007\n",
      "Epoch [23/300], Step [11700/27733], Loss: 3.0578\n",
      "Epoch [23/300], Step [11800/27733], Loss: 2.9252\n",
      "Epoch [23/300], Step [11900/27733], Loss: 2.5909\n",
      "Epoch [23/300], Step [12000/27733], Loss: 2.9418\n",
      "Epoch [23/300], Step [12100/27733], Loss: 2.9942\n",
      "Epoch [23/300], Step [12200/27733], Loss: 2.7824\n",
      "Epoch [23/300], Step [12300/27733], Loss: 3.0414\n",
      "Epoch [23/300], Step [12400/27733], Loss: 3.2913\n",
      "Epoch [23/300], Step [12500/27733], Loss: 3.1842\n",
      "Epoch [23/300], Step [12600/27733], Loss: 2.8416\n",
      "Epoch [23/300], Step [12700/27733], Loss: 2.4665\n",
      "Epoch [23/300], Step [12800/27733], Loss: 2.8133\n",
      "Epoch [23/300], Step [12900/27733], Loss: 4.2891\n",
      "Epoch [23/300], Step [13000/27733], Loss: 2.5293\n",
      "Epoch [23/300], Step [13100/27733], Loss: 3.2432\n",
      "Epoch [23/300], Step [13200/27733], Loss: 3.5333\n",
      "Epoch [23/300], Step [13300/27733], Loss: 3.6071\n",
      "Epoch [23/300], Step [13400/27733], Loss: 2.3686\n",
      "Epoch [23/300], Step [13500/27733], Loss: 3.4542\n",
      "Epoch [23/300], Step [13600/27733], Loss: 3.1719\n",
      "Epoch [23/300], Step [13700/27733], Loss: 3.0062\n",
      "Epoch [23/300], Step [13800/27733], Loss: 2.5294\n",
      "Epoch [23/300], Step [13900/27733], Loss: 3.4000\n",
      "Epoch [23/300], Step [14000/27733], Loss: 3.7549\n",
      "Epoch [23/300], Step [14100/27733], Loss: 3.1962\n",
      "Epoch [23/300], Step [14200/27733], Loss: 2.5548\n",
      "Epoch [23/300], Step [14300/27733], Loss: 3.1614\n",
      "Epoch [23/300], Step [14400/27733], Loss: 2.6681\n",
      "Epoch [23/300], Step [14500/27733], Loss: 3.8898\n",
      "Epoch [23/300], Step [14600/27733], Loss: 2.2033\n",
      "Epoch [23/300], Step [14700/27733], Loss: 3.9086\n",
      "Epoch [23/300], Step [14800/27733], Loss: 2.2743\n",
      "Epoch [23/300], Step [14900/27733], Loss: 3.1418\n",
      "Epoch [23/300], Step [15000/27733], Loss: 3.3000\n",
      "Epoch [23/300], Step [15100/27733], Loss: 2.9814\n",
      "Epoch [23/300], Step [15200/27733], Loss: 3.4887\n",
      "Epoch [23/300], Step [15300/27733], Loss: 3.0403\n",
      "Epoch [23/300], Step [15400/27733], Loss: 2.5485\n",
      "Epoch [23/300], Step [15500/27733], Loss: 3.5614\n",
      "Epoch [23/300], Step [15600/27733], Loss: 2.9519\n",
      "Epoch [23/300], Step [15700/27733], Loss: 2.6748\n",
      "Epoch [23/300], Step [15800/27733], Loss: 3.0895\n",
      "Epoch [23/300], Step [15900/27733], Loss: 3.4953\n",
      "Epoch [23/300], Step [16000/27733], Loss: 3.4656\n",
      "Epoch [23/300], Step [16100/27733], Loss: 3.2783\n",
      "Epoch [23/300], Step [16200/27733], Loss: 2.0155\n",
      "Epoch [23/300], Step [16300/27733], Loss: 2.7641\n",
      "Epoch [23/300], Step [16400/27733], Loss: 4.2875\n",
      "Epoch [23/300], Step [16500/27733], Loss: 2.8832\n",
      "Epoch [23/300], Step [16600/27733], Loss: 2.8387\n",
      "Epoch [23/300], Step [16700/27733], Loss: 2.5699\n",
      "Epoch [23/300], Step [16800/27733], Loss: 3.7168\n",
      "Epoch [23/300], Step [16900/27733], Loss: 2.9682\n",
      "Epoch [23/300], Step [17000/27733], Loss: 2.3381\n",
      "Epoch [23/300], Step [17100/27733], Loss: 3.6497\n",
      "Epoch [23/300], Step [17200/27733], Loss: 3.1772\n",
      "Epoch [23/300], Step [17300/27733], Loss: 4.1188\n",
      "Epoch [23/300], Step [17400/27733], Loss: 2.4613\n",
      "Epoch [23/300], Step [17500/27733], Loss: 3.1549\n",
      "Epoch [23/300], Step [17600/27733], Loss: 3.2001\n",
      "Epoch [23/300], Step [17700/27733], Loss: 3.5557\n",
      "Epoch [23/300], Step [17800/27733], Loss: 3.1033\n",
      "Epoch [23/300], Step [17900/27733], Loss: 2.0215\n",
      "Epoch [23/300], Step [18000/27733], Loss: 3.1366\n",
      "Epoch [23/300], Step [18100/27733], Loss: 2.9089\n",
      "Epoch [23/300], Step [18200/27733], Loss: 3.4737\n",
      "Epoch [23/300], Step [18300/27733], Loss: 2.2077\n",
      "Epoch [23/300], Step [18400/27733], Loss: 2.9032\n",
      "Epoch [23/300], Step [18500/27733], Loss: 3.2112\n",
      "Epoch [23/300], Step [18600/27733], Loss: 3.0181\n",
      "Epoch [23/300], Step [18700/27733], Loss: 3.3076\n",
      "Epoch [23/300], Step [18800/27733], Loss: 3.1409\n",
      "Epoch [23/300], Step [18900/27733], Loss: 3.5235\n",
      "Epoch [23/300], Step [19000/27733], Loss: 3.2734\n",
      "Epoch [23/300], Step [19100/27733], Loss: 2.6168\n",
      "Epoch [23/300], Step [19200/27733], Loss: 3.2929\n",
      "Epoch [23/300], Step [19300/27733], Loss: 2.7782\n",
      "Epoch [23/300], Step [19400/27733], Loss: 2.6429\n",
      "Epoch [23/300], Step [19500/27733], Loss: 3.6331\n",
      "Epoch [23/300], Step [19600/27733], Loss: 3.5373\n",
      "Epoch [23/300], Step [19700/27733], Loss: 2.8855\n",
      "Epoch [23/300], Step [19800/27733], Loss: 3.1517\n",
      "Epoch [23/300], Step [19900/27733], Loss: 2.6466\n",
      "Epoch [23/300], Step [20000/27733], Loss: 3.1739\n",
      "Epoch [23/300], Step [20100/27733], Loss: 3.7809\n",
      "Epoch [23/300], Step [20200/27733], Loss: 2.5243\n",
      "Epoch [23/300], Step [20300/27733], Loss: 2.9601\n",
      "Epoch [23/300], Step [20400/27733], Loss: 2.5026\n",
      "Epoch [23/300], Step [20500/27733], Loss: 2.9933\n",
      "Epoch [23/300], Step [20600/27733], Loss: 2.7049\n",
      "Epoch [23/300], Step [20700/27733], Loss: 2.9945\n",
      "Epoch [23/300], Step [20800/27733], Loss: 3.5985\n",
      "Epoch [23/300], Step [20900/27733], Loss: 2.6835\n",
      "Epoch [23/300], Step [21000/27733], Loss: 3.3772\n",
      "Epoch [23/300], Step [21100/27733], Loss: 2.9633\n",
      "Epoch [23/300], Step [21200/27733], Loss: 3.5578\n",
      "Epoch [23/300], Step [21300/27733], Loss: 3.1197\n",
      "Epoch [23/300], Step [21400/27733], Loss: 3.2171\n",
      "Epoch [23/300], Step [21500/27733], Loss: 3.0354\n",
      "Epoch [23/300], Step [21600/27733], Loss: 2.7764\n",
      "Epoch [23/300], Step [21700/27733], Loss: 2.3524\n",
      "Epoch [23/300], Step [21800/27733], Loss: 2.8069\n",
      "Epoch [23/300], Step [21900/27733], Loss: 3.3464\n",
      "Epoch [23/300], Step [22000/27733], Loss: 3.3579\n",
      "Epoch [23/300], Step [22100/27733], Loss: 3.4879\n",
      "Epoch [23/300], Step [22200/27733], Loss: 2.8094\n",
      "Epoch [23/300], Step [22300/27733], Loss: 3.1107\n",
      "Epoch [23/300], Step [22400/27733], Loss: 3.1211\n",
      "Epoch [23/300], Step [22500/27733], Loss: 3.0699\n",
      "Epoch [23/300], Step [22600/27733], Loss: 3.2511\n",
      "Epoch [23/300], Step [22700/27733], Loss: 2.7707\n",
      "Epoch [23/300], Step [22800/27733], Loss: 3.4836\n",
      "Epoch [23/300], Step [22900/27733], Loss: 4.3909\n",
      "Epoch [23/300], Step [23000/27733], Loss: 3.4193\n",
      "Epoch [23/300], Step [23100/27733], Loss: 3.1377\n",
      "Epoch [23/300], Step [23200/27733], Loss: 3.7953\n",
      "Epoch [23/300], Step [23300/27733], Loss: 2.5555\n",
      "Epoch [23/300], Step [23400/27733], Loss: 3.4077\n",
      "Epoch [23/300], Step [23500/27733], Loss: 3.0147\n",
      "Epoch [23/300], Step [23600/27733], Loss: 3.1070\n",
      "Epoch [23/300], Step [23700/27733], Loss: 2.4145\n",
      "Epoch [23/300], Step [23800/27733], Loss: 3.9859\n",
      "Epoch [23/300], Step [23900/27733], Loss: 2.8493\n",
      "Epoch [23/300], Step [24000/27733], Loss: 3.4053\n",
      "Epoch [23/300], Step [24100/27733], Loss: 3.4475\n",
      "Epoch [23/300], Step [24200/27733], Loss: 3.3838\n",
      "Epoch [23/300], Step [24300/27733], Loss: 3.4501\n",
      "Epoch [23/300], Step [24400/27733], Loss: 2.6137\n",
      "Epoch [23/300], Step [24500/27733], Loss: 3.1540\n",
      "Epoch [23/300], Step [24600/27733], Loss: 3.1783\n",
      "Epoch [23/300], Step [24700/27733], Loss: 3.4540\n",
      "Epoch [23/300], Step [24800/27733], Loss: 3.6894\n",
      "Epoch [23/300], Step [24900/27733], Loss: 3.7552\n",
      "Epoch [23/300], Step [25000/27733], Loss: 2.9357\n",
      "Epoch [23/300], Step [25100/27733], Loss: 2.7528\n",
      "Epoch [23/300], Step [25200/27733], Loss: 2.2073\n",
      "Epoch [23/300], Step [25300/27733], Loss: 3.3301\n",
      "Epoch [23/300], Step [25400/27733], Loss: 2.7399\n",
      "Epoch [23/300], Step [25500/27733], Loss: 3.4769\n",
      "Epoch [23/300], Step [25600/27733], Loss: 2.5060\n",
      "Epoch [23/300], Step [25700/27733], Loss: 3.0090\n",
      "Epoch [23/300], Step [25800/27733], Loss: 2.5441\n",
      "Epoch [23/300], Step [25900/27733], Loss: 3.1300\n",
      "Epoch [23/300], Step [26000/27733], Loss: 4.1117\n",
      "Epoch [23/300], Step [26100/27733], Loss: 2.9466\n",
      "Epoch [23/300], Step [26200/27733], Loss: 3.0763\n",
      "Epoch [23/300], Step [26300/27733], Loss: 2.9997\n",
      "Epoch [23/300], Step [26400/27733], Loss: 1.9303\n",
      "Epoch [23/300], Step [26500/27733], Loss: 3.7701\n",
      "Epoch [23/300], Step [26600/27733], Loss: 3.3901\n",
      "Epoch [23/300], Step [26700/27733], Loss: 3.2949\n",
      "Epoch [23/300], Step [26800/27733], Loss: 2.8434\n",
      "Epoch [23/300], Step [26900/27733], Loss: 3.4114\n",
      "Epoch [23/300], Step [27000/27733], Loss: 2.6525\n",
      "Epoch [23/300], Step [27100/27733], Loss: 3.4776\n",
      "Epoch [23/300], Step [27200/27733], Loss: 3.7653\n",
      "Epoch [23/300], Step [27300/27733], Loss: 3.3054\n",
      "Epoch [23/300], Step [27400/27733], Loss: 3.0545\n",
      "Epoch [23/300], Step [27500/27733], Loss: 3.5436\n",
      "Epoch [23/300], Step [27600/27733], Loss: 3.0129\n",
      "Epoch [23/300], Step [27700/27733], Loss: 2.8626\n",
      "Epoch [24/300], Step [100/27733], Loss: 2.7907\n",
      "Epoch [24/300], Step [200/27733], Loss: 2.0398\n",
      "Epoch [24/300], Step [300/27733], Loss: 2.7051\n",
      "Epoch [24/300], Step [400/27733], Loss: 2.8182\n",
      "Epoch [24/300], Step [500/27733], Loss: 2.4797\n",
      "Epoch [24/300], Step [600/27733], Loss: 2.3687\n",
      "Epoch [24/300], Step [700/27733], Loss: 2.9158\n",
      "Epoch [24/300], Step [800/27733], Loss: 2.2826\n",
      "Epoch [24/300], Step [900/27733], Loss: 2.5322\n",
      "Epoch [24/300], Step [1000/27733], Loss: 2.3321\n",
      "Epoch [24/300], Step [1100/27733], Loss: 3.2896\n",
      "Epoch [24/300], Step [1200/27733], Loss: 2.4098\n",
      "Epoch [24/300], Step [1300/27733], Loss: 2.5240\n",
      "Epoch [24/300], Step [1400/27733], Loss: 2.3839\n",
      "Epoch [24/300], Step [1500/27733], Loss: 2.4185\n",
      "Epoch [24/300], Step [1600/27733], Loss: 3.0887\n",
      "Epoch [24/300], Step [1700/27733], Loss: 2.4746\n",
      "Epoch [24/300], Step [1800/27733], Loss: 3.3404\n",
      "Epoch [24/300], Step [1900/27733], Loss: 2.7106\n",
      "Epoch [24/300], Step [2000/27733], Loss: 2.7776\n",
      "Epoch [24/300], Step [2100/27733], Loss: 2.6137\n",
      "Epoch [24/300], Step [2200/27733], Loss: 3.0534\n",
      "Epoch [24/300], Step [2300/27733], Loss: 3.0195\n",
      "Epoch [24/300], Step [2400/27733], Loss: 3.2524\n",
      "Epoch [24/300], Step [2500/27733], Loss: 2.3606\n",
      "Epoch [24/300], Step [2600/27733], Loss: 3.3851\n",
      "Epoch [24/300], Step [2700/27733], Loss: 2.4748\n",
      "Epoch [24/300], Step [2800/27733], Loss: 2.4013\n",
      "Epoch [24/300], Step [2900/27733], Loss: 2.2042\n",
      "Epoch [24/300], Step [3000/27733], Loss: 2.6011\n",
      "Epoch [24/300], Step [3100/27733], Loss: 2.6012\n",
      "Epoch [24/300], Step [3200/27733], Loss: 2.4390\n",
      "Epoch [24/300], Step [3300/27733], Loss: 3.2262\n",
      "Epoch [24/300], Step [3400/27733], Loss: 1.9179\n",
      "Epoch [24/300], Step [3500/27733], Loss: 2.4424\n",
      "Epoch [24/300], Step [3600/27733], Loss: 3.3902\n",
      "Epoch [24/300], Step [3700/27733], Loss: 2.4886\n",
      "Epoch [24/300], Step [3800/27733], Loss: 2.0853\n",
      "Epoch [24/300], Step [3900/27733], Loss: 2.3638\n",
      "Epoch [24/300], Step [4000/27733], Loss: 2.2705\n",
      "Epoch [24/300], Step [4100/27733], Loss: 2.7993\n",
      "Epoch [24/300], Step [4200/27733], Loss: 2.6332\n",
      "Epoch [24/300], Step [4300/27733], Loss: 1.8461\n",
      "Epoch [24/300], Step [4400/27733], Loss: 3.1931\n",
      "Epoch [24/300], Step [4500/27733], Loss: 2.4725\n",
      "Epoch [24/300], Step [4600/27733], Loss: 2.9707\n",
      "Epoch [24/300], Step [4700/27733], Loss: 2.6310\n",
      "Epoch [24/300], Step [4800/27733], Loss: 2.4732\n",
      "Epoch [24/300], Step [4900/27733], Loss: 2.1275\n",
      "Epoch [24/300], Step [5000/27733], Loss: 2.9763\n",
      "Epoch [24/300], Step [5100/27733], Loss: 2.3047\n",
      "Epoch [24/300], Step [5200/27733], Loss: 2.3417\n",
      "Epoch [24/300], Step [5300/27733], Loss: 2.6292\n",
      "Epoch [24/300], Step [5400/27733], Loss: 2.8901\n",
      "Epoch [24/300], Step [5500/27733], Loss: 2.4735\n",
      "Epoch [24/300], Step [5600/27733], Loss: 2.8932\n",
      "Epoch [24/300], Step [5700/27733], Loss: 2.4822\n",
      "Epoch [24/300], Step [5800/27733], Loss: 2.6045\n",
      "Epoch [24/300], Step [5900/27733], Loss: 3.4169\n",
      "Epoch [24/300], Step [6000/27733], Loss: 2.7263\n",
      "Epoch [24/300], Step [6100/27733], Loss: 2.4848\n",
      "Epoch [24/300], Step [6200/27733], Loss: 2.8586\n",
      "Epoch [24/300], Step [6300/27733], Loss: 3.7152\n",
      "Epoch [24/300], Step [6400/27733], Loss: 3.2479\n",
      "Epoch [24/300], Step [6500/27733], Loss: 2.6912\n",
      "Epoch [24/300], Step [6600/27733], Loss: 2.5393\n",
      "Epoch [24/300], Step [6700/27733], Loss: 3.0995\n",
      "Epoch [24/300], Step [6800/27733], Loss: 3.1784\n",
      "Epoch [24/300], Step [6900/27733], Loss: 2.5104\n",
      "Epoch [24/300], Step [7000/27733], Loss: 2.8527\n",
      "Epoch [24/300], Step [7100/27733], Loss: 3.0957\n",
      "Epoch [24/300], Step [7200/27733], Loss: 2.5412\n",
      "Epoch [24/300], Step [7300/27733], Loss: 2.2900\n",
      "Epoch [24/300], Step [7400/27733], Loss: 2.7378\n",
      "Epoch [24/300], Step [7500/27733], Loss: 4.5369\n",
      "Epoch [24/300], Step [7600/27733], Loss: 3.3282\n",
      "Epoch [24/300], Step [7700/27733], Loss: 2.8100\n",
      "Epoch [24/300], Step [7800/27733], Loss: 2.9349\n",
      "Epoch [24/300], Step [7900/27733], Loss: 3.2648\n",
      "Epoch [24/300], Step [8000/27733], Loss: 3.2577\n",
      "Epoch [24/300], Step [8100/27733], Loss: 2.9504\n",
      "Epoch [24/300], Step [8200/27733], Loss: 3.3334\n",
      "Epoch [24/300], Step [8300/27733], Loss: 2.6385\n",
      "Epoch [24/300], Step [8400/27733], Loss: 3.0782\n",
      "Epoch [24/300], Step [8500/27733], Loss: 3.3627\n",
      "Epoch [24/300], Step [8600/27733], Loss: 3.4546\n",
      "Epoch [24/300], Step [8700/27733], Loss: 2.1967\n",
      "Epoch [24/300], Step [8800/27733], Loss: 2.9107\n",
      "Epoch [24/300], Step [8900/27733], Loss: 2.8457\n",
      "Epoch [24/300], Step [9000/27733], Loss: 2.8945\n",
      "Epoch [24/300], Step [9100/27733], Loss: 2.6010\n",
      "Epoch [24/300], Step [9200/27733], Loss: 3.1134\n",
      "Epoch [24/300], Step [9300/27733], Loss: 2.1000\n",
      "Epoch [24/300], Step [9400/27733], Loss: 2.0113\n",
      "Epoch [24/300], Step [9500/27733], Loss: 3.2934\n",
      "Epoch [24/300], Step [9600/27733], Loss: 3.0192\n",
      "Epoch [24/300], Step [9700/27733], Loss: 2.0490\n",
      "Epoch [24/300], Step [9800/27733], Loss: 2.6679\n",
      "Epoch [24/300], Step [9900/27733], Loss: 2.7625\n",
      "Epoch [24/300], Step [10000/27733], Loss: 2.3757\n",
      "Epoch [24/300], Step [10100/27733], Loss: 2.8360\n",
      "Epoch [24/300], Step [10200/27733], Loss: 3.5444\n",
      "Epoch [24/300], Step [10300/27733], Loss: 2.5289\n",
      "Epoch [24/300], Step [10400/27733], Loss: 1.8536\n",
      "Epoch [24/300], Step [10500/27733], Loss: 2.6082\n",
      "Epoch [24/300], Step [10600/27733], Loss: 2.9062\n",
      "Epoch [24/300], Step [10700/27733], Loss: 3.2740\n",
      "Epoch [24/300], Step [10800/27733], Loss: 2.1852\n",
      "Epoch [24/300], Step [10900/27733], Loss: 2.6413\n",
      "Epoch [24/300], Step [11000/27733], Loss: 2.8814\n",
      "Epoch [24/300], Step [11100/27733], Loss: 3.2379\n",
      "Epoch [24/300], Step [11200/27733], Loss: 3.1200\n",
      "Epoch [24/300], Step [11300/27733], Loss: 3.3438\n",
      "Epoch [24/300], Step [11400/27733], Loss: 2.5979\n",
      "Epoch [24/300], Step [11500/27733], Loss: 3.2751\n",
      "Epoch [24/300], Step [11600/27733], Loss: 3.1148\n",
      "Epoch [24/300], Step [11700/27733], Loss: 3.1351\n",
      "Epoch [24/300], Step [11800/27733], Loss: 3.2602\n",
      "Epoch [24/300], Step [11900/27733], Loss: 2.3570\n",
      "Epoch [24/300], Step [12000/27733], Loss: 3.6799\n",
      "Epoch [24/300], Step [12100/27733], Loss: 3.2330\n",
      "Epoch [24/300], Step [12200/27733], Loss: 2.7831\n",
      "Epoch [24/300], Step [12300/27733], Loss: 2.4240\n",
      "Epoch [24/300], Step [12400/27733], Loss: 2.6521\n",
      "Epoch [24/300], Step [12500/27733], Loss: 3.1977\n",
      "Epoch [24/300], Step [12600/27733], Loss: 2.3953\n",
      "Epoch [24/300], Step [12700/27733], Loss: 2.5066\n",
      "Epoch [24/300], Step [12800/27733], Loss: 3.3460\n",
      "Epoch [24/300], Step [12900/27733], Loss: 3.3313\n",
      "Epoch [24/300], Step [13000/27733], Loss: 3.4539\n",
      "Epoch [24/300], Step [13100/27733], Loss: 2.5598\n",
      "Epoch [24/300], Step [13200/27733], Loss: 2.6595\n",
      "Epoch [24/300], Step [13300/27733], Loss: 2.8040\n",
      "Epoch [24/300], Step [13400/27733], Loss: 2.7965\n",
      "Epoch [24/300], Step [13500/27733], Loss: 3.4404\n",
      "Epoch [24/300], Step [13600/27733], Loss: 2.7120\n",
      "Epoch [24/300], Step [13700/27733], Loss: 3.3202\n",
      "Epoch [24/300], Step [13800/27733], Loss: 3.7205\n",
      "Epoch [24/300], Step [13900/27733], Loss: 3.2263\n",
      "Epoch [24/300], Step [14000/27733], Loss: 2.4038\n",
      "Epoch [24/300], Step [14100/27733], Loss: 2.5007\n",
      "Epoch [24/300], Step [14200/27733], Loss: 2.7258\n",
      "Epoch [24/300], Step [14300/27733], Loss: 2.0236\n",
      "Epoch [24/300], Step [14400/27733], Loss: 3.1992\n",
      "Epoch [24/300], Step [14500/27733], Loss: 3.5651\n",
      "Epoch [24/300], Step [14600/27733], Loss: 2.6302\n",
      "Epoch [24/300], Step [14700/27733], Loss: 3.0537\n",
      "Epoch [24/300], Step [14800/27733], Loss: 3.2322\n",
      "Epoch [24/300], Step [14900/27733], Loss: 3.6415\n",
      "Epoch [24/300], Step [15000/27733], Loss: 3.0759\n",
      "Epoch [24/300], Step [15100/27733], Loss: 3.2179\n",
      "Epoch [24/300], Step [15200/27733], Loss: 2.8463\n",
      "Epoch [24/300], Step [15300/27733], Loss: 3.1742\n",
      "Epoch [24/300], Step [15400/27733], Loss: 3.0021\n",
      "Epoch [24/300], Step [15500/27733], Loss: 2.6805\n",
      "Epoch [24/300], Step [15600/27733], Loss: 3.1553\n",
      "Epoch [24/300], Step [15700/27733], Loss: 2.8101\n",
      "Epoch [24/300], Step [15800/27733], Loss: 2.9085\n",
      "Epoch [24/300], Step [15900/27733], Loss: 3.1906\n",
      "Epoch [24/300], Step [16000/27733], Loss: 2.5658\n",
      "Epoch [24/300], Step [16100/27733], Loss: 2.7360\n",
      "Epoch [24/300], Step [16200/27733], Loss: 3.5668\n",
      "Epoch [24/300], Step [16300/27733], Loss: 2.8151\n",
      "Epoch [24/300], Step [16400/27733], Loss: 3.2503\n",
      "Epoch [24/300], Step [16500/27733], Loss: 3.1933\n",
      "Epoch [24/300], Step [16600/27733], Loss: 2.6918\n",
      "Epoch [24/300], Step [16700/27733], Loss: 3.4473\n",
      "Epoch [24/300], Step [16800/27733], Loss: 2.2110\n",
      "Epoch [24/300], Step [16900/27733], Loss: 3.0949\n",
      "Epoch [24/300], Step [17000/27733], Loss: 3.2092\n",
      "Epoch [24/300], Step [17100/27733], Loss: 3.2185\n",
      "Epoch [24/300], Step [17200/27733], Loss: 2.7273\n",
      "Epoch [24/300], Step [17300/27733], Loss: 2.8795\n",
      "Epoch [24/300], Step [17400/27733], Loss: 2.7664\n",
      "Epoch [24/300], Step [17500/27733], Loss: 3.1045\n",
      "Epoch [24/300], Step [17600/27733], Loss: 3.4540\n",
      "Epoch [24/300], Step [17700/27733], Loss: 2.9715\n",
      "Epoch [24/300], Step [17800/27733], Loss: 2.6532\n",
      "Epoch [24/300], Step [17900/27733], Loss: 2.9132\n",
      "Epoch [24/300], Step [18000/27733], Loss: 3.2928\n",
      "Epoch [24/300], Step [18100/27733], Loss: 2.1876\n",
      "Epoch [24/300], Step [18200/27733], Loss: 2.7185\n",
      "Epoch [24/300], Step [18300/27733], Loss: 3.2077\n",
      "Epoch [24/300], Step [18400/27733], Loss: 3.3236\n",
      "Epoch [24/300], Step [18500/27733], Loss: 2.0968\n",
      "Epoch [24/300], Step [18600/27733], Loss: 3.7224\n",
      "Epoch [24/300], Step [18700/27733], Loss: 3.4610\n",
      "Epoch [24/300], Step [18800/27733], Loss: 3.2602\n",
      "Epoch [24/300], Step [18900/27733], Loss: 4.0266\n",
      "Epoch [24/300], Step [19000/27733], Loss: 2.7459\n",
      "Epoch [24/300], Step [19100/27733], Loss: 2.9499\n",
      "Epoch [24/300], Step [19200/27733], Loss: 3.4526\n",
      "Epoch [24/300], Step [19300/27733], Loss: 2.6587\n",
      "Epoch [24/300], Step [19400/27733], Loss: 3.6566\n",
      "Epoch [24/300], Step [19500/27733], Loss: 2.8820\n",
      "Epoch [24/300], Step [19600/27733], Loss: 2.5314\n",
      "Epoch [24/300], Step [19700/27733], Loss: 3.5630\n",
      "Epoch [24/300], Step [19800/27733], Loss: 3.3048\n",
      "Epoch [24/300], Step [19900/27733], Loss: 3.0356\n",
      "Epoch [24/300], Step [20000/27733], Loss: 3.0190\n",
      "Epoch [24/300], Step [20100/27733], Loss: 2.6018\n",
      "Epoch [24/300], Step [20200/27733], Loss: 3.3493\n",
      "Epoch [24/300], Step [20300/27733], Loss: 3.1763\n",
      "Epoch [24/300], Step [20400/27733], Loss: 3.4006\n",
      "Epoch [24/300], Step [20500/27733], Loss: 3.8407\n",
      "Epoch [24/300], Step [20600/27733], Loss: 3.6176\n",
      "Epoch [24/300], Step [20700/27733], Loss: 2.5711\n",
      "Epoch [24/300], Step [20800/27733], Loss: 3.1976\n",
      "Epoch [24/300], Step [20900/27733], Loss: 2.9635\n",
      "Epoch [24/300], Step [21000/27733], Loss: 3.6414\n",
      "Epoch [24/300], Step [21100/27733], Loss: 3.3834\n",
      "Epoch [24/300], Step [21200/27733], Loss: 3.3829\n",
      "Epoch [24/300], Step [21300/27733], Loss: 3.1285\n",
      "Epoch [24/300], Step [21400/27733], Loss: 3.2674\n",
      "Epoch [24/300], Step [21500/27733], Loss: 3.3392\n",
      "Epoch [24/300], Step [21600/27733], Loss: 2.7908\n",
      "Epoch [24/300], Step [21700/27733], Loss: 2.4875\n",
      "Epoch [24/300], Step [21800/27733], Loss: 3.2120\n",
      "Epoch [24/300], Step [21900/27733], Loss: 2.8169\n",
      "Epoch [24/300], Step [22000/27733], Loss: 2.8014\n",
      "Epoch [24/300], Step [22100/27733], Loss: 3.3839\n",
      "Epoch [24/300], Step [22200/27733], Loss: 2.4182\n",
      "Epoch [24/300], Step [22300/27733], Loss: 3.5808\n",
      "Epoch [24/300], Step [22400/27733], Loss: 2.7880\n",
      "Epoch [24/300], Step [22500/27733], Loss: 3.4055\n",
      "Epoch [24/300], Step [22600/27733], Loss: 3.5084\n",
      "Epoch [24/300], Step [22700/27733], Loss: 3.1269\n",
      "Epoch [24/300], Step [22800/27733], Loss: 3.6377\n",
      "Epoch [24/300], Step [22900/27733], Loss: 3.3581\n",
      "Epoch [24/300], Step [23000/27733], Loss: 3.0436\n",
      "Epoch [24/300], Step [23100/27733], Loss: 2.9474\n",
      "Epoch [24/300], Step [23200/27733], Loss: 2.8499\n",
      "Epoch [24/300], Step [23300/27733], Loss: 3.3611\n",
      "Epoch [24/300], Step [23400/27733], Loss: 3.6503\n",
      "Epoch [24/300], Step [23500/27733], Loss: 3.8451\n",
      "Epoch [24/300], Step [23600/27733], Loss: 3.4438\n",
      "Epoch [24/300], Step [23700/27733], Loss: 3.8476\n",
      "Epoch [24/300], Step [23800/27733], Loss: 2.4472\n",
      "Epoch [24/300], Step [23900/27733], Loss: 2.9498\n",
      "Epoch [24/300], Step [24000/27733], Loss: 2.9235\n",
      "Epoch [24/300], Step [24100/27733], Loss: 2.8333\n",
      "Epoch [24/300], Step [24200/27733], Loss: 2.8442\n",
      "Epoch [24/300], Step [24300/27733], Loss: 4.1179\n",
      "Epoch [24/300], Step [24400/27733], Loss: 2.4753\n",
      "Epoch [24/300], Step [24500/27733], Loss: 3.0040\n",
      "Epoch [24/300], Step [24600/27733], Loss: 3.6989\n",
      "Epoch [24/300], Step [24700/27733], Loss: 3.5945\n",
      "Epoch [24/300], Step [24800/27733], Loss: 3.6066\n",
      "Epoch [24/300], Step [24900/27733], Loss: 2.7337\n",
      "Epoch [24/300], Step [25000/27733], Loss: 3.8896\n",
      "Epoch [24/300], Step [25100/27733], Loss: 3.2437\n",
      "Epoch [24/300], Step [25200/27733], Loss: 2.9217\n",
      "Epoch [24/300], Step [25300/27733], Loss: 3.1485\n",
      "Epoch [24/300], Step [25400/27733], Loss: 3.2478\n",
      "Epoch [24/300], Step [25500/27733], Loss: 3.3617\n",
      "Epoch [24/300], Step [25600/27733], Loss: 3.2629\n",
      "Epoch [24/300], Step [25700/27733], Loss: 3.4741\n",
      "Epoch [24/300], Step [25800/27733], Loss: 3.3674\n",
      "Epoch [24/300], Step [25900/27733], Loss: 2.6283\n",
      "Epoch [24/300], Step [26000/27733], Loss: 2.8778\n",
      "Epoch [24/300], Step [26100/27733], Loss: 3.1735\n",
      "Epoch [24/300], Step [26200/27733], Loss: 2.4951\n",
      "Epoch [24/300], Step [26300/27733], Loss: 3.2557\n",
      "Epoch [24/300], Step [26400/27733], Loss: 2.9770\n",
      "Epoch [24/300], Step [26500/27733], Loss: 3.1059\n",
      "Epoch [24/300], Step [26600/27733], Loss: 2.9719\n",
      "Epoch [24/300], Step [26700/27733], Loss: 3.3818\n",
      "Epoch [24/300], Step [26800/27733], Loss: 2.9655\n",
      "Epoch [24/300], Step [26900/27733], Loss: 3.2377\n",
      "Epoch [24/300], Step [27000/27733], Loss: 3.7933\n",
      "Epoch [24/300], Step [27100/27733], Loss: 3.2769\n",
      "Epoch [24/300], Step [27200/27733], Loss: 3.6788\n",
      "Epoch [24/300], Step [27300/27733], Loss: 2.9541\n",
      "Epoch [24/300], Step [27400/27733], Loss: 3.5150\n",
      "Epoch [24/300], Step [27500/27733], Loss: 2.9722\n",
      "Epoch [24/300], Step [27600/27733], Loss: 2.8324\n",
      "Epoch [24/300], Step [27700/27733], Loss: 3.3643\n",
      "Epoch [25/300], Step [100/27733], Loss: 2.2089\n",
      "Epoch [25/300], Step [200/27733], Loss: 1.9209\n",
      "Epoch [25/300], Step [300/27733], Loss: 2.3214\n",
      "Epoch [25/300], Step [400/27733], Loss: 2.7122\n",
      "Epoch [25/300], Step [500/27733], Loss: 2.0817\n",
      "Epoch [25/300], Step [600/27733], Loss: 2.4636\n",
      "Epoch [25/300], Step [700/27733], Loss: 2.5272\n",
      "Epoch [25/300], Step [800/27733], Loss: 2.8257\n",
      "Epoch [25/300], Step [900/27733], Loss: 2.4309\n",
      "Epoch [25/300], Step [1000/27733], Loss: 2.6616\n",
      "Epoch [25/300], Step [1100/27733], Loss: 2.2519\n",
      "Epoch [25/300], Step [1200/27733], Loss: 2.4377\n",
      "Epoch [25/300], Step [1300/27733], Loss: 3.2127\n",
      "Epoch [25/300], Step [1400/27733], Loss: 2.5415\n",
      "Epoch [25/300], Step [1500/27733], Loss: 1.8790\n",
      "Epoch [25/300], Step [1600/27733], Loss: 2.7879\n",
      "Epoch [25/300], Step [1700/27733], Loss: 2.3900\n",
      "Epoch [25/300], Step [1800/27733], Loss: 2.5635\n",
      "Epoch [25/300], Step [1900/27733], Loss: 2.4531\n",
      "Epoch [25/300], Step [2000/27733], Loss: 2.4269\n",
      "Epoch [25/300], Step [2100/27733], Loss: 3.2368\n",
      "Epoch [25/300], Step [2200/27733], Loss: 2.4219\n",
      "Epoch [25/300], Step [2300/27733], Loss: 2.5313\n",
      "Epoch [25/300], Step [2400/27733], Loss: 2.3253\n",
      "Epoch [25/300], Step [2500/27733], Loss: 2.3989\n",
      "Epoch [25/300], Step [2600/27733], Loss: 2.1745\n",
      "Epoch [25/300], Step [2700/27733], Loss: 3.2155\n",
      "Epoch [25/300], Step [2800/27733], Loss: 2.9985\n",
      "Epoch [25/300], Step [2900/27733], Loss: 3.0435\n",
      "Epoch [25/300], Step [3000/27733], Loss: 2.6702\n",
      "Epoch [25/300], Step [3100/27733], Loss: 2.3786\n",
      "Epoch [25/300], Step [3200/27733], Loss: 3.1033\n",
      "Epoch [25/300], Step [3300/27733], Loss: 2.9092\n",
      "Epoch [25/300], Step [3400/27733], Loss: 3.9264\n",
      "Epoch [25/300], Step [3500/27733], Loss: 3.1810\n",
      "Epoch [25/300], Step [3600/27733], Loss: 2.7346\n",
      "Epoch [25/300], Step [3700/27733], Loss: 2.9395\n",
      "Epoch [25/300], Step [3800/27733], Loss: 2.0971\n",
      "Epoch [25/300], Step [3900/27733], Loss: 1.8522\n",
      "Epoch [25/300], Step [4000/27733], Loss: 2.8269\n",
      "Epoch [25/300], Step [4100/27733], Loss: 2.7338\n",
      "Epoch [25/300], Step [4200/27733], Loss: 2.8471\n",
      "Epoch [25/300], Step [4300/27733], Loss: 2.4633\n",
      "Epoch [25/300], Step [4400/27733], Loss: 2.7340\n",
      "Epoch [25/300], Step [4500/27733], Loss: 3.0052\n",
      "Epoch [25/300], Step [4600/27733], Loss: 2.6972\n",
      "Epoch [25/300], Step [4700/27733], Loss: 3.1877\n",
      "Epoch [25/300], Step [4800/27733], Loss: 2.1639\n",
      "Epoch [25/300], Step [4900/27733], Loss: 2.3732\n",
      "Epoch [25/300], Step [5000/27733], Loss: 3.3195\n",
      "Epoch [25/300], Step [5100/27733], Loss: 2.5940\n",
      "Epoch [25/300], Step [5200/27733], Loss: 2.6101\n",
      "Epoch [25/300], Step [5300/27733], Loss: 2.7539\n",
      "Epoch [25/300], Step [5400/27733], Loss: 2.4414\n",
      "Epoch [25/300], Step [5500/27733], Loss: 3.2257\n",
      "Epoch [25/300], Step [5600/27733], Loss: 2.8850\n",
      "Epoch [25/300], Step [5700/27733], Loss: 2.1530\n",
      "Epoch [25/300], Step [5800/27733], Loss: 2.3704\n",
      "Epoch [25/300], Step [5900/27733], Loss: 2.2694\n",
      "Epoch [25/300], Step [6000/27733], Loss: 2.5594\n",
      "Epoch [25/300], Step [6100/27733], Loss: 2.7360\n",
      "Epoch [25/300], Step [6200/27733], Loss: 3.6820\n",
      "Epoch [25/300], Step [6300/27733], Loss: 2.9053\n",
      "Epoch [25/300], Step [6400/27733], Loss: 2.5731\n",
      "Epoch [25/300], Step [6500/27733], Loss: 3.2244\n",
      "Epoch [25/300], Step [6600/27733], Loss: 2.7159\n",
      "Epoch [25/300], Step [6700/27733], Loss: 2.3112\n",
      "Epoch [25/300], Step [6800/27733], Loss: 3.3744\n",
      "Epoch [25/300], Step [6900/27733], Loss: 2.5533\n",
      "Epoch [25/300], Step [7000/27733], Loss: 3.0638\n",
      "Epoch [25/300], Step [7100/27733], Loss: 2.3702\n",
      "Epoch [25/300], Step [7200/27733], Loss: 2.5804\n",
      "Epoch [25/300], Step [7300/27733], Loss: 3.5312\n",
      "Epoch [25/300], Step [7400/27733], Loss: 2.3916\n",
      "Epoch [25/300], Step [7500/27733], Loss: 3.0004\n",
      "Epoch [25/300], Step [7600/27733], Loss: 2.8271\n",
      "Epoch [25/300], Step [7700/27733], Loss: 2.4232\n",
      "Epoch [25/300], Step [7800/27733], Loss: 2.6769\n",
      "Epoch [25/300], Step [7900/27733], Loss: 3.1362\n",
      "Epoch [25/300], Step [8000/27733], Loss: 2.7405\n",
      "Epoch [25/300], Step [8100/27733], Loss: 2.3020\n",
      "Epoch [25/300], Step [8200/27733], Loss: 3.3778\n",
      "Epoch [25/300], Step [8300/27733], Loss: 2.7537\n",
      "Epoch [25/300], Step [8400/27733], Loss: 3.5564\n",
      "Epoch [25/300], Step [8500/27733], Loss: 2.8106\n",
      "Epoch [25/300], Step [8600/27733], Loss: 2.8540\n",
      "Epoch [25/300], Step [8700/27733], Loss: 2.4259\n",
      "Epoch [25/300], Step [8800/27733], Loss: 3.0523\n",
      "Epoch [25/300], Step [8900/27733], Loss: 2.6472\n",
      "Epoch [25/300], Step [9000/27733], Loss: 3.3248\n",
      "Epoch [25/300], Step [9100/27733], Loss: 3.3023\n",
      "Epoch [25/300], Step [9200/27733], Loss: 2.9379\n",
      "Epoch [25/300], Step [9300/27733], Loss: 2.0782\n",
      "Epoch [25/300], Step [9400/27733], Loss: 3.3906\n",
      "Epoch [25/300], Step [9500/27733], Loss: 3.6471\n",
      "Epoch [25/300], Step [9600/27733], Loss: 3.5671\n",
      "Epoch [25/300], Step [9700/27733], Loss: 3.1599\n",
      "Epoch [25/300], Step [9800/27733], Loss: 3.6473\n",
      "Epoch [25/300], Step [9900/27733], Loss: 3.2972\n",
      "Epoch [25/300], Step [10000/27733], Loss: 2.2270\n",
      "Epoch [25/300], Step [10100/27733], Loss: 3.4075\n",
      "Epoch [25/300], Step [10200/27733], Loss: 2.1801\n",
      "Epoch [25/300], Step [10300/27733], Loss: 2.9704\n",
      "Epoch [25/300], Step [10400/27733], Loss: 3.3121\n",
      "Epoch [25/300], Step [10500/27733], Loss: 3.2353\n",
      "Epoch [25/300], Step [10600/27733], Loss: 2.3673\n",
      "Epoch [25/300], Step [10700/27733], Loss: 2.6652\n",
      "Epoch [25/300], Step [10800/27733], Loss: 3.5068\n",
      "Epoch [25/300], Step [10900/27733], Loss: 2.6561\n",
      "Epoch [25/300], Step [11000/27733], Loss: 3.0368\n",
      "Epoch [25/300], Step [11100/27733], Loss: 3.1778\n",
      "Epoch [25/300], Step [11200/27733], Loss: 3.1599\n",
      "Epoch [25/300], Step [11300/27733], Loss: 3.0047\n",
      "Epoch [25/300], Step [11400/27733], Loss: 2.9841\n",
      "Epoch [25/300], Step [11500/27733], Loss: 2.3984\n",
      "Epoch [25/300], Step [11600/27733], Loss: 3.0902\n",
      "Epoch [25/300], Step [11700/27733], Loss: 3.7794\n",
      "Epoch [25/300], Step [11800/27733], Loss: 2.7046\n",
      "Epoch [25/300], Step [11900/27733], Loss: 3.2834\n",
      "Epoch [25/300], Step [12000/27733], Loss: 3.0065\n",
      "Epoch [25/300], Step [12100/27733], Loss: 2.8643\n",
      "Epoch [25/300], Step [12200/27733], Loss: 3.1545\n",
      "Epoch [25/300], Step [12300/27733], Loss: 3.7668\n",
      "Epoch [25/300], Step [12400/27733], Loss: 2.8862\n",
      "Epoch [25/300], Step [12500/27733], Loss: 3.7463\n",
      "Epoch [25/300], Step [12600/27733], Loss: 3.2435\n",
      "Epoch [25/300], Step [12700/27733], Loss: 3.2426\n",
      "Epoch [25/300], Step [12800/27733], Loss: 2.9698\n",
      "Epoch [25/300], Step [12900/27733], Loss: 3.1747\n",
      "Epoch [25/300], Step [13000/27733], Loss: 3.3442\n",
      "Epoch [25/300], Step [13100/27733], Loss: 3.7112\n",
      "Epoch [25/300], Step [13200/27733], Loss: 3.4923\n",
      "Epoch [25/300], Step [13300/27733], Loss: 2.9191\n",
      "Epoch [25/300], Step [13400/27733], Loss: 2.9541\n",
      "Epoch [25/300], Step [13500/27733], Loss: 3.1557\n",
      "Epoch [25/300], Step [13600/27733], Loss: 3.2119\n",
      "Epoch [25/300], Step [13700/27733], Loss: 3.4217\n",
      "Epoch [25/300], Step [13800/27733], Loss: 2.9957\n",
      "Epoch [25/300], Step [13900/27733], Loss: 2.4810\n",
      "Epoch [25/300], Step [14000/27733], Loss: 2.9887\n",
      "Epoch [25/300], Step [14100/27733], Loss: 2.9659\n",
      "Epoch [25/300], Step [14200/27733], Loss: 2.6474\n",
      "Epoch [25/300], Step [14300/27733], Loss: 2.7742\n",
      "Epoch [25/300], Step [14400/27733], Loss: 2.5983\n",
      "Epoch [25/300], Step [14500/27733], Loss: 3.7231\n",
      "Epoch [25/300], Step [14600/27733], Loss: 3.3078\n",
      "Epoch [25/300], Step [14700/27733], Loss: 3.6035\n",
      "Epoch [25/300], Step [14800/27733], Loss: 2.5723\n",
      "Epoch [25/300], Step [14900/27733], Loss: 3.0052\n",
      "Epoch [25/300], Step [15000/27733], Loss: 2.8839\n",
      "Epoch [25/300], Step [15100/27733], Loss: 2.7542\n",
      "Epoch [25/300], Step [15200/27733], Loss: 2.7631\n",
      "Epoch [25/300], Step [15300/27733], Loss: 2.5004\n",
      "Epoch [25/300], Step [15400/27733], Loss: 2.3487\n",
      "Epoch [25/300], Step [15500/27733], Loss: 2.7460\n",
      "Epoch [25/300], Step [15600/27733], Loss: 2.5744\n",
      "Epoch [25/300], Step [15700/27733], Loss: 3.3223\n",
      "Epoch [25/300], Step [15800/27733], Loss: 2.7341\n",
      "Epoch [25/300], Step [15900/27733], Loss: 2.5600\n",
      "Epoch [25/300], Step [16000/27733], Loss: 3.4239\n",
      "Epoch [25/300], Step [16100/27733], Loss: 1.8423\n",
      "Epoch [25/300], Step [16200/27733], Loss: 3.6280\n",
      "Epoch [25/300], Step [16300/27733], Loss: 2.5336\n",
      "Epoch [25/300], Step [16400/27733], Loss: 2.9207\n",
      "Epoch [25/300], Step [16500/27733], Loss: 3.5329\n",
      "Epoch [25/300], Step [16600/27733], Loss: 2.6847\n",
      "Epoch [25/300], Step [16700/27733], Loss: 3.0982\n",
      "Epoch [25/300], Step [16800/27733], Loss: 3.7730\n",
      "Epoch [25/300], Step [16900/27733], Loss: 2.9547\n",
      "Epoch [25/300], Step [17000/27733], Loss: 2.3053\n",
      "Epoch [25/300], Step [17100/27733], Loss: 2.8233\n",
      "Epoch [25/300], Step [17200/27733], Loss: 3.3520\n",
      "Epoch [25/300], Step [17300/27733], Loss: 2.2827\n",
      "Epoch [25/300], Step [17400/27733], Loss: 2.7867\n",
      "Epoch [25/300], Step [17500/27733], Loss: 2.5034\n",
      "Epoch [25/300], Step [17600/27733], Loss: 2.6080\n",
      "Epoch [25/300], Step [17700/27733], Loss: 2.8152\n",
      "Epoch [25/300], Step [17800/27733], Loss: 2.6628\n",
      "Epoch [25/300], Step [17900/27733], Loss: 2.6445\n",
      "Epoch [25/300], Step [18000/27733], Loss: 3.0408\n",
      "Epoch [25/300], Step [18100/27733], Loss: 2.4405\n",
      "Epoch [25/300], Step [18200/27733], Loss: 2.4982\n",
      "Epoch [25/300], Step [18300/27733], Loss: 2.6077\n",
      "Epoch [25/300], Step [18400/27733], Loss: 2.8861\n",
      "Epoch [25/300], Step [18500/27733], Loss: 3.2358\n",
      "Epoch [25/300], Step [18600/27733], Loss: 3.3175\n",
      "Epoch [25/300], Step [18700/27733], Loss: 3.8672\n",
      "Epoch [25/300], Step [18800/27733], Loss: 2.7899\n",
      "Epoch [25/300], Step [18900/27733], Loss: 2.6512\n",
      "Epoch [25/300], Step [19000/27733], Loss: 2.6670\n",
      "Epoch [25/300], Step [19100/27733], Loss: 3.8503\n",
      "Epoch [25/300], Step [19200/27733], Loss: 3.6433\n",
      "Epoch [25/300], Step [19300/27733], Loss: 3.7112\n",
      "Epoch [25/300], Step [19400/27733], Loss: 3.0559\n",
      "Epoch [25/300], Step [19500/27733], Loss: 3.0038\n",
      "Epoch [25/300], Step [19600/27733], Loss: 2.7842\n",
      "Epoch [25/300], Step [19700/27733], Loss: 3.6903\n",
      "Epoch [25/300], Step [19800/27733], Loss: 3.1283\n",
      "Epoch [25/300], Step [19900/27733], Loss: 3.0100\n",
      "Epoch [25/300], Step [20000/27733], Loss: 2.2126\n",
      "Epoch [25/300], Step [20100/27733], Loss: 4.1468\n",
      "Epoch [25/300], Step [20200/27733], Loss: 2.4798\n",
      "Epoch [25/300], Step [20300/27733], Loss: 3.1891\n",
      "Epoch [25/300], Step [20400/27733], Loss: 3.2201\n",
      "Epoch [25/300], Step [20500/27733], Loss: 3.2836\n",
      "Epoch [25/300], Step [20600/27733], Loss: 3.0454\n",
      "Epoch [25/300], Step [20700/27733], Loss: 2.9747\n",
      "Epoch [25/300], Step [20800/27733], Loss: 2.9917\n",
      "Epoch [25/300], Step [20900/27733], Loss: 3.5357\n",
      "Epoch [25/300], Step [21000/27733], Loss: 2.4312\n",
      "Epoch [25/300], Step [21100/27733], Loss: 3.4443\n",
      "Epoch [25/300], Step [21200/27733], Loss: 3.5035\n",
      "Epoch [25/300], Step [21300/27733], Loss: 3.5963\n",
      "Epoch [25/300], Step [21400/27733], Loss: 2.9422\n",
      "Epoch [25/300], Step [21500/27733], Loss: 2.5018\n",
      "Epoch [25/300], Step [21600/27733], Loss: 3.3255\n",
      "Epoch [25/300], Step [21700/27733], Loss: 2.8322\n",
      "Epoch [25/300], Step [21800/27733], Loss: 2.8761\n",
      "Epoch [25/300], Step [21900/27733], Loss: 2.6064\n",
      "Epoch [25/300], Step [22000/27733], Loss: 2.3612\n",
      "Epoch [25/300], Step [22100/27733], Loss: 2.9758\n",
      "Epoch [25/300], Step [22200/27733], Loss: 2.9363\n",
      "Epoch [25/300], Step [22300/27733], Loss: 2.8964\n",
      "Epoch [25/300], Step [22400/27733], Loss: 2.7744\n",
      "Epoch [25/300], Step [22500/27733], Loss: 2.8605\n",
      "Epoch [25/300], Step [22600/27733], Loss: 2.9529\n",
      "Epoch [25/300], Step [22700/27733], Loss: 3.2143\n",
      "Epoch [25/300], Step [22800/27733], Loss: 2.4991\n",
      "Epoch [25/300], Step [22900/27733], Loss: 2.4933\n",
      "Epoch [25/300], Step [23000/27733], Loss: 3.0200\n",
      "Epoch [25/300], Step [23100/27733], Loss: 3.4068\n",
      "Epoch [25/300], Step [23200/27733], Loss: 2.9005\n",
      "Epoch [25/300], Step [23300/27733], Loss: 2.9305\n",
      "Epoch [25/300], Step [23400/27733], Loss: 2.9656\n",
      "Epoch [25/300], Step [23500/27733], Loss: 2.9003\n",
      "Epoch [25/300], Step [23600/27733], Loss: 3.1431\n",
      "Epoch [25/300], Step [23700/27733], Loss: 2.8198\n",
      "Epoch [25/300], Step [23800/27733], Loss: 3.3868\n",
      "Epoch [25/300], Step [23900/27733], Loss: 3.9242\n",
      "Epoch [25/300], Step [24000/27733], Loss: 3.2722\n",
      "Epoch [25/300], Step [24100/27733], Loss: 3.6245\n",
      "Epoch [25/300], Step [24200/27733], Loss: 3.8842\n",
      "Epoch [25/300], Step [24300/27733], Loss: 2.6024\n",
      "Epoch [25/300], Step [24400/27733], Loss: 3.5897\n",
      "Epoch [25/300], Step [24500/27733], Loss: 3.7459\n",
      "Epoch [25/300], Step [24600/27733], Loss: 3.5439\n",
      "Epoch [25/300], Step [24700/27733], Loss: 2.2403\n",
      "Epoch [25/300], Step [24800/27733], Loss: 3.5811\n",
      "Epoch [25/300], Step [24900/27733], Loss: 3.1409\n",
      "Epoch [25/300], Step [25000/27733], Loss: 3.2065\n",
      "Epoch [25/300], Step [25100/27733], Loss: 3.1406\n",
      "Epoch [25/300], Step [25200/27733], Loss: 2.9482\n",
      "Epoch [25/300], Step [25300/27733], Loss: 3.0586\n",
      "Epoch [25/300], Step [25400/27733], Loss: 3.5725\n",
      "Epoch [25/300], Step [25500/27733], Loss: 3.4452\n",
      "Epoch [25/300], Step [25600/27733], Loss: 2.7341\n",
      "Epoch [25/300], Step [25700/27733], Loss: 2.8746\n",
      "Epoch [25/300], Step [25800/27733], Loss: 3.2704\n",
      "Epoch [25/300], Step [25900/27733], Loss: 2.9786\n",
      "Epoch [25/300], Step [26000/27733], Loss: 3.2361\n",
      "Epoch [25/300], Step [26100/27733], Loss: 3.9371\n",
      "Epoch [25/300], Step [26200/27733], Loss: 2.6215\n",
      "Epoch [25/300], Step [26300/27733], Loss: 2.9060\n",
      "Epoch [25/300], Step [26400/27733], Loss: 2.3439\n",
      "Epoch [25/300], Step [26500/27733], Loss: 3.4975\n",
      "Epoch [25/300], Step [26600/27733], Loss: 3.1536\n",
      "Epoch [25/300], Step [26700/27733], Loss: 3.6957\n",
      "Epoch [25/300], Step [26800/27733], Loss: 2.9165\n",
      "Epoch [25/300], Step [26900/27733], Loss: 3.3984\n",
      "Epoch [25/300], Step [27000/27733], Loss: 2.7466\n",
      "Epoch [25/300], Step [27100/27733], Loss: 3.6970\n",
      "Epoch [25/300], Step [27200/27733], Loss: 2.6785\n",
      "Epoch [25/300], Step [27300/27733], Loss: 2.8151\n",
      "Epoch [25/300], Step [27400/27733], Loss: 3.3135\n",
      "Epoch [25/300], Step [27500/27733], Loss: 2.7135\n",
      "Epoch [25/300], Step [27600/27733], Loss: 2.9589\n",
      "Epoch [25/300], Step [27700/27733], Loss: 3.0066\n",
      "Epoch [26/300], Step [100/27733], Loss: 2.6038\n",
      "Epoch [26/300], Step [200/27733], Loss: 2.3948\n",
      "Epoch [26/300], Step [300/27733], Loss: 2.5459\n",
      "Epoch [26/300], Step [400/27733], Loss: 2.4991\n",
      "Epoch [26/300], Step [500/27733], Loss: 2.2174\n",
      "Epoch [26/300], Step [600/27733], Loss: 2.3125\n",
      "Epoch [26/300], Step [700/27733], Loss: 2.0791\n",
      "Epoch [26/300], Step [800/27733], Loss: 2.6235\n",
      "Epoch [26/300], Step [900/27733], Loss: 3.3772\n",
      "Epoch [26/300], Step [1000/27733], Loss: 3.3439\n",
      "Epoch [26/300], Step [1100/27733], Loss: 2.7923\n",
      "Epoch [26/300], Step [1200/27733], Loss: 2.1345\n",
      "Epoch [26/300], Step [1300/27733], Loss: 1.8364\n",
      "Epoch [26/300], Step [1400/27733], Loss: 2.6929\n",
      "Epoch [26/300], Step [1500/27733], Loss: 2.5565\n",
      "Epoch [26/300], Step [1600/27733], Loss: 2.4789\n",
      "Epoch [26/300], Step [1700/27733], Loss: 2.5263\n",
      "Epoch [26/300], Step [1800/27733], Loss: 2.5441\n",
      "Epoch [26/300], Step [1900/27733], Loss: 2.8136\n",
      "Epoch [26/300], Step [2000/27733], Loss: 2.3824\n",
      "Epoch [26/300], Step [2100/27733], Loss: 2.7257\n",
      "Epoch [26/300], Step [2200/27733], Loss: 2.9516\n",
      "Epoch [26/300], Step [2300/27733], Loss: 2.6391\n",
      "Epoch [26/300], Step [2400/27733], Loss: 2.1491\n",
      "Epoch [26/300], Step [2500/27733], Loss: 3.2468\n",
      "Epoch [26/300], Step [2600/27733], Loss: 2.1119\n",
      "Epoch [26/300], Step [2700/27733], Loss: 2.7090\n",
      "Epoch [26/300], Step [2800/27733], Loss: 2.7452\n",
      "Epoch [26/300], Step [2900/27733], Loss: 1.9863\n",
      "Epoch [26/300], Step [3000/27733], Loss: 2.5732\n",
      "Epoch [26/300], Step [3100/27733], Loss: 2.5605\n",
      "Epoch [26/300], Step [3200/27733], Loss: 2.5395\n",
      "Epoch [26/300], Step [3300/27733], Loss: 2.6266\n",
      "Epoch [26/300], Step [3400/27733], Loss: 2.3743\n",
      "Epoch [26/300], Step [3500/27733], Loss: 2.3009\n",
      "Epoch [26/300], Step [3600/27733], Loss: 3.4359\n",
      "Epoch [26/300], Step [3700/27733], Loss: 2.8743\n",
      "Epoch [26/300], Step [3800/27733], Loss: 2.4474\n",
      "Epoch [26/300], Step [3900/27733], Loss: 2.3399\n",
      "Epoch [26/300], Step [4000/27733], Loss: 2.2571\n",
      "Epoch [26/300], Step [4100/27733], Loss: 2.5511\n",
      "Epoch [26/300], Step [4200/27733], Loss: 2.8530\n",
      "Epoch [26/300], Step [4300/27733], Loss: 3.3013\n",
      "Epoch [26/300], Step [4400/27733], Loss: 2.6358\n",
      "Epoch [26/300], Step [4500/27733], Loss: 2.2022\n",
      "Epoch [26/300], Step [4600/27733], Loss: 2.5927\n",
      "Epoch [26/300], Step [4700/27733], Loss: 3.0450\n",
      "Epoch [26/300], Step [4800/27733], Loss: 3.8194\n",
      "Epoch [26/300], Step [4900/27733], Loss: 2.9763\n",
      "Epoch [26/300], Step [5000/27733], Loss: 2.6825\n",
      "Epoch [26/300], Step [5100/27733], Loss: 2.7038\n",
      "Epoch [26/300], Step [5200/27733], Loss: 2.7352\n",
      "Epoch [26/300], Step [5300/27733], Loss: 2.5573\n",
      "Epoch [26/300], Step [5400/27733], Loss: 3.8474\n",
      "Epoch [26/300], Step [5500/27733], Loss: 3.7802\n",
      "Epoch [26/300], Step [5600/27733], Loss: 2.4133\n",
      "Epoch [26/300], Step [5700/27733], Loss: 3.6912\n",
      "Epoch [26/300], Step [5800/27733], Loss: 3.4924\n",
      "Epoch [26/300], Step [5900/27733], Loss: 2.5077\n",
      "Epoch [26/300], Step [6000/27733], Loss: 3.0540\n",
      "Epoch [26/300], Step [6100/27733], Loss: 1.9842\n",
      "Epoch [26/300], Step [6200/27733], Loss: 3.1060\n",
      "Epoch [26/300], Step [6300/27733], Loss: 3.2293\n",
      "Epoch [26/300], Step [6400/27733], Loss: 2.4114\n",
      "Epoch [26/300], Step [6500/27733], Loss: 3.0093\n",
      "Epoch [26/300], Step [6600/27733], Loss: 3.5160\n",
      "Epoch [26/300], Step [6700/27733], Loss: 2.4212\n",
      "Epoch [26/300], Step [6800/27733], Loss: 3.1031\n",
      "Epoch [26/300], Step [6900/27733], Loss: 2.8258\n",
      "Epoch [26/300], Step [7000/27733], Loss: 3.5123\n",
      "Epoch [26/300], Step [7100/27733], Loss: 2.6567\n",
      "Epoch [26/300], Step [7200/27733], Loss: 2.8768\n",
      "Epoch [26/300], Step [7300/27733], Loss: 3.7804\n",
      "Epoch [26/300], Step [7400/27733], Loss: 3.0504\n",
      "Epoch [26/300], Step [7500/27733], Loss: 2.7228\n",
      "Epoch [26/300], Step [7600/27733], Loss: 2.9741\n",
      "Epoch [26/300], Step [7700/27733], Loss: 2.8974\n",
      "Epoch [26/300], Step [7800/27733], Loss: 2.2686\n",
      "Epoch [26/300], Step [7900/27733], Loss: 2.5164\n",
      "Epoch [26/300], Step [8000/27733], Loss: 2.8587\n",
      "Epoch [26/300], Step [8100/27733], Loss: 3.3114\n",
      "Epoch [26/300], Step [8200/27733], Loss: 1.9774\n",
      "Epoch [26/300], Step [8300/27733], Loss: 2.2467\n",
      "Epoch [26/300], Step [8400/27733], Loss: 2.3482\n",
      "Epoch [26/300], Step [8500/27733], Loss: 3.1728\n",
      "Epoch [26/300], Step [8600/27733], Loss: 2.4012\n",
      "Epoch [26/300], Step [8700/27733], Loss: 2.3642\n",
      "Epoch [26/300], Step [8800/27733], Loss: 3.2301\n",
      "Epoch [26/300], Step [8900/27733], Loss: 2.3528\n",
      "Epoch [26/300], Step [9000/27733], Loss: 3.0873\n",
      "Epoch [26/300], Step [9100/27733], Loss: 2.5305\n",
      "Epoch [26/300], Step [9200/27733], Loss: 2.9844\n",
      "Epoch [26/300], Step [9300/27733], Loss: 4.1238\n",
      "Epoch [26/300], Step [9400/27733], Loss: 2.4873\n",
      "Epoch [26/300], Step [9500/27733], Loss: 3.2290\n",
      "Epoch [26/300], Step [9600/27733], Loss: 2.5211\n",
      "Epoch [26/300], Step [9700/27733], Loss: 4.3673\n",
      "Epoch [26/300], Step [9800/27733], Loss: 2.4883\n",
      "Epoch [26/300], Step [9900/27733], Loss: 2.7094\n",
      "Epoch [26/300], Step [10000/27733], Loss: 2.9759\n",
      "Epoch [26/300], Step [10100/27733], Loss: 3.0563\n",
      "Epoch [26/300], Step [10200/27733], Loss: 2.4172\n",
      "Epoch [26/300], Step [10300/27733], Loss: 2.1979\n",
      "Epoch [26/300], Step [10400/27733], Loss: 2.4533\n",
      "Epoch [26/300], Step [10500/27733], Loss: 3.1984\n",
      "Epoch [26/300], Step [10600/27733], Loss: 2.8772\n",
      "Epoch [26/300], Step [10700/27733], Loss: 2.8505\n",
      "Epoch [26/300], Step [10800/27733], Loss: 3.0630\n",
      "Epoch [26/300], Step [10900/27733], Loss: 3.6141\n",
      "Epoch [26/300], Step [11000/27733], Loss: 2.4375\n",
      "Epoch [26/300], Step [11100/27733], Loss: 3.2310\n",
      "Epoch [26/300], Step [11200/27733], Loss: 2.4067\n",
      "Epoch [26/300], Step [11300/27733], Loss: 2.9831\n",
      "Epoch [26/300], Step [11400/27733], Loss: 3.8532\n",
      "Epoch [26/300], Step [11500/27733], Loss: 3.2172\n",
      "Epoch [26/300], Step [11600/27733], Loss: 2.7544\n",
      "Epoch [26/300], Step [11700/27733], Loss: 2.4191\n",
      "Epoch [26/300], Step [11800/27733], Loss: 2.7417\n",
      "Epoch [26/300], Step [11900/27733], Loss: 2.6321\n",
      "Epoch [26/300], Step [12000/27733], Loss: 2.7047\n",
      "Epoch [26/300], Step [12100/27733], Loss: 2.7840\n",
      "Epoch [26/300], Step [12200/27733], Loss: 2.6399\n",
      "Epoch [26/300], Step [12300/27733], Loss: 3.1908\n",
      "Epoch [26/300], Step [12400/27733], Loss: 2.9754\n",
      "Epoch [26/300], Step [12500/27733], Loss: 2.4459\n",
      "Epoch [26/300], Step [12600/27733], Loss: 2.2576\n",
      "Epoch [26/300], Step [12700/27733], Loss: 3.6741\n",
      "Epoch [26/300], Step [12800/27733], Loss: 3.2150\n",
      "Epoch [26/300], Step [12900/27733], Loss: 3.0490\n",
      "Epoch [26/300], Step [13000/27733], Loss: 2.8256\n",
      "Epoch [26/300], Step [13100/27733], Loss: 2.3999\n",
      "Epoch [26/300], Step [13200/27733], Loss: 2.9886\n",
      "Epoch [26/300], Step [13300/27733], Loss: 3.2750\n",
      "Epoch [26/300], Step [13400/27733], Loss: 2.8888\n",
      "Epoch [26/300], Step [13500/27733], Loss: 2.0741\n",
      "Epoch [26/300], Step [13600/27733], Loss: 3.2099\n",
      "Epoch [26/300], Step [13700/27733], Loss: 2.5709\n",
      "Epoch [26/300], Step [13800/27733], Loss: 3.4155\n",
      "Epoch [26/300], Step [13900/27733], Loss: 3.2097\n",
      "Epoch [26/300], Step [14000/27733], Loss: 2.6180\n",
      "Epoch [26/300], Step [14100/27733], Loss: 2.9188\n",
      "Epoch [26/300], Step [14200/27733], Loss: 2.9918\n",
      "Epoch [26/300], Step [14300/27733], Loss: 3.3094\n",
      "Epoch [26/300], Step [14400/27733], Loss: 3.0852\n",
      "Epoch [26/300], Step [14500/27733], Loss: 2.4804\n",
      "Epoch [26/300], Step [14600/27733], Loss: 2.5142\n",
      "Epoch [26/300], Step [14700/27733], Loss: 3.2286\n",
      "Epoch [26/300], Step [14800/27733], Loss: 2.7694\n",
      "Epoch [26/300], Step [14900/27733], Loss: 3.0055\n",
      "Epoch [26/300], Step [15000/27733], Loss: 3.1009\n",
      "Epoch [26/300], Step [15100/27733], Loss: 3.1106\n",
      "Epoch [26/300], Step [15200/27733], Loss: 3.2208\n",
      "Epoch [26/300], Step [15300/27733], Loss: 3.2964\n",
      "Epoch [26/300], Step [15400/27733], Loss: 3.4116\n",
      "Epoch [26/300], Step [15500/27733], Loss: 2.9743\n",
      "Epoch [26/300], Step [15600/27733], Loss: 3.1631\n",
      "Epoch [26/300], Step [15700/27733], Loss: 3.5357\n",
      "Epoch [26/300], Step [15800/27733], Loss: 3.8284\n",
      "Epoch [26/300], Step [15900/27733], Loss: 3.0617\n",
      "Epoch [26/300], Step [16000/27733], Loss: 2.8398\n",
      "Epoch [26/300], Step [16100/27733], Loss: 2.0398\n",
      "Epoch [26/300], Step [16200/27733], Loss: 3.7152\n",
      "Epoch [26/300], Step [16300/27733], Loss: 3.1865\n",
      "Epoch [26/300], Step [16400/27733], Loss: 2.4843\n",
      "Epoch [26/300], Step [16500/27733], Loss: 2.7579\n",
      "Epoch [26/300], Step [16600/27733], Loss: 3.0791\n",
      "Epoch [26/300], Step [16700/27733], Loss: 3.0238\n",
      "Epoch [26/300], Step [16800/27733], Loss: 3.1859\n",
      "Epoch [26/300], Step [16900/27733], Loss: 3.0374\n",
      "Epoch [26/300], Step [17000/27733], Loss: 2.8045\n",
      "Epoch [26/300], Step [17100/27733], Loss: 2.9145\n",
      "Epoch [26/300], Step [17200/27733], Loss: 2.5384\n",
      "Epoch [26/300], Step [17300/27733], Loss: 3.0556\n",
      "Epoch [26/300], Step [17400/27733], Loss: 3.2798\n",
      "Epoch [26/300], Step [17500/27733], Loss: 3.2562\n",
      "Epoch [26/300], Step [17600/27733], Loss: 3.2722\n",
      "Epoch [26/300], Step [17700/27733], Loss: 2.8252\n",
      "Epoch [26/300], Step [17800/27733], Loss: 2.0569\n",
      "Epoch [26/300], Step [17900/27733], Loss: 3.9865\n",
      "Epoch [26/300], Step [18000/27733], Loss: 2.6510\n",
      "Epoch [26/300], Step [18100/27733], Loss: 3.2966\n",
      "Epoch [26/300], Step [18200/27733], Loss: 2.7830\n",
      "Epoch [26/300], Step [18300/27733], Loss: 2.9778\n",
      "Epoch [26/300], Step [18400/27733], Loss: 3.3800\n",
      "Epoch [26/300], Step [18500/27733], Loss: 3.2349\n",
      "Epoch [26/300], Step [18600/27733], Loss: 2.8126\n",
      "Epoch [26/300], Step [18700/27733], Loss: 2.8516\n",
      "Epoch [26/300], Step [18800/27733], Loss: 2.7439\n",
      "Epoch [26/300], Step [18900/27733], Loss: 3.7422\n",
      "Epoch [26/300], Step [19000/27733], Loss: 3.7356\n",
      "Epoch [26/300], Step [19100/27733], Loss: 3.8011\n",
      "Epoch [26/300], Step [19200/27733], Loss: 3.0135\n",
      "Epoch [26/300], Step [19300/27733], Loss: 2.9922\n",
      "Epoch [26/300], Step [19400/27733], Loss: 2.9979\n",
      "Epoch [26/300], Step [19500/27733], Loss: 3.2922\n",
      "Epoch [26/300], Step [19600/27733], Loss: 3.0789\n",
      "Epoch [26/300], Step [19700/27733], Loss: 2.7471\n",
      "Epoch [26/300], Step [19800/27733], Loss: 3.0891\n",
      "Epoch [26/300], Step [19900/27733], Loss: 3.1098\n",
      "Epoch [26/300], Step [20000/27733], Loss: 3.7576\n",
      "Epoch [26/300], Step [20100/27733], Loss: 2.9002\n",
      "Epoch [26/300], Step [20200/27733], Loss: 3.0972\n",
      "Epoch [26/300], Step [20300/27733], Loss: 2.8571\n",
      "Epoch [26/300], Step [20400/27733], Loss: 2.5235\n",
      "Epoch [26/300], Step [20500/27733], Loss: 2.1105\n",
      "Epoch [26/300], Step [20600/27733], Loss: 3.0915\n",
      "Epoch [26/300], Step [20700/27733], Loss: 2.4973\n",
      "Epoch [26/300], Step [20800/27733], Loss: 2.5864\n",
      "Epoch [26/300], Step [20900/27733], Loss: 3.5451\n",
      "Epoch [26/300], Step [21000/27733], Loss: 3.7040\n",
      "Epoch [26/300], Step [21100/27733], Loss: 3.2433\n",
      "Epoch [26/300], Step [21200/27733], Loss: 2.9823\n",
      "Epoch [26/300], Step [21300/27733], Loss: 3.1257\n",
      "Epoch [26/300], Step [21400/27733], Loss: 3.1487\n",
      "Epoch [26/300], Step [21500/27733], Loss: 3.7655\n",
      "Epoch [26/300], Step [21600/27733], Loss: 2.8732\n",
      "Epoch [26/300], Step [21700/27733], Loss: 3.1198\n",
      "Epoch [26/300], Step [21800/27733], Loss: 3.3622\n",
      "Epoch [26/300], Step [21900/27733], Loss: 2.8591\n",
      "Epoch [26/300], Step [22000/27733], Loss: 3.8522\n",
      "Epoch [26/300], Step [22100/27733], Loss: 3.3326\n",
      "Epoch [26/300], Step [22200/27733], Loss: 3.2439\n",
      "Epoch [26/300], Step [22300/27733], Loss: 3.6648\n",
      "Epoch [26/300], Step [22400/27733], Loss: 2.4985\n",
      "Epoch [26/300], Step [22500/27733], Loss: 3.7086\n",
      "Epoch [26/300], Step [22600/27733], Loss: 3.1368\n",
      "Epoch [26/300], Step [22700/27733], Loss: 2.9148\n",
      "Epoch [26/300], Step [22800/27733], Loss: 3.1409\n",
      "Epoch [26/300], Step [22900/27733], Loss: 3.7082\n",
      "Epoch [26/300], Step [23000/27733], Loss: 2.4609\n",
      "Epoch [26/300], Step [23100/27733], Loss: 3.0247\n",
      "Epoch [26/300], Step [23200/27733], Loss: 3.4426\n",
      "Epoch [26/300], Step [23300/27733], Loss: 3.2956\n",
      "Epoch [26/300], Step [23400/27733], Loss: 2.9879\n",
      "Epoch [26/300], Step [23500/27733], Loss: 3.1530\n",
      "Epoch [26/300], Step [23600/27733], Loss: 3.2750\n",
      "Epoch [26/300], Step [23700/27733], Loss: 3.5118\n",
      "Epoch [26/300], Step [23800/27733], Loss: 3.9948\n",
      "Epoch [26/300], Step [23900/27733], Loss: 2.6314\n",
      "Epoch [26/300], Step [24000/27733], Loss: 3.6329\n",
      "Epoch [26/300], Step [24100/27733], Loss: 2.9370\n",
      "Epoch [26/300], Step [24200/27733], Loss: 3.0045\n",
      "Epoch [26/300], Step [24300/27733], Loss: 2.8276\n",
      "Epoch [26/300], Step [24400/27733], Loss: 2.6118\n",
      "Epoch [26/300], Step [24500/27733], Loss: 3.0365\n",
      "Epoch [26/300], Step [24600/27733], Loss: 2.2734\n",
      "Epoch [26/300], Step [24700/27733], Loss: 3.2967\n",
      "Epoch [26/300], Step [24800/27733], Loss: 3.7897\n",
      "Epoch [26/300], Step [24900/27733], Loss: 3.3147\n",
      "Epoch [26/300], Step [25000/27733], Loss: 3.1145\n",
      "Epoch [26/300], Step [25100/27733], Loss: 3.2941\n",
      "Epoch [26/300], Step [25200/27733], Loss: 3.6134\n",
      "Epoch [26/300], Step [25300/27733], Loss: 3.3857\n",
      "Epoch [26/300], Step [25400/27733], Loss: 3.0292\n",
      "Epoch [26/300], Step [25500/27733], Loss: 3.4084\n",
      "Epoch [26/300], Step [25600/27733], Loss: 3.0212\n",
      "Epoch [26/300], Step [25700/27733], Loss: 3.4267\n",
      "Epoch [26/300], Step [25800/27733], Loss: 3.4165\n",
      "Epoch [26/300], Step [25900/27733], Loss: 3.2530\n",
      "Epoch [26/300], Step [26000/27733], Loss: 4.7966\n",
      "Epoch [26/300], Step [26100/27733], Loss: 3.7101\n",
      "Epoch [26/300], Step [26200/27733], Loss: 4.0141\n",
      "Epoch [26/300], Step [26300/27733], Loss: 2.7627\n",
      "Epoch [26/300], Step [26400/27733], Loss: 2.5345\n",
      "Epoch [26/300], Step [26500/27733], Loss: 3.8336\n",
      "Epoch [26/300], Step [26600/27733], Loss: 3.0193\n",
      "Epoch [26/300], Step [26700/27733], Loss: 3.4957\n",
      "Epoch [26/300], Step [26800/27733], Loss: 3.9456\n",
      "Epoch [26/300], Step [26900/27733], Loss: 3.2392\n",
      "Epoch [26/300], Step [27000/27733], Loss: 4.0132\n",
      "Epoch [26/300], Step [27100/27733], Loss: 4.3008\n",
      "Epoch [26/300], Step [27200/27733], Loss: 3.4426\n",
      "Epoch [26/300], Step [27300/27733], Loss: 2.5497\n",
      "Epoch [26/300], Step [27400/27733], Loss: 3.9934\n",
      "Epoch [26/300], Step [27500/27733], Loss: 5.2453\n",
      "Epoch [26/300], Step [27600/27733], Loss: 3.2960\n",
      "Epoch [26/300], Step [27700/27733], Loss: 3.2784\n",
      "Epoch [27/300], Step [100/27733], Loss: 1.8366\n",
      "Epoch [27/300], Step [200/27733], Loss: 2.5885\n",
      "Epoch [27/300], Step [300/27733], Loss: 3.2233\n",
      "Epoch [27/300], Step [400/27733], Loss: 2.5387\n",
      "Epoch [27/300], Step [500/27733], Loss: 2.1101\n",
      "Epoch [27/300], Step [600/27733], Loss: 3.3040\n",
      "Epoch [27/300], Step [700/27733], Loss: 3.0546\n",
      "Epoch [27/300], Step [800/27733], Loss: 2.0094\n",
      "Epoch [27/300], Step [900/27733], Loss: 2.3020\n",
      "Epoch [27/300], Step [1000/27733], Loss: 2.8813\n",
      "Epoch [27/300], Step [1100/27733], Loss: 3.1515\n",
      "Epoch [27/300], Step [1200/27733], Loss: 2.7075\n",
      "Epoch [27/300], Step [1300/27733], Loss: 2.1982\n",
      "Epoch [27/300], Step [1400/27733], Loss: 2.3302\n",
      "Epoch [27/300], Step [1500/27733], Loss: 2.2065\n",
      "Epoch [27/300], Step [1600/27733], Loss: 3.4054\n",
      "Epoch [27/300], Step [1700/27733], Loss: 2.3692\n",
      "Epoch [27/300], Step [1800/27733], Loss: 2.3931\n",
      "Epoch [27/300], Step [1900/27733], Loss: 2.7938\n",
      "Epoch [27/300], Step [2000/27733], Loss: 3.4328\n",
      "Epoch [27/300], Step [2100/27733], Loss: 2.7694\n",
      "Epoch [27/300], Step [2200/27733], Loss: 2.5573\n",
      "Epoch [27/300], Step [2300/27733], Loss: 2.2011\n",
      "Epoch [27/300], Step [2400/27733], Loss: 2.4687\n",
      "Epoch [27/300], Step [2500/27733], Loss: 2.1644\n",
      "Epoch [27/300], Step [2600/27733], Loss: 2.4032\n",
      "Epoch [27/300], Step [2700/27733], Loss: 2.6622\n",
      "Epoch [27/300], Step [2800/27733], Loss: 2.4243\n",
      "Epoch [27/300], Step [2900/27733], Loss: 1.9670\n",
      "Epoch [27/300], Step [3000/27733], Loss: 2.7612\n",
      "Epoch [27/300], Step [3100/27733], Loss: 3.0970\n",
      "Epoch [27/300], Step [3200/27733], Loss: 2.4521\n",
      "Epoch [27/300], Step [3300/27733], Loss: 2.5473\n",
      "Epoch [27/300], Step [3400/27733], Loss: 2.3148\n",
      "Epoch [27/300], Step [3500/27733], Loss: 2.5303\n",
      "Epoch [27/300], Step [3600/27733], Loss: 2.9774\n",
      "Epoch [27/300], Step [3700/27733], Loss: 2.4651\n",
      "Epoch [27/300], Step [3800/27733], Loss: 2.3587\n",
      "Epoch [27/300], Step [3900/27733], Loss: 3.5806\n",
      "Epoch [27/300], Step [4000/27733], Loss: 2.8392\n",
      "Epoch [27/300], Step [4100/27733], Loss: 2.4136\n",
      "Epoch [27/300], Step [4200/27733], Loss: 2.9120\n",
      "Epoch [27/300], Step [4300/27733], Loss: 2.4962\n",
      "Epoch [27/300], Step [4400/27733], Loss: 3.2400\n",
      "Epoch [27/300], Step [4500/27733], Loss: 2.9656\n",
      "Epoch [27/300], Step [4600/27733], Loss: 3.1560\n",
      "Epoch [27/300], Step [4700/27733], Loss: 2.4166\n",
      "Epoch [27/300], Step [4800/27733], Loss: 2.1817\n",
      "Epoch [27/300], Step [4900/27733], Loss: 2.4592\n",
      "Epoch [27/300], Step [5000/27733], Loss: 3.2780\n",
      "Epoch [27/300], Step [5100/27733], Loss: 2.4832\n",
      "Epoch [27/300], Step [5200/27733], Loss: 2.9813\n",
      "Epoch [27/300], Step [5300/27733], Loss: 2.5090\n",
      "Epoch [27/300], Step [5400/27733], Loss: 2.4846\n",
      "Epoch [27/300], Step [5500/27733], Loss: 2.1509\n",
      "Epoch [27/300], Step [5600/27733], Loss: 2.7896\n",
      "Epoch [27/300], Step [5700/27733], Loss: 2.0960\n",
      "Epoch [27/300], Step [5800/27733], Loss: 2.5142\n",
      "Epoch [27/300], Step [5900/27733], Loss: 3.0080\n",
      "Epoch [27/300], Step [6000/27733], Loss: 2.1096\n",
      "Epoch [27/300], Step [6100/27733], Loss: 2.6510\n",
      "Epoch [27/300], Step [6200/27733], Loss: 2.7534\n",
      "Epoch [27/300], Step [6300/27733], Loss: 2.8722\n",
      "Epoch [27/300], Step [6400/27733], Loss: 2.3253\n",
      "Epoch [27/300], Step [6500/27733], Loss: 2.3603\n",
      "Epoch [27/300], Step [6600/27733], Loss: 2.4462\n",
      "Epoch [27/300], Step [6700/27733], Loss: 2.6413\n",
      "Epoch [27/300], Step [6800/27733], Loss: 2.9366\n",
      "Epoch [27/300], Step [6900/27733], Loss: 2.8368\n",
      "Epoch [27/300], Step [7000/27733], Loss: 3.5338\n",
      "Epoch [27/300], Step [7100/27733], Loss: 2.1938\n",
      "Epoch [27/300], Step [7200/27733], Loss: 2.7205\n",
      "Epoch [27/300], Step [7300/27733], Loss: 3.5059\n",
      "Epoch [27/300], Step [7400/27733], Loss: 2.3190\n",
      "Epoch [27/300], Step [7500/27733], Loss: 3.2594\n",
      "Epoch [27/300], Step [7600/27733], Loss: 2.3439\n",
      "Epoch [27/300], Step [7700/27733], Loss: 3.1987\n",
      "Epoch [27/300], Step [7800/27733], Loss: 3.2269\n",
      "Epoch [27/300], Step [7900/27733], Loss: 2.5578\n",
      "Epoch [27/300], Step [8000/27733], Loss: 2.9995\n",
      "Epoch [27/300], Step [8100/27733], Loss: 2.2721\n",
      "Epoch [27/300], Step [8200/27733], Loss: 3.2464\n",
      "Epoch [27/300], Step [8300/27733], Loss: 3.1343\n",
      "Epoch [27/300], Step [8400/27733], Loss: 3.2557\n",
      "Epoch [27/300], Step [8500/27733], Loss: 2.7956\n",
      "Epoch [27/300], Step [8600/27733], Loss: 2.6918\n",
      "Epoch [27/300], Step [8700/27733], Loss: 2.5134\n",
      "Epoch [27/300], Step [8800/27733], Loss: 2.6310\n",
      "Epoch [27/300], Step [8900/27733], Loss: 2.7299\n",
      "Epoch [27/300], Step [9000/27733], Loss: 3.1358\n",
      "Epoch [27/300], Step [9100/27733], Loss: 1.8914\n",
      "Epoch [27/300], Step [9200/27733], Loss: 2.7632\n",
      "Epoch [27/300], Step [9300/27733], Loss: 3.2720\n",
      "Epoch [27/300], Step [9400/27733], Loss: 2.7115\n",
      "Epoch [27/300], Step [9500/27733], Loss: 1.7476\n",
      "Epoch [27/300], Step [9600/27733], Loss: 2.7087\n",
      "Epoch [27/300], Step [9700/27733], Loss: 3.2711\n",
      "Epoch [27/300], Step [9800/27733], Loss: 2.2179\n",
      "Epoch [27/300], Step [9900/27733], Loss: 3.1754\n",
      "Epoch [27/300], Step [10000/27733], Loss: 2.2172\n",
      "Epoch [27/300], Step [10100/27733], Loss: 2.6734\n",
      "Epoch [27/300], Step [10200/27733], Loss: 2.8045\n",
      "Epoch [27/300], Step [10300/27733], Loss: 3.4530\n",
      "Epoch [27/300], Step [10400/27733], Loss: 3.1228\n",
      "Epoch [27/300], Step [10500/27733], Loss: 2.4407\n",
      "Epoch [27/300], Step [10600/27733], Loss: 2.8725\n",
      "Epoch [27/300], Step [10700/27733], Loss: 3.1670\n",
      "Epoch [27/300], Step [10800/27733], Loss: 2.9738\n",
      "Epoch [27/300], Step [10900/27733], Loss: 3.0118\n",
      "Epoch [27/300], Step [11000/27733], Loss: 2.8674\n",
      "Epoch [27/300], Step [11100/27733], Loss: 2.4804\n",
      "Epoch [27/300], Step [11200/27733], Loss: 2.5058\n",
      "Epoch [27/300], Step [11300/27733], Loss: 2.5422\n",
      "Epoch [27/300], Step [11400/27733], Loss: 2.3368\n",
      "Epoch [27/300], Step [11500/27733], Loss: 3.4673\n",
      "Epoch [27/300], Step [11600/27733], Loss: 2.5942\n",
      "Epoch [27/300], Step [11700/27733], Loss: 2.5664\n",
      "Epoch [27/300], Step [11800/27733], Loss: 3.7726\n",
      "Epoch [27/300], Step [11900/27733], Loss: 3.6024\n",
      "Epoch [27/300], Step [12000/27733], Loss: 2.4062\n",
      "Epoch [27/300], Step [12100/27733], Loss: 3.4151\n",
      "Epoch [27/300], Step [12200/27733], Loss: 2.8138\n",
      "Epoch [27/300], Step [12300/27733], Loss: 2.2563\n",
      "Epoch [27/300], Step [12400/27733], Loss: 3.2925\n",
      "Epoch [27/300], Step [12500/27733], Loss: 2.4827\n",
      "Epoch [27/300], Step [12600/27733], Loss: 3.1968\n",
      "Epoch [27/300], Step [12700/27733], Loss: 3.2282\n",
      "Epoch [27/300], Step [12800/27733], Loss: 3.2493\n",
      "Epoch [27/300], Step [12900/27733], Loss: 2.7734\n",
      "Epoch [27/300], Step [13000/27733], Loss: 2.7388\n",
      "Epoch [27/300], Step [13100/27733], Loss: 3.6033\n",
      "Epoch [27/300], Step [13200/27733], Loss: 2.8379\n",
      "Epoch [27/300], Step [13300/27733], Loss: 2.1751\n",
      "Epoch [27/300], Step [13400/27733], Loss: 2.7338\n",
      "Epoch [27/300], Step [13500/27733], Loss: 2.8770\n",
      "Epoch [27/300], Step [13600/27733], Loss: 3.6199\n",
      "Epoch [27/300], Step [13700/27733], Loss: 2.8143\n",
      "Epoch [27/300], Step [13800/27733], Loss: 2.9834\n",
      "Epoch [27/300], Step [13900/27733], Loss: 2.7921\n",
      "Epoch [27/300], Step [14000/27733], Loss: 3.7478\n",
      "Epoch [27/300], Step [14100/27733], Loss: 2.0268\n",
      "Epoch [27/300], Step [14200/27733], Loss: 3.1344\n",
      "Epoch [27/300], Step [14300/27733], Loss: 3.1607\n",
      "Epoch [27/300], Step [14400/27733], Loss: 3.2720\n",
      "Epoch [27/300], Step [14500/27733], Loss: 2.8993\n",
      "Epoch [27/300], Step [14600/27733], Loss: 3.5087\n",
      "Epoch [27/300], Step [14700/27733], Loss: 3.2537\n",
      "Epoch [27/300], Step [14800/27733], Loss: 2.4726\n",
      "Epoch [27/300], Step [14900/27733], Loss: 2.2119\n",
      "Epoch [27/300], Step [15000/27733], Loss: 3.2706\n",
      "Epoch [27/300], Step [15100/27733], Loss: 2.8644\n",
      "Epoch [27/300], Step [15200/27733], Loss: 3.7836\n",
      "Epoch [27/300], Step [15300/27733], Loss: 2.9838\n",
      "Epoch [27/300], Step [15400/27733], Loss: 3.1324\n",
      "Epoch [27/300], Step [15500/27733], Loss: 2.9902\n",
      "Epoch [27/300], Step [15600/27733], Loss: 2.3506\n",
      "Epoch [27/300], Step [15700/27733], Loss: 3.8122\n",
      "Epoch [27/300], Step [15800/27733], Loss: 2.1647\n",
      "Epoch [27/300], Step [15900/27733], Loss: 2.7224\n",
      "Epoch [27/300], Step [16000/27733], Loss: 3.2675\n",
      "Epoch [27/300], Step [16100/27733], Loss: 2.5808\n",
      "Epoch [27/300], Step [16200/27733], Loss: 3.0829\n",
      "Epoch [27/300], Step [16300/27733], Loss: 3.3873\n",
      "Epoch [27/300], Step [16400/27733], Loss: 2.6611\n",
      "Epoch [27/300], Step [16500/27733], Loss: 2.9788\n",
      "Epoch [27/300], Step [16600/27733], Loss: 3.0720\n",
      "Epoch [27/300], Step [16700/27733], Loss: 2.7883\n",
      "Epoch [27/300], Step [16800/27733], Loss: 3.7534\n",
      "Epoch [27/300], Step [16900/27733], Loss: 3.0911\n",
      "Epoch [27/300], Step [17000/27733], Loss: 2.7523\n",
      "Epoch [27/300], Step [17100/27733], Loss: 2.2423\n",
      "Epoch [27/300], Step [17200/27733], Loss: 2.7429\n",
      "Epoch [27/300], Step [17300/27733], Loss: 2.7073\n",
      "Epoch [27/300], Step [17400/27733], Loss: 3.5534\n",
      "Epoch [27/300], Step [17500/27733], Loss: 3.7363\n",
      "Epoch [27/300], Step [17600/27733], Loss: 3.3211\n",
      "Epoch [27/300], Step [17700/27733], Loss: 2.6799\n",
      "Epoch [27/300], Step [17800/27733], Loss: 2.9441\n",
      "Epoch [27/300], Step [17900/27733], Loss: 3.6901\n",
      "Epoch [27/300], Step [18000/27733], Loss: 2.6078\n",
      "Epoch [27/300], Step [18100/27733], Loss: 2.6235\n",
      "Epoch [27/300], Step [18200/27733], Loss: 3.6781\n",
      "Epoch [27/300], Step [18300/27733], Loss: 3.2010\n",
      "Epoch [27/300], Step [18400/27733], Loss: 2.7302\n",
      "Epoch [27/300], Step [18500/27733], Loss: 3.3861\n",
      "Epoch [27/300], Step [18600/27733], Loss: 2.4449\n",
      "Epoch [27/300], Step [18700/27733], Loss: 2.8649\n",
      "Epoch [27/300], Step [18800/27733], Loss: 3.3704\n",
      "Epoch [27/300], Step [18900/27733], Loss: 3.1558\n",
      "Epoch [27/300], Step [19000/27733], Loss: 2.7618\n",
      "Epoch [27/300], Step [19100/27733], Loss: 3.6277\n",
      "Epoch [27/300], Step [19200/27733], Loss: 2.6016\n",
      "Epoch [27/300], Step [19300/27733], Loss: 2.9937\n",
      "Epoch [27/300], Step [19400/27733], Loss: 2.8212\n",
      "Epoch [27/300], Step [19500/27733], Loss: 4.0385\n",
      "Epoch [27/300], Step [19600/27733], Loss: 3.4052\n",
      "Epoch [27/300], Step [19700/27733], Loss: 3.3007\n",
      "Epoch [27/300], Step [19800/27733], Loss: 3.3268\n",
      "Epoch [27/300], Step [19900/27733], Loss: 2.3054\n",
      "Epoch [27/300], Step [20000/27733], Loss: 3.3873\n",
      "Epoch [27/300], Step [20100/27733], Loss: 3.6008\n",
      "Epoch [27/300], Step [20200/27733], Loss: 3.7086\n",
      "Epoch [27/300], Step [20300/27733], Loss: 3.4810\n",
      "Epoch [27/300], Step [20400/27733], Loss: 3.0735\n",
      "Epoch [27/300], Step [20500/27733], Loss: 2.9495\n",
      "Epoch [27/300], Step [20600/27733], Loss: 3.2339\n",
      "Epoch [27/300], Step [20700/27733], Loss: 2.8217\n",
      "Epoch [27/300], Step [20800/27733], Loss: 2.8041\n",
      "Epoch [27/300], Step [20900/27733], Loss: 3.2769\n",
      "Epoch [27/300], Step [21000/27733], Loss: 3.2272\n",
      "Epoch [27/300], Step [21100/27733], Loss: 3.2078\n",
      "Epoch [27/300], Step [21200/27733], Loss: 3.3327\n",
      "Epoch [27/300], Step [21300/27733], Loss: 3.7175\n",
      "Epoch [27/300], Step [21400/27733], Loss: 2.5827\n",
      "Epoch [27/300], Step [21500/27733], Loss: 2.0520\n",
      "Epoch [27/300], Step [21600/27733], Loss: 3.3247\n",
      "Epoch [27/300], Step [21700/27733], Loss: 3.2302\n",
      "Epoch [27/300], Step [21800/27733], Loss: 2.7382\n",
      "Epoch [27/300], Step [21900/27733], Loss: 3.3412\n",
      "Epoch [27/300], Step [22000/27733], Loss: 2.9363\n",
      "Epoch [27/300], Step [22100/27733], Loss: 4.1192\n",
      "Epoch [27/300], Step [22200/27733], Loss: 3.9384\n",
      "Epoch [27/300], Step [22300/27733], Loss: 3.1228\n",
      "Epoch [27/300], Step [22400/27733], Loss: 3.3265\n",
      "Epoch [27/300], Step [22500/27733], Loss: 3.4096\n",
      "Epoch [27/300], Step [22600/27733], Loss: 2.9836\n",
      "Epoch [27/300], Step [22700/27733], Loss: 3.4138\n",
      "Epoch [27/300], Step [22800/27733], Loss: 2.3748\n",
      "Epoch [27/300], Step [22900/27733], Loss: 2.9306\n",
      "Epoch [27/300], Step [23000/27733], Loss: 3.9936\n",
      "Epoch [27/300], Step [23100/27733], Loss: 3.3131\n",
      "Epoch [27/300], Step [23200/27733], Loss: 3.5712\n",
      "Epoch [27/300], Step [23300/27733], Loss: 2.4871\n",
      "Epoch [27/300], Step [23400/27733], Loss: 2.1970\n",
      "Epoch [27/300], Step [23500/27733], Loss: 3.4819\n",
      "Epoch [27/300], Step [23600/27733], Loss: 3.2051\n",
      "Epoch [27/300], Step [23700/27733], Loss: 3.2411\n",
      "Epoch [27/300], Step [23800/27733], Loss: 3.7450\n",
      "Epoch [27/300], Step [23900/27733], Loss: 2.6188\n",
      "Epoch [27/300], Step [24000/27733], Loss: 2.5710\n",
      "Epoch [27/300], Step [24100/27733], Loss: 3.4614\n",
      "Epoch [27/300], Step [24200/27733], Loss: 3.1158\n",
      "Epoch [27/300], Step [24300/27733], Loss: 3.7996\n",
      "Epoch [27/300], Step [24400/27733], Loss: 2.9361\n",
      "Epoch [27/300], Step [24500/27733], Loss: 3.4960\n",
      "Epoch [27/300], Step [24600/27733], Loss: 4.2683\n",
      "Epoch [27/300], Step [24700/27733], Loss: 2.8969\n",
      "Epoch [27/300], Step [24800/27733], Loss: 3.0632\n",
      "Epoch [27/300], Step [24900/27733], Loss: 4.2134\n",
      "Epoch [27/300], Step [25000/27733], Loss: 2.5710\n",
      "Epoch [27/300], Step [25100/27733], Loss: 4.0301\n",
      "Epoch [27/300], Step [25200/27733], Loss: 3.7957\n",
      "Epoch [27/300], Step [25300/27733], Loss: 3.1455\n",
      "Epoch [27/300], Step [25400/27733], Loss: 3.8648\n",
      "Epoch [27/300], Step [25500/27733], Loss: 3.5885\n",
      "Epoch [27/300], Step [25600/27733], Loss: 3.2415\n",
      "Epoch [27/300], Step [25700/27733], Loss: 4.1431\n",
      "Epoch [27/300], Step [25800/27733], Loss: 2.8876\n",
      "Epoch [27/300], Step [25900/27733], Loss: 3.1509\n",
      "Epoch [27/300], Step [26000/27733], Loss: 2.6892\n",
      "Epoch [27/300], Step [26100/27733], Loss: 3.7101\n",
      "Epoch [27/300], Step [26200/27733], Loss: 3.7182\n",
      "Epoch [27/300], Step [26300/27733], Loss: 3.1947\n",
      "Epoch [27/300], Step [26400/27733], Loss: 3.0567\n",
      "Epoch [27/300], Step [26500/27733], Loss: 3.3912\n",
      "Epoch [27/300], Step [26600/27733], Loss: 3.3320\n",
      "Epoch [27/300], Step [26700/27733], Loss: 2.9847\n",
      "Epoch [27/300], Step [26800/27733], Loss: 4.2194\n",
      "Epoch [27/300], Step [26900/27733], Loss: 2.8076\n",
      "Epoch [27/300], Step [27000/27733], Loss: 2.5779\n",
      "Epoch [27/300], Step [27100/27733], Loss: 3.1684\n",
      "Epoch [27/300], Step [27200/27733], Loss: 3.0387\n",
      "Epoch [27/300], Step [27300/27733], Loss: 3.2070\n",
      "Epoch [27/300], Step [27400/27733], Loss: 3.0469\n",
      "Epoch [27/300], Step [27500/27733], Loss: 2.8657\n",
      "Epoch [27/300], Step [27600/27733], Loss: 2.7129\n",
      "Epoch [27/300], Step [27700/27733], Loss: 2.0443\n",
      "Epoch [28/300], Step [100/27733], Loss: 2.7673\n",
      "Epoch [28/300], Step [200/27733], Loss: 2.4743\n",
      "Epoch [28/300], Step [300/27733], Loss: 2.6398\n",
      "Epoch [28/300], Step [400/27733], Loss: 2.6350\n",
      "Epoch [28/300], Step [500/27733], Loss: 3.3766\n",
      "Epoch [28/300], Step [600/27733], Loss: 2.5513\n",
      "Epoch [28/300], Step [700/27733], Loss: 2.6352\n",
      "Epoch [28/300], Step [800/27733], Loss: 2.8353\n",
      "Epoch [28/300], Step [900/27733], Loss: 3.1145\n",
      "Epoch [28/300], Step [1000/27733], Loss: 2.0261\n",
      "Epoch [28/300], Step [1100/27733], Loss: 2.9753\n",
      "Epoch [28/300], Step [1200/27733], Loss: 2.1950\n",
      "Epoch [28/300], Step [1300/27733], Loss: 2.8720\n",
      "Epoch [28/300], Step [1400/27733], Loss: 2.4872\n",
      "Epoch [28/300], Step [1500/27733], Loss: 2.6932\n",
      "Epoch [28/300], Step [1600/27733], Loss: 2.7322\n",
      "Epoch [28/300], Step [1700/27733], Loss: 1.9237\n",
      "Epoch [28/300], Step [1800/27733], Loss: 2.5996\n",
      "Epoch [28/300], Step [1900/27733], Loss: 2.3487\n",
      "Epoch [28/300], Step [2000/27733], Loss: 2.1214\n",
      "Epoch [28/300], Step [2100/27733], Loss: 2.4020\n",
      "Epoch [28/300], Step [2200/27733], Loss: 2.1929\n",
      "Epoch [28/300], Step [2300/27733], Loss: 2.3948\n",
      "Epoch [28/300], Step [2400/27733], Loss: 2.9909\n",
      "Epoch [28/300], Step [2500/27733], Loss: 2.4578\n",
      "Epoch [28/300], Step [2600/27733], Loss: 2.9437\n",
      "Epoch [28/300], Step [2700/27733], Loss: 2.3545\n",
      "Epoch [28/300], Step [2800/27733], Loss: 1.8906\n",
      "Epoch [28/300], Step [2900/27733], Loss: 2.2132\n",
      "Epoch [28/300], Step [3000/27733], Loss: 3.1905\n",
      "Epoch [28/300], Step [3100/27733], Loss: 2.9975\n",
      "Epoch [28/300], Step [3200/27733], Loss: 2.7740\n",
      "Epoch [28/300], Step [3300/27733], Loss: 2.2878\n",
      "Epoch [28/300], Step [3400/27733], Loss: 2.5557\n",
      "Epoch [28/300], Step [3500/27733], Loss: 3.4914\n",
      "Epoch [28/300], Step [3600/27733], Loss: 2.1948\n",
      "Epoch [28/300], Step [3700/27733], Loss: 2.9682\n",
      "Epoch [28/300], Step [3800/27733], Loss: 2.8848\n",
      "Epoch [28/300], Step [3900/27733], Loss: 2.7737\n",
      "Epoch [28/300], Step [4000/27733], Loss: 3.2792\n",
      "Epoch [28/300], Step [4100/27733], Loss: 2.9290\n",
      "Epoch [28/300], Step [4200/27733], Loss: 2.4230\n",
      "Epoch [28/300], Step [4300/27733], Loss: 2.4542\n",
      "Epoch [28/300], Step [4400/27733], Loss: 2.8352\n",
      "Epoch [28/300], Step [4500/27733], Loss: 2.2087\n",
      "Epoch [28/300], Step [4600/27733], Loss: 3.8331\n",
      "Epoch [28/300], Step [4700/27733], Loss: 2.4961\n",
      "Epoch [28/300], Step [4800/27733], Loss: 2.9780\n",
      "Epoch [28/300], Step [4900/27733], Loss: 2.5823\n",
      "Epoch [28/300], Step [5000/27733], Loss: 3.0180\n",
      "Epoch [28/300], Step [5100/27733], Loss: 2.3351\n",
      "Epoch [28/300], Step [5200/27733], Loss: 3.1040\n",
      "Epoch [28/300], Step [5300/27733], Loss: 3.4954\n",
      "Epoch [28/300], Step [5400/27733], Loss: 2.6009\n",
      "Epoch [28/300], Step [5500/27733], Loss: 2.4922\n",
      "Epoch [28/300], Step [5600/27733], Loss: 3.1226\n",
      "Epoch [28/300], Step [5700/27733], Loss: 3.0225\n",
      "Epoch [28/300], Step [5800/27733], Loss: 2.5335\n",
      "Epoch [28/300], Step [5900/27733], Loss: 2.6347\n",
      "Epoch [28/300], Step [6000/27733], Loss: 2.4419\n",
      "Epoch [28/300], Step [6100/27733], Loss: 2.9452\n",
      "Epoch [28/300], Step [6200/27733], Loss: 2.0853\n",
      "Epoch [28/300], Step [6300/27733], Loss: 2.4661\n",
      "Epoch [28/300], Step [6400/27733], Loss: 2.5721\n",
      "Epoch [28/300], Step [6500/27733], Loss: 2.2867\n",
      "Epoch [28/300], Step [6600/27733], Loss: 2.7214\n",
      "Epoch [28/300], Step [6700/27733], Loss: 2.7623\n",
      "Epoch [28/300], Step [6800/27733], Loss: 2.8793\n",
      "Epoch [28/300], Step [6900/27733], Loss: 3.4485\n",
      "Epoch [28/300], Step [7000/27733], Loss: 3.0944\n",
      "Epoch [28/300], Step [7100/27733], Loss: 3.3144\n",
      "Epoch [28/300], Step [7200/27733], Loss: 2.7384\n",
      "Epoch [28/300], Step [7300/27733], Loss: 3.8226\n",
      "Epoch [28/300], Step [7400/27733], Loss: 2.3891\n",
      "Epoch [28/300], Step [7500/27733], Loss: 3.3142\n",
      "Epoch [28/300], Step [7600/27733], Loss: 3.1795\n",
      "Epoch [28/300], Step [7700/27733], Loss: 2.3179\n",
      "Epoch [28/300], Step [7800/27733], Loss: 2.8332\n",
      "Epoch [28/300], Step [7900/27733], Loss: 2.2839\n",
      "Epoch [28/300], Step [8000/27733], Loss: 3.3574\n",
      "Epoch [28/300], Step [8100/27733], Loss: 2.8056\n",
      "Epoch [28/300], Step [8200/27733], Loss: 2.9513\n",
      "Epoch [28/300], Step [8300/27733], Loss: 3.3496\n",
      "Epoch [28/300], Step [8400/27733], Loss: 2.1316\n",
      "Epoch [28/300], Step [8500/27733], Loss: 2.9966\n",
      "Epoch [28/300], Step [8600/27733], Loss: 3.4429\n",
      "Epoch [28/300], Step [8700/27733], Loss: 2.2510\n",
      "Epoch [28/300], Step [8800/27733], Loss: 3.2645\n",
      "Epoch [28/300], Step [8900/27733], Loss: 2.9598\n",
      "Epoch [28/300], Step [9000/27733], Loss: 3.7730\n",
      "Epoch [28/300], Step [9100/27733], Loss: 2.1737\n",
      "Epoch [28/300], Step [9200/27733], Loss: 2.8577\n",
      "Epoch [28/300], Step [9300/27733], Loss: 2.4539\n",
      "Epoch [28/300], Step [9400/27733], Loss: 3.1893\n",
      "Epoch [28/300], Step [9500/27733], Loss: 3.2589\n",
      "Epoch [28/300], Step [9600/27733], Loss: 2.5035\n",
      "Epoch [28/300], Step [9700/27733], Loss: 2.6191\n",
      "Epoch [28/300], Step [9800/27733], Loss: 2.9326\n",
      "Epoch [28/300], Step [9900/27733], Loss: 2.4006\n",
      "Epoch [28/300], Step [10000/27733], Loss: 2.9744\n",
      "Epoch [28/300], Step [10100/27733], Loss: 3.2609\n",
      "Epoch [28/300], Step [10200/27733], Loss: 3.4121\n",
      "Epoch [28/300], Step [10300/27733], Loss: 2.5232\n",
      "Epoch [28/300], Step [10400/27733], Loss: 3.0099\n",
      "Epoch [28/300], Step [10500/27733], Loss: 2.6829\n",
      "Epoch [28/300], Step [10600/27733], Loss: 3.0437\n",
      "Epoch [28/300], Step [10700/27733], Loss: 3.3538\n",
      "Epoch [28/300], Step [10800/27733], Loss: 2.8440\n",
      "Epoch [28/300], Step [10900/27733], Loss: 2.6624\n",
      "Epoch [28/300], Step [11000/27733], Loss: 3.2192\n",
      "Epoch [28/300], Step [11100/27733], Loss: 2.9516\n",
      "Epoch [28/300], Step [11200/27733], Loss: 3.4410\n",
      "Epoch [28/300], Step [11300/27733], Loss: 2.4189\n",
      "Epoch [28/300], Step [11400/27733], Loss: 2.6174\n",
      "Epoch [28/300], Step [11500/27733], Loss: 2.8762\n",
      "Epoch [28/300], Step [11600/27733], Loss: 3.2100\n",
      "Epoch [28/300], Step [11700/27733], Loss: 1.8144\n",
      "Epoch [28/300], Step [11800/27733], Loss: 2.7134\n",
      "Epoch [28/300], Step [11900/27733], Loss: 2.2417\n",
      "Epoch [28/300], Step [12000/27733], Loss: 2.8231\n",
      "Epoch [28/300], Step [12100/27733], Loss: 2.8172\n",
      "Epoch [28/300], Step [12200/27733], Loss: 3.2014\n",
      "Epoch [28/300], Step [12300/27733], Loss: 2.4024\n",
      "Epoch [28/300], Step [12400/27733], Loss: 3.2863\n",
      "Epoch [28/300], Step [12500/27733], Loss: 3.2842\n",
      "Epoch [28/300], Step [12600/27733], Loss: 3.1312\n",
      "Epoch [28/300], Step [12700/27733], Loss: 2.7600\n",
      "Epoch [28/300], Step [12800/27733], Loss: 3.5058\n",
      "Epoch [28/300], Step [12900/27733], Loss: 3.0973\n",
      "Epoch [28/300], Step [13000/27733], Loss: 2.9405\n",
      "Epoch [28/300], Step [13100/27733], Loss: 2.5452\n",
      "Epoch [28/300], Step [13200/27733], Loss: 2.8220\n",
      "Epoch [28/300], Step [13300/27733], Loss: 1.9427\n",
      "Epoch [28/300], Step [13400/27733], Loss: 3.0291\n",
      "Epoch [28/300], Step [13500/27733], Loss: 2.7224\n",
      "Epoch [28/300], Step [13600/27733], Loss: 2.7336\n",
      "Epoch [28/300], Step [13700/27733], Loss: 3.8721\n",
      "Epoch [28/300], Step [13800/27733], Loss: 3.3825\n",
      "Epoch [28/300], Step [13900/27733], Loss: 2.9637\n",
      "Epoch [28/300], Step [14000/27733], Loss: 2.9090\n",
      "Epoch [28/300], Step [14100/27733], Loss: 2.8211\n",
      "Epoch [28/300], Step [14200/27733], Loss: 3.2530\n",
      "Epoch [28/300], Step [14300/27733], Loss: 3.0654\n",
      "Epoch [28/300], Step [14400/27733], Loss: 3.0997\n",
      "Epoch [28/300], Step [14500/27733], Loss: 2.6757\n",
      "Epoch [28/300], Step [14600/27733], Loss: 3.3892\n",
      "Epoch [28/300], Step [14700/27733], Loss: 3.5111\n",
      "Epoch [28/300], Step [14800/27733], Loss: 2.7736\n",
      "Epoch [28/300], Step [14900/27733], Loss: 2.5321\n",
      "Epoch [28/300], Step [15000/27733], Loss: 3.0649\n",
      "Epoch [28/300], Step [15100/27733], Loss: 2.4722\n",
      "Epoch [28/300], Step [15200/27733], Loss: 3.1130\n",
      "Epoch [28/300], Step [15300/27733], Loss: 2.8231\n",
      "Epoch [28/300], Step [15400/27733], Loss: 3.0065\n",
      "Epoch [28/300], Step [15500/27733], Loss: 3.1023\n",
      "Epoch [28/300], Step [15600/27733], Loss: 3.4491\n",
      "Epoch [28/300], Step [15700/27733], Loss: 3.0654\n",
      "Epoch [28/300], Step [15800/27733], Loss: 4.0890\n",
      "Epoch [28/300], Step [15900/27733], Loss: 3.4140\n",
      "Epoch [28/300], Step [16000/27733], Loss: 3.0257\n",
      "Epoch [28/300], Step [16100/27733], Loss: 3.2098\n",
      "Epoch [28/300], Step [16200/27733], Loss: 2.8110\n",
      "Epoch [28/300], Step [16300/27733], Loss: 3.4926\n",
      "Epoch [28/300], Step [16400/27733], Loss: 3.0331\n",
      "Epoch [28/300], Step [16500/27733], Loss: 3.3817\n",
      "Epoch [28/300], Step [16600/27733], Loss: 2.8035\n",
      "Epoch [28/300], Step [16700/27733], Loss: 2.3330\n",
      "Epoch [28/300], Step [16800/27733], Loss: 2.3986\n",
      "Epoch [28/300], Step [16900/27733], Loss: 2.5560\n",
      "Epoch [28/300], Step [17000/27733], Loss: 3.5096\n",
      "Epoch [28/300], Step [17100/27733], Loss: 2.2061\n",
      "Epoch [28/300], Step [17200/27733], Loss: 2.4401\n",
      "Epoch [28/300], Step [17300/27733], Loss: 2.3249\n",
      "Epoch [28/300], Step [17400/27733], Loss: 3.2192\n",
      "Epoch [28/300], Step [17500/27733], Loss: 3.6268\n",
      "Epoch [28/300], Step [17600/27733], Loss: 2.7420\n",
      "Epoch [28/300], Step [17700/27733], Loss: 2.8287\n",
      "Epoch [28/300], Step [17800/27733], Loss: 2.3607\n",
      "Epoch [28/300], Step [17900/27733], Loss: 3.4958\n",
      "Epoch [28/300], Step [18000/27733], Loss: 2.0633\n",
      "Epoch [28/300], Step [18100/27733], Loss: 2.9057\n",
      "Epoch [28/300], Step [18200/27733], Loss: 2.8670\n",
      "Epoch [28/300], Step [18300/27733], Loss: 2.7238\n",
      "Epoch [28/300], Step [18400/27733], Loss: 3.6146\n",
      "Epoch [28/300], Step [18500/27733], Loss: 2.4998\n",
      "Epoch [28/300], Step [18600/27733], Loss: 3.4186\n",
      "Epoch [28/300], Step [18700/27733], Loss: 3.7163\n",
      "Epoch [28/300], Step [18800/27733], Loss: 3.6045\n",
      "Epoch [28/300], Step [18900/27733], Loss: 3.0982\n",
      "Epoch [28/300], Step [19000/27733], Loss: 3.0915\n",
      "Epoch [28/300], Step [19100/27733], Loss: 2.6130\n",
      "Epoch [28/300], Step [19200/27733], Loss: 3.6447\n",
      "Epoch [28/300], Step [19300/27733], Loss: 3.7613\n",
      "Epoch [28/300], Step [19400/27733], Loss: 2.8305\n",
      "Epoch [28/300], Step [19500/27733], Loss: 2.9659\n",
      "Epoch [28/300], Step [19600/27733], Loss: 2.8654\n",
      "Epoch [28/300], Step [19700/27733], Loss: 2.8375\n",
      "Epoch [28/300], Step [19800/27733], Loss: 2.4382\n",
      "Epoch [28/300], Step [19900/27733], Loss: 2.8297\n",
      "Epoch [28/300], Step [20000/27733], Loss: 3.3302\n",
      "Epoch [28/300], Step [20100/27733], Loss: 3.7509\n",
      "Epoch [28/300], Step [20200/27733], Loss: 3.1612\n",
      "Epoch [28/300], Step [20300/27733], Loss: 3.3532\n",
      "Epoch [28/300], Step [20400/27733], Loss: 3.0805\n",
      "Epoch [28/300], Step [20500/27733], Loss: 2.9204\n",
      "Epoch [28/300], Step [20600/27733], Loss: 3.0972\n",
      "Epoch [28/300], Step [20700/27733], Loss: 3.0083\n",
      "Epoch [28/300], Step [20800/27733], Loss: 3.1239\n",
      "Epoch [28/300], Step [20900/27733], Loss: 3.3902\n",
      "Epoch [28/300], Step [21000/27733], Loss: 2.3401\n",
      "Epoch [28/300], Step [21100/27733], Loss: 3.4165\n",
      "Epoch [28/300], Step [21200/27733], Loss: 3.5183\n",
      "Epoch [28/300], Step [21300/27733], Loss: 3.3850\n",
      "Epoch [28/300], Step [21400/27733], Loss: 2.4407\n",
      "Epoch [28/300], Step [21500/27733], Loss: 3.7353\n",
      "Epoch [28/300], Step [21600/27733], Loss: 3.7360\n",
      "Epoch [28/300], Step [21700/27733], Loss: 3.4698\n",
      "Epoch [28/300], Step [21800/27733], Loss: 2.6023\n",
      "Epoch [28/300], Step [21900/27733], Loss: 2.8393\n",
      "Epoch [28/300], Step [22000/27733], Loss: 3.6654\n",
      "Epoch [28/300], Step [22100/27733], Loss: 3.5612\n",
      "Epoch [28/300], Step [22200/27733], Loss: 3.7197\n",
      "Epoch [28/300], Step [22300/27733], Loss: 3.2736\n",
      "Epoch [28/300], Step [22400/27733], Loss: 3.3828\n",
      "Epoch [28/300], Step [22500/27733], Loss: 3.0112\n",
      "Epoch [28/300], Step [22600/27733], Loss: 3.3071\n",
      "Epoch [28/300], Step [22700/27733], Loss: 3.8202\n",
      "Epoch [28/300], Step [22800/27733], Loss: 2.8758\n",
      "Epoch [28/300], Step [22900/27733], Loss: 3.3616\n",
      "Epoch [28/300], Step [23000/27733], Loss: 2.5987\n",
      "Epoch [28/300], Step [23100/27733], Loss: 3.5595\n",
      "Epoch [28/300], Step [23200/27733], Loss: 2.8950\n",
      "Epoch [28/300], Step [23300/27733], Loss: 2.8033\n",
      "Epoch [28/300], Step [23400/27733], Loss: 2.8747\n",
      "Epoch [28/300], Step [23500/27733], Loss: 3.1805\n",
      "Epoch [28/300], Step [23600/27733], Loss: 4.0353\n",
      "Epoch [28/300], Step [23700/27733], Loss: 3.5410\n",
      "Epoch [28/300], Step [23800/27733], Loss: 2.7682\n",
      "Epoch [28/300], Step [23900/27733], Loss: 3.3432\n",
      "Epoch [28/300], Step [24000/27733], Loss: 3.3037\n",
      "Epoch [28/300], Step [24100/27733], Loss: 3.3996\n",
      "Epoch [28/300], Step [24200/27733], Loss: 3.7000\n",
      "Epoch [28/300], Step [24300/27733], Loss: 3.7147\n",
      "Epoch [28/300], Step [24400/27733], Loss: 3.3687\n",
      "Epoch [28/300], Step [24500/27733], Loss: 3.3309\n",
      "Epoch [28/300], Step [24600/27733], Loss: 2.5645\n",
      "Epoch [28/300], Step [24700/27733], Loss: 2.3535\n",
      "Epoch [28/300], Step [24800/27733], Loss: 2.5812\n",
      "Epoch [28/300], Step [24900/27733], Loss: 2.9916\n",
      "Epoch [28/300], Step [25000/27733], Loss: 3.4363\n",
      "Epoch [28/300], Step [25100/27733], Loss: 3.0848\n",
      "Epoch [28/300], Step [25200/27733], Loss: 3.2989\n",
      "Epoch [28/300], Step [25300/27733], Loss: 3.2142\n",
      "Epoch [28/300], Step [25400/27733], Loss: 3.2064\n",
      "Epoch [28/300], Step [25500/27733], Loss: 2.8055\n",
      "Epoch [28/300], Step [25600/27733], Loss: 3.4577\n",
      "Epoch [28/300], Step [25700/27733], Loss: 2.5387\n",
      "Epoch [28/300], Step [25800/27733], Loss: 3.0330\n",
      "Epoch [28/300], Step [25900/27733], Loss: 3.6396\n",
      "Epoch [28/300], Step [26000/27733], Loss: 2.8443\n",
      "Epoch [28/300], Step [26100/27733], Loss: 2.7309\n",
      "Epoch [28/300], Step [26200/27733], Loss: 3.0613\n",
      "Epoch [28/300], Step [26300/27733], Loss: 2.6434\n",
      "Epoch [28/300], Step [26400/27733], Loss: 2.7729\n",
      "Epoch [28/300], Step [26500/27733], Loss: 3.1955\n",
      "Epoch [28/300], Step [26600/27733], Loss: 2.7905\n",
      "Epoch [28/300], Step [26700/27733], Loss: 2.5869\n",
      "Epoch [28/300], Step [26800/27733], Loss: 3.1333\n",
      "Epoch [28/300], Step [26900/27733], Loss: 2.8469\n",
      "Epoch [28/300], Step [27000/27733], Loss: 3.9023\n",
      "Epoch [28/300], Step [27100/27733], Loss: 3.5363\n",
      "Epoch [28/300], Step [27200/27733], Loss: 2.8578\n",
      "Epoch [28/300], Step [27300/27733], Loss: 2.6174\n",
      "Epoch [28/300], Step [27400/27733], Loss: 2.6652\n",
      "Epoch [28/300], Step [27500/27733], Loss: 3.6539\n",
      "Epoch [28/300], Step [27600/27733], Loss: 3.4658\n",
      "Epoch [28/300], Step [27700/27733], Loss: 2.9553\n",
      "Epoch [29/300], Step [100/27733], Loss: 3.0937\n",
      "Epoch [29/300], Step [200/27733], Loss: 2.8740\n",
      "Epoch [29/300], Step [300/27733], Loss: 2.0530\n",
      "Epoch [29/300], Step [400/27733], Loss: 2.4259\n",
      "Epoch [29/300], Step [500/27733], Loss: 2.7157\n",
      "Epoch [29/300], Step [600/27733], Loss: 2.5432\n",
      "Epoch [29/300], Step [700/27733], Loss: 2.7256\n",
      "Epoch [29/300], Step [800/27733], Loss: 2.7493\n",
      "Epoch [29/300], Step [900/27733], Loss: 2.0089\n",
      "Epoch [29/300], Step [1000/27733], Loss: 3.3165\n",
      "Epoch [29/300], Step [1100/27733], Loss: 2.5440\n",
      "Epoch [29/300], Step [1200/27733], Loss: 2.2846\n",
      "Epoch [29/300], Step [1300/27733], Loss: 2.8098\n",
      "Epoch [29/300], Step [1400/27733], Loss: 2.3991\n",
      "Epoch [29/300], Step [1500/27733], Loss: 2.2813\n",
      "Epoch [29/300], Step [1600/27733], Loss: 2.7309\n",
      "Epoch [29/300], Step [1700/27733], Loss: 2.7142\n",
      "Epoch [29/300], Step [1800/27733], Loss: 1.9320\n",
      "Epoch [29/300], Step [1900/27733], Loss: 2.5043\n",
      "Epoch [29/300], Step [2000/27733], Loss: 3.0325\n",
      "Epoch [29/300], Step [2100/27733], Loss: 2.6042\n",
      "Epoch [29/300], Step [2200/27733], Loss: 2.5398\n",
      "Epoch [29/300], Step [2300/27733], Loss: 2.6676\n",
      "Epoch [29/300], Step [2400/27733], Loss: 2.7157\n",
      "Epoch [29/300], Step [2500/27733], Loss: 2.6499\n",
      "Epoch [29/300], Step [2600/27733], Loss: 3.1035\n",
      "Epoch [29/300], Step [2700/27733], Loss: 2.5070\n",
      "Epoch [29/300], Step [2800/27733], Loss: 2.9666\n",
      "Epoch [29/300], Step [2900/27733], Loss: 2.2524\n",
      "Epoch [29/300], Step [3000/27733], Loss: 2.5131\n",
      "Epoch [29/300], Step [3100/27733], Loss: 3.1341\n",
      "Epoch [29/300], Step [3200/27733], Loss: 3.2179\n",
      "Epoch [29/300], Step [3300/27733], Loss: 3.5031\n",
      "Epoch [29/300], Step [3400/27733], Loss: 2.6098\n",
      "Epoch [29/300], Step [3500/27733], Loss: 3.0311\n",
      "Epoch [29/300], Step [3600/27733], Loss: 2.1690\n",
      "Epoch [29/300], Step [3700/27733], Loss: 2.3221\n",
      "Epoch [29/300], Step [3800/27733], Loss: 3.2609\n",
      "Epoch [29/300], Step [3900/27733], Loss: 3.2344\n",
      "Epoch [29/300], Step [4000/27733], Loss: 3.0056\n",
      "Epoch [29/300], Step [4100/27733], Loss: 2.8065\n",
      "Epoch [29/300], Step [4200/27733], Loss: 1.4991\n",
      "Epoch [29/300], Step [4300/27733], Loss: 3.2060\n",
      "Epoch [29/300], Step [4400/27733], Loss: 3.5994\n",
      "Epoch [29/300], Step [4500/27733], Loss: 2.5637\n",
      "Epoch [29/300], Step [4600/27733], Loss: 2.7336\n",
      "Epoch [29/300], Step [4700/27733], Loss: 1.9553\n",
      "Epoch [29/300], Step [4800/27733], Loss: 3.0261\n",
      "Epoch [29/300], Step [4900/27733], Loss: 2.5045\n",
      "Epoch [29/300], Step [5000/27733], Loss: 3.4218\n",
      "Epoch [29/300], Step [5100/27733], Loss: 2.3941\n",
      "Epoch [29/300], Step [5200/27733], Loss: 2.5696\n",
      "Epoch [29/300], Step [5300/27733], Loss: 2.2350\n",
      "Epoch [29/300], Step [5400/27733], Loss: 2.4775\n",
      "Epoch [29/300], Step [5500/27733], Loss: 3.0141\n",
      "Epoch [29/300], Step [5600/27733], Loss: 3.6828\n",
      "Epoch [29/300], Step [5700/27733], Loss: 2.0976\n",
      "Epoch [29/300], Step [5800/27733], Loss: 2.9207\n",
      "Epoch [29/300], Step [5900/27733], Loss: 2.8840\n",
      "Epoch [29/300], Step [6000/27733], Loss: 2.5656\n",
      "Epoch [29/300], Step [6100/27733], Loss: 2.1346\n",
      "Epoch [29/300], Step [6200/27733], Loss: 2.5965\n",
      "Epoch [29/300], Step [6300/27733], Loss: 2.6831\n",
      "Epoch [29/300], Step [6400/27733], Loss: 2.6125\n",
      "Epoch [29/300], Step [6500/27733], Loss: 2.6681\n",
      "Epoch [29/300], Step [6600/27733], Loss: 2.8640\n",
      "Epoch [29/300], Step [6700/27733], Loss: 2.0732\n",
      "Epoch [29/300], Step [6800/27733], Loss: 3.2855\n",
      "Epoch [29/300], Step [6900/27733], Loss: 3.2375\n",
      "Epoch [29/300], Step [7000/27733], Loss: 2.1574\n",
      "Epoch [29/300], Step [7100/27733], Loss: 2.6635\n",
      "Epoch [29/300], Step [7200/27733], Loss: 2.6849\n",
      "Epoch [29/300], Step [7300/27733], Loss: 3.0185\n",
      "Epoch [29/300], Step [7400/27733], Loss: 3.4202\n",
      "Epoch [29/300], Step [7500/27733], Loss: 3.3311\n",
      "Epoch [29/300], Step [7600/27733], Loss: 2.5423\n",
      "Epoch [29/300], Step [7700/27733], Loss: 2.8906\n",
      "Epoch [29/300], Step [7800/27733], Loss: 3.2231\n",
      "Epoch [29/300], Step [7900/27733], Loss: 2.3074\n",
      "Epoch [29/300], Step [8000/27733], Loss: 2.0908\n",
      "Epoch [29/300], Step [8100/27733], Loss: 1.8580\n",
      "Epoch [29/300], Step [8200/27733], Loss: 3.1414\n",
      "Epoch [29/300], Step [8300/27733], Loss: 3.0539\n",
      "Epoch [29/300], Step [8400/27733], Loss: 3.4232\n",
      "Epoch [29/300], Step [8500/27733], Loss: 2.8123\n",
      "Epoch [29/300], Step [8600/27733], Loss: 2.0916\n",
      "Epoch [29/300], Step [8700/27733], Loss: 3.2680\n",
      "Epoch [29/300], Step [8800/27733], Loss: 2.7686\n",
      "Epoch [29/300], Step [8900/27733], Loss: 3.0048\n",
      "Epoch [29/300], Step [9000/27733], Loss: 2.7278\n",
      "Epoch [29/300], Step [9100/27733], Loss: 2.8526\n",
      "Epoch [29/300], Step [9200/27733], Loss: 2.9336\n",
      "Epoch [29/300], Step [9300/27733], Loss: 2.4125\n",
      "Epoch [29/300], Step [9400/27733], Loss: 2.7560\n",
      "Epoch [29/300], Step [9500/27733], Loss: 2.1607\n",
      "Epoch [29/300], Step [9600/27733], Loss: 3.4079\n",
      "Epoch [29/300], Step [9700/27733], Loss: 3.1687\n",
      "Epoch [29/300], Step [9800/27733], Loss: 3.5103\n",
      "Epoch [29/300], Step [9900/27733], Loss: 2.8787\n",
      "Epoch [29/300], Step [10000/27733], Loss: 2.4651\n",
      "Epoch [29/300], Step [10100/27733], Loss: 2.8939\n",
      "Epoch [29/300], Step [10200/27733], Loss: 2.4066\n",
      "Epoch [29/300], Step [10300/27733], Loss: 2.6205\n",
      "Epoch [29/300], Step [10400/27733], Loss: 2.7368\n",
      "Epoch [29/300], Step [10500/27733], Loss: 2.5211\n",
      "Epoch [29/300], Step [10600/27733], Loss: 2.5750\n",
      "Epoch [29/300], Step [10700/27733], Loss: 2.6468\n",
      "Epoch [29/300], Step [10800/27733], Loss: 3.6173\n",
      "Epoch [29/300], Step [10900/27733], Loss: 3.0860\n",
      "Epoch [29/300], Step [11000/27733], Loss: 2.6179\n",
      "Epoch [29/300], Step [11100/27733], Loss: 3.1700\n",
      "Epoch [29/300], Step [11200/27733], Loss: 2.7616\n",
      "Epoch [29/300], Step [11300/27733], Loss: 2.7873\n",
      "Epoch [29/300], Step [11400/27733], Loss: 3.5063\n",
      "Epoch [29/300], Step [11500/27733], Loss: 2.7414\n",
      "Epoch [29/300], Step [11600/27733], Loss: 2.2489\n",
      "Epoch [29/300], Step [11700/27733], Loss: 2.5486\n",
      "Epoch [29/300], Step [11800/27733], Loss: 2.9869\n",
      "Epoch [29/300], Step [11900/27733], Loss: 2.3641\n",
      "Epoch [29/300], Step [12000/27733], Loss: 2.4930\n",
      "Epoch [29/300], Step [12100/27733], Loss: 3.8816\n",
      "Epoch [29/300], Step [12200/27733], Loss: 3.0592\n",
      "Epoch [29/300], Step [12300/27733], Loss: 2.2918\n",
      "Epoch [29/300], Step [12400/27733], Loss: 3.0396\n",
      "Epoch [29/300], Step [12500/27733], Loss: 3.4962\n",
      "Epoch [29/300], Step [12600/27733], Loss: 2.0647\n",
      "Epoch [29/300], Step [12700/27733], Loss: 3.7010\n",
      "Epoch [29/300], Step [12800/27733], Loss: 2.3700\n",
      "Epoch [29/300], Step [12900/27733], Loss: 2.4947\n",
      "Epoch [29/300], Step [13000/27733], Loss: 2.7889\n",
      "Epoch [29/300], Step [13100/27733], Loss: 2.2941\n",
      "Epoch [29/300], Step [13200/27733], Loss: 3.1161\n",
      "Epoch [29/300], Step [13300/27733], Loss: 2.3923\n",
      "Epoch [29/300], Step [13400/27733], Loss: 3.2060\n",
      "Epoch [29/300], Step [13500/27733], Loss: 3.0707\n",
      "Epoch [29/300], Step [13600/27733], Loss: 3.0986\n",
      "Epoch [29/300], Step [13700/27733], Loss: 3.6137\n",
      "Epoch [29/300], Step [13800/27733], Loss: 2.7518\n",
      "Epoch [29/300], Step [13900/27733], Loss: 4.3512\n",
      "Epoch [29/300], Step [14000/27733], Loss: 3.4586\n",
      "Epoch [29/300], Step [14100/27733], Loss: 3.0041\n",
      "Epoch [29/300], Step [14200/27733], Loss: 2.5181\n",
      "Epoch [29/300], Step [14300/27733], Loss: 2.8471\n",
      "Epoch [29/300], Step [14400/27733], Loss: 3.1284\n",
      "Epoch [29/300], Step [14500/27733], Loss: 3.6115\n",
      "Epoch [29/300], Step [14600/27733], Loss: 2.9843\n",
      "Epoch [29/300], Step [14700/27733], Loss: 3.3278\n",
      "Epoch [29/300], Step [14800/27733], Loss: 2.7056\n",
      "Epoch [29/300], Step [14900/27733], Loss: 3.0589\n",
      "Epoch [29/300], Step [15000/27733], Loss: 2.5770\n",
      "Epoch [29/300], Step [15100/27733], Loss: 2.2696\n",
      "Epoch [29/300], Step [15200/27733], Loss: 3.3253\n",
      "Epoch [29/300], Step [15300/27733], Loss: 2.7392\n",
      "Epoch [29/300], Step [15400/27733], Loss: 2.7723\n",
      "Epoch [29/300], Step [15500/27733], Loss: 3.2348\n",
      "Epoch [29/300], Step [15600/27733], Loss: 2.1434\n",
      "Epoch [29/300], Step [15700/27733], Loss: 3.2387\n",
      "Epoch [29/300], Step [15800/27733], Loss: 2.9312\n",
      "Epoch [29/300], Step [15900/27733], Loss: 2.8323\n",
      "Epoch [29/300], Step [16000/27733], Loss: 4.3257\n",
      "Epoch [29/300], Step [16100/27733], Loss: 2.6160\n",
      "Epoch [29/300], Step [16200/27733], Loss: 3.7363\n",
      "Epoch [29/300], Step [16300/27733], Loss: 2.7953\n",
      "Epoch [29/300], Step [16400/27733], Loss: 3.2520\n",
      "Epoch [29/300], Step [16500/27733], Loss: 3.7304\n",
      "Epoch [29/300], Step [16600/27733], Loss: 4.1576\n",
      "Epoch [29/300], Step [16700/27733], Loss: 2.7700\n",
      "Epoch [29/300], Step [16800/27733], Loss: 3.6524\n",
      "Epoch [29/300], Step [16900/27733], Loss: 3.6049\n",
      "Epoch [29/300], Step [17000/27733], Loss: 3.1650\n",
      "Epoch [29/300], Step [17100/27733], Loss: 2.7179\n",
      "Epoch [29/300], Step [17200/27733], Loss: 2.9289\n",
      "Epoch [29/300], Step [17300/27733], Loss: 3.1075\n",
      "Epoch [29/300], Step [17400/27733], Loss: 3.3821\n",
      "Epoch [29/300], Step [17500/27733], Loss: 3.5624\n",
      "Epoch [29/300], Step [17600/27733], Loss: 3.1584\n",
      "Epoch [29/300], Step [17700/27733], Loss: 3.0604\n",
      "Epoch [29/300], Step [17800/27733], Loss: 3.1991\n",
      "Epoch [29/300], Step [17900/27733], Loss: 3.0253\n",
      "Epoch [29/300], Step [18000/27733], Loss: 3.2833\n",
      "Epoch [29/300], Step [18100/27733], Loss: 3.2642\n",
      "Epoch [29/300], Step [18200/27733], Loss: 3.3184\n",
      "Epoch [29/300], Step [18300/27733], Loss: 3.1772\n",
      "Epoch [29/300], Step [18400/27733], Loss: 3.1955\n",
      "Epoch [29/300], Step [18500/27733], Loss: 2.2184\n",
      "Epoch [29/300], Step [18600/27733], Loss: 4.3981\n",
      "Epoch [29/300], Step [18700/27733], Loss: 3.2269\n",
      "Epoch [29/300], Step [18800/27733], Loss: 3.5775\n",
      "Epoch [29/300], Step [18900/27733], Loss: 3.8695\n",
      "Epoch [29/300], Step [19000/27733], Loss: 3.0150\n",
      "Epoch [29/300], Step [19100/27733], Loss: 2.7292\n",
      "Epoch [29/300], Step [19200/27733], Loss: 2.5217\n",
      "Epoch [29/300], Step [19300/27733], Loss: 3.2985\n",
      "Epoch [29/300], Step [19400/27733], Loss: 2.5762\n",
      "Epoch [29/300], Step [19500/27733], Loss: 2.7222\n",
      "Epoch [29/300], Step [19600/27733], Loss: 2.3540\n",
      "Epoch [29/300], Step [19700/27733], Loss: 2.0834\n",
      "Epoch [29/300], Step [19800/27733], Loss: 2.6714\n",
      "Epoch [29/300], Step [19900/27733], Loss: 3.4980\n",
      "Epoch [29/300], Step [20000/27733], Loss: 2.9700\n",
      "Epoch [29/300], Step [20100/27733], Loss: 2.9738\n",
      "Epoch [29/300], Step [20200/27733], Loss: 3.7078\n",
      "Epoch [29/300], Step [20300/27733], Loss: 2.5979\n",
      "Epoch [29/300], Step [20400/27733], Loss: 3.6799\n",
      "Epoch [29/300], Step [20500/27733], Loss: 3.2895\n",
      "Epoch [29/300], Step [20600/27733], Loss: 2.1944\n",
      "Epoch [29/300], Step [20700/27733], Loss: 2.7577\n",
      "Epoch [29/300], Step [20800/27733], Loss: 3.7393\n",
      "Epoch [29/300], Step [20900/27733], Loss: 2.8185\n",
      "Epoch [29/300], Step [21000/27733], Loss: 2.9265\n",
      "Epoch [29/300], Step [21100/27733], Loss: 3.2891\n",
      "Epoch [29/300], Step [21200/27733], Loss: 3.6180\n",
      "Epoch [29/300], Step [21300/27733], Loss: 2.2734\n",
      "Epoch [29/300], Step [21400/27733], Loss: 2.8023\n",
      "Epoch [29/300], Step [21500/27733], Loss: 2.9828\n",
      "Epoch [29/300], Step [21600/27733], Loss: 3.2825\n",
      "Epoch [29/300], Step [21700/27733], Loss: 2.9665\n",
      "Epoch [29/300], Step [21800/27733], Loss: 3.2134\n",
      "Epoch [29/300], Step [21900/27733], Loss: 3.6208\n",
      "Epoch [29/300], Step [22000/27733], Loss: 3.4230\n",
      "Epoch [29/300], Step [22100/27733], Loss: 3.1761\n",
      "Epoch [29/300], Step [22200/27733], Loss: 2.7612\n",
      "Epoch [29/300], Step [22300/27733], Loss: 3.3114\n",
      "Epoch [29/300], Step [22400/27733], Loss: 3.9179\n",
      "Epoch [29/300], Step [22500/27733], Loss: 1.9866\n",
      "Epoch [29/300], Step [22600/27733], Loss: 3.7262\n",
      "Epoch [29/300], Step [22700/27733], Loss: 2.6865\n",
      "Epoch [29/300], Step [22800/27733], Loss: 3.4903\n",
      "Epoch [29/300], Step [22900/27733], Loss: 2.6426\n",
      "Epoch [29/300], Step [23000/27733], Loss: 3.1771\n",
      "Epoch [29/300], Step [23100/27733], Loss: 3.2619\n",
      "Epoch [29/300], Step [23200/27733], Loss: 3.6879\n",
      "Epoch [29/300], Step [23300/27733], Loss: 3.3768\n",
      "Epoch [29/300], Step [23400/27733], Loss: 3.1051\n",
      "Epoch [29/300], Step [23500/27733], Loss: 2.8299\n",
      "Epoch [29/300], Step [23600/27733], Loss: 2.7572\n",
      "Epoch [29/300], Step [23700/27733], Loss: 2.9997\n",
      "Epoch [29/300], Step [23800/27733], Loss: 2.6498\n",
      "Epoch [29/300], Step [23900/27733], Loss: 2.4244\n",
      "Epoch [29/300], Step [24000/27733], Loss: 2.9953\n",
      "Epoch [29/300], Step [24100/27733], Loss: 3.7087\n",
      "Epoch [29/300], Step [24200/27733], Loss: 2.4634\n",
      "Epoch [29/300], Step [24300/27733], Loss: 3.6198\n",
      "Epoch [29/300], Step [24400/27733], Loss: 3.7364\n",
      "Epoch [29/300], Step [24500/27733], Loss: 3.3807\n",
      "Epoch [29/300], Step [24600/27733], Loss: 2.9813\n",
      "Epoch [29/300], Step [24700/27733], Loss: 2.9699\n",
      "Epoch [29/300], Step [24800/27733], Loss: 3.0767\n",
      "Epoch [29/300], Step [24900/27733], Loss: 2.3505\n",
      "Epoch [29/300], Step [25000/27733], Loss: 2.4536\n",
      "Epoch [29/300], Step [25100/27733], Loss: 3.0945\n",
      "Epoch [29/300], Step [25200/27733], Loss: 2.8850\n",
      "Epoch [29/300], Step [25300/27733], Loss: 1.9765\n",
      "Epoch [29/300], Step [25400/27733], Loss: 2.9026\n",
      "Epoch [29/300], Step [25500/27733], Loss: 3.4636\n",
      "Epoch [29/300], Step [25600/27733], Loss: 3.1035\n",
      "Epoch [29/300], Step [25700/27733], Loss: 3.8998\n",
      "Epoch [29/300], Step [25800/27733], Loss: 2.5270\n",
      "Epoch [29/300], Step [25900/27733], Loss: 2.8950\n",
      "Epoch [29/300], Step [26000/27733], Loss: 2.9112\n",
      "Epoch [29/300], Step [26100/27733], Loss: 2.7475\n",
      "Epoch [29/300], Step [26200/27733], Loss: 2.9152\n",
      "Epoch [29/300], Step [26300/27733], Loss: 2.9754\n",
      "Epoch [29/300], Step [26400/27733], Loss: 2.9497\n",
      "Epoch [29/300], Step [26500/27733], Loss: 3.7222\n",
      "Epoch [29/300], Step [26600/27733], Loss: 2.9513\n",
      "Epoch [29/300], Step [26700/27733], Loss: 2.7904\n",
      "Epoch [29/300], Step [26800/27733], Loss: 3.7267\n",
      "Epoch [29/300], Step [26900/27733], Loss: 3.2890\n",
      "Epoch [29/300], Step [27000/27733], Loss: 2.5155\n",
      "Epoch [29/300], Step [27100/27733], Loss: 2.1195\n",
      "Epoch [29/300], Step [27200/27733], Loss: 3.9418\n",
      "Epoch [29/300], Step [27300/27733], Loss: 2.4731\n",
      "Epoch [29/300], Step [27400/27733], Loss: 3.2769\n",
      "Epoch [29/300], Step [27500/27733], Loss: 3.1144\n",
      "Epoch [29/300], Step [27600/27733], Loss: 3.9270\n",
      "Epoch [29/300], Step [27700/27733], Loss: 2.9678\n",
      "Epoch [30/300], Step [100/27733], Loss: 1.8354\n",
      "Epoch [30/300], Step [200/27733], Loss: 1.9422\n",
      "Epoch [30/300], Step [300/27733], Loss: 2.9004\n",
      "Epoch [30/300], Step [400/27733], Loss: 2.2978\n",
      "Epoch [30/300], Step [500/27733], Loss: 2.9741\n",
      "Epoch [30/300], Step [600/27733], Loss: 2.7841\n",
      "Epoch [30/300], Step [700/27733], Loss: 2.3152\n",
      "Epoch [30/300], Step [800/27733], Loss: 3.0755\n",
      "Epoch [30/300], Step [900/27733], Loss: 1.5830\n",
      "Epoch [30/300], Step [1000/27733], Loss: 2.4640\n",
      "Epoch [30/300], Step [1100/27733], Loss: 2.0951\n",
      "Epoch [30/300], Step [1200/27733], Loss: 2.4584\n",
      "Epoch [30/300], Step [1300/27733], Loss: 1.6181\n",
      "Epoch [30/300], Step [1400/27733], Loss: 2.4835\n",
      "Epoch [30/300], Step [1500/27733], Loss: 2.0006\n",
      "Epoch [30/300], Step [1600/27733], Loss: 2.7604\n",
      "Epoch [30/300], Step [1700/27733], Loss: 3.6622\n",
      "Epoch [30/300], Step [1800/27733], Loss: 3.0022\n",
      "Epoch [30/300], Step [1900/27733], Loss: 3.1643\n",
      "Epoch [30/300], Step [2000/27733], Loss: 2.4766\n",
      "Epoch [30/300], Step [2100/27733], Loss: 2.5691\n",
      "Epoch [30/300], Step [2200/27733], Loss: 3.2349\n",
      "Epoch [30/300], Step [2300/27733], Loss: 2.3131\n",
      "Epoch [30/300], Step [2400/27733], Loss: 2.8432\n",
      "Epoch [30/300], Step [2500/27733], Loss: 2.5347\n",
      "Epoch [30/300], Step [2600/27733], Loss: 2.6854\n",
      "Epoch [30/300], Step [2700/27733], Loss: 3.3088\n",
      "Epoch [30/300], Step [2800/27733], Loss: 1.9751\n",
      "Epoch [30/300], Step [2900/27733], Loss: 2.0177\n",
      "Epoch [30/300], Step [3000/27733], Loss: 2.3982\n",
      "Epoch [30/300], Step [3100/27733], Loss: 2.5140\n",
      "Epoch [30/300], Step [3200/27733], Loss: 2.9077\n",
      "Epoch [30/300], Step [3300/27733], Loss: 2.7800\n",
      "Epoch [30/300], Step [3400/27733], Loss: 2.9794\n",
      "Epoch [30/300], Step [3500/27733], Loss: 2.4216\n",
      "Epoch [30/300], Step [3600/27733], Loss: 2.7444\n",
      "Epoch [30/300], Step [3700/27733], Loss: 2.8847\n",
      "Epoch [30/300], Step [3800/27733], Loss: 2.3735\n",
      "Epoch [30/300], Step [3900/27733], Loss: 2.3876\n",
      "Epoch [30/300], Step [4000/27733], Loss: 2.6841\n",
      "Epoch [30/300], Step [4100/27733], Loss: 2.6621\n",
      "Epoch [30/300], Step [4200/27733], Loss: 3.2522\n",
      "Epoch [30/300], Step [4300/27733], Loss: 2.7083\n",
      "Epoch [30/300], Step [4400/27733], Loss: 3.3976\n",
      "Epoch [30/300], Step [4500/27733], Loss: 2.6926\n",
      "Epoch [30/300], Step [4600/27733], Loss: 3.2623\n",
      "Epoch [30/300], Step [4700/27733], Loss: 2.0780\n",
      "Epoch [30/300], Step [4800/27733], Loss: 2.9031\n",
      "Epoch [30/300], Step [4900/27733], Loss: 2.4081\n",
      "Epoch [30/300], Step [5000/27733], Loss: 2.5639\n",
      "Epoch [30/300], Step [5100/27733], Loss: 2.1722\n",
      "Epoch [30/300], Step [5200/27733], Loss: 2.9296\n",
      "Epoch [30/300], Step [5300/27733], Loss: 2.7445\n",
      "Epoch [30/300], Step [5400/27733], Loss: 2.2384\n",
      "Epoch [30/300], Step [5500/27733], Loss: 3.1243\n",
      "Epoch [30/300], Step [5600/27733], Loss: 2.5019\n",
      "Epoch [30/300], Step [5700/27733], Loss: 3.0515\n",
      "Epoch [30/300], Step [5800/27733], Loss: 3.2324\n",
      "Epoch [30/300], Step [5900/27733], Loss: 2.4945\n",
      "Epoch [30/300], Step [6000/27733], Loss: 2.8951\n",
      "Epoch [30/300], Step [6100/27733], Loss: 2.6676\n",
      "Epoch [30/300], Step [6200/27733], Loss: 2.5540\n",
      "Epoch [30/300], Step [6300/27733], Loss: 3.2127\n",
      "Epoch [30/300], Step [6400/27733], Loss: 2.2324\n",
      "Epoch [30/300], Step [6500/27733], Loss: 2.4669\n",
      "Epoch [30/300], Step [6600/27733], Loss: 2.1167\n",
      "Epoch [30/300], Step [6700/27733], Loss: 2.5128\n",
      "Epoch [30/300], Step [6800/27733], Loss: 3.0389\n",
      "Epoch [30/300], Step [6900/27733], Loss: 2.4212\n",
      "Epoch [30/300], Step [7000/27733], Loss: 3.1779\n",
      "Epoch [30/300], Step [7100/27733], Loss: 3.3755\n",
      "Epoch [30/300], Step [7200/27733], Loss: 2.3999\n",
      "Epoch [30/300], Step [7300/27733], Loss: 3.3623\n",
      "Epoch [30/300], Step [7400/27733], Loss: 2.0670\n",
      "Epoch [30/300], Step [7500/27733], Loss: 2.6272\n",
      "Epoch [30/300], Step [7600/27733], Loss: 2.5085\n",
      "Epoch [30/300], Step [7700/27733], Loss: 2.9530\n",
      "Epoch [30/300], Step [7800/27733], Loss: 2.2930\n",
      "Epoch [30/300], Step [7900/27733], Loss: 1.7343\n",
      "Epoch [30/300], Step [8000/27733], Loss: 2.9785\n",
      "Epoch [30/300], Step [8100/27733], Loss: 3.1070\n",
      "Epoch [30/300], Step [8200/27733], Loss: 2.8264\n",
      "Epoch [30/300], Step [8300/27733], Loss: 2.7525\n",
      "Epoch [30/300], Step [8400/27733], Loss: 2.6051\n",
      "Epoch [30/300], Step [8500/27733], Loss: 3.0686\n",
      "Epoch [30/300], Step [8600/27733], Loss: 3.6981\n",
      "Epoch [30/300], Step [8700/27733], Loss: 2.7544\n",
      "Epoch [30/300], Step [8800/27733], Loss: 2.4867\n",
      "Epoch [30/300], Step [8900/27733], Loss: 3.0209\n",
      "Epoch [30/300], Step [9000/27733], Loss: 2.9419\n",
      "Epoch [30/300], Step [9100/27733], Loss: 2.9633\n",
      "Epoch [30/300], Step [9200/27733], Loss: 2.8947\n",
      "Epoch [30/300], Step [9300/27733], Loss: 2.8044\n",
      "Epoch [30/300], Step [9400/27733], Loss: 3.5263\n",
      "Epoch [30/300], Step [9500/27733], Loss: 1.8694\n",
      "Epoch [30/300], Step [9600/27733], Loss: 2.9600\n",
      "Epoch [30/300], Step [9700/27733], Loss: 2.6798\n",
      "Epoch [30/300], Step [9800/27733], Loss: 3.2096\n",
      "Epoch [30/300], Step [9900/27733], Loss: 3.7260\n",
      "Epoch [30/300], Step [10000/27733], Loss: 2.4232\n",
      "Epoch [30/300], Step [10100/27733], Loss: 2.2140\n",
      "Epoch [30/300], Step [10200/27733], Loss: 2.0741\n",
      "Epoch [30/300], Step [10300/27733], Loss: 3.9971\n",
      "Epoch [30/300], Step [10400/27733], Loss: 2.4223\n",
      "Epoch [30/300], Step [10500/27733], Loss: 3.5909\n",
      "Epoch [30/300], Step [10600/27733], Loss: 2.4131\n",
      "Epoch [30/300], Step [10700/27733], Loss: 2.9221\n",
      "Epoch [30/300], Step [10800/27733], Loss: 3.3304\n",
      "Epoch [30/300], Step [10900/27733], Loss: 3.5283\n",
      "Epoch [30/300], Step [11000/27733], Loss: 2.5700\n",
      "Epoch [30/300], Step [11100/27733], Loss: 2.9766\n",
      "Epoch [30/300], Step [11200/27733], Loss: 3.0521\n",
      "Epoch [30/300], Step [11300/27733], Loss: 2.8133\n",
      "Epoch [30/300], Step [11400/27733], Loss: 2.9101\n",
      "Epoch [30/300], Step [11500/27733], Loss: 2.8318\n",
      "Epoch [30/300], Step [11600/27733], Loss: 2.1942\n",
      "Epoch [30/300], Step [11700/27733], Loss: 3.0531\n",
      "Epoch [30/300], Step [11800/27733], Loss: 2.1691\n",
      "Epoch [30/300], Step [11900/27733], Loss: 3.0594\n",
      "Epoch [30/300], Step [12000/27733], Loss: 2.8719\n",
      "Epoch [30/300], Step [12100/27733], Loss: 2.3511\n",
      "Epoch [30/300], Step [12200/27733], Loss: 2.1936\n",
      "Epoch [30/300], Step [12300/27733], Loss: 2.9146\n",
      "Epoch [30/300], Step [12400/27733], Loss: 2.2577\n",
      "Epoch [30/300], Step [12500/27733], Loss: 2.8468\n",
      "Epoch [30/300], Step [12600/27733], Loss: 2.7327\n",
      "Epoch [30/300], Step [12700/27733], Loss: 2.9519\n",
      "Epoch [30/300], Step [12800/27733], Loss: 2.5076\n",
      "Epoch [30/300], Step [12900/27733], Loss: 3.0269\n",
      "Epoch [30/300], Step [13000/27733], Loss: 2.9776\n",
      "Epoch [30/300], Step [13100/27733], Loss: 2.0030\n",
      "Epoch [30/300], Step [13200/27733], Loss: 3.0240\n",
      "Epoch [30/300], Step [13300/27733], Loss: 3.6839\n",
      "Epoch [30/300], Step [13400/27733], Loss: 3.3151\n",
      "Epoch [30/300], Step [13500/27733], Loss: 2.8213\n",
      "Epoch [30/300], Step [13600/27733], Loss: 3.1893\n",
      "Epoch [30/300], Step [13700/27733], Loss: 2.6191\n",
      "Epoch [30/300], Step [13800/27733], Loss: 3.0948\n",
      "Epoch [30/300], Step [13900/27733], Loss: 2.8245\n",
      "Epoch [30/300], Step [14000/27733], Loss: 3.7783\n",
      "Epoch [30/300], Step [14100/27733], Loss: 3.5584\n",
      "Epoch [30/300], Step [14200/27733], Loss: 3.3589\n",
      "Epoch [30/300], Step [14300/27733], Loss: 3.4996\n",
      "Epoch [30/300], Step [14400/27733], Loss: 3.5439\n",
      "Epoch [30/300], Step [14500/27733], Loss: 3.4670\n",
      "Epoch [30/300], Step [14600/27733], Loss: 2.9804\n",
      "Epoch [30/300], Step [14700/27733], Loss: 3.0960\n",
      "Epoch [30/300], Step [14800/27733], Loss: 2.8898\n",
      "Epoch [30/300], Step [14900/27733], Loss: 3.0751\n",
      "Epoch [30/300], Step [15000/27733], Loss: 3.3635\n",
      "Epoch [30/300], Step [15100/27733], Loss: 2.5065\n",
      "Epoch [30/300], Step [15200/27733], Loss: 4.4224\n",
      "Epoch [30/300], Step [15300/27733], Loss: 2.1435\n",
      "Epoch [30/300], Step [15400/27733], Loss: 2.7150\n",
      "Epoch [30/300], Step [15500/27733], Loss: 2.5663\n",
      "Epoch [30/300], Step [15600/27733], Loss: 3.2627\n",
      "Epoch [30/300], Step [15700/27733], Loss: 2.7465\n",
      "Epoch [30/300], Step [15800/27733], Loss: 3.2508\n",
      "Epoch [30/300], Step [15900/27733], Loss: 2.8735\n",
      "Epoch [30/300], Step [16000/27733], Loss: 2.7628\n",
      "Epoch [30/300], Step [16100/27733], Loss: 2.3600\n",
      "Epoch [30/300], Step [16200/27733], Loss: 2.0876\n",
      "Epoch [30/300], Step [16300/27733], Loss: 3.3726\n",
      "Epoch [30/300], Step [16400/27733], Loss: 2.7781\n",
      "Epoch [30/300], Step [16500/27733], Loss: 2.7463\n",
      "Epoch [30/300], Step [16600/27733], Loss: 3.5397\n",
      "Epoch [30/300], Step [16700/27733], Loss: 2.9575\n",
      "Epoch [30/300], Step [16800/27733], Loss: 3.1081\n",
      "Epoch [30/300], Step [16900/27733], Loss: 2.4763\n",
      "Epoch [30/300], Step [17000/27733], Loss: 2.6408\n",
      "Epoch [30/300], Step [17100/27733], Loss: 2.3353\n",
      "Epoch [30/300], Step [17200/27733], Loss: 2.8412\n",
      "Epoch [30/300], Step [17300/27733], Loss: 2.7614\n",
      "Epoch [30/300], Step [17400/27733], Loss: 2.1149\n",
      "Epoch [30/300], Step [17500/27733], Loss: 3.1918\n",
      "Epoch [30/300], Step [17600/27733], Loss: 3.1724\n",
      "Epoch [30/300], Step [17700/27733], Loss: 3.1820\n",
      "Epoch [30/300], Step [17800/27733], Loss: 2.6739\n",
      "Epoch [30/300], Step [17900/27733], Loss: 2.9274\n",
      "Epoch [30/300], Step [18000/27733], Loss: 2.9812\n",
      "Epoch [30/300], Step [18100/27733], Loss: 3.2029\n",
      "Epoch [30/300], Step [18200/27733], Loss: 3.4502\n",
      "Epoch [30/300], Step [18300/27733], Loss: 2.8990\n",
      "Epoch [30/300], Step [18400/27733], Loss: 2.8007\n",
      "Epoch [30/300], Step [18500/27733], Loss: 3.0123\n",
      "Epoch [30/300], Step [18600/27733], Loss: 2.7069\n",
      "Epoch [30/300], Step [18700/27733], Loss: 2.9493\n",
      "Epoch [30/300], Step [18800/27733], Loss: 3.8323\n",
      "Epoch [30/300], Step [18900/27733], Loss: 2.9162\n",
      "Epoch [30/300], Step [19000/27733], Loss: 3.2851\n",
      "Epoch [30/300], Step [19100/27733], Loss: 3.0480\n",
      "Epoch [30/300], Step [19200/27733], Loss: 2.8926\n",
      "Epoch [30/300], Step [19300/27733], Loss: 3.2806\n",
      "Epoch [30/300], Step [19400/27733], Loss: 3.2241\n",
      "Epoch [30/300], Step [19500/27733], Loss: 3.4060\n",
      "Epoch [30/300], Step [19600/27733], Loss: 3.5543\n",
      "Epoch [30/300], Step [19700/27733], Loss: 3.0615\n",
      "Epoch [30/300], Step [19800/27733], Loss: 2.8653\n",
      "Epoch [30/300], Step [19900/27733], Loss: 2.6392\n",
      "Epoch [30/300], Step [20000/27733], Loss: 2.5369\n",
      "Epoch [30/300], Step [20100/27733], Loss: 3.1193\n",
      "Epoch [30/300], Step [20200/27733], Loss: 3.0533\n",
      "Epoch [30/300], Step [20300/27733], Loss: 2.8061\n",
      "Epoch [30/300], Step [20400/27733], Loss: 2.5974\n",
      "Epoch [30/300], Step [20500/27733], Loss: 3.0448\n",
      "Epoch [30/300], Step [20600/27733], Loss: 3.4331\n",
      "Epoch [30/300], Step [20700/27733], Loss: 2.6865\n",
      "Epoch [30/300], Step [20800/27733], Loss: 4.4675\n",
      "Epoch [30/300], Step [20900/27733], Loss: 2.5427\n",
      "Epoch [30/300], Step [21000/27733], Loss: 2.8528\n",
      "Epoch [30/300], Step [21100/27733], Loss: 2.8095\n",
      "Epoch [30/300], Step [21200/27733], Loss: 3.8106\n",
      "Epoch [30/300], Step [21300/27733], Loss: 2.5793\n",
      "Epoch [30/300], Step [21400/27733], Loss: 2.9142\n",
      "Epoch [30/300], Step [21500/27733], Loss: 3.7015\n",
      "Epoch [30/300], Step [21600/27733], Loss: 2.9054\n",
      "Epoch [30/300], Step [21700/27733], Loss: 2.0827\n",
      "Epoch [30/300], Step [21800/27733], Loss: 2.5347\n",
      "Epoch [30/300], Step [21900/27733], Loss: 3.3906\n",
      "Epoch [30/300], Step [22000/27733], Loss: 2.4407\n",
      "Epoch [30/300], Step [22100/27733], Loss: 2.0961\n",
      "Epoch [30/300], Step [22200/27733], Loss: 2.5152\n",
      "Epoch [30/300], Step [22300/27733], Loss: 3.3989\n",
      "Epoch [30/300], Step [22400/27733], Loss: 2.7699\n",
      "Epoch [30/300], Step [22500/27733], Loss: 3.3561\n",
      "Epoch [30/300], Step [22600/27733], Loss: 2.5494\n",
      "Epoch [30/300], Step [22700/27733], Loss: 3.1896\n",
      "Epoch [30/300], Step [22800/27733], Loss: 1.9654\n",
      "Epoch [30/300], Step [22900/27733], Loss: 2.6211\n",
      "Epoch [30/300], Step [23000/27733], Loss: 3.2527\n",
      "Epoch [30/300], Step [23100/27733], Loss: 3.2472\n",
      "Epoch [30/300], Step [23200/27733], Loss: 2.8447\n",
      "Epoch [30/300], Step [23300/27733], Loss: 2.4465\n",
      "Epoch [30/300], Step [23400/27733], Loss: 2.6345\n",
      "Epoch [30/300], Step [23500/27733], Loss: 2.6510\n",
      "Epoch [30/300], Step [23600/27733], Loss: 2.9760\n",
      "Epoch [30/300], Step [23700/27733], Loss: 2.8998\n",
      "Epoch [30/300], Step [23800/27733], Loss: 3.5647\n",
      "Epoch [30/300], Step [23900/27733], Loss: 3.4712\n",
      "Epoch [30/300], Step [24000/27733], Loss: 3.7300\n",
      "Epoch [30/300], Step [24100/27733], Loss: 2.7216\n",
      "Epoch [30/300], Step [24200/27733], Loss: 3.1784\n",
      "Epoch [30/300], Step [24300/27733], Loss: 3.0288\n",
      "Epoch [30/300], Step [24400/27733], Loss: 2.5203\n",
      "Epoch [30/300], Step [24500/27733], Loss: 3.0771\n",
      "Epoch [30/300], Step [24600/27733], Loss: 3.4846\n",
      "Epoch [30/300], Step [24700/27733], Loss: 3.0996\n",
      "Epoch [30/300], Step [24800/27733], Loss: 2.8730\n",
      "Epoch [30/300], Step [24900/27733], Loss: 3.6065\n",
      "Epoch [30/300], Step [25000/27733], Loss: 2.9354\n",
      "Epoch [30/300], Step [25100/27733], Loss: 2.8143\n",
      "Epoch [30/300], Step [25200/27733], Loss: 3.1794\n",
      "Epoch [30/300], Step [25300/27733], Loss: 3.5504\n",
      "Epoch [30/300], Step [25400/27733], Loss: 2.5276\n",
      "Epoch [30/300], Step [25500/27733], Loss: 2.7920\n",
      "Epoch [30/300], Step [25600/27733], Loss: 3.1695\n",
      "Epoch [30/300], Step [25700/27733], Loss: 3.3090\n",
      "Epoch [30/300], Step [25800/27733], Loss: 3.1205\n",
      "Epoch [30/300], Step [25900/27733], Loss: 3.8550\n",
      "Epoch [30/300], Step [26000/27733], Loss: 3.8415\n",
      "Epoch [30/300], Step [26100/27733], Loss: 3.4677\n",
      "Epoch [30/300], Step [26200/27733], Loss: 2.8629\n",
      "Epoch [30/300], Step [26300/27733], Loss: 3.9621\n",
      "Epoch [30/300], Step [26400/27733], Loss: 3.3743\n",
      "Epoch [30/300], Step [26500/27733], Loss: 2.7792\n",
      "Epoch [30/300], Step [26600/27733], Loss: 2.4794\n",
      "Epoch [30/300], Step [26700/27733], Loss: 2.5735\n",
      "Epoch [30/300], Step [26800/27733], Loss: 3.4076\n",
      "Epoch [30/300], Step [26900/27733], Loss: 3.9392\n",
      "Epoch [30/300], Step [27000/27733], Loss: 3.2498\n",
      "Epoch [30/300], Step [27100/27733], Loss: 2.5263\n",
      "Epoch [30/300], Step [27200/27733], Loss: 2.2722\n",
      "Epoch [30/300], Step [27300/27733], Loss: 1.9734\n",
      "Epoch [30/300], Step [27400/27733], Loss: 3.6732\n",
      "Epoch [30/300], Step [27500/27733], Loss: 3.2702\n",
      "Epoch [30/300], Step [27600/27733], Loss: 3.4806\n",
      "Epoch [30/300], Step [27700/27733], Loss: 3.6824\n",
      "Epoch [31/300], Step [100/27733], Loss: 3.1386\n",
      "Epoch [31/300], Step [200/27733], Loss: 2.0630\n",
      "Epoch [31/300], Step [300/27733], Loss: 2.9987\n",
      "Epoch [31/300], Step [400/27733], Loss: 2.7803\n",
      "Epoch [31/300], Step [500/27733], Loss: 2.5334\n",
      "Epoch [31/300], Step [600/27733], Loss: 2.1545\n",
      "Epoch [31/300], Step [700/27733], Loss: 2.2373\n",
      "Epoch [31/300], Step [800/27733], Loss: 2.3527\n",
      "Epoch [31/300], Step [900/27733], Loss: 1.9425\n",
      "Epoch [31/300], Step [1000/27733], Loss: 3.0228\n",
      "Epoch [31/300], Step [1100/27733], Loss: 2.3103\n",
      "Epoch [31/300], Step [1200/27733], Loss: 2.0911\n",
      "Epoch [31/300], Step [1300/27733], Loss: 2.3385\n",
      "Epoch [31/300], Step [1400/27733], Loss: 3.0450\n",
      "Epoch [31/300], Step [1500/27733], Loss: 2.4823\n",
      "Epoch [31/300], Step [1600/27733], Loss: 1.9549\n",
      "Epoch [31/300], Step [1700/27733], Loss: 2.6140\n",
      "Epoch [31/300], Step [1800/27733], Loss: 2.0032\n",
      "Epoch [31/300], Step [1900/27733], Loss: 2.9995\n",
      "Epoch [31/300], Step [2000/27733], Loss: 2.3861\n",
      "Epoch [31/300], Step [2100/27733], Loss: 2.0232\n",
      "Epoch [31/300], Step [2200/27733], Loss: 2.5253\n",
      "Epoch [31/300], Step [2300/27733], Loss: 2.6322\n",
      "Epoch [31/300], Step [2400/27733], Loss: 3.2185\n",
      "Epoch [31/300], Step [2500/27733], Loss: 2.2708\n",
      "Epoch [31/300], Step [2600/27733], Loss: 2.2772\n",
      "Epoch [31/300], Step [2700/27733], Loss: 2.8534\n",
      "Epoch [31/300], Step [2800/27733], Loss: 1.8998\n",
      "Epoch [31/300], Step [2900/27733], Loss: 3.3135\n",
      "Epoch [31/300], Step [3000/27733], Loss: 2.2300\n",
      "Epoch [31/300], Step [3100/27733], Loss: 2.8600\n",
      "Epoch [31/300], Step [3200/27733], Loss: 2.7568\n",
      "Epoch [31/300], Step [3300/27733], Loss: 2.2962\n",
      "Epoch [31/300], Step [3400/27733], Loss: 3.2811\n",
      "Epoch [31/300], Step [3500/27733], Loss: 3.4561\n",
      "Epoch [31/300], Step [3600/27733], Loss: 2.7354\n",
      "Epoch [31/300], Step [3700/27733], Loss: 2.7190\n",
      "Epoch [31/300], Step [3800/27733], Loss: 1.8760\n",
      "Epoch [31/300], Step [3900/27733], Loss: 2.7577\n",
      "Epoch [31/300], Step [4000/27733], Loss: 2.7263\n",
      "Epoch [31/300], Step [4100/27733], Loss: 2.7492\n",
      "Epoch [31/300], Step [4200/27733], Loss: 2.9209\n",
      "Epoch [31/300], Step [4300/27733], Loss: 2.8792\n",
      "Epoch [31/300], Step [4400/27733], Loss: 2.1755\n",
      "Epoch [31/300], Step [4500/27733], Loss: 2.5784\n",
      "Epoch [31/300], Step [4600/27733], Loss: 2.1366\n",
      "Epoch [31/300], Step [4700/27733], Loss: 2.3577\n",
      "Epoch [31/300], Step [4800/27733], Loss: 2.2810\n",
      "Epoch [31/300], Step [4900/27733], Loss: 2.6635\n",
      "Epoch [31/300], Step [5000/27733], Loss: 2.6436\n",
      "Epoch [31/300], Step [5100/27733], Loss: 2.8760\n",
      "Epoch [31/300], Step [5200/27733], Loss: 3.0412\n",
      "Epoch [31/300], Step [5300/27733], Loss: 3.4521\n",
      "Epoch [31/300], Step [5400/27733], Loss: 3.2675\n",
      "Epoch [31/300], Step [5500/27733], Loss: 2.7675\n",
      "Epoch [31/300], Step [5600/27733], Loss: 2.7755\n",
      "Epoch [31/300], Step [5700/27733], Loss: 3.4015\n",
      "Epoch [31/300], Step [5800/27733], Loss: 2.9653\n",
      "Epoch [31/300], Step [5900/27733], Loss: 2.9007\n",
      "Epoch [31/300], Step [6000/27733], Loss: 2.8385\n",
      "Epoch [31/300], Step [6100/27733], Loss: 2.7133\n",
      "Epoch [31/300], Step [6200/27733], Loss: 3.2329\n",
      "Epoch [31/300], Step [6300/27733], Loss: 2.0568\n",
      "Epoch [31/300], Step [6400/27733], Loss: 2.9626\n",
      "Epoch [31/300], Step [6500/27733], Loss: 2.8405\n",
      "Epoch [31/300], Step [6600/27733], Loss: 3.0645\n",
      "Epoch [31/300], Step [6700/27733], Loss: 3.1610\n",
      "Epoch [31/300], Step [6800/27733], Loss: 2.7107\n",
      "Epoch [31/300], Step [6900/27733], Loss: 2.5789\n",
      "Epoch [31/300], Step [7000/27733], Loss: 2.0905\n",
      "Epoch [31/300], Step [7100/27733], Loss: 2.6877\n",
      "Epoch [31/300], Step [7200/27733], Loss: 2.9703\n",
      "Epoch [31/300], Step [7300/27733], Loss: 2.3118\n",
      "Epoch [31/300], Step [7400/27733], Loss: 3.0182\n",
      "Epoch [31/300], Step [7500/27733], Loss: 3.4894\n",
      "Epoch [31/300], Step [7600/27733], Loss: 1.9466\n",
      "Epoch [31/300], Step [7700/27733], Loss: 3.2823\n",
      "Epoch [31/300], Step [7800/27733], Loss: 3.2271\n",
      "Epoch [31/300], Step [7900/27733], Loss: 3.1449\n",
      "Epoch [31/300], Step [8000/27733], Loss: 2.5167\n",
      "Epoch [31/300], Step [8100/27733], Loss: 2.5602\n",
      "Epoch [31/300], Step [8200/27733], Loss: 2.5367\n",
      "Epoch [31/300], Step [8300/27733], Loss: 2.9097\n",
      "Epoch [31/300], Step [8400/27733], Loss: 1.8508\n",
      "Epoch [31/300], Step [8500/27733], Loss: 2.2401\n",
      "Epoch [31/300], Step [8600/27733], Loss: 2.5142\n",
      "Epoch [31/300], Step [8700/27733], Loss: 3.0586\n",
      "Epoch [31/300], Step [8800/27733], Loss: 3.4699\n",
      "Epoch [31/300], Step [8900/27733], Loss: 2.4866\n",
      "Epoch [31/300], Step [9000/27733], Loss: 2.4077\n",
      "Epoch [31/300], Step [9100/27733], Loss: 2.4298\n",
      "Epoch [31/300], Step [9200/27733], Loss: 2.7276\n",
      "Epoch [31/300], Step [9300/27733], Loss: 2.7010\n",
      "Epoch [31/300], Step [9400/27733], Loss: 2.8273\n",
      "Epoch [31/300], Step [9500/27733], Loss: 2.4417\n",
      "Epoch [31/300], Step [9600/27733], Loss: 3.7069\n",
      "Epoch [31/300], Step [9700/27733], Loss: 2.7516\n",
      "Epoch [31/300], Step [9800/27733], Loss: 2.7461\n",
      "Epoch [31/300], Step [9900/27733], Loss: 2.7536\n",
      "Epoch [31/300], Step [10000/27733], Loss: 2.9009\n",
      "Epoch [31/300], Step [10100/27733], Loss: 3.8496\n",
      "Epoch [31/300], Step [10200/27733], Loss: 2.6561\n",
      "Epoch [31/300], Step [10300/27733], Loss: 3.0096\n",
      "Epoch [31/300], Step [10400/27733], Loss: 2.6035\n",
      "Epoch [31/300], Step [10500/27733], Loss: 3.2325\n",
      "Epoch [31/300], Step [10600/27733], Loss: 2.7342\n",
      "Epoch [31/300], Step [10700/27733], Loss: 3.3957\n",
      "Epoch [31/300], Step [10800/27733], Loss: 2.2705\n",
      "Epoch [31/300], Step [10900/27733], Loss: 3.5439\n",
      "Epoch [31/300], Step [11000/27733], Loss: 3.7650\n",
      "Epoch [31/300], Step [11100/27733], Loss: 3.3928\n",
      "Epoch [31/300], Step [11200/27733], Loss: 2.4322\n",
      "Epoch [31/300], Step [11300/27733], Loss: 2.4216\n",
      "Epoch [31/300], Step [11400/27733], Loss: 2.9622\n",
      "Epoch [31/300], Step [11500/27733], Loss: 2.8976\n",
      "Epoch [31/300], Step [11600/27733], Loss: 2.9397\n",
      "Epoch [31/300], Step [11700/27733], Loss: 2.5592\n",
      "Epoch [31/300], Step [11800/27733], Loss: 3.3056\n",
      "Epoch [31/300], Step [11900/27733], Loss: 3.5573\n",
      "Epoch [31/300], Step [12000/27733], Loss: 3.3449\n",
      "Epoch [31/300], Step [12100/27733], Loss: 2.7243\n",
      "Epoch [31/300], Step [12200/27733], Loss: 2.6017\n",
      "Epoch [31/300], Step [12300/27733], Loss: 2.3837\n",
      "Epoch [31/300], Step [12400/27733], Loss: 3.5120\n",
      "Epoch [31/300], Step [12500/27733], Loss: 2.1912\n",
      "Epoch [31/300], Step [12600/27733], Loss: 2.5687\n",
      "Epoch [31/300], Step [12700/27733], Loss: 2.7897\n",
      "Epoch [31/300], Step [12800/27733], Loss: 2.9403\n",
      "Epoch [31/300], Step [12900/27733], Loss: 3.3917\n",
      "Epoch [31/300], Step [13000/27733], Loss: 2.8632\n",
      "Epoch [31/300], Step [13100/27733], Loss: 2.8759\n",
      "Epoch [31/300], Step [13200/27733], Loss: 3.0029\n",
      "Epoch [31/300], Step [13300/27733], Loss: 2.2472\n",
      "Epoch [31/300], Step [13400/27733], Loss: 2.2976\n",
      "Epoch [31/300], Step [13500/27733], Loss: 2.6776\n",
      "Epoch [31/300], Step [13600/27733], Loss: 2.8559\n",
      "Epoch [31/300], Step [13700/27733], Loss: 2.8682\n",
      "Epoch [31/300], Step [13800/27733], Loss: 2.8730\n",
      "Epoch [31/300], Step [13900/27733], Loss: 2.9788\n",
      "Epoch [31/300], Step [14000/27733], Loss: 2.3678\n",
      "Epoch [31/300], Step [14100/27733], Loss: 3.0980\n",
      "Epoch [31/300], Step [14200/27733], Loss: 2.4287\n",
      "Epoch [31/300], Step [14300/27733], Loss: 3.0238\n",
      "Epoch [31/300], Step [14400/27733], Loss: 3.2691\n",
      "Epoch [31/300], Step [14500/27733], Loss: 2.8987\n",
      "Epoch [31/300], Step [14600/27733], Loss: 3.4158\n",
      "Epoch [31/300], Step [14700/27733], Loss: 2.9440\n",
      "Epoch [31/300], Step [14800/27733], Loss: 2.4332\n",
      "Epoch [31/300], Step [14900/27733], Loss: 2.8402\n",
      "Epoch [31/300], Step [15000/27733], Loss: 3.4849\n",
      "Epoch [31/300], Step [15100/27733], Loss: 2.9516\n",
      "Epoch [31/300], Step [15200/27733], Loss: 3.1549\n",
      "Epoch [31/300], Step [15300/27733], Loss: 3.2418\n",
      "Epoch [31/300], Step [15400/27733], Loss: 2.8714\n",
      "Epoch [31/300], Step [15500/27733], Loss: 1.9463\n",
      "Epoch [31/300], Step [15600/27733], Loss: 2.3791\n",
      "Epoch [31/300], Step [15700/27733], Loss: 3.2934\n",
      "Epoch [31/300], Step [15800/27733], Loss: 2.8158\n",
      "Epoch [31/300], Step [15900/27733], Loss: 3.7315\n",
      "Epoch [31/300], Step [16000/27733], Loss: 3.2845\n",
      "Epoch [31/300], Step [16100/27733], Loss: 1.6044\n",
      "Epoch [31/300], Step [16200/27733], Loss: 3.4454\n",
      "Epoch [31/300], Step [16300/27733], Loss: 2.8022\n",
      "Epoch [31/300], Step [16400/27733], Loss: 2.0329\n",
      "Epoch [31/300], Step [16500/27733], Loss: 3.1891\n",
      "Epoch [31/300], Step [16600/27733], Loss: 3.3746\n",
      "Epoch [31/300], Step [16700/27733], Loss: 3.3512\n",
      "Epoch [31/300], Step [16800/27733], Loss: 2.9987\n",
      "Epoch [31/300], Step [16900/27733], Loss: 2.5827\n",
      "Epoch [31/300], Step [17000/27733], Loss: 2.8737\n",
      "Epoch [31/300], Step [17100/27733], Loss: 2.9600\n",
      "Epoch [31/300], Step [17200/27733], Loss: 2.2095\n",
      "Epoch [31/300], Step [17300/27733], Loss: 3.1442\n",
      "Epoch [31/300], Step [17400/27733], Loss: 2.8065\n",
      "Epoch [31/300], Step [17500/27733], Loss: 3.7154\n",
      "Epoch [31/300], Step [17600/27733], Loss: 3.8721\n",
      "Epoch [31/300], Step [17700/27733], Loss: 2.0237\n",
      "Epoch [31/300], Step [17800/27733], Loss: 2.0024\n",
      "Epoch [31/300], Step [17900/27733], Loss: 2.2578\n",
      "Epoch [31/300], Step [18000/27733], Loss: 3.3281\n",
      "Epoch [31/300], Step [18100/27733], Loss: 2.9927\n",
      "Epoch [31/300], Step [18200/27733], Loss: 2.2755\n",
      "Epoch [31/300], Step [18300/27733], Loss: 3.4084\n",
      "Epoch [31/300], Step [18400/27733], Loss: 3.3261\n",
      "Epoch [31/300], Step [18500/27733], Loss: 3.5553\n",
      "Epoch [31/300], Step [18600/27733], Loss: 3.1933\n",
      "Epoch [31/300], Step [18700/27733], Loss: 3.4694\n",
      "Epoch [31/300], Step [18800/27733], Loss: 2.3462\n",
      "Epoch [31/300], Step [18900/27733], Loss: 2.7114\n",
      "Epoch [31/300], Step [19000/27733], Loss: 2.3309\n",
      "Epoch [31/300], Step [19100/27733], Loss: 2.6616\n",
      "Epoch [31/300], Step [19200/27733], Loss: 2.6051\n",
      "Epoch [31/300], Step [19300/27733], Loss: 2.1155\n",
      "Epoch [31/300], Step [19400/27733], Loss: 3.0928\n",
      "Epoch [31/300], Step [19500/27733], Loss: 2.6425\n",
      "Epoch [31/300], Step [19600/27733], Loss: 2.9673\n",
      "Epoch [31/300], Step [19700/27733], Loss: 2.7027\n",
      "Epoch [31/300], Step [19800/27733], Loss: 2.4487\n",
      "Epoch [31/300], Step [19900/27733], Loss: 2.1875\n",
      "Epoch [31/300], Step [20000/27733], Loss: 3.2738\n",
      "Epoch [31/300], Step [20100/27733], Loss: 2.8954\n",
      "Epoch [31/300], Step [20200/27733], Loss: 3.3719\n",
      "Epoch [31/300], Step [20300/27733], Loss: 3.2893\n",
      "Epoch [31/300], Step [20400/27733], Loss: 3.0655\n",
      "Epoch [31/300], Step [20500/27733], Loss: 3.4895\n",
      "Epoch [31/300], Step [20600/27733], Loss: 2.7732\n",
      "Epoch [31/300], Step [20700/27733], Loss: 2.9599\n",
      "Epoch [31/300], Step [20800/27733], Loss: 3.0170\n",
      "Epoch [31/300], Step [20900/27733], Loss: 2.9054\n",
      "Epoch [31/300], Step [21000/27733], Loss: 2.7599\n",
      "Epoch [31/300], Step [21100/27733], Loss: 2.7922\n",
      "Epoch [31/300], Step [21200/27733], Loss: 2.7307\n",
      "Epoch [31/300], Step [21300/27733], Loss: 2.8148\n",
      "Epoch [31/300], Step [21400/27733], Loss: 2.7657\n",
      "Epoch [31/300], Step [21500/27733], Loss: 2.9584\n",
      "Epoch [31/300], Step [21600/27733], Loss: 3.4656\n",
      "Epoch [31/300], Step [21700/27733], Loss: 3.0464\n",
      "Epoch [31/300], Step [21800/27733], Loss: 2.8274\n",
      "Epoch [31/300], Step [21900/27733], Loss: 3.1777\n",
      "Epoch [31/300], Step [22000/27733], Loss: 3.3133\n",
      "Epoch [31/300], Step [22100/27733], Loss: 3.0562\n",
      "Epoch [31/300], Step [22200/27733], Loss: 2.7768\n",
      "Epoch [31/300], Step [22300/27733], Loss: 3.3418\n",
      "Epoch [31/300], Step [22400/27733], Loss: 2.8415\n",
      "Epoch [31/300], Step [22500/27733], Loss: 3.1869\n",
      "Epoch [31/300], Step [22600/27733], Loss: 2.9419\n",
      "Epoch [31/300], Step [22700/27733], Loss: 2.8370\n",
      "Epoch [31/300], Step [22800/27733], Loss: 2.6580\n",
      "Epoch [31/300], Step [22900/27733], Loss: 3.0329\n",
      "Epoch [31/300], Step [23000/27733], Loss: 3.6772\n",
      "Epoch [31/300], Step [23100/27733], Loss: 3.4974\n",
      "Epoch [31/300], Step [23200/27733], Loss: 2.5270\n",
      "Epoch [31/300], Step [23300/27733], Loss: 2.7083\n",
      "Epoch [31/300], Step [23400/27733], Loss: 2.7194\n",
      "Epoch [31/300], Step [23500/27733], Loss: 3.0380\n",
      "Epoch [31/300], Step [23600/27733], Loss: 2.8014\n",
      "Epoch [31/300], Step [23700/27733], Loss: 2.7101\n",
      "Epoch [31/300], Step [23800/27733], Loss: 3.2720\n",
      "Epoch [31/300], Step [23900/27733], Loss: 2.5944\n",
      "Epoch [31/300], Step [24000/27733], Loss: 3.4220\n",
      "Epoch [31/300], Step [24100/27733], Loss: 2.7257\n",
      "Epoch [31/300], Step [24200/27733], Loss: 2.9977\n",
      "Epoch [31/300], Step [24300/27733], Loss: 2.8744\n",
      "Epoch [31/300], Step [24400/27733], Loss: 3.0397\n",
      "Epoch [31/300], Step [24500/27733], Loss: 1.8222\n",
      "Epoch [31/300], Step [24600/27733], Loss: 2.6552\n",
      "Epoch [31/300], Step [24700/27733], Loss: 3.1087\n",
      "Epoch [31/300], Step [24800/27733], Loss: 2.7590\n",
      "Epoch [31/300], Step [24900/27733], Loss: 3.2182\n",
      "Epoch [31/300], Step [25000/27733], Loss: 3.6115\n",
      "Epoch [31/300], Step [25100/27733], Loss: 4.1254\n",
      "Epoch [31/300], Step [25200/27733], Loss: 3.6772\n",
      "Epoch [31/300], Step [25300/27733], Loss: 3.9392\n",
      "Epoch [31/300], Step [25400/27733], Loss: 3.3613\n",
      "Epoch [31/300], Step [25500/27733], Loss: 3.2588\n",
      "Epoch [31/300], Step [25600/27733], Loss: 2.9792\n",
      "Epoch [31/300], Step [25700/27733], Loss: 3.1508\n",
      "Epoch [31/300], Step [25800/27733], Loss: 2.7403\n",
      "Epoch [31/300], Step [25900/27733], Loss: 1.8565\n",
      "Epoch [31/300], Step [26000/27733], Loss: 2.8268\n",
      "Epoch [31/300], Step [26100/27733], Loss: 2.9576\n",
      "Epoch [31/300], Step [26200/27733], Loss: 3.4178\n",
      "Epoch [31/300], Step [26300/27733], Loss: 3.2195\n",
      "Epoch [31/300], Step [26400/27733], Loss: 2.8105\n",
      "Epoch [31/300], Step [26500/27733], Loss: 2.6707\n",
      "Epoch [31/300], Step [26600/27733], Loss: 3.9459\n",
      "Epoch [31/300], Step [26700/27733], Loss: 3.0647\n",
      "Epoch [31/300], Step [26800/27733], Loss: 3.5256\n",
      "Epoch [31/300], Step [26900/27733], Loss: 3.7077\n",
      "Epoch [31/300], Step [27000/27733], Loss: 2.7671\n",
      "Epoch [31/300], Step [27100/27733], Loss: 3.7307\n",
      "Epoch [31/300], Step [27200/27733], Loss: 3.1691\n",
      "Epoch [31/300], Step [27300/27733], Loss: 3.5584\n",
      "Epoch [31/300], Step [27400/27733], Loss: 3.5961\n",
      "Epoch [31/300], Step [27500/27733], Loss: 2.8509\n",
      "Epoch [31/300], Step [27600/27733], Loss: 2.8049\n",
      "Epoch [31/300], Step [27700/27733], Loss: 2.5499\n",
      "Epoch [32/300], Step [100/27733], Loss: 2.1309\n",
      "Epoch [32/300], Step [200/27733], Loss: 2.5405\n",
      "Epoch [32/300], Step [300/27733], Loss: 2.5087\n",
      "Epoch [32/300], Step [400/27733], Loss: 2.6009\n",
      "Epoch [32/300], Step [500/27733], Loss: 2.6743\n",
      "Epoch [32/300], Step [600/27733], Loss: 2.1463\n",
      "Epoch [32/300], Step [700/27733], Loss: 2.4079\n",
      "Epoch [32/300], Step [800/27733], Loss: 2.3238\n",
      "Epoch [32/300], Step [900/27733], Loss: 2.5422\n",
      "Epoch [32/300], Step [1000/27733], Loss: 2.6279\n",
      "Epoch [32/300], Step [1100/27733], Loss: 2.7698\n",
      "Epoch [32/300], Step [1200/27733], Loss: 1.8587\n",
      "Epoch [32/300], Step [1300/27733], Loss: 2.6394\n",
      "Epoch [32/300], Step [1400/27733], Loss: 1.8094\n",
      "Epoch [32/300], Step [1500/27733], Loss: 2.5646\n",
      "Epoch [32/300], Step [1600/27733], Loss: 2.0629\n",
      "Epoch [32/300], Step [1700/27733], Loss: 2.1289\n",
      "Epoch [32/300], Step [1800/27733], Loss: 2.5154\n",
      "Epoch [32/300], Step [1900/27733], Loss: 1.9755\n",
      "Epoch [32/300], Step [2000/27733], Loss: 1.8470\n",
      "Epoch [32/300], Step [2100/27733], Loss: 2.9651\n",
      "Epoch [32/300], Step [2200/27733], Loss: 2.3935\n",
      "Epoch [32/300], Step [2300/27733], Loss: 2.4657\n",
      "Epoch [32/300], Step [2400/27733], Loss: 3.1106\n",
      "Epoch [32/300], Step [2500/27733], Loss: 2.9138\n",
      "Epoch [32/300], Step [2600/27733], Loss: 2.0556\n",
      "Epoch [32/300], Step [2700/27733], Loss: 2.8326\n",
      "Epoch [32/300], Step [2800/27733], Loss: 2.0445\n",
      "Epoch [32/300], Step [2900/27733], Loss: 3.0151\n",
      "Epoch [32/300], Step [3000/27733], Loss: 1.8896\n",
      "Epoch [32/300], Step [3100/27733], Loss: 2.2329\n",
      "Epoch [32/300], Step [3200/27733], Loss: 2.5388\n",
      "Epoch [32/300], Step [3300/27733], Loss: 2.0102\n",
      "Epoch [32/300], Step [3400/27733], Loss: 2.2082\n",
      "Epoch [32/300], Step [3500/27733], Loss: 2.5837\n",
      "Epoch [32/300], Step [3600/27733], Loss: 2.7562\n",
      "Epoch [32/300], Step [3700/27733], Loss: 3.1855\n",
      "Epoch [32/300], Step [3800/27733], Loss: 2.8289\n",
      "Epoch [32/300], Step [3900/27733], Loss: 2.2719\n",
      "Epoch [32/300], Step [4000/27733], Loss: 2.8989\n",
      "Epoch [32/300], Step [4100/27733], Loss: 2.9931\n",
      "Epoch [32/300], Step [4200/27733], Loss: 2.8031\n",
      "Epoch [32/300], Step [4300/27733], Loss: 2.4997\n",
      "Epoch [32/300], Step [4400/27733], Loss: 3.0291\n",
      "Epoch [32/300], Step [4500/27733], Loss: 3.4838\n",
      "Epoch [32/300], Step [4600/27733], Loss: 2.4148\n",
      "Epoch [32/300], Step [4700/27733], Loss: 2.9965\n",
      "Epoch [32/300], Step [4800/27733], Loss: 2.0980\n",
      "Epoch [32/300], Step [4900/27733], Loss: 2.4794\n",
      "Epoch [32/300], Step [5000/27733], Loss: 2.5290\n",
      "Epoch [32/300], Step [5100/27733], Loss: 2.4557\n",
      "Epoch [32/300], Step [5200/27733], Loss: 2.5962\n",
      "Epoch [32/300], Step [5300/27733], Loss: 2.8563\n",
      "Epoch [32/300], Step [5400/27733], Loss: 3.4080\n",
      "Epoch [32/300], Step [5500/27733], Loss: 2.0702\n",
      "Epoch [32/300], Step [5600/27733], Loss: 2.9213\n",
      "Epoch [32/300], Step [5700/27733], Loss: 3.1020\n",
      "Epoch [32/300], Step [5800/27733], Loss: 2.9762\n",
      "Epoch [32/300], Step [5900/27733], Loss: 2.3880\n",
      "Epoch [32/300], Step [6000/27733], Loss: 2.7970\n",
      "Epoch [32/300], Step [6100/27733], Loss: 3.2345\n",
      "Epoch [32/300], Step [6200/27733], Loss: 2.8471\n",
      "Epoch [32/300], Step [6300/27733], Loss: 2.6474\n",
      "Epoch [32/300], Step [6400/27733], Loss: 2.9163\n",
      "Epoch [32/300], Step [6500/27733], Loss: 3.2034\n",
      "Epoch [32/300], Step [6600/27733], Loss: 2.2962\n",
      "Epoch [32/300], Step [6700/27733], Loss: 2.8688\n",
      "Epoch [32/300], Step [6800/27733], Loss: 3.2957\n",
      "Epoch [32/300], Step [6900/27733], Loss: 3.2276\n",
      "Epoch [32/300], Step [7000/27733], Loss: 1.7588\n",
      "Epoch [32/300], Step [7100/27733], Loss: 2.7964\n",
      "Epoch [32/300], Step [7200/27733], Loss: 2.5607\n",
      "Epoch [32/300], Step [7300/27733], Loss: 1.8564\n",
      "Epoch [32/300], Step [7400/27733], Loss: 2.6595\n",
      "Epoch [32/300], Step [7500/27733], Loss: 3.0117\n",
      "Epoch [32/300], Step [7600/27733], Loss: 2.5470\n",
      "Epoch [32/300], Step [7700/27733], Loss: 1.9367\n",
      "Epoch [32/300], Step [7800/27733], Loss: 2.8349\n",
      "Epoch [32/300], Step [7900/27733], Loss: 3.0558\n",
      "Epoch [32/300], Step [8000/27733], Loss: 2.0552\n",
      "Epoch [32/300], Step [8100/27733], Loss: 2.5312\n",
      "Epoch [32/300], Step [8200/27733], Loss: 2.9016\n",
      "Epoch [32/300], Step [8300/27733], Loss: 2.5797\n",
      "Epoch [32/300], Step [8400/27733], Loss: 3.1637\n",
      "Epoch [32/300], Step [8500/27733], Loss: 2.1060\n",
      "Epoch [32/300], Step [8600/27733], Loss: 2.6964\n",
      "Epoch [32/300], Step [8700/27733], Loss: 3.2283\n",
      "Epoch [32/300], Step [8800/27733], Loss: 2.7081\n",
      "Epoch [32/300], Step [8900/27733], Loss: 2.7071\n",
      "Epoch [32/300], Step [9000/27733], Loss: 2.7969\n",
      "Epoch [32/300], Step [9100/27733], Loss: 2.2863\n",
      "Epoch [32/300], Step [9200/27733], Loss: 2.2710\n",
      "Epoch [32/300], Step [9300/27733], Loss: 3.6539\n",
      "Epoch [32/300], Step [9400/27733], Loss: 2.5107\n",
      "Epoch [32/300], Step [9500/27733], Loss: 2.6299\n",
      "Epoch [32/300], Step [9600/27733], Loss: 2.7309\n",
      "Epoch [32/300], Step [9700/27733], Loss: 2.7654\n",
      "Epoch [32/300], Step [9800/27733], Loss: 2.7352\n",
      "Epoch [32/300], Step [9900/27733], Loss: 2.9050\n",
      "Epoch [32/300], Step [10000/27733], Loss: 1.9679\n",
      "Epoch [32/300], Step [10100/27733], Loss: 2.6038\n",
      "Epoch [32/300], Step [10200/27733], Loss: 2.4917\n",
      "Epoch [32/300], Step [10300/27733], Loss: 2.9664\n",
      "Epoch [32/300], Step [10400/27733], Loss: 2.6739\n",
      "Epoch [32/300], Step [10500/27733], Loss: 2.9913\n",
      "Epoch [32/300], Step [10600/27733], Loss: 3.3401\n",
      "Epoch [32/300], Step [10700/27733], Loss: 3.1877\n",
      "Epoch [32/300], Step [10800/27733], Loss: 3.0985\n",
      "Epoch [32/300], Step [10900/27733], Loss: 2.2752\n",
      "Epoch [32/300], Step [11000/27733], Loss: 3.5651\n",
      "Epoch [32/300], Step [11100/27733], Loss: 2.7579\n",
      "Epoch [32/300], Step [11200/27733], Loss: 3.0780\n",
      "Epoch [32/300], Step [11300/27733], Loss: 3.4046\n",
      "Epoch [32/300], Step [11400/27733], Loss: 2.9387\n",
      "Epoch [32/300], Step [11500/27733], Loss: 2.9709\n",
      "Epoch [32/300], Step [11600/27733], Loss: 3.0898\n",
      "Epoch [32/300], Step [11700/27733], Loss: 2.1918\n",
      "Epoch [32/300], Step [11800/27733], Loss: 3.3433\n",
      "Epoch [32/300], Step [11900/27733], Loss: 2.7659\n",
      "Epoch [32/300], Step [12000/27733], Loss: 2.9374\n",
      "Epoch [32/300], Step [12100/27733], Loss: 2.6600\n",
      "Epoch [32/300], Step [12200/27733], Loss: 3.3728\n",
      "Epoch [32/300], Step [12300/27733], Loss: 2.7563\n",
      "Epoch [32/300], Step [12400/27733], Loss: 2.9687\n",
      "Epoch [32/300], Step [12500/27733], Loss: 3.2216\n",
      "Epoch [32/300], Step [12600/27733], Loss: 3.6919\n",
      "Epoch [32/300], Step [12700/27733], Loss: 2.5918\n",
      "Epoch [32/300], Step [12800/27733], Loss: 3.9880\n",
      "Epoch [32/300], Step [12900/27733], Loss: 2.0727\n",
      "Epoch [32/300], Step [13000/27733], Loss: 3.2751\n",
      "Epoch [32/300], Step [13100/27733], Loss: 2.3460\n",
      "Epoch [32/300], Step [13200/27733], Loss: 2.9123\n",
      "Epoch [32/300], Step [13300/27733], Loss: 3.7955\n",
      "Epoch [32/300], Step [13400/27733], Loss: 2.5944\n",
      "Epoch [32/300], Step [13500/27733], Loss: 3.0349\n",
      "Epoch [32/300], Step [13600/27733], Loss: 2.7928\n",
      "Epoch [32/300], Step [13700/27733], Loss: 2.6308\n",
      "Epoch [32/300], Step [13800/27733], Loss: 3.5828\n",
      "Epoch [32/300], Step [13900/27733], Loss: 2.8566\n",
      "Epoch [32/300], Step [14000/27733], Loss: 2.3654\n",
      "Epoch [32/300], Step [14100/27733], Loss: 3.2203\n",
      "Epoch [32/300], Step [14200/27733], Loss: 3.0502\n",
      "Epoch [32/300], Step [14300/27733], Loss: 3.0490\n",
      "Epoch [32/300], Step [14400/27733], Loss: 3.5510\n",
      "Epoch [32/300], Step [14500/27733], Loss: 2.1113\n",
      "Epoch [32/300], Step [14600/27733], Loss: 3.0236\n",
      "Epoch [32/300], Step [14700/27733], Loss: 3.1124\n",
      "Epoch [32/300], Step [14800/27733], Loss: 3.6268\n",
      "Epoch [32/300], Step [14900/27733], Loss: 3.1088\n",
      "Epoch [32/300], Step [15000/27733], Loss: 2.5025\n",
      "Epoch [32/300], Step [15100/27733], Loss: 2.3591\n",
      "Epoch [32/300], Step [15200/27733], Loss: 3.1984\n",
      "Epoch [32/300], Step [15300/27733], Loss: 2.3524\n",
      "Epoch [32/300], Step [15400/27733], Loss: 2.7839\n",
      "Epoch [32/300], Step [15500/27733], Loss: 2.9191\n",
      "Epoch [32/300], Step [15600/27733], Loss: 3.3320\n",
      "Epoch [32/300], Step [15700/27733], Loss: 3.3138\n",
      "Epoch [32/300], Step [15800/27733], Loss: 2.7754\n",
      "Epoch [32/300], Step [15900/27733], Loss: 2.8876\n",
      "Epoch [32/300], Step [16000/27733], Loss: 2.9703\n",
      "Epoch [32/300], Step [16100/27733], Loss: 2.5466\n",
      "Epoch [32/300], Step [16200/27733], Loss: 3.2040\n",
      "Epoch [32/300], Step [16300/27733], Loss: 3.0805\n",
      "Epoch [32/300], Step [16400/27733], Loss: 3.1334\n",
      "Epoch [32/300], Step [16500/27733], Loss: 2.1911\n",
      "Epoch [32/300], Step [16600/27733], Loss: 2.6679\n",
      "Epoch [32/300], Step [16700/27733], Loss: 2.6205\n",
      "Epoch [32/300], Step [16800/27733], Loss: 2.8485\n",
      "Epoch [32/300], Step [16900/27733], Loss: 2.4361\n",
      "Epoch [32/300], Step [17000/27733], Loss: 3.5741\n",
      "Epoch [32/300], Step [17100/27733], Loss: 3.0592\n",
      "Epoch [32/300], Step [17200/27733], Loss: 2.3841\n",
      "Epoch [32/300], Step [17300/27733], Loss: 3.5952\n",
      "Epoch [32/300], Step [17400/27733], Loss: 2.3291\n",
      "Epoch [32/300], Step [17500/27733], Loss: 2.5186\n",
      "Epoch [32/300], Step [17600/27733], Loss: 1.9781\n",
      "Epoch [32/300], Step [17700/27733], Loss: 2.7597\n",
      "Epoch [32/300], Step [17800/27733], Loss: 3.2308\n",
      "Epoch [32/300], Step [17900/27733], Loss: 3.8092\n",
      "Epoch [32/300], Step [18000/27733], Loss: 2.4209\n",
      "Epoch [32/300], Step [18100/27733], Loss: 2.6049\n",
      "Epoch [32/300], Step [18200/27733], Loss: 3.1303\n",
      "Epoch [32/300], Step [18300/27733], Loss: 3.8944\n",
      "Epoch [32/300], Step [18400/27733], Loss: 1.9318\n",
      "Epoch [32/300], Step [18500/27733], Loss: 3.1948\n",
      "Epoch [32/300], Step [18600/27733], Loss: 2.8954\n",
      "Epoch [32/300], Step [18700/27733], Loss: 3.0916\n",
      "Epoch [32/300], Step [18800/27733], Loss: 3.7848\n",
      "Epoch [32/300], Step [18900/27733], Loss: 3.3978\n",
      "Epoch [32/300], Step [19000/27733], Loss: 3.6539\n",
      "Epoch [32/300], Step [19100/27733], Loss: 2.3580\n",
      "Epoch [32/300], Step [19200/27733], Loss: 2.6294\n",
      "Epoch [32/300], Step [19300/27733], Loss: 3.2186\n",
      "Epoch [32/300], Step [19400/27733], Loss: 2.9775\n",
      "Epoch [32/300], Step [19500/27733], Loss: 2.6551\n",
      "Epoch [32/300], Step [19600/27733], Loss: 3.2557\n",
      "Epoch [32/300], Step [19700/27733], Loss: 2.3438\n",
      "Epoch [32/300], Step [19800/27733], Loss: 3.0358\n",
      "Epoch [32/300], Step [19900/27733], Loss: 2.6609\n",
      "Epoch [32/300], Step [20000/27733], Loss: 3.9276\n",
      "Epoch [32/300], Step [20100/27733], Loss: 2.9399\n",
      "Epoch [32/300], Step [20200/27733], Loss: 3.5170\n",
      "Epoch [32/300], Step [20300/27733], Loss: 1.9249\n",
      "Epoch [32/300], Step [20400/27733], Loss: 3.7516\n",
      "Epoch [32/300], Step [20500/27733], Loss: 2.7487\n",
      "Epoch [32/300], Step [20600/27733], Loss: 2.7711\n",
      "Epoch [32/300], Step [20700/27733], Loss: 2.7763\n",
      "Epoch [32/300], Step [20800/27733], Loss: 3.0788\n",
      "Epoch [32/300], Step [20900/27733], Loss: 3.1605\n",
      "Epoch [32/300], Step [21000/27733], Loss: 2.9277\n",
      "Epoch [32/300], Step [21100/27733], Loss: 3.3860\n",
      "Epoch [32/300], Step [21200/27733], Loss: 2.7456\n",
      "Epoch [32/300], Step [21300/27733], Loss: 2.8947\n",
      "Epoch [32/300], Step [21400/27733], Loss: 3.0709\n",
      "Epoch [32/300], Step [21500/27733], Loss: 3.7514\n",
      "Epoch [32/300], Step [21600/27733], Loss: 3.1147\n",
      "Epoch [32/300], Step [21700/27733], Loss: 2.0047\n",
      "Epoch [32/300], Step [21800/27733], Loss: 2.6627\n",
      "Epoch [32/300], Step [21900/27733], Loss: 3.2798\n",
      "Epoch [32/300], Step [22000/27733], Loss: 3.2488\n",
      "Epoch [32/300], Step [22100/27733], Loss: 2.4668\n",
      "Epoch [32/300], Step [22200/27733], Loss: 3.1786\n",
      "Epoch [32/300], Step [22300/27733], Loss: 2.6318\n",
      "Epoch [32/300], Step [22400/27733], Loss: 2.6580\n",
      "Epoch [32/300], Step [22500/27733], Loss: 2.7336\n",
      "Epoch [32/300], Step [22600/27733], Loss: 2.8595\n",
      "Epoch [32/300], Step [22700/27733], Loss: 3.9306\n",
      "Epoch [32/300], Step [22800/27733], Loss: 2.5960\n",
      "Epoch [32/300], Step [22900/27733], Loss: 3.1050\n",
      "Epoch [32/300], Step [23000/27733], Loss: 2.6732\n",
      "Epoch [32/300], Step [23100/27733], Loss: 3.1314\n",
      "Epoch [32/300], Step [23200/27733], Loss: 2.4873\n",
      "Epoch [32/300], Step [23300/27733], Loss: 3.4729\n",
      "Epoch [32/300], Step [23400/27733], Loss: 4.1834\n",
      "Epoch [32/300], Step [23500/27733], Loss: 4.6947\n",
      "Epoch [32/300], Step [23600/27733], Loss: 3.1267\n",
      "Epoch [32/300], Step [23700/27733], Loss: 3.3134\n",
      "Epoch [32/300], Step [23800/27733], Loss: 2.8111\n",
      "Epoch [32/300], Step [23900/27733], Loss: 2.3315\n",
      "Epoch [32/300], Step [24000/27733], Loss: 2.5279\n",
      "Epoch [32/300], Step [24100/27733], Loss: 3.0609\n",
      "Epoch [32/300], Step [24200/27733], Loss: 2.9447\n",
      "Epoch [32/300], Step [24300/27733], Loss: 3.3742\n",
      "Epoch [32/300], Step [24400/27733], Loss: 3.2795\n",
      "Epoch [32/300], Step [24500/27733], Loss: 2.5102\n",
      "Epoch [32/300], Step [24600/27733], Loss: 3.1154\n",
      "Epoch [32/300], Step [24700/27733], Loss: 3.1647\n",
      "Epoch [32/300], Step [24800/27733], Loss: 3.1778\n",
      "Epoch [32/300], Step [24900/27733], Loss: 2.6449\n",
      "Epoch [32/300], Step [25000/27733], Loss: 2.8618\n",
      "Epoch [32/300], Step [25100/27733], Loss: 2.9818\n",
      "Epoch [32/300], Step [25200/27733], Loss: 3.0682\n",
      "Epoch [32/300], Step [25300/27733], Loss: 3.2211\n",
      "Epoch [32/300], Step [25400/27733], Loss: 3.5242\n",
      "Epoch [32/300], Step [25500/27733], Loss: 3.7091\n",
      "Epoch [32/300], Step [25600/27733], Loss: 3.2841\n",
      "Epoch [32/300], Step [25700/27733], Loss: 3.6276\n",
      "Epoch [32/300], Step [25800/27733], Loss: 3.9561\n",
      "Epoch [32/300], Step [25900/27733], Loss: 2.8204\n",
      "Epoch [32/300], Step [26000/27733], Loss: 2.8495\n",
      "Epoch [32/300], Step [26100/27733], Loss: 2.8056\n",
      "Epoch [32/300], Step [26200/27733], Loss: 3.1630\n",
      "Epoch [32/300], Step [26300/27733], Loss: 3.3559\n",
      "Epoch [32/300], Step [26400/27733], Loss: 3.1996\n",
      "Epoch [32/300], Step [26500/27733], Loss: 3.5288\n",
      "Epoch [32/300], Step [26600/27733], Loss: 3.6514\n",
      "Epoch [32/300], Step [26700/27733], Loss: 2.8895\n",
      "Epoch [32/300], Step [26800/27733], Loss: 2.9629\n",
      "Epoch [32/300], Step [26900/27733], Loss: 2.6733\n",
      "Epoch [32/300], Step [27000/27733], Loss: 3.0733\n",
      "Epoch [32/300], Step [27100/27733], Loss: 3.4608\n",
      "Epoch [32/300], Step [27200/27733], Loss: 2.6061\n",
      "Epoch [32/300], Step [27300/27733], Loss: 2.9646\n",
      "Epoch [32/300], Step [27400/27733], Loss: 3.5961\n",
      "Epoch [32/300], Step [27500/27733], Loss: 3.2791\n",
      "Epoch [32/300], Step [27600/27733], Loss: 3.7807\n",
      "Epoch [32/300], Step [27700/27733], Loss: 3.0897\n",
      "Epoch [33/300], Step [100/27733], Loss: 2.1071\n",
      "Epoch [33/300], Step [200/27733], Loss: 2.4806\n",
      "Epoch [33/300], Step [300/27733], Loss: 2.1250\n",
      "Epoch [33/300], Step [400/27733], Loss: 2.8185\n",
      "Epoch [33/300], Step [500/27733], Loss: 2.3986\n",
      "Epoch [33/300], Step [600/27733], Loss: 2.9533\n",
      "Epoch [33/300], Step [700/27733], Loss: 2.3023\n",
      "Epoch [33/300], Step [800/27733], Loss: 1.9659\n",
      "Epoch [33/300], Step [900/27733], Loss: 2.6569\n",
      "Epoch [33/300], Step [1000/27733], Loss: 2.3338\n",
      "Epoch [33/300], Step [1100/27733], Loss: 2.1171\n",
      "Epoch [33/300], Step [1200/27733], Loss: 2.2831\n",
      "Epoch [33/300], Step [1300/27733], Loss: 2.2122\n",
      "Epoch [33/300], Step [1400/27733], Loss: 3.1847\n",
      "Epoch [33/300], Step [1500/27733], Loss: 3.6674\n",
      "Epoch [33/300], Step [1600/27733], Loss: 1.7651\n",
      "Epoch [33/300], Step [1700/27733], Loss: 2.1026\n",
      "Epoch [33/300], Step [1800/27733], Loss: 2.4002\n",
      "Epoch [33/300], Step [1900/27733], Loss: 2.3393\n",
      "Epoch [33/300], Step [2000/27733], Loss: 2.3290\n",
      "Epoch [33/300], Step [2100/27733], Loss: 2.6122\n",
      "Epoch [33/300], Step [2200/27733], Loss: 2.0740\n",
      "Epoch [33/300], Step [2300/27733], Loss: 2.7761\n",
      "Epoch [33/300], Step [2400/27733], Loss: 3.1978\n",
      "Epoch [33/300], Step [2500/27733], Loss: 2.2037\n",
      "Epoch [33/300], Step [2600/27733], Loss: 3.6448\n",
      "Epoch [33/300], Step [2700/27733], Loss: 2.3947\n",
      "Epoch [33/300], Step [2800/27733], Loss: 2.7176\n",
      "Epoch [33/300], Step [2900/27733], Loss: 2.5948\n",
      "Epoch [33/300], Step [3000/27733], Loss: 2.7163\n",
      "Epoch [33/300], Step [3100/27733], Loss: 2.3039\n",
      "Epoch [33/300], Step [3200/27733], Loss: 2.3508\n",
      "Epoch [33/300], Step [3300/27733], Loss: 3.1059\n",
      "Epoch [33/300], Step [3400/27733], Loss: 3.1220\n",
      "Epoch [33/300], Step [3500/27733], Loss: 2.2321\n",
      "Epoch [33/300], Step [3600/27733], Loss: 2.4693\n",
      "Epoch [33/300], Step [3700/27733], Loss: 2.6730\n",
      "Epoch [33/300], Step [3800/27733], Loss: 3.4553\n",
      "Epoch [33/300], Step [3900/27733], Loss: 2.6213\n",
      "Epoch [33/300], Step [4000/27733], Loss: 3.3465\n",
      "Epoch [33/300], Step [4100/27733], Loss: 3.2631\n",
      "Epoch [33/300], Step [4200/27733], Loss: 2.9176\n",
      "Epoch [33/300], Step [4300/27733], Loss: 1.7454\n",
      "Epoch [33/300], Step [4400/27733], Loss: 1.8743\n",
      "Epoch [33/300], Step [4500/27733], Loss: 2.9109\n",
      "Epoch [33/300], Step [4600/27733], Loss: 3.4509\n",
      "Epoch [33/300], Step [4700/27733], Loss: 2.6620\n",
      "Epoch [33/300], Step [4800/27733], Loss: 2.5270\n",
      "Epoch [33/300], Step [4900/27733], Loss: 2.7305\n",
      "Epoch [33/300], Step [5000/27733], Loss: 3.1045\n",
      "Epoch [33/300], Step [5100/27733], Loss: 2.6013\n",
      "Epoch [33/300], Step [5200/27733], Loss: 2.9585\n",
      "Epoch [33/300], Step [5300/27733], Loss: 1.9674\n",
      "Epoch [33/300], Step [5400/27733], Loss: 3.4176\n",
      "Epoch [33/300], Step [5500/27733], Loss: 1.8170\n",
      "Epoch [33/300], Step [5600/27733], Loss: 2.6342\n",
      "Epoch [33/300], Step [5700/27733], Loss: 2.3058\n",
      "Epoch [33/300], Step [5800/27733], Loss: 2.7810\n",
      "Epoch [33/300], Step [5900/27733], Loss: 2.5752\n",
      "Epoch [33/300], Step [6000/27733], Loss: 2.4655\n",
      "Epoch [33/300], Step [6100/27733], Loss: 2.5564\n",
      "Epoch [33/300], Step [6200/27733], Loss: 3.5817\n",
      "Epoch [33/300], Step [6300/27733], Loss: 2.6495\n",
      "Epoch [33/300], Step [6400/27733], Loss: 2.7520\n",
      "Epoch [33/300], Step [6500/27733], Loss: 2.1403\n",
      "Epoch [33/300], Step [6600/27733], Loss: 2.6221\n",
      "Epoch [33/300], Step [6700/27733], Loss: 3.0327\n",
      "Epoch [33/300], Step [6800/27733], Loss: 3.0678\n",
      "Epoch [33/300], Step [6900/27733], Loss: 2.9840\n",
      "Epoch [33/300], Step [7000/27733], Loss: 3.1249\n",
      "Epoch [33/300], Step [7100/27733], Loss: 2.4377\n",
      "Epoch [33/300], Step [7200/27733], Loss: 2.4410\n",
      "Epoch [33/300], Step [7300/27733], Loss: 3.0576\n",
      "Epoch [33/300], Step [7400/27733], Loss: 2.9938\n",
      "Epoch [33/300], Step [7500/27733], Loss: 2.8766\n",
      "Epoch [33/300], Step [7600/27733], Loss: 2.8024\n",
      "Epoch [33/300], Step [7700/27733], Loss: 2.7865\n",
      "Epoch [33/300], Step [7800/27733], Loss: 2.9413\n",
      "Epoch [33/300], Step [7900/27733], Loss: 2.1599\n",
      "Epoch [33/300], Step [8000/27733], Loss: 1.9052\n",
      "Epoch [33/300], Step [8100/27733], Loss: 2.2245\n",
      "Epoch [33/300], Step [8200/27733], Loss: 2.3403\n",
      "Epoch [33/300], Step [8300/27733], Loss: 2.3657\n",
      "Epoch [33/300], Step [8400/27733], Loss: 2.5393\n",
      "Epoch [33/300], Step [8500/27733], Loss: 2.0576\n",
      "Epoch [33/300], Step [8600/27733], Loss: 2.7731\n",
      "Epoch [33/300], Step [8700/27733], Loss: 2.3823\n",
      "Epoch [33/300], Step [8800/27733], Loss: 2.5825\n",
      "Epoch [33/300], Step [8900/27733], Loss: 2.1215\n",
      "Epoch [33/300], Step [9000/27733], Loss: 2.7736\n",
      "Epoch [33/300], Step [9100/27733], Loss: 3.1478\n",
      "Epoch [33/300], Step [9200/27733], Loss: 3.1927\n",
      "Epoch [33/300], Step [9300/27733], Loss: 3.0025\n",
      "Epoch [33/300], Step [9400/27733], Loss: 3.3739\n",
      "Epoch [33/300], Step [9500/27733], Loss: 2.4216\n",
      "Epoch [33/300], Step [9600/27733], Loss: 1.9957\n",
      "Epoch [33/300], Step [9700/27733], Loss: 2.9784\n",
      "Epoch [33/300], Step [9800/27733], Loss: 3.1229\n",
      "Epoch [33/300], Step [9900/27733], Loss: 2.9232\n",
      "Epoch [33/300], Step [10000/27733], Loss: 3.0672\n",
      "Epoch [33/300], Step [10100/27733], Loss: 3.0721\n",
      "Epoch [33/300], Step [10200/27733], Loss: 2.7396\n",
      "Epoch [33/300], Step [10300/27733], Loss: 3.1898\n",
      "Epoch [33/300], Step [10400/27733], Loss: 2.9487\n",
      "Epoch [33/300], Step [10500/27733], Loss: 2.7004\n",
      "Epoch [33/300], Step [10600/27733], Loss: 2.6264\n",
      "Epoch [33/300], Step [10700/27733], Loss: 3.1138\n",
      "Epoch [33/300], Step [10800/27733], Loss: 2.5282\n",
      "Epoch [33/300], Step [10900/27733], Loss: 2.5654\n",
      "Epoch [33/300], Step [11000/27733], Loss: 3.8109\n",
      "Epoch [33/300], Step [11100/27733], Loss: 3.2855\n",
      "Epoch [33/300], Step [11200/27733], Loss: 3.4262\n",
      "Epoch [33/300], Step [11300/27733], Loss: 3.0841\n",
      "Epoch [33/300], Step [11400/27733], Loss: 2.6545\n",
      "Epoch [33/300], Step [11500/27733], Loss: 3.3034\n",
      "Epoch [33/300], Step [11600/27733], Loss: 2.6208\n",
      "Epoch [33/300], Step [11700/27733], Loss: 3.3094\n",
      "Epoch [33/300], Step [11800/27733], Loss: 3.1144\n",
      "Epoch [33/300], Step [11900/27733], Loss: 3.2819\n",
      "Epoch [33/300], Step [12000/27733], Loss: 2.5268\n",
      "Epoch [33/300], Step [12100/27733], Loss: 2.0855\n",
      "Epoch [33/300], Step [12200/27733], Loss: 2.9312\n",
      "Epoch [33/300], Step [12300/27733], Loss: 3.1217\n",
      "Epoch [33/300], Step [12400/27733], Loss: 2.4583\n",
      "Epoch [33/300], Step [12500/27733], Loss: 2.7115\n",
      "Epoch [33/300], Step [12600/27733], Loss: 2.3501\n",
      "Epoch [33/300], Step [12700/27733], Loss: 2.4471\n",
      "Epoch [33/300], Step [12800/27733], Loss: 2.5397\n",
      "Epoch [33/300], Step [12900/27733], Loss: 3.1029\n",
      "Epoch [33/300], Step [13000/27733], Loss: 3.1318\n",
      "Epoch [33/300], Step [13100/27733], Loss: 3.0753\n",
      "Epoch [33/300], Step [13200/27733], Loss: 3.2604\n",
      "Epoch [33/300], Step [13300/27733], Loss: 3.6182\n",
      "Epoch [33/300], Step [13400/27733], Loss: 2.9241\n",
      "Epoch [33/300], Step [13500/27733], Loss: 2.6094\n",
      "Epoch [33/300], Step [13600/27733], Loss: 2.4605\n",
      "Epoch [33/300], Step [13700/27733], Loss: 2.5557\n",
      "Epoch [33/300], Step [13800/27733], Loss: 3.0851\n",
      "Epoch [33/300], Step [13900/27733], Loss: 2.8811\n",
      "Epoch [33/300], Step [14000/27733], Loss: 2.8260\n",
      "Epoch [33/300], Step [14100/27733], Loss: 2.9130\n",
      "Epoch [33/300], Step [14200/27733], Loss: 2.7984\n",
      "Epoch [33/300], Step [14300/27733], Loss: 2.8669\n",
      "Epoch [33/300], Step [14400/27733], Loss: 2.3206\n",
      "Epoch [33/300], Step [14500/27733], Loss: 2.9711\n",
      "Epoch [33/300], Step [14600/27733], Loss: 1.8216\n",
      "Epoch [33/300], Step [14700/27733], Loss: 2.8073\n",
      "Epoch [33/300], Step [14800/27733], Loss: 3.5351\n",
      "Epoch [33/300], Step [14900/27733], Loss: 2.8600\n",
      "Epoch [33/300], Step [15000/27733], Loss: 3.0942\n",
      "Epoch [33/300], Step [15100/27733], Loss: 2.6526\n",
      "Epoch [33/300], Step [15200/27733], Loss: 2.9859\n",
      "Epoch [33/300], Step [15300/27733], Loss: 3.2906\n",
      "Epoch [33/300], Step [15400/27733], Loss: 2.1666\n",
      "Epoch [33/300], Step [15500/27733], Loss: 2.5884\n",
      "Epoch [33/300], Step [15600/27733], Loss: 2.3883\n",
      "Epoch [33/300], Step [15700/27733], Loss: 3.1331\n",
      "Epoch [33/300], Step [15800/27733], Loss: 2.3676\n",
      "Epoch [33/300], Step [15900/27733], Loss: 3.4494\n",
      "Epoch [33/300], Step [16000/27733], Loss: 3.0828\n",
      "Epoch [33/300], Step [16100/27733], Loss: 3.6611\n",
      "Epoch [33/300], Step [16200/27733], Loss: 3.3320\n",
      "Epoch [33/300], Step [16300/27733], Loss: 3.0824\n",
      "Epoch [33/300], Step [16400/27733], Loss: 3.4141\n",
      "Epoch [33/300], Step [16500/27733], Loss: 3.0695\n",
      "Epoch [33/300], Step [16600/27733], Loss: 2.0375\n",
      "Epoch [33/300], Step [16700/27733], Loss: 2.6841\n",
      "Epoch [33/300], Step [16800/27733], Loss: 4.0683\n",
      "Epoch [33/300], Step [16900/27733], Loss: 2.3509\n",
      "Epoch [33/300], Step [17000/27733], Loss: 3.1659\n",
      "Epoch [33/300], Step [17100/27733], Loss: 2.4978\n",
      "Epoch [33/300], Step [17200/27733], Loss: 2.1349\n",
      "Epoch [33/300], Step [17300/27733], Loss: 2.9141\n",
      "Epoch [33/300], Step [17400/27733], Loss: 2.8708\n",
      "Epoch [33/300], Step [17500/27733], Loss: 2.5958\n",
      "Epoch [33/300], Step [17600/27733], Loss: 3.0291\n",
      "Epoch [33/300], Step [17700/27733], Loss: 3.6254\n",
      "Epoch [33/300], Step [17800/27733], Loss: 3.7938\n",
      "Epoch [33/300], Step [17900/27733], Loss: 2.6322\n",
      "Epoch [33/300], Step [18000/27733], Loss: 3.0358\n",
      "Epoch [33/300], Step [18100/27733], Loss: 3.1401\n",
      "Epoch [33/300], Step [18200/27733], Loss: 3.9268\n",
      "Epoch [33/300], Step [18300/27733], Loss: 2.8229\n",
      "Epoch [33/300], Step [18400/27733], Loss: 3.4738\n",
      "Epoch [33/300], Step [18500/27733], Loss: 3.2534\n",
      "Epoch [33/300], Step [18600/27733], Loss: 2.7577\n",
      "Epoch [33/300], Step [18700/27733], Loss: 2.6643\n",
      "Epoch [33/300], Step [18800/27733], Loss: 3.3094\n",
      "Epoch [33/300], Step [18900/27733], Loss: 3.7156\n",
      "Epoch [33/300], Step [19000/27733], Loss: 2.8051\n",
      "Epoch [33/300], Step [19100/27733], Loss: 2.9133\n",
      "Epoch [33/300], Step [19200/27733], Loss: 3.1887\n",
      "Epoch [33/300], Step [19300/27733], Loss: 2.2228\n",
      "Epoch [33/300], Step [19400/27733], Loss: 2.7546\n",
      "Epoch [33/300], Step [19500/27733], Loss: 1.9253\n",
      "Epoch [33/300], Step [19600/27733], Loss: 2.3608\n",
      "Epoch [33/300], Step [19700/27733], Loss: 3.0156\n",
      "Epoch [33/300], Step [19800/27733], Loss: 2.8221\n",
      "Epoch [33/300], Step [19900/27733], Loss: 1.9298\n",
      "Epoch [33/300], Step [20000/27733], Loss: 3.4801\n",
      "Epoch [33/300], Step [20100/27733], Loss: 3.3523\n",
      "Epoch [33/300], Step [20200/27733], Loss: 2.9827\n",
      "Epoch [33/300], Step [20300/27733], Loss: 2.9644\n",
      "Epoch [33/300], Step [20400/27733], Loss: 3.3968\n",
      "Epoch [33/300], Step [20500/27733], Loss: 2.8319\n",
      "Epoch [33/300], Step [20600/27733], Loss: 2.5381\n",
      "Epoch [33/300], Step [20700/27733], Loss: 2.5916\n",
      "Epoch [33/300], Step [20800/27733], Loss: 3.7688\n",
      "Epoch [33/300], Step [20900/27733], Loss: 3.0702\n",
      "Epoch [33/300], Step [21000/27733], Loss: 3.6083\n",
      "Epoch [33/300], Step [21100/27733], Loss: 2.6949\n",
      "Epoch [33/300], Step [21200/27733], Loss: 3.7981\n",
      "Epoch [33/300], Step [21300/27733], Loss: 3.2058\n",
      "Epoch [33/300], Step [21400/27733], Loss: 2.9111\n",
      "Epoch [33/300], Step [21500/27733], Loss: 3.0867\n",
      "Epoch [33/300], Step [21600/27733], Loss: 2.5786\n",
      "Epoch [33/300], Step [21700/27733], Loss: 2.9566\n",
      "Epoch [33/300], Step [21800/27733], Loss: 3.1686\n",
      "Epoch [33/300], Step [21900/27733], Loss: 2.6568\n",
      "Epoch [33/300], Step [22000/27733], Loss: 3.8106\n",
      "Epoch [33/300], Step [22100/27733], Loss: 3.6861\n",
      "Epoch [33/300], Step [22200/27733], Loss: 2.4660\n",
      "Epoch [33/300], Step [22300/27733], Loss: 2.8412\n",
      "Epoch [33/300], Step [22400/27733], Loss: 3.2824\n",
      "Epoch [33/300], Step [22500/27733], Loss: 3.0305\n",
      "Epoch [33/300], Step [22600/27733], Loss: 2.5832\n",
      "Epoch [33/300], Step [22700/27733], Loss: 3.0730\n",
      "Epoch [33/300], Step [22800/27733], Loss: 2.6837\n",
      "Epoch [33/300], Step [22900/27733], Loss: 2.8950\n",
      "Epoch [33/300], Step [23000/27733], Loss: 3.1055\n",
      "Epoch [33/300], Step [23100/27733], Loss: 2.7771\n",
      "Epoch [33/300], Step [23200/27733], Loss: 3.6186\n",
      "Epoch [33/300], Step [23300/27733], Loss: 2.8798\n",
      "Epoch [33/300], Step [23400/27733], Loss: 2.9596\n",
      "Epoch [33/300], Step [23500/27733], Loss: 3.1901\n",
      "Epoch [33/300], Step [23600/27733], Loss: 2.8105\n",
      "Epoch [33/300], Step [23700/27733], Loss: 3.1459\n",
      "Epoch [33/300], Step [23800/27733], Loss: 3.4883\n",
      "Epoch [33/300], Step [23900/27733], Loss: 2.9723\n",
      "Epoch [33/300], Step [24000/27733], Loss: 3.1714\n",
      "Epoch [33/300], Step [24100/27733], Loss: 3.0921\n",
      "Epoch [33/300], Step [24200/27733], Loss: 2.5620\n",
      "Epoch [33/300], Step [24300/27733], Loss: 2.9903\n",
      "Epoch [33/300], Step [24400/27733], Loss: 2.5944\n",
      "Epoch [33/300], Step [24500/27733], Loss: 2.7077\n",
      "Epoch [33/300], Step [24600/27733], Loss: 2.7504\n",
      "Epoch [33/300], Step [24700/27733], Loss: 3.3698\n",
      "Epoch [33/300], Step [24800/27733], Loss: 2.8533\n",
      "Epoch [33/300], Step [24900/27733], Loss: 2.7533\n",
      "Epoch [33/300], Step [25000/27733], Loss: 3.2002\n",
      "Epoch [33/300], Step [25100/27733], Loss: 3.1529\n",
      "Epoch [33/300], Step [25200/27733], Loss: 3.1385\n",
      "Epoch [33/300], Step [25300/27733], Loss: 3.2474\n",
      "Epoch [33/300], Step [25400/27733], Loss: 3.0570\n",
      "Epoch [33/300], Step [25500/27733], Loss: 2.8128\n",
      "Epoch [33/300], Step [25600/27733], Loss: 3.4768\n",
      "Epoch [33/300], Step [25700/27733], Loss: 2.7827\n",
      "Epoch [33/300], Step [25800/27733], Loss: 3.8826\n",
      "Epoch [33/300], Step [25900/27733], Loss: 2.5698\n",
      "Epoch [33/300], Step [26000/27733], Loss: 4.0857\n",
      "Epoch [33/300], Step [26100/27733], Loss: 4.4162\n",
      "Epoch [33/300], Step [26200/27733], Loss: 2.8059\n",
      "Epoch [33/300], Step [26300/27733], Loss: 2.4031\n",
      "Epoch [33/300], Step [26400/27733], Loss: 3.1090\n",
      "Epoch [33/300], Step [26500/27733], Loss: 2.9669\n",
      "Epoch [33/300], Step [26600/27733], Loss: 2.6953\n",
      "Epoch [33/300], Step [26700/27733], Loss: 3.2441\n",
      "Epoch [33/300], Step [26800/27733], Loss: 3.0980\n",
      "Epoch [33/300], Step [26900/27733], Loss: 3.3017\n",
      "Epoch [33/300], Step [27000/27733], Loss: 2.6890\n",
      "Epoch [33/300], Step [27100/27733], Loss: 2.9895\n",
      "Epoch [33/300], Step [27200/27733], Loss: 3.6542\n",
      "Epoch [33/300], Step [27300/27733], Loss: 3.1515\n",
      "Epoch [33/300], Step [27400/27733], Loss: 2.2072\n",
      "Epoch [33/300], Step [27500/27733], Loss: 2.6621\n",
      "Epoch [33/300], Step [27600/27733], Loss: 2.6434\n",
      "Epoch [33/300], Step [27700/27733], Loss: 2.8566\n",
      "Epoch [34/300], Step [100/27733], Loss: 2.7176\n",
      "Epoch [34/300], Step [200/27733], Loss: 2.3732\n",
      "Epoch [34/300], Step [300/27733], Loss: 2.3023\n",
      "Epoch [34/300], Step [400/27733], Loss: 2.5035\n",
      "Epoch [34/300], Step [500/27733], Loss: 2.8778\n",
      "Epoch [34/300], Step [600/27733], Loss: 1.9969\n",
      "Epoch [34/300], Step [700/27733], Loss: 2.5765\n",
      "Epoch [34/300], Step [800/27733], Loss: 2.5945\n",
      "Epoch [34/300], Step [900/27733], Loss: 1.8807\n",
      "Epoch [34/300], Step [1000/27733], Loss: 2.9694\n",
      "Epoch [34/300], Step [1100/27733], Loss: 2.9080\n",
      "Epoch [34/300], Step [1200/27733], Loss: 3.4214\n",
      "Epoch [34/300], Step [1300/27733], Loss: 2.7140\n",
      "Epoch [34/300], Step [1400/27733], Loss: 1.8458\n",
      "Epoch [34/300], Step [1500/27733], Loss: 3.0978\n",
      "Epoch [34/300], Step [1600/27733], Loss: 2.9988\n",
      "Epoch [34/300], Step [1700/27733], Loss: 1.8919\n",
      "Epoch [34/300], Step [1800/27733], Loss: 2.6730\n",
      "Epoch [34/300], Step [1900/27733], Loss: 2.2380\n",
      "Epoch [34/300], Step [2000/27733], Loss: 2.7983\n",
      "Epoch [34/300], Step [2100/27733], Loss: 2.8163\n",
      "Epoch [34/300], Step [2200/27733], Loss: 2.6028\n",
      "Epoch [34/300], Step [2300/27733], Loss: 2.3428\n",
      "Epoch [34/300], Step [2400/27733], Loss: 2.4723\n",
      "Epoch [34/300], Step [2500/27733], Loss: 3.2272\n",
      "Epoch [34/300], Step [2600/27733], Loss: 2.7615\n",
      "Epoch [34/300], Step [2700/27733], Loss: 2.8848\n",
      "Epoch [34/300], Step [2800/27733], Loss: 2.2867\n",
      "Epoch [34/300], Step [2900/27733], Loss: 2.2198\n",
      "Epoch [34/300], Step [3000/27733], Loss: 2.1579\n",
      "Epoch [34/300], Step [3100/27733], Loss: 3.3528\n",
      "Epoch [34/300], Step [3200/27733], Loss: 3.2713\n",
      "Epoch [34/300], Step [3300/27733], Loss: 1.8348\n",
      "Epoch [34/300], Step [3400/27733], Loss: 2.7188\n",
      "Epoch [34/300], Step [3500/27733], Loss: 2.2461\n",
      "Epoch [34/300], Step [3600/27733], Loss: 2.6798\n",
      "Epoch [34/300], Step [3700/27733], Loss: 2.8676\n",
      "Epoch [34/300], Step [3800/27733], Loss: 2.8575\n",
      "Epoch [34/300], Step [3900/27733], Loss: 2.3657\n",
      "Epoch [34/300], Step [4000/27733], Loss: 2.7701\n",
      "Epoch [34/300], Step [4100/27733], Loss: 2.5961\n",
      "Epoch [34/300], Step [4200/27733], Loss: 2.9165\n",
      "Epoch [34/300], Step [4300/27733], Loss: 2.7187\n",
      "Epoch [34/300], Step [4400/27733], Loss: 1.8976\n",
      "Epoch [34/300], Step [4500/27733], Loss: 1.9571\n",
      "Epoch [34/300], Step [4600/27733], Loss: 3.1373\n",
      "Epoch [34/300], Step [4700/27733], Loss: 1.5646\n",
      "Epoch [34/300], Step [4800/27733], Loss: 2.8651\n",
      "Epoch [34/300], Step [4900/27733], Loss: 2.7039\n",
      "Epoch [34/300], Step [5000/27733], Loss: 2.3343\n",
      "Epoch [34/300], Step [5100/27733], Loss: 2.3525\n",
      "Epoch [34/300], Step [5200/27733], Loss: 3.0545\n",
      "Epoch [34/300], Step [5300/27733], Loss: 2.2090\n",
      "Epoch [34/300], Step [5400/27733], Loss: 2.2550\n",
      "Epoch [34/300], Step [5500/27733], Loss: 1.8698\n",
      "Epoch [34/300], Step [5600/27733], Loss: 2.6465\n",
      "Epoch [34/300], Step [5700/27733], Loss: 2.3126\n",
      "Epoch [34/300], Step [5800/27733], Loss: 2.2938\n",
      "Epoch [34/300], Step [5900/27733], Loss: 2.7715\n",
      "Epoch [34/300], Step [6000/27733], Loss: 2.7756\n",
      "Epoch [34/300], Step [6100/27733], Loss: 3.0607\n",
      "Epoch [34/300], Step [6200/27733], Loss: 2.6940\n",
      "Epoch [34/300], Step [6300/27733], Loss: 3.0367\n",
      "Epoch [34/300], Step [6400/27733], Loss: 2.5985\n",
      "Epoch [34/300], Step [6500/27733], Loss: 3.5656\n",
      "Epoch [34/300], Step [6600/27733], Loss: 3.2747\n",
      "Epoch [34/300], Step [6700/27733], Loss: 2.9123\n",
      "Epoch [34/300], Step [6800/27733], Loss: 2.5251\n",
      "Epoch [34/300], Step [6900/27733], Loss: 2.8650\n",
      "Epoch [34/300], Step [7000/27733], Loss: 2.1885\n",
      "Epoch [34/300], Step [7100/27733], Loss: 3.6506\n",
      "Epoch [34/300], Step [7200/27733], Loss: 2.7868\n",
      "Epoch [34/300], Step [7300/27733], Loss: 2.7733\n",
      "Epoch [34/300], Step [7400/27733], Loss: 2.9411\n",
      "Epoch [34/300], Step [7500/27733], Loss: 2.8875\n",
      "Epoch [34/300], Step [7600/27733], Loss: 2.0570\n",
      "Epoch [34/300], Step [7700/27733], Loss: 2.7020\n",
      "Epoch [34/300], Step [7800/27733], Loss: 2.3135\n",
      "Epoch [34/300], Step [7900/27733], Loss: 2.3136\n",
      "Epoch [34/300], Step [8000/27733], Loss: 2.9767\n",
      "Epoch [34/300], Step [8100/27733], Loss: 1.9862\n",
      "Epoch [34/300], Step [8200/27733], Loss: 3.7060\n",
      "Epoch [34/300], Step [8300/27733], Loss: 2.5154\n",
      "Epoch [34/300], Step [8400/27733], Loss: 2.0918\n",
      "Epoch [34/300], Step [8500/27733], Loss: 2.0589\n",
      "Epoch [34/300], Step [8600/27733], Loss: 1.8994\n",
      "Epoch [34/300], Step [8700/27733], Loss: 2.7331\n",
      "Epoch [34/300], Step [8800/27733], Loss: 3.0448\n",
      "Epoch [34/300], Step [8900/27733], Loss: 2.9853\n",
      "Epoch [34/300], Step [9000/27733], Loss: 3.0495\n",
      "Epoch [34/300], Step [9100/27733], Loss: 3.0981\n",
      "Epoch [34/300], Step [9200/27733], Loss: 2.6299\n",
      "Epoch [34/300], Step [9300/27733], Loss: 2.9953\n",
      "Epoch [34/300], Step [9400/27733], Loss: 3.4613\n",
      "Epoch [34/300], Step [9500/27733], Loss: 2.6748\n",
      "Epoch [34/300], Step [9600/27733], Loss: 2.6404\n",
      "Epoch [34/300], Step [9700/27733], Loss: 2.6981\n",
      "Epoch [34/300], Step [9800/27733], Loss: 3.0328\n",
      "Epoch [34/300], Step [9900/27733], Loss: 2.3386\n",
      "Epoch [34/300], Step [10000/27733], Loss: 2.8841\n",
      "Epoch [34/300], Step [10100/27733], Loss: 2.6733\n",
      "Epoch [34/300], Step [10200/27733], Loss: 3.1176\n",
      "Epoch [34/300], Step [10300/27733], Loss: 2.7366\n",
      "Epoch [34/300], Step [10400/27733], Loss: 2.5988\n",
      "Epoch [34/300], Step [10500/27733], Loss: 2.9635\n",
      "Epoch [34/300], Step [10600/27733], Loss: 2.3693\n",
      "Epoch [34/300], Step [10700/27733], Loss: 3.5379\n",
      "Epoch [34/300], Step [10800/27733], Loss: 3.2859\n",
      "Epoch [34/300], Step [10900/27733], Loss: 2.5353\n",
      "Epoch [34/300], Step [11000/27733], Loss: 2.7498\n",
      "Epoch [34/300], Step [11100/27733], Loss: 2.8551\n",
      "Epoch [34/300], Step [11200/27733], Loss: 2.5830\n",
      "Epoch [34/300], Step [11300/27733], Loss: 2.2337\n",
      "Epoch [34/300], Step [11400/27733], Loss: 2.0226\n",
      "Epoch [34/300], Step [11500/27733], Loss: 3.1560\n",
      "Epoch [34/300], Step [11600/27733], Loss: 2.9096\n",
      "Epoch [34/300], Step [11700/27733], Loss: 2.8032\n",
      "Epoch [34/300], Step [11800/27733], Loss: 3.0975\n",
      "Epoch [34/300], Step [11900/27733], Loss: 2.3857\n",
      "Epoch [34/300], Step [12000/27733], Loss: 3.5772\n",
      "Epoch [34/300], Step [12100/27733], Loss: 2.7237\n",
      "Epoch [34/300], Step [12200/27733], Loss: 3.6567\n",
      "Epoch [34/300], Step [12300/27733], Loss: 2.6313\n",
      "Epoch [34/300], Step [12400/27733], Loss: 2.8951\n",
      "Epoch [34/300], Step [12500/27733], Loss: 2.6867\n",
      "Epoch [34/300], Step [12600/27733], Loss: 3.0906\n",
      "Epoch [34/300], Step [12700/27733], Loss: 3.0675\n",
      "Epoch [34/300], Step [12800/27733], Loss: 3.2567\n",
      "Epoch [34/300], Step [12900/27733], Loss: 2.6750\n",
      "Epoch [34/300], Step [13000/27733], Loss: 2.9698\n",
      "Epoch [34/300], Step [13100/27733], Loss: 3.2274\n",
      "Epoch [34/300], Step [13200/27733], Loss: 2.7076\n",
      "Epoch [34/300], Step [13300/27733], Loss: 3.1052\n",
      "Epoch [34/300], Step [13400/27733], Loss: 1.9696\n",
      "Epoch [34/300], Step [13500/27733], Loss: 2.9737\n",
      "Epoch [34/300], Step [13600/27733], Loss: 3.0643\n",
      "Epoch [34/300], Step [13700/27733], Loss: 3.4541\n",
      "Epoch [34/300], Step [13800/27733], Loss: 2.3899\n",
      "Epoch [34/300], Step [13900/27733], Loss: 3.2467\n",
      "Epoch [34/300], Step [14000/27733], Loss: 3.2521\n",
      "Epoch [34/300], Step [14100/27733], Loss: 2.8660\n",
      "Epoch [34/300], Step [14200/27733], Loss: 2.7822\n",
      "Epoch [34/300], Step [14300/27733], Loss: 3.4115\n",
      "Epoch [34/300], Step [14400/27733], Loss: 2.7329\n",
      "Epoch [34/300], Step [14500/27733], Loss: 2.4012\n",
      "Epoch [34/300], Step [14600/27733], Loss: 3.6700\n",
      "Epoch [34/300], Step [14700/27733], Loss: 3.4880\n",
      "Epoch [34/300], Step [14800/27733], Loss: 3.0884\n",
      "Epoch [34/300], Step [14900/27733], Loss: 3.3916\n",
      "Epoch [34/300], Step [15000/27733], Loss: 2.6157\n",
      "Epoch [34/300], Step [15100/27733], Loss: 3.3707\n",
      "Epoch [34/300], Step [15200/27733], Loss: 2.3401\n",
      "Epoch [34/300], Step [15300/27733], Loss: 2.7841\n",
      "Epoch [34/300], Step [15400/27733], Loss: 3.3359\n",
      "Epoch [34/300], Step [15500/27733], Loss: 2.9715\n",
      "Epoch [34/300], Step [15600/27733], Loss: 3.9705\n",
      "Epoch [34/300], Step [15700/27733], Loss: 2.6199\n",
      "Epoch [34/300], Step [15800/27733], Loss: 2.7917\n",
      "Epoch [34/300], Step [15900/27733], Loss: 2.6618\n",
      "Epoch [34/300], Step [16000/27733], Loss: 3.2787\n",
      "Epoch [34/300], Step [16100/27733], Loss: 2.9239\n",
      "Epoch [34/300], Step [16200/27733], Loss: 2.6924\n",
      "Epoch [34/300], Step [16300/27733], Loss: 3.2750\n",
      "Epoch [34/300], Step [16400/27733], Loss: 2.8399\n",
      "Epoch [34/300], Step [16500/27733], Loss: 2.6854\n",
      "Epoch [34/300], Step [16600/27733], Loss: 3.2715\n",
      "Epoch [34/300], Step [16700/27733], Loss: 2.5807\n",
      "Epoch [34/300], Step [16800/27733], Loss: 2.7672\n",
      "Epoch [34/300], Step [16900/27733], Loss: 2.6568\n",
      "Epoch [34/300], Step [17000/27733], Loss: 3.1192\n",
      "Epoch [34/300], Step [17100/27733], Loss: 2.3127\n",
      "Epoch [34/300], Step [17200/27733], Loss: 3.3375\n",
      "Epoch [34/300], Step [17300/27733], Loss: 2.8376\n",
      "Epoch [34/300], Step [17400/27733], Loss: 2.5168\n",
      "Epoch [34/300], Step [17500/27733], Loss: 3.5783\n",
      "Epoch [34/300], Step [17600/27733], Loss: 3.4665\n",
      "Epoch [34/300], Step [17700/27733], Loss: 3.8671\n",
      "Epoch [34/300], Step [17800/27733], Loss: 2.9849\n",
      "Epoch [34/300], Step [17900/27733], Loss: 3.1764\n",
      "Epoch [34/300], Step [18000/27733], Loss: 3.5015\n",
      "Epoch [34/300], Step [18100/27733], Loss: 3.4074\n",
      "Epoch [34/300], Step [18200/27733], Loss: 4.0259\n",
      "Epoch [34/300], Step [18300/27733], Loss: 2.7333\n",
      "Epoch [34/300], Step [18400/27733], Loss: 2.9211\n",
      "Epoch [34/300], Step [18500/27733], Loss: 3.0814\n",
      "Epoch [34/300], Step [18600/27733], Loss: 2.9616\n",
      "Epoch [34/300], Step [18700/27733], Loss: 3.4075\n",
      "Epoch [34/300], Step [18800/27733], Loss: 3.1946\n",
      "Epoch [34/300], Step [18900/27733], Loss: 3.7975\n",
      "Epoch [34/300], Step [19000/27733], Loss: 3.3026\n",
      "Epoch [34/300], Step [19100/27733], Loss: 2.5614\n",
      "Epoch [34/300], Step [19200/27733], Loss: 3.0146\n",
      "Epoch [34/300], Step [19300/27733], Loss: 2.4576\n",
      "Epoch [34/300], Step [19400/27733], Loss: 3.0293\n",
      "Epoch [34/300], Step [19500/27733], Loss: 3.2048\n",
      "Epoch [34/300], Step [19600/27733], Loss: 2.6212\n",
      "Epoch [34/300], Step [19700/27733], Loss: 2.5667\n",
      "Epoch [34/300], Step [19800/27733], Loss: 2.9163\n",
      "Epoch [34/300], Step [19900/27733], Loss: 3.1384\n",
      "Epoch [34/300], Step [20000/27733], Loss: 3.1082\n",
      "Epoch [34/300], Step [20100/27733], Loss: 2.5501\n",
      "Epoch [34/300], Step [20200/27733], Loss: 2.7222\n",
      "Epoch [34/300], Step [20300/27733], Loss: 2.6482\n",
      "Epoch [34/300], Step [20400/27733], Loss: 3.7925\n",
      "Epoch [34/300], Step [20500/27733], Loss: 3.5359\n",
      "Epoch [34/300], Step [20600/27733], Loss: 3.3197\n",
      "Epoch [34/300], Step [20700/27733], Loss: 3.1049\n",
      "Epoch [34/300], Step [20800/27733], Loss: 3.0921\n",
      "Epoch [34/300], Step [20900/27733], Loss: 3.4168\n",
      "Epoch [34/300], Step [21000/27733], Loss: 4.2402\n",
      "Epoch [34/300], Step [21100/27733], Loss: 3.7738\n",
      "Epoch [34/300], Step [21200/27733], Loss: 2.4166\n",
      "Epoch [34/300], Step [21300/27733], Loss: 2.8242\n",
      "Epoch [34/300], Step [21400/27733], Loss: 3.2130\n",
      "Epoch [34/300], Step [21500/27733], Loss: 2.8432\n",
      "Epoch [34/300], Step [21600/27733], Loss: 2.9812\n",
      "Epoch [34/300], Step [21700/27733], Loss: 3.1802\n",
      "Epoch [34/300], Step [21800/27733], Loss: 3.5548\n",
      "Epoch [34/300], Step [21900/27733], Loss: 2.5821\n",
      "Epoch [34/300], Step [22000/27733], Loss: 3.1894\n",
      "Epoch [34/300], Step [22100/27733], Loss: 3.0813\n",
      "Epoch [34/300], Step [22200/27733], Loss: 2.9627\n",
      "Epoch [34/300], Step [22300/27733], Loss: 3.9739\n",
      "Epoch [34/300], Step [22400/27733], Loss: 3.1693\n",
      "Epoch [34/300], Step [22500/27733], Loss: 3.0974\n",
      "Epoch [34/300], Step [22600/27733], Loss: 3.4609\n",
      "Epoch [34/300], Step [22700/27733], Loss: 2.7537\n",
      "Epoch [34/300], Step [22800/27733], Loss: 3.2643\n",
      "Epoch [34/300], Step [22900/27733], Loss: 2.8559\n",
      "Epoch [34/300], Step [23000/27733], Loss: 3.7861\n",
      "Epoch [34/300], Step [23100/27733], Loss: 3.6700\n",
      "Epoch [34/300], Step [23200/27733], Loss: 2.5586\n",
      "Epoch [34/300], Step [23300/27733], Loss: 1.9069\n",
      "Epoch [34/300], Step [23400/27733], Loss: 4.1066\n",
      "Epoch [34/300], Step [23500/27733], Loss: 2.7156\n",
      "Epoch [34/300], Step [23600/27733], Loss: 3.2042\n",
      "Epoch [34/300], Step [23700/27733], Loss: 3.7148\n",
      "Epoch [34/300], Step [23800/27733], Loss: 3.3268\n",
      "Epoch [34/300], Step [23900/27733], Loss: 2.9460\n",
      "Epoch [34/300], Step [24000/27733], Loss: 3.1885\n",
      "Epoch [34/300], Step [24100/27733], Loss: 2.8978\n",
      "Epoch [34/300], Step [24200/27733], Loss: 2.6345\n",
      "Epoch [34/300], Step [24300/27733], Loss: 2.7968\n",
      "Epoch [34/300], Step [24400/27733], Loss: 3.1670\n",
      "Epoch [34/300], Step [24500/27733], Loss: 3.6153\n",
      "Epoch [34/300], Step [24600/27733], Loss: 2.3682\n",
      "Epoch [34/300], Step [24700/27733], Loss: 2.8471\n",
      "Epoch [34/300], Step [24800/27733], Loss: 2.5728\n",
      "Epoch [34/300], Step [24900/27733], Loss: 3.2710\n",
      "Epoch [34/300], Step [25000/27733], Loss: 3.0625\n",
      "Epoch [34/300], Step [25100/27733], Loss: 2.4107\n",
      "Epoch [34/300], Step [25200/27733], Loss: 3.3448\n",
      "Epoch [34/300], Step [25300/27733], Loss: 3.1524\n",
      "Epoch [34/300], Step [25400/27733], Loss: 2.7456\n",
      "Epoch [34/300], Step [25500/27733], Loss: 2.2245\n",
      "Epoch [34/300], Step [25600/27733], Loss: 3.3679\n",
      "Epoch [34/300], Step [25700/27733], Loss: 2.7384\n",
      "Epoch [34/300], Step [25800/27733], Loss: 3.7764\n",
      "Epoch [34/300], Step [25900/27733], Loss: 2.7858\n",
      "Epoch [34/300], Step [26000/27733], Loss: 2.5515\n",
      "Epoch [34/300], Step [26100/27733], Loss: 2.8737\n",
      "Epoch [34/300], Step [26200/27733], Loss: 2.9218\n",
      "Epoch [34/300], Step [26300/27733], Loss: 3.6284\n",
      "Epoch [34/300], Step [26400/27733], Loss: 2.6959\n",
      "Epoch [34/300], Step [26500/27733], Loss: 2.6620\n",
      "Epoch [34/300], Step [26600/27733], Loss: 3.5730\n",
      "Epoch [34/300], Step [26700/27733], Loss: 2.9266\n",
      "Epoch [34/300], Step [26800/27733], Loss: 2.9794\n",
      "Epoch [34/300], Step [26900/27733], Loss: 3.6963\n",
      "Epoch [34/300], Step [27000/27733], Loss: 3.3526\n",
      "Epoch [34/300], Step [27100/27733], Loss: 2.6457\n",
      "Epoch [34/300], Step [27200/27733], Loss: 3.3186\n",
      "Epoch [34/300], Step [27300/27733], Loss: 3.7922\n",
      "Epoch [34/300], Step [27400/27733], Loss: 3.3306\n",
      "Epoch [34/300], Step [27500/27733], Loss: 2.7706\n",
      "Epoch [34/300], Step [27600/27733], Loss: 3.5849\n",
      "Epoch [34/300], Step [27700/27733], Loss: 2.3694\n",
      "Epoch [35/300], Step [100/27733], Loss: 2.2643\n",
      "Epoch [35/300], Step [200/27733], Loss: 2.3036\n",
      "Epoch [35/300], Step [300/27733], Loss: 2.3897\n",
      "Epoch [35/300], Step [400/27733], Loss: 3.2735\n",
      "Epoch [35/300], Step [500/27733], Loss: 2.9042\n",
      "Epoch [35/300], Step [600/27733], Loss: 2.7331\n",
      "Epoch [35/300], Step [700/27733], Loss: 2.5943\n",
      "Epoch [35/300], Step [800/27733], Loss: 2.5920\n",
      "Epoch [35/300], Step [900/27733], Loss: 2.3923\n",
      "Epoch [35/300], Step [1000/27733], Loss: 1.8012\n",
      "Epoch [35/300], Step [1100/27733], Loss: 2.2460\n",
      "Epoch [35/300], Step [1200/27733], Loss: 2.4632\n",
      "Epoch [35/300], Step [1300/27733], Loss: 2.5318\n",
      "Epoch [35/300], Step [1400/27733], Loss: 2.1265\n",
      "Epoch [35/300], Step [1500/27733], Loss: 2.5960\n",
      "Epoch [35/300], Step [1600/27733], Loss: 2.3568\n",
      "Epoch [35/300], Step [1700/27733], Loss: 2.3588\n",
      "Epoch [35/300], Step [1800/27733], Loss: 3.0423\n",
      "Epoch [35/300], Step [1900/27733], Loss: 2.5493\n",
      "Epoch [35/300], Step [2000/27733], Loss: 2.0731\n",
      "Epoch [35/300], Step [2100/27733], Loss: 2.1625\n",
      "Epoch [35/300], Step [2200/27733], Loss: 3.3354\n",
      "Epoch [35/300], Step [2300/27733], Loss: 1.6860\n",
      "Epoch [35/300], Step [2400/27733], Loss: 2.5444\n",
      "Epoch [35/300], Step [2500/27733], Loss: 1.8430\n",
      "Epoch [35/300], Step [2600/27733], Loss: 1.8550\n",
      "Epoch [35/300], Step [2700/27733], Loss: 2.7818\n",
      "Epoch [35/300], Step [2800/27733], Loss: 3.3001\n",
      "Epoch [35/300], Step [2900/27733], Loss: 3.2751\n",
      "Epoch [35/300], Step [3000/27733], Loss: 2.0210\n",
      "Epoch [35/300], Step [3100/27733], Loss: 3.4965\n",
      "Epoch [35/300], Step [3200/27733], Loss: 1.7702\n",
      "Epoch [35/300], Step [3300/27733], Loss: 2.4519\n",
      "Epoch [35/300], Step [3400/27733], Loss: 2.5151\n",
      "Epoch [35/300], Step [3500/27733], Loss: 2.9574\n",
      "Epoch [35/300], Step [3600/27733], Loss: 2.2792\n",
      "Epoch [35/300], Step [3700/27733], Loss: 2.0930\n",
      "Epoch [35/300], Step [3800/27733], Loss: 2.7402\n",
      "Epoch [35/300], Step [3900/27733], Loss: 2.3924\n",
      "Epoch [35/300], Step [4000/27733], Loss: 2.7926\n",
      "Epoch [35/300], Step [4100/27733], Loss: 3.5856\n",
      "Epoch [35/300], Step [4200/27733], Loss: 2.4933\n",
      "Epoch [35/300], Step [4300/27733], Loss: 2.3815\n",
      "Epoch [35/300], Step [4400/27733], Loss: 2.5097\n",
      "Epoch [35/300], Step [4500/27733], Loss: 2.4824\n",
      "Epoch [35/300], Step [4600/27733], Loss: 2.5887\n",
      "Epoch [35/300], Step [4700/27733], Loss: 2.5935\n",
      "Epoch [35/300], Step [4800/27733], Loss: 2.3720\n",
      "Epoch [35/300], Step [4900/27733], Loss: 2.1185\n",
      "Epoch [35/300], Step [5000/27733], Loss: 3.3466\n",
      "Epoch [35/300], Step [5100/27733], Loss: 2.6409\n",
      "Epoch [35/300], Step [5200/27733], Loss: 2.2682\n",
      "Epoch [35/300], Step [5300/27733], Loss: 2.1037\n",
      "Epoch [35/300], Step [5400/27733], Loss: 3.4276\n",
      "Epoch [35/300], Step [5500/27733], Loss: 2.7083\n",
      "Epoch [35/300], Step [5600/27733], Loss: 3.4517\n",
      "Epoch [35/300], Step [5700/27733], Loss: 2.1092\n",
      "Epoch [35/300], Step [5800/27733], Loss: 2.9635\n",
      "Epoch [35/300], Step [5900/27733], Loss: 3.0744\n",
      "Epoch [35/300], Step [6000/27733], Loss: 3.0707\n",
      "Epoch [35/300], Step [6100/27733], Loss: 2.1723\n",
      "Epoch [35/300], Step [6200/27733], Loss: 2.4038\n",
      "Epoch [35/300], Step [6300/27733], Loss: 2.4808\n",
      "Epoch [35/300], Step [6400/27733], Loss: 2.5978\n",
      "Epoch [35/300], Step [6500/27733], Loss: 2.1741\n",
      "Epoch [35/300], Step [6600/27733], Loss: 2.6803\n",
      "Epoch [35/300], Step [6700/27733], Loss: 3.6766\n",
      "Epoch [35/300], Step [6800/27733], Loss: 2.4892\n",
      "Epoch [35/300], Step [6900/27733], Loss: 2.6053\n",
      "Epoch [35/300], Step [7000/27733], Loss: 2.8223\n",
      "Epoch [35/300], Step [7100/27733], Loss: 3.2921\n",
      "Epoch [35/300], Step [7200/27733], Loss: 2.1556\n",
      "Epoch [35/300], Step [7300/27733], Loss: 2.9012\n",
      "Epoch [35/300], Step [7400/27733], Loss: 2.5897\n",
      "Epoch [35/300], Step [7500/27733], Loss: 3.6588\n",
      "Epoch [35/300], Step [7600/27733], Loss: 2.2114\n",
      "Epoch [35/300], Step [7700/27733], Loss: 2.9239\n",
      "Epoch [35/300], Step [7800/27733], Loss: 2.7005\n",
      "Epoch [35/300], Step [7900/27733], Loss: 2.9469\n",
      "Epoch [35/300], Step [8000/27733], Loss: 3.2312\n",
      "Epoch [35/300], Step [8100/27733], Loss: 2.3351\n",
      "Epoch [35/300], Step [8200/27733], Loss: 2.4544\n",
      "Epoch [35/300], Step [8300/27733], Loss: 2.8818\n",
      "Epoch [35/300], Step [8400/27733], Loss: 3.0001\n",
      "Epoch [35/300], Step [8500/27733], Loss: 3.1365\n",
      "Epoch [35/300], Step [8600/27733], Loss: 2.0580\n",
      "Epoch [35/300], Step [8700/27733], Loss: 2.8858\n",
      "Epoch [35/300], Step [8800/27733], Loss: 2.5488\n",
      "Epoch [35/300], Step [8900/27733], Loss: 2.4546\n",
      "Epoch [35/300], Step [9000/27733], Loss: 3.0138\n",
      "Epoch [35/300], Step [9100/27733], Loss: 2.7051\n",
      "Epoch [35/300], Step [9200/27733], Loss: 2.6567\n",
      "Epoch [35/300], Step [9300/27733], Loss: 2.9542\n",
      "Epoch [35/300], Step [9400/27733], Loss: 3.2898\n",
      "Epoch [35/300], Step [9500/27733], Loss: 2.6801\n",
      "Epoch [35/300], Step [9600/27733], Loss: 2.2549\n",
      "Epoch [35/300], Step [9700/27733], Loss: 3.3048\n",
      "Epoch [35/300], Step [9800/27733], Loss: 2.5382\n",
      "Epoch [35/300], Step [9900/27733], Loss: 2.9382\n",
      "Epoch [35/300], Step [10000/27733], Loss: 2.6750\n",
      "Epoch [35/300], Step [10100/27733], Loss: 2.7526\n",
      "Epoch [35/300], Step [10200/27733], Loss: 2.8678\n",
      "Epoch [35/300], Step [10300/27733], Loss: 3.0556\n",
      "Epoch [35/300], Step [10400/27733], Loss: 2.8308\n",
      "Epoch [35/300], Step [10500/27733], Loss: 2.7868\n",
      "Epoch [35/300], Step [10600/27733], Loss: 2.2744\n",
      "Epoch [35/300], Step [10700/27733], Loss: 2.5259\n",
      "Epoch [35/300], Step [10800/27733], Loss: 3.1039\n",
      "Epoch [35/300], Step [10900/27733], Loss: 2.9313\n",
      "Epoch [35/300], Step [11000/27733], Loss: 3.2783\n",
      "Epoch [35/300], Step [11100/27733], Loss: 2.0736\n",
      "Epoch [35/300], Step [11200/27733], Loss: 2.1525\n",
      "Epoch [35/300], Step [11300/27733], Loss: 2.4695\n",
      "Epoch [35/300], Step [11400/27733], Loss: 2.5299\n",
      "Epoch [35/300], Step [11500/27733], Loss: 3.0047\n",
      "Epoch [35/300], Step [11600/27733], Loss: 3.3163\n",
      "Epoch [35/300], Step [11700/27733], Loss: 3.7174\n",
      "Epoch [35/300], Step [11800/27733], Loss: 2.5308\n",
      "Epoch [35/300], Step [11900/27733], Loss: 2.8188\n",
      "Epoch [35/300], Step [12000/27733], Loss: 2.2954\n",
      "Epoch [35/300], Step [12100/27733], Loss: 3.7563\n",
      "Epoch [35/300], Step [12200/27733], Loss: 2.1732\n",
      "Epoch [35/300], Step [12300/27733], Loss: 2.4938\n",
      "Epoch [35/300], Step [12400/27733], Loss: 2.1818\n",
      "Epoch [35/300], Step [12500/27733], Loss: 3.4472\n",
      "Epoch [35/300], Step [12600/27733], Loss: 2.9753\n",
      "Epoch [35/300], Step [12700/27733], Loss: 2.6082\n",
      "Epoch [35/300], Step [12800/27733], Loss: 2.9154\n",
      "Epoch [35/300], Step [12900/27733], Loss: 2.5104\n",
      "Epoch [35/300], Step [13000/27733], Loss: 2.4095\n",
      "Epoch [35/300], Step [13100/27733], Loss: 2.8988\n",
      "Epoch [35/300], Step [13200/27733], Loss: 3.4575\n",
      "Epoch [35/300], Step [13300/27733], Loss: 2.0187\n",
      "Epoch [35/300], Step [13400/27733], Loss: 3.2639\n",
      "Epoch [35/300], Step [13500/27733], Loss: 1.3928\n",
      "Epoch [35/300], Step [13600/27733], Loss: 2.7856\n",
      "Epoch [35/300], Step [13700/27733], Loss: 2.4565\n",
      "Epoch [35/300], Step [13800/27733], Loss: 2.4176\n",
      "Epoch [35/300], Step [13900/27733], Loss: 2.2899\n",
      "Epoch [35/300], Step [14000/27733], Loss: 3.3935\n",
      "Epoch [35/300], Step [14100/27733], Loss: 3.1640\n",
      "Epoch [35/300], Step [14200/27733], Loss: 2.7328\n",
      "Epoch [35/300], Step [14300/27733], Loss: 2.4056\n",
      "Epoch [35/300], Step [14400/27733], Loss: 3.1995\n",
      "Epoch [35/300], Step [14500/27733], Loss: 2.8526\n",
      "Epoch [35/300], Step [14600/27733], Loss: 2.8280\n",
      "Epoch [35/300], Step [14700/27733], Loss: 3.1191\n",
      "Epoch [35/300], Step [14800/27733], Loss: 3.8889\n",
      "Epoch [35/300], Step [14900/27733], Loss: 2.5049\n",
      "Epoch [35/300], Step [15000/27733], Loss: 2.4415\n",
      "Epoch [35/300], Step [15100/27733], Loss: 2.7021\n",
      "Epoch [35/300], Step [15200/27733], Loss: 3.1499\n",
      "Epoch [35/300], Step [15300/27733], Loss: 3.0111\n",
      "Epoch [35/300], Step [15400/27733], Loss: 3.2895\n",
      "Epoch [35/300], Step [15500/27733], Loss: 2.5890\n",
      "Epoch [35/300], Step [15600/27733], Loss: 2.7755\n",
      "Epoch [35/300], Step [15700/27733], Loss: 2.8005\n",
      "Epoch [35/300], Step [15800/27733], Loss: 2.5703\n",
      "Epoch [35/300], Step [15900/27733], Loss: 2.9725\n",
      "Epoch [35/300], Step [16000/27733], Loss: 3.0299\n",
      "Epoch [35/300], Step [16100/27733], Loss: 2.0802\n",
      "Epoch [35/300], Step [16200/27733], Loss: 3.4064\n",
      "Epoch [35/300], Step [16300/27733], Loss: 2.8150\n",
      "Epoch [35/300], Step [16400/27733], Loss: 2.4059\n",
      "Epoch [35/300], Step [16500/27733], Loss: 3.1419\n",
      "Epoch [35/300], Step [16600/27733], Loss: 2.1024\n",
      "Epoch [35/300], Step [16700/27733], Loss: 2.4920\n",
      "Epoch [35/300], Step [16800/27733], Loss: 2.3109\n",
      "Epoch [35/300], Step [16900/27733], Loss: 2.2172\n",
      "Epoch [35/300], Step [17000/27733], Loss: 2.1672\n",
      "Epoch [35/300], Step [17100/27733], Loss: 3.1340\n",
      "Epoch [35/300], Step [17200/27733], Loss: 3.0414\n",
      "Epoch [35/300], Step [17300/27733], Loss: 3.0915\n",
      "Epoch [35/300], Step [17400/27733], Loss: 2.9616\n",
      "Epoch [35/300], Step [17500/27733], Loss: 2.0917\n",
      "Epoch [35/300], Step [17600/27733], Loss: 2.5960\n",
      "Epoch [35/300], Step [17700/27733], Loss: 3.0220\n",
      "Epoch [35/300], Step [17800/27733], Loss: 2.5783\n",
      "Epoch [35/300], Step [17900/27733], Loss: 3.2323\n",
      "Epoch [35/300], Step [18000/27733], Loss: 2.8789\n",
      "Epoch [35/300], Step [18100/27733], Loss: 3.1370\n",
      "Epoch [35/300], Step [18200/27733], Loss: 2.8764\n",
      "Epoch [35/300], Step [18300/27733], Loss: 3.7411\n",
      "Epoch [35/300], Step [18400/27733], Loss: 2.2136\n",
      "Epoch [35/300], Step [18500/27733], Loss: 3.5968\n",
      "Epoch [35/300], Step [18600/27733], Loss: 3.0738\n",
      "Epoch [35/300], Step [18700/27733], Loss: 2.6694\n",
      "Epoch [35/300], Step [18800/27733], Loss: 3.3974\n",
      "Epoch [35/300], Step [18900/27733], Loss: 3.5984\n",
      "Epoch [35/300], Step [19000/27733], Loss: 2.9921\n",
      "Epoch [35/300], Step [19100/27733], Loss: 3.0719\n",
      "Epoch [35/300], Step [19200/27733], Loss: 2.8132\n",
      "Epoch [35/300], Step [19300/27733], Loss: 2.7188\n",
      "Epoch [35/300], Step [19400/27733], Loss: 2.8368\n",
      "Epoch [35/300], Step [19500/27733], Loss: 2.6288\n",
      "Epoch [35/300], Step [19600/27733], Loss: 3.3758\n",
      "Epoch [35/300], Step [19700/27733], Loss: 3.3368\n",
      "Epoch [35/300], Step [19800/27733], Loss: 3.3425\n",
      "Epoch [35/300], Step [19900/27733], Loss: 3.1294\n",
      "Epoch [35/300], Step [20000/27733], Loss: 3.2495\n",
      "Epoch [35/300], Step [20100/27733], Loss: 3.1016\n",
      "Epoch [35/300], Step [20200/27733], Loss: 2.4053\n",
      "Epoch [35/300], Step [20300/27733], Loss: 2.3448\n",
      "Epoch [35/300], Step [20400/27733], Loss: 3.1275\n",
      "Epoch [35/300], Step [20500/27733], Loss: 3.1338\n",
      "Epoch [35/300], Step [20600/27733], Loss: 2.9780\n",
      "Epoch [35/300], Step [20700/27733], Loss: 3.9188\n",
      "Epoch [35/300], Step [20800/27733], Loss: 3.5525\n",
      "Epoch [35/300], Step [20900/27733], Loss: 3.6649\n",
      "Epoch [35/300], Step [21000/27733], Loss: 3.2628\n",
      "Epoch [35/300], Step [21100/27733], Loss: 3.0298\n",
      "Epoch [35/300], Step [21200/27733], Loss: 2.5869\n",
      "Epoch [35/300], Step [21300/27733], Loss: 2.8137\n",
      "Epoch [35/300], Step [21400/27733], Loss: 3.0576\n",
      "Epoch [35/300], Step [21500/27733], Loss: 2.8343\n",
      "Epoch [35/300], Step [21600/27733], Loss: 3.4477\n",
      "Epoch [35/300], Step [21700/27733], Loss: 2.4901\n",
      "Epoch [35/300], Step [21800/27733], Loss: 2.9101\n",
      "Epoch [35/300], Step [21900/27733], Loss: 2.9203\n",
      "Epoch [35/300], Step [22000/27733], Loss: 3.1306\n",
      "Epoch [35/300], Step [22100/27733], Loss: 2.1411\n",
      "Epoch [35/300], Step [22200/27733], Loss: 3.9305\n",
      "Epoch [35/300], Step [22300/27733], Loss: 2.8741\n",
      "Epoch [35/300], Step [22400/27733], Loss: 3.3836\n",
      "Epoch [35/300], Step [22500/27733], Loss: 2.9937\n",
      "Epoch [35/300], Step [22600/27733], Loss: 3.6484\n",
      "Epoch [35/300], Step [22700/27733], Loss: 3.4334\n",
      "Epoch [35/300], Step [22800/27733], Loss: 3.6305\n",
      "Epoch [35/300], Step [22900/27733], Loss: 3.0069\n",
      "Epoch [35/300], Step [23000/27733], Loss: 2.8569\n",
      "Epoch [35/300], Step [23100/27733], Loss: 2.7586\n",
      "Epoch [35/300], Step [23200/27733], Loss: 3.7890\n",
      "Epoch [35/300], Step [23300/27733], Loss: 4.0379\n",
      "Epoch [35/300], Step [23400/27733], Loss: 3.7931\n",
      "Epoch [35/300], Step [23500/27733], Loss: 2.4886\n",
      "Epoch [35/300], Step [23600/27733], Loss: 2.9847\n",
      "Epoch [35/300], Step [23700/27733], Loss: 3.3180\n",
      "Epoch [35/300], Step [23800/27733], Loss: 2.1340\n",
      "Epoch [35/300], Step [23900/27733], Loss: 3.4704\n",
      "Epoch [35/300], Step [24000/27733], Loss: 2.9810\n",
      "Epoch [35/300], Step [24100/27733], Loss: 2.5811\n",
      "Epoch [35/300], Step [24200/27733], Loss: 2.9536\n",
      "Epoch [35/300], Step [24300/27733], Loss: 3.3830\n",
      "Epoch [35/300], Step [24400/27733], Loss: 3.0323\n",
      "Epoch [35/300], Step [24500/27733], Loss: 3.3876\n",
      "Epoch [35/300], Step [24600/27733], Loss: 3.2745\n",
      "Epoch [35/300], Step [24700/27733], Loss: 2.3773\n",
      "Epoch [35/300], Step [24800/27733], Loss: 3.6882\n",
      "Epoch [35/300], Step [24900/27733], Loss: 2.7119\n",
      "Epoch [35/300], Step [25000/27733], Loss: 2.8303\n",
      "Epoch [35/300], Step [25100/27733], Loss: 3.8485\n",
      "Epoch [35/300], Step [25200/27733], Loss: 2.7233\n",
      "Epoch [35/300], Step [25300/27733], Loss: 2.5582\n",
      "Epoch [35/300], Step [25400/27733], Loss: 3.1359\n",
      "Epoch [35/300], Step [25500/27733], Loss: 2.4182\n",
      "Epoch [35/300], Step [25600/27733], Loss: 3.2797\n",
      "Epoch [35/300], Step [25700/27733], Loss: 3.0444\n",
      "Epoch [35/300], Step [25800/27733], Loss: 2.5934\n",
      "Epoch [35/300], Step [25900/27733], Loss: 2.2792\n",
      "Epoch [35/300], Step [26000/27733], Loss: 2.8115\n",
      "Epoch [35/300], Step [26100/27733], Loss: 3.1007\n",
      "Epoch [35/300], Step [26200/27733], Loss: 3.0980\n",
      "Epoch [35/300], Step [26300/27733], Loss: 2.9582\n",
      "Epoch [35/300], Step [26400/27733], Loss: 3.0186\n",
      "Epoch [35/300], Step [26500/27733], Loss: 2.4635\n",
      "Epoch [35/300], Step [26600/27733], Loss: 3.0873\n",
      "Epoch [35/300], Step [26700/27733], Loss: 2.7832\n",
      "Epoch [35/300], Step [26800/27733], Loss: 2.5393\n",
      "Epoch [35/300], Step [26900/27733], Loss: 3.6655\n",
      "Epoch [35/300], Step [27000/27733], Loss: 2.8735\n",
      "Epoch [35/300], Step [27100/27733], Loss: 3.5536\n",
      "Epoch [35/300], Step [27200/27733], Loss: 3.6607\n",
      "Epoch [35/300], Step [27300/27733], Loss: 2.4827\n",
      "Epoch [35/300], Step [27400/27733], Loss: 3.7942\n",
      "Epoch [35/300], Step [27500/27733], Loss: 3.3731\n",
      "Epoch [35/300], Step [27600/27733], Loss: 3.9264\n",
      "Epoch [35/300], Step [27700/27733], Loss: 3.3155\n",
      "Epoch [36/300], Step [100/27733], Loss: 1.5020\n",
      "Epoch [36/300], Step [200/27733], Loss: 2.7627\n",
      "Epoch [36/300], Step [300/27733], Loss: 1.8341\n",
      "Epoch [36/300], Step [400/27733], Loss: 2.5421\n",
      "Epoch [36/300], Step [500/27733], Loss: 2.4648\n",
      "Epoch [36/300], Step [600/27733], Loss: 2.8134\n",
      "Epoch [36/300], Step [700/27733], Loss: 2.4131\n",
      "Epoch [36/300], Step [800/27733], Loss: 2.4457\n",
      "Epoch [36/300], Step [900/27733], Loss: 2.3241\n",
      "Epoch [36/300], Step [1000/27733], Loss: 1.6084\n",
      "Epoch [36/300], Step [1100/27733], Loss: 2.6211\n",
      "Epoch [36/300], Step [1200/27733], Loss: 2.3741\n",
      "Epoch [36/300], Step [1300/27733], Loss: 2.6770\n",
      "Epoch [36/300], Step [1400/27733], Loss: 2.4660\n",
      "Epoch [36/300], Step [1500/27733], Loss: 3.1239\n",
      "Epoch [36/300], Step [1600/27733], Loss: 2.7609\n",
      "Epoch [36/300], Step [1700/27733], Loss: 3.0247\n",
      "Epoch [36/300], Step [1800/27733], Loss: 1.9218\n",
      "Epoch [36/300], Step [1900/27733], Loss: 2.9374\n",
      "Epoch [36/300], Step [2000/27733], Loss: 1.9312\n",
      "Epoch [36/300], Step [2100/27733], Loss: 2.2611\n",
      "Epoch [36/300], Step [2200/27733], Loss: 2.1621\n",
      "Epoch [36/300], Step [2300/27733], Loss: 2.7852\n",
      "Epoch [36/300], Step [2400/27733], Loss: 2.1722\n",
      "Epoch [36/300], Step [2500/27733], Loss: 2.4812\n",
      "Epoch [36/300], Step [2600/27733], Loss: 2.9229\n",
      "Epoch [36/300], Step [2700/27733], Loss: 1.8501\n",
      "Epoch [36/300], Step [2800/27733], Loss: 2.5512\n",
      "Epoch [36/300], Step [2900/27733], Loss: 2.4569\n",
      "Epoch [36/300], Step [3000/27733], Loss: 2.6529\n",
      "Epoch [36/300], Step [3100/27733], Loss: 2.2608\n",
      "Epoch [36/300], Step [3200/27733], Loss: 2.5940\n",
      "Epoch [36/300], Step [3300/27733], Loss: 2.6118\n",
      "Epoch [36/300], Step [3400/27733], Loss: 1.9730\n",
      "Epoch [36/300], Step [3500/27733], Loss: 2.9389\n",
      "Epoch [36/300], Step [3600/27733], Loss: 2.5946\n",
      "Epoch [36/300], Step [3700/27733], Loss: 2.5434\n",
      "Epoch [36/300], Step [3800/27733], Loss: 2.0877\n",
      "Epoch [36/300], Step [3900/27733], Loss: 2.5017\n",
      "Epoch [36/300], Step [4000/27733], Loss: 1.9302\n",
      "Epoch [36/300], Step [4100/27733], Loss: 2.9157\n",
      "Epoch [36/300], Step [4200/27733], Loss: 2.4052\n",
      "Epoch [36/300], Step [4300/27733], Loss: 2.4746\n",
      "Epoch [36/300], Step [4400/27733], Loss: 2.8621\n",
      "Epoch [36/300], Step [4500/27733], Loss: 2.7603\n",
      "Epoch [36/300], Step [4600/27733], Loss: 2.1211\n",
      "Epoch [36/300], Step [4700/27733], Loss: 2.4644\n",
      "Epoch [36/300], Step [4800/27733], Loss: 2.4082\n",
      "Epoch [36/300], Step [4900/27733], Loss: 2.5064\n",
      "Epoch [36/300], Step [5000/27733], Loss: 2.0936\n",
      "Epoch [36/300], Step [5100/27733], Loss: 2.1995\n",
      "Epoch [36/300], Step [5200/27733], Loss: 3.4313\n",
      "Epoch [36/300], Step [5300/27733], Loss: 2.8711\n",
      "Epoch [36/300], Step [5400/27733], Loss: 2.6849\n",
      "Epoch [36/300], Step [5500/27733], Loss: 2.3368\n",
      "Epoch [36/300], Step [5600/27733], Loss: 2.3017\n",
      "Epoch [36/300], Step [5700/27733], Loss: 2.7684\n",
      "Epoch [36/300], Step [5800/27733], Loss: 2.6616\n",
      "Epoch [36/300], Step [5900/27733], Loss: 3.0977\n",
      "Epoch [36/300], Step [6000/27733], Loss: 2.2009\n",
      "Epoch [36/300], Step [6100/27733], Loss: 3.6156\n",
      "Epoch [36/300], Step [6200/27733], Loss: 3.3850\n",
      "Epoch [36/300], Step [6300/27733], Loss: 3.2069\n",
      "Epoch [36/300], Step [6400/27733], Loss: 1.6717\n",
      "Epoch [36/300], Step [6500/27733], Loss: 2.3285\n",
      "Epoch [36/300], Step [6600/27733], Loss: 2.3239\n",
      "Epoch [36/300], Step [6700/27733], Loss: 2.9646\n",
      "Epoch [36/300], Step [6800/27733], Loss: 3.1475\n",
      "Epoch [36/300], Step [6900/27733], Loss: 2.7094\n",
      "Epoch [36/300], Step [7000/27733], Loss: 2.7000\n",
      "Epoch [36/300], Step [7100/27733], Loss: 2.8553\n",
      "Epoch [36/300], Step [7200/27733], Loss: 2.2037\n",
      "Epoch [36/300], Step [7300/27733], Loss: 2.3867\n",
      "Epoch [36/300], Step [7400/27733], Loss: 2.3381\n",
      "Epoch [36/300], Step [7500/27733], Loss: 3.2282\n",
      "Epoch [36/300], Step [7600/27733], Loss: 2.3681\n",
      "Epoch [36/300], Step [7700/27733], Loss: 2.4211\n",
      "Epoch [36/300], Step [7800/27733], Loss: 2.7969\n",
      "Epoch [36/300], Step [7900/27733], Loss: 2.2096\n",
      "Epoch [36/300], Step [8000/27733], Loss: 2.8670\n",
      "Epoch [36/300], Step [8100/27733], Loss: 3.0144\n",
      "Epoch [36/300], Step [8200/27733], Loss: 3.2911\n",
      "Epoch [36/300], Step [8300/27733], Loss: 2.8380\n",
      "Epoch [36/300], Step [8400/27733], Loss: 2.0471\n",
      "Epoch [36/300], Step [8500/27733], Loss: 2.2571\n",
      "Epoch [36/300], Step [8600/27733], Loss: 3.2663\n",
      "Epoch [36/300], Step [8700/27733], Loss: 2.6920\n",
      "Epoch [36/300], Step [8800/27733], Loss: 2.8220\n",
      "Epoch [36/300], Step [8900/27733], Loss: 2.8411\n",
      "Epoch [36/300], Step [9000/27733], Loss: 2.6206\n",
      "Epoch [36/300], Step [9100/27733], Loss: 2.8486\n",
      "Epoch [36/300], Step [9200/27733], Loss: 1.8115\n",
      "Epoch [36/300], Step [9300/27733], Loss: 2.1007\n",
      "Epoch [36/300], Step [9400/27733], Loss: 1.7285\n",
      "Epoch [36/300], Step [9500/27733], Loss: 2.3191\n",
      "Epoch [36/300], Step [9600/27733], Loss: 3.0517\n",
      "Epoch [36/300], Step [9700/27733], Loss: 2.5466\n",
      "Epoch [36/300], Step [9800/27733], Loss: 2.7147\n",
      "Epoch [36/300], Step [9900/27733], Loss: 2.0477\n",
      "Epoch [36/300], Step [10000/27733], Loss: 3.3429\n",
      "Epoch [36/300], Step [10100/27733], Loss: 2.7557\n",
      "Epoch [36/300], Step [10200/27733], Loss: 2.3378\n",
      "Epoch [36/300], Step [10300/27733], Loss: 2.6072\n",
      "Epoch [36/300], Step [10400/27733], Loss: 2.4743\n",
      "Epoch [36/300], Step [10500/27733], Loss: 3.2827\n",
      "Epoch [36/300], Step [10600/27733], Loss: 2.7051\n",
      "Epoch [36/300], Step [10700/27733], Loss: 2.7412\n",
      "Epoch [36/300], Step [10800/27733], Loss: 2.3438\n",
      "Epoch [36/300], Step [10900/27733], Loss: 3.1344\n",
      "Epoch [36/300], Step [11000/27733], Loss: 2.4880\n",
      "Epoch [36/300], Step [11100/27733], Loss: 2.6810\n",
      "Epoch [36/300], Step [11200/27733], Loss: 3.1671\n",
      "Epoch [36/300], Step [11300/27733], Loss: 2.3174\n",
      "Epoch [36/300], Step [11400/27733], Loss: 2.5388\n",
      "Epoch [36/300], Step [11500/27733], Loss: 2.8042\n",
      "Epoch [36/300], Step [11600/27733], Loss: 2.6466\n",
      "Epoch [36/300], Step [11700/27733], Loss: 2.3132\n",
      "Epoch [36/300], Step [11800/27733], Loss: 3.5273\n",
      "Epoch [36/300], Step [11900/27733], Loss: 2.9327\n",
      "Epoch [36/300], Step [12000/27733], Loss: 2.9034\n",
      "Epoch [36/300], Step [12100/27733], Loss: 2.5908\n",
      "Epoch [36/300], Step [12200/27733], Loss: 2.1796\n",
      "Epoch [36/300], Step [12300/27733], Loss: 2.4196\n",
      "Epoch [36/300], Step [12400/27733], Loss: 2.3305\n",
      "Epoch [36/300], Step [12500/27733], Loss: 2.4519\n",
      "Epoch [36/300], Step [12600/27733], Loss: 2.6380\n",
      "Epoch [36/300], Step [12700/27733], Loss: 2.6470\n",
      "Epoch [36/300], Step [12800/27733], Loss: 3.1380\n",
      "Epoch [36/300], Step [12900/27733], Loss: 2.7855\n",
      "Epoch [36/300], Step [13000/27733], Loss: 2.3333\n",
      "Epoch [36/300], Step [13100/27733], Loss: 2.0066\n",
      "Epoch [36/300], Step [13200/27733], Loss: 3.0407\n",
      "Epoch [36/300], Step [13300/27733], Loss: 2.8874\n",
      "Epoch [36/300], Step [13400/27733], Loss: 2.7416\n",
      "Epoch [36/300], Step [13500/27733], Loss: 3.2440\n",
      "Epoch [36/300], Step [13600/27733], Loss: 3.0656\n",
      "Epoch [36/300], Step [13700/27733], Loss: 2.5645\n",
      "Epoch [36/300], Step [13800/27733], Loss: 3.2548\n",
      "Epoch [36/300], Step [13900/27733], Loss: 2.5088\n",
      "Epoch [36/300], Step [14000/27733], Loss: 3.9511\n",
      "Epoch [36/300], Step [14100/27733], Loss: 3.1436\n",
      "Epoch [36/300], Step [14200/27733], Loss: 3.1510\n",
      "Epoch [36/300], Step [14300/27733], Loss: 3.8796\n",
      "Epoch [36/300], Step [14400/27733], Loss: 1.8866\n",
      "Epoch [36/300], Step [14500/27733], Loss: 2.9926\n",
      "Epoch [36/300], Step [14600/27733], Loss: 2.9925\n",
      "Epoch [36/300], Step [14700/27733], Loss: 3.3526\n",
      "Epoch [36/300], Step [14800/27733], Loss: 2.7029\n",
      "Epoch [36/300], Step [14900/27733], Loss: 2.4576\n",
      "Epoch [36/300], Step [15000/27733], Loss: 2.8775\n",
      "Epoch [36/300], Step [15100/27733], Loss: 3.4570\n",
      "Epoch [36/300], Step [15200/27733], Loss: 2.8855\n",
      "Epoch [36/300], Step [15300/27733], Loss: 2.7480\n",
      "Epoch [36/300], Step [15400/27733], Loss: 2.7797\n",
      "Epoch [36/300], Step [15500/27733], Loss: 2.8259\n",
      "Epoch [36/300], Step [15600/27733], Loss: 2.6791\n",
      "Epoch [36/300], Step [15700/27733], Loss: 3.3587\n",
      "Epoch [36/300], Step [15800/27733], Loss: 3.4535\n",
      "Epoch [36/300], Step [15900/27733], Loss: 3.0883\n",
      "Epoch [36/300], Step [16000/27733], Loss: 2.7075\n",
      "Epoch [36/300], Step [16100/27733], Loss: 3.5554\n",
      "Epoch [36/300], Step [16200/27733], Loss: 3.6996\n",
      "Epoch [36/300], Step [16300/27733], Loss: 2.3357\n",
      "Epoch [36/300], Step [16400/27733], Loss: 3.1215\n",
      "Epoch [36/300], Step [16500/27733], Loss: 3.8017\n",
      "Epoch [36/300], Step [16600/27733], Loss: 3.0466\n",
      "Epoch [36/300], Step [16700/27733], Loss: 2.4674\n",
      "Epoch [36/300], Step [16800/27733], Loss: 3.0635\n",
      "Epoch [36/300], Step [16900/27733], Loss: 3.5255\n",
      "Epoch [36/300], Step [17000/27733], Loss: 3.2378\n",
      "Epoch [36/300], Step [17100/27733], Loss: 2.9584\n",
      "Epoch [36/300], Step [17200/27733], Loss: 3.8510\n",
      "Epoch [36/300], Step [17300/27733], Loss: 3.3400\n",
      "Epoch [36/300], Step [17400/27733], Loss: 3.3128\n",
      "Epoch [36/300], Step [17500/27733], Loss: 2.9622\n",
      "Epoch [36/300], Step [17600/27733], Loss: 2.7745\n",
      "Epoch [36/300], Step [17700/27733], Loss: 2.0491\n",
      "Epoch [36/300], Step [17800/27733], Loss: 2.7054\n",
      "Epoch [36/300], Step [17900/27733], Loss: 2.5523\n",
      "Epoch [36/300], Step [18000/27733], Loss: 3.2988\n",
      "Epoch [36/300], Step [18100/27733], Loss: 2.7211\n",
      "Epoch [36/300], Step [18200/27733], Loss: 3.0823\n",
      "Epoch [36/300], Step [18300/27733], Loss: 3.0191\n",
      "Epoch [36/300], Step [18400/27733], Loss: 3.1974\n",
      "Epoch [36/300], Step [18500/27733], Loss: 2.5337\n",
      "Epoch [36/300], Step [18600/27733], Loss: 2.7088\n",
      "Epoch [36/300], Step [18700/27733], Loss: 3.3868\n",
      "Epoch [36/300], Step [18800/27733], Loss: 2.3058\n",
      "Epoch [36/300], Step [18900/27733], Loss: 2.8870\n",
      "Epoch [36/300], Step [19000/27733], Loss: 4.0914\n",
      "Epoch [36/300], Step [19100/27733], Loss: 2.3960\n",
      "Epoch [36/300], Step [19200/27733], Loss: 3.3583\n",
      "Epoch [36/300], Step [19300/27733], Loss: 3.0549\n",
      "Epoch [36/300], Step [19400/27733], Loss: 2.7220\n",
      "Epoch [36/300], Step [19500/27733], Loss: 3.1969\n",
      "Epoch [36/300], Step [19600/27733], Loss: 2.5725\n",
      "Epoch [36/300], Step [19700/27733], Loss: 2.8942\n",
      "Epoch [36/300], Step [19800/27733], Loss: 2.8679\n",
      "Epoch [36/300], Step [19900/27733], Loss: 3.4473\n",
      "Epoch [36/300], Step [20000/27733], Loss: 2.3541\n",
      "Epoch [36/300], Step [20100/27733], Loss: 3.7605\n",
      "Epoch [36/300], Step [20200/27733], Loss: 3.0849\n",
      "Epoch [36/300], Step [20300/27733], Loss: 2.2293\n",
      "Epoch [36/300], Step [20400/27733], Loss: 2.7314\n",
      "Epoch [36/300], Step [20500/27733], Loss: 3.0967\n",
      "Epoch [36/300], Step [20600/27733], Loss: 3.5901\n",
      "Epoch [36/300], Step [20700/27733], Loss: 2.7820\n",
      "Epoch [36/300], Step [20800/27733], Loss: 3.3945\n",
      "Epoch [36/300], Step [20900/27733], Loss: 1.9400\n",
      "Epoch [36/300], Step [21000/27733], Loss: 3.0465\n",
      "Epoch [36/300], Step [21100/27733], Loss: 2.4347\n",
      "Epoch [36/300], Step [21200/27733], Loss: 3.7319\n",
      "Epoch [36/300], Step [21300/27733], Loss: 2.8684\n",
      "Epoch [36/300], Step [21400/27733], Loss: 2.4444\n",
      "Epoch [36/300], Step [21500/27733], Loss: 3.0353\n",
      "Epoch [36/300], Step [21600/27733], Loss: 2.8955\n",
      "Epoch [36/300], Step [21700/27733], Loss: 2.8028\n",
      "Epoch [36/300], Step [21800/27733], Loss: 2.9995\n",
      "Epoch [36/300], Step [21900/27733], Loss: 2.7832\n",
      "Epoch [36/300], Step [22000/27733], Loss: 3.5180\n",
      "Epoch [36/300], Step [22100/27733], Loss: 3.0769\n",
      "Epoch [36/300], Step [22200/27733], Loss: 2.9574\n",
      "Epoch [36/300], Step [22300/27733], Loss: 3.0536\n",
      "Epoch [36/300], Step [22400/27733], Loss: 2.5106\n",
      "Epoch [36/300], Step [22500/27733], Loss: 3.2836\n",
      "Epoch [36/300], Step [22600/27733], Loss: 2.6832\n",
      "Epoch [36/300], Step [22700/27733], Loss: 2.6184\n",
      "Epoch [36/300], Step [22800/27733], Loss: 3.0189\n",
      "Epoch [36/300], Step [22900/27733], Loss: 2.4356\n",
      "Epoch [36/300], Step [23000/27733], Loss: 3.0496\n",
      "Epoch [36/300], Step [23100/27733], Loss: 2.4882\n",
      "Epoch [36/300], Step [23200/27733], Loss: 2.5744\n",
      "Epoch [36/300], Step [23300/27733], Loss: 3.4029\n",
      "Epoch [36/300], Step [23400/27733], Loss: 2.5428\n",
      "Epoch [36/300], Step [23500/27733], Loss: 2.6757\n",
      "Epoch [36/300], Step [23600/27733], Loss: 2.4203\n",
      "Epoch [36/300], Step [23700/27733], Loss: 3.1234\n",
      "Epoch [36/300], Step [23800/27733], Loss: 3.2987\n",
      "Epoch [36/300], Step [23900/27733], Loss: 2.8895\n",
      "Epoch [36/300], Step [24000/27733], Loss: 2.5190\n",
      "Epoch [36/300], Step [24100/27733], Loss: 3.2633\n",
      "Epoch [36/300], Step [24200/27733], Loss: 3.3115\n",
      "Epoch [36/300], Step [24300/27733], Loss: 3.0788\n",
      "Epoch [36/300], Step [24400/27733], Loss: 2.5386\n",
      "Epoch [36/300], Step [24500/27733], Loss: 3.2382\n",
      "Epoch [36/300], Step [24600/27733], Loss: 3.8794\n",
      "Epoch [36/300], Step [24700/27733], Loss: 2.7910\n",
      "Epoch [36/300], Step [24800/27733], Loss: 2.2933\n",
      "Epoch [36/300], Step [24900/27733], Loss: 2.6951\n",
      "Epoch [36/300], Step [25000/27733], Loss: 3.1414\n",
      "Epoch [36/300], Step [25100/27733], Loss: 2.8680\n",
      "Epoch [36/300], Step [25200/27733], Loss: 3.0765\n",
      "Epoch [36/300], Step [25300/27733], Loss: 2.5314\n",
      "Epoch [36/300], Step [25400/27733], Loss: 3.0760\n",
      "Epoch [36/300], Step [25500/27733], Loss: 2.5192\n",
      "Epoch [36/300], Step [25600/27733], Loss: 2.3857\n",
      "Epoch [36/300], Step [25700/27733], Loss: 3.1404\n",
      "Epoch [36/300], Step [25800/27733], Loss: 3.0727\n",
      "Epoch [36/300], Step [25900/27733], Loss: 2.6420\n",
      "Epoch [36/300], Step [26000/27733], Loss: 3.5023\n",
      "Epoch [36/300], Step [26100/27733], Loss: 3.0377\n",
      "Epoch [36/300], Step [26200/27733], Loss: 2.6750\n",
      "Epoch [36/300], Step [26300/27733], Loss: 2.7581\n",
      "Epoch [36/300], Step [26400/27733], Loss: 2.9098\n",
      "Epoch [36/300], Step [26500/27733], Loss: 4.2755\n",
      "Epoch [36/300], Step [26600/27733], Loss: 3.3571\n",
      "Epoch [36/300], Step [26700/27733], Loss: 2.7837\n",
      "Epoch [36/300], Step [26800/27733], Loss: 2.3496\n",
      "Epoch [36/300], Step [26900/27733], Loss: 3.5015\n",
      "Epoch [36/300], Step [27000/27733], Loss: 3.2338\n",
      "Epoch [36/300], Step [27100/27733], Loss: 3.8689\n",
      "Epoch [36/300], Step [27200/27733], Loss: 2.7803\n",
      "Epoch [36/300], Step [27300/27733], Loss: 3.4540\n",
      "Epoch [36/300], Step [27400/27733], Loss: 2.7794\n",
      "Epoch [36/300], Step [27500/27733], Loss: 2.9222\n",
      "Epoch [36/300], Step [27600/27733], Loss: 3.1072\n",
      "Epoch [36/300], Step [27700/27733], Loss: 3.8167\n",
      "Epoch [37/300], Step [100/27733], Loss: 2.7917\n",
      "Epoch [37/300], Step [200/27733], Loss: 2.8666\n",
      "Epoch [37/300], Step [300/27733], Loss: 2.6801\n",
      "Epoch [37/300], Step [400/27733], Loss: 2.4608\n",
      "Epoch [37/300], Step [500/27733], Loss: 1.8460\n",
      "Epoch [37/300], Step [600/27733], Loss: 2.9219\n",
      "Epoch [37/300], Step [700/27733], Loss: 2.6462\n",
      "Epoch [37/300], Step [800/27733], Loss: 2.6291\n",
      "Epoch [37/300], Step [900/27733], Loss: 2.5781\n",
      "Epoch [37/300], Step [1000/27733], Loss: 2.6501\n",
      "Epoch [37/300], Step [1100/27733], Loss: 2.6809\n",
      "Epoch [37/300], Step [1200/27733], Loss: 2.6194\n",
      "Epoch [37/300], Step [1300/27733], Loss: 2.5726\n",
      "Epoch [37/300], Step [1400/27733], Loss: 2.8195\n",
      "Epoch [37/300], Step [1500/27733], Loss: 2.6711\n",
      "Epoch [37/300], Step [1600/27733], Loss: 3.0463\n",
      "Epoch [37/300], Step [1700/27733], Loss: 2.5415\n",
      "Epoch [37/300], Step [1800/27733], Loss: 2.8341\n",
      "Epoch [37/300], Step [1900/27733], Loss: 3.1813\n",
      "Epoch [37/300], Step [2000/27733], Loss: 2.1604\n",
      "Epoch [37/300], Step [2100/27733], Loss: 2.8699\n",
      "Epoch [37/300], Step [2200/27733], Loss: 2.4822\n",
      "Epoch [37/300], Step [2300/27733], Loss: 2.1586\n",
      "Epoch [37/300], Step [2400/27733], Loss: 2.1989\n",
      "Epoch [37/300], Step [2500/27733], Loss: 2.5772\n",
      "Epoch [37/300], Step [2600/27733], Loss: 2.8899\n",
      "Epoch [37/300], Step [2700/27733], Loss: 2.8757\n",
      "Epoch [37/300], Step [2800/27733], Loss: 2.3313\n",
      "Epoch [37/300], Step [2900/27733], Loss: 2.1810\n",
      "Epoch [37/300], Step [3000/27733], Loss: 3.1776\n",
      "Epoch [37/300], Step [3100/27733], Loss: 2.6438\n",
      "Epoch [37/300], Step [3200/27733], Loss: 2.3056\n",
      "Epoch [37/300], Step [3300/27733], Loss: 2.3673\n",
      "Epoch [37/300], Step [3400/27733], Loss: 2.3134\n",
      "Epoch [37/300], Step [3500/27733], Loss: 2.0034\n",
      "Epoch [37/300], Step [3600/27733], Loss: 3.5104\n",
      "Epoch [37/300], Step [3700/27733], Loss: 2.0324\n",
      "Epoch [37/300], Step [3800/27733], Loss: 2.5620\n",
      "Epoch [37/300], Step [3900/27733], Loss: 2.9045\n",
      "Epoch [37/300], Step [4000/27733], Loss: 2.7790\n",
      "Epoch [37/300], Step [4100/27733], Loss: 2.1524\n",
      "Epoch [37/300], Step [4200/27733], Loss: 2.8909\n",
      "Epoch [37/300], Step [4300/27733], Loss: 2.5383\n",
      "Epoch [37/300], Step [4400/27733], Loss: 2.9063\n",
      "Epoch [37/300], Step [4500/27733], Loss: 2.5847\n",
      "Epoch [37/300], Step [4600/27733], Loss: 2.7552\n",
      "Epoch [37/300], Step [4700/27733], Loss: 2.4074\n",
      "Epoch [37/300], Step [4800/27733], Loss: 1.8551\n",
      "Epoch [37/300], Step [4900/27733], Loss: 2.2426\n",
      "Epoch [37/300], Step [5000/27733], Loss: 2.7074\n",
      "Epoch [37/300], Step [5100/27733], Loss: 3.3223\n",
      "Epoch [37/300], Step [5200/27733], Loss: 2.6610\n",
      "Epoch [37/300], Step [5300/27733], Loss: 2.4040\n",
      "Epoch [37/300], Step [5400/27733], Loss: 2.6486\n",
      "Epoch [37/300], Step [5500/27733], Loss: 2.8217\n",
      "Epoch [37/300], Step [5600/27733], Loss: 2.1531\n",
      "Epoch [37/300], Step [5700/27733], Loss: 2.1220\n",
      "Epoch [37/300], Step [5800/27733], Loss: 2.1148\n",
      "Epoch [37/300], Step [5900/27733], Loss: 2.7290\n",
      "Epoch [37/300], Step [6000/27733], Loss: 2.9892\n",
      "Epoch [37/300], Step [6100/27733], Loss: 2.3736\n",
      "Epoch [37/300], Step [6200/27733], Loss: 2.8836\n",
      "Epoch [37/300], Step [6300/27733], Loss: 3.2711\n",
      "Epoch [37/300], Step [6400/27733], Loss: 1.8598\n",
      "Epoch [37/300], Step [6500/27733], Loss: 3.1417\n",
      "Epoch [37/300], Step [6600/27733], Loss: 2.5845\n",
      "Epoch [37/300], Step [6700/27733], Loss: 2.5507\n",
      "Epoch [37/300], Step [6800/27733], Loss: 3.0309\n",
      "Epoch [37/300], Step [6900/27733], Loss: 2.7533\n",
      "Epoch [37/300], Step [7000/27733], Loss: 1.9773\n",
      "Epoch [37/300], Step [7100/27733], Loss: 1.9189\n",
      "Epoch [37/300], Step [7200/27733], Loss: 2.8931\n",
      "Epoch [37/300], Step [7300/27733], Loss: 2.2359\n",
      "Epoch [37/300], Step [7400/27733], Loss: 3.7536\n",
      "Epoch [37/300], Step [7500/27733], Loss: 2.4281\n",
      "Epoch [37/300], Step [7600/27733], Loss: 3.5365\n",
      "Epoch [37/300], Step [7700/27733], Loss: 2.3573\n",
      "Epoch [37/300], Step [7800/27733], Loss: 2.8760\n",
      "Epoch [37/300], Step [7900/27733], Loss: 2.4332\n",
      "Epoch [37/300], Step [8000/27733], Loss: 1.9461\n",
      "Epoch [37/300], Step [8100/27733], Loss: 3.2510\n",
      "Epoch [37/300], Step [8200/27733], Loss: 2.4981\n",
      "Epoch [37/300], Step [8300/27733], Loss: 3.1278\n",
      "Epoch [37/300], Step [8400/27733], Loss: 2.5414\n",
      "Epoch [37/300], Step [8500/27733], Loss: 2.1547\n",
      "Epoch [37/300], Step [8600/27733], Loss: 2.7801\n",
      "Epoch [37/300], Step [8700/27733], Loss: 3.5738\n",
      "Epoch [37/300], Step [8800/27733], Loss: 2.6018\n",
      "Epoch [37/300], Step [8900/27733], Loss: 1.8586\n",
      "Epoch [37/300], Step [9000/27733], Loss: 3.0210\n",
      "Epoch [37/300], Step [9100/27733], Loss: 2.8191\n",
      "Epoch [37/300], Step [9200/27733], Loss: 2.5324\n",
      "Epoch [37/300], Step [9300/27733], Loss: 2.5929\n",
      "Epoch [37/300], Step [9400/27733], Loss: 2.6662\n",
      "Epoch [37/300], Step [9500/27733], Loss: 2.4028\n",
      "Epoch [37/300], Step [9600/27733], Loss: 2.7400\n",
      "Epoch [37/300], Step [9700/27733], Loss: 2.9738\n",
      "Epoch [37/300], Step [9800/27733], Loss: 2.2907\n",
      "Epoch [37/300], Step [9900/27733], Loss: 2.4606\n",
      "Epoch [37/300], Step [10000/27733], Loss: 3.2299\n",
      "Epoch [37/300], Step [10100/27733], Loss: 2.7657\n",
      "Epoch [37/300], Step [10200/27733], Loss: 3.1562\n",
      "Epoch [37/300], Step [10300/27733], Loss: 2.4631\n",
      "Epoch [37/300], Step [10400/27733], Loss: 2.3283\n",
      "Epoch [37/300], Step [10500/27733], Loss: 1.8619\n",
      "Epoch [37/300], Step [10600/27733], Loss: 2.9884\n",
      "Epoch [37/300], Step [10700/27733], Loss: 2.6620\n",
      "Epoch [37/300], Step [10800/27733], Loss: 2.7168\n",
      "Epoch [37/300], Step [10900/27733], Loss: 2.2335\n",
      "Epoch [37/300], Step [11000/27733], Loss: 2.2041\n",
      "Epoch [37/300], Step [11100/27733], Loss: 3.3535\n",
      "Epoch [37/300], Step [11200/27733], Loss: 1.5110\n",
      "Epoch [37/300], Step [11300/27733], Loss: 2.3080\n",
      "Epoch [37/300], Step [11400/27733], Loss: 3.1230\n",
      "Epoch [37/300], Step [11500/27733], Loss: 3.0036\n",
      "Epoch [37/300], Step [11600/27733], Loss: 2.6940\n",
      "Epoch [37/300], Step [11700/27733], Loss: 3.3202\n",
      "Epoch [37/300], Step [11800/27733], Loss: 2.4790\n",
      "Epoch [37/300], Step [11900/27733], Loss: 2.6638\n",
      "Epoch [37/300], Step [12000/27733], Loss: 3.0022\n",
      "Epoch [37/300], Step [12100/27733], Loss: 2.1010\n",
      "Epoch [37/300], Step [12200/27733], Loss: 2.6538\n",
      "Epoch [37/300], Step [12300/27733], Loss: 2.3473\n",
      "Epoch [37/300], Step [12400/27733], Loss: 2.1755\n",
      "Epoch [37/300], Step [12500/27733], Loss: 2.8914\n",
      "Epoch [37/300], Step [12600/27733], Loss: 2.4061\n",
      "Epoch [37/300], Step [12700/27733], Loss: 3.1841\n",
      "Epoch [37/300], Step [12800/27733], Loss: 2.8306\n",
      "Epoch [37/300], Step [12900/27733], Loss: 2.7884\n",
      "Epoch [37/300], Step [13000/27733], Loss: 4.0563\n",
      "Epoch [37/300], Step [13100/27733], Loss: 2.8670\n",
      "Epoch [37/300], Step [13200/27733], Loss: 2.2946\n",
      "Epoch [37/300], Step [13300/27733], Loss: 2.2531\n",
      "Epoch [37/300], Step [13400/27733], Loss: 3.2212\n",
      "Epoch [37/300], Step [13500/27733], Loss: 2.8431\n",
      "Epoch [37/300], Step [13600/27733], Loss: 3.0990\n",
      "Epoch [37/300], Step [13700/27733], Loss: 3.1207\n",
      "Epoch [37/300], Step [13800/27733], Loss: 2.8148\n",
      "Epoch [37/300], Step [13900/27733], Loss: 2.7330\n",
      "Epoch [37/300], Step [14000/27733], Loss: 2.6351\n",
      "Epoch [37/300], Step [14100/27733], Loss: 3.5501\n",
      "Epoch [37/300], Step [14200/27733], Loss: 3.0066\n",
      "Epoch [37/300], Step [14300/27733], Loss: 1.9560\n",
      "Epoch [37/300], Step [14400/27733], Loss: 2.3949\n",
      "Epoch [37/300], Step [14500/27733], Loss: 3.1303\n",
      "Epoch [37/300], Step [14600/27733], Loss: 3.1965\n",
      "Epoch [37/300], Step [14700/27733], Loss: 3.1692\n",
      "Epoch [37/300], Step [14800/27733], Loss: 2.7587\n",
      "Epoch [37/300], Step [14900/27733], Loss: 2.4810\n",
      "Epoch [37/300], Step [15000/27733], Loss: 2.7942\n",
      "Epoch [37/300], Step [15100/27733], Loss: 3.4986\n",
      "Epoch [37/300], Step [15200/27733], Loss: 2.1349\n",
      "Epoch [37/300], Step [15300/27733], Loss: 2.6866\n",
      "Epoch [37/300], Step [15400/27733], Loss: 2.7514\n",
      "Epoch [37/300], Step [15500/27733], Loss: 2.5259\n",
      "Epoch [37/300], Step [15600/27733], Loss: 3.0066\n",
      "Epoch [37/300], Step [15700/27733], Loss: 2.7842\n",
      "Epoch [37/300], Step [15800/27733], Loss: 3.2237\n",
      "Epoch [37/300], Step [15900/27733], Loss: 2.2472\n",
      "Epoch [37/300], Step [16000/27733], Loss: 2.8992\n",
      "Epoch [37/300], Step [16100/27733], Loss: 2.5230\n",
      "Epoch [37/300], Step [16200/27733], Loss: 3.2108\n",
      "Epoch [37/300], Step [16300/27733], Loss: 2.9613\n",
      "Epoch [37/300], Step [16400/27733], Loss: 2.4417\n",
      "Epoch [37/300], Step [16500/27733], Loss: 3.1090\n",
      "Epoch [37/300], Step [16600/27733], Loss: 3.3340\n",
      "Epoch [37/300], Step [16700/27733], Loss: 2.5239\n",
      "Epoch [37/300], Step [16800/27733], Loss: 2.6663\n",
      "Epoch [37/300], Step [16900/27733], Loss: 3.0951\n",
      "Epoch [37/300], Step [17000/27733], Loss: 3.6212\n",
      "Epoch [37/300], Step [17100/27733], Loss: 2.5956\n",
      "Epoch [37/300], Step [17200/27733], Loss: 3.0631\n",
      "Epoch [37/300], Step [17300/27733], Loss: 2.8047\n",
      "Epoch [37/300], Step [17400/27733], Loss: 2.9520\n",
      "Epoch [37/300], Step [17500/27733], Loss: 2.8331\n",
      "Epoch [37/300], Step [17600/27733], Loss: 3.5646\n",
      "Epoch [37/300], Step [17700/27733], Loss: 2.4357\n",
      "Epoch [37/300], Step [17800/27733], Loss: 2.6954\n",
      "Epoch [37/300], Step [17900/27733], Loss: 2.8895\n",
      "Epoch [37/300], Step [18000/27733], Loss: 2.6567\n",
      "Epoch [37/300], Step [18100/27733], Loss: 2.5242\n",
      "Epoch [37/300], Step [18200/27733], Loss: 2.5382\n",
      "Epoch [37/300], Step [18300/27733], Loss: 2.8590\n",
      "Epoch [37/300], Step [18400/27733], Loss: 3.3377\n",
      "Epoch [37/300], Step [18500/27733], Loss: 3.5138\n",
      "Epoch [37/300], Step [18600/27733], Loss: 2.7790\n",
      "Epoch [37/300], Step [18700/27733], Loss: 2.5705\n",
      "Epoch [37/300], Step [18800/27733], Loss: 3.1039\n",
      "Epoch [37/300], Step [18900/27733], Loss: 2.8887\n",
      "Epoch [37/300], Step [19000/27733], Loss: 3.0766\n",
      "Epoch [37/300], Step [19100/27733], Loss: 3.0815\n",
      "Epoch [37/300], Step [19200/27733], Loss: 2.6570\n",
      "Epoch [37/300], Step [19300/27733], Loss: 3.4846\n",
      "Epoch [37/300], Step [19400/27733], Loss: 3.1062\n",
      "Epoch [37/300], Step [19500/27733], Loss: 2.9180\n",
      "Epoch [37/300], Step [19600/27733], Loss: 3.0256\n",
      "Epoch [37/300], Step [19700/27733], Loss: 3.1980\n",
      "Epoch [37/300], Step [19800/27733], Loss: 2.1492\n",
      "Epoch [37/300], Step [19900/27733], Loss: 2.8104\n",
      "Epoch [37/300], Step [20000/27733], Loss: 2.9750\n",
      "Epoch [37/300], Step [20100/27733], Loss: 4.0665\n",
      "Epoch [37/300], Step [20200/27733], Loss: 3.1953\n",
      "Epoch [37/300], Step [20300/27733], Loss: 2.8275\n",
      "Epoch [37/300], Step [20400/27733], Loss: 2.8317\n",
      "Epoch [37/300], Step [20500/27733], Loss: 2.9787\n",
      "Epoch [37/300], Step [20600/27733], Loss: 3.4895\n",
      "Epoch [37/300], Step [20700/27733], Loss: 2.7670\n",
      "Epoch [37/300], Step [20800/27733], Loss: 2.8634\n",
      "Epoch [37/300], Step [20900/27733], Loss: 3.0930\n",
      "Epoch [37/300], Step [21000/27733], Loss: 3.5196\n",
      "Epoch [37/300], Step [21100/27733], Loss: 3.2953\n",
      "Epoch [37/300], Step [21200/27733], Loss: 3.5801\n",
      "Epoch [37/300], Step [21300/27733], Loss: 3.8271\n",
      "Epoch [37/300], Step [21400/27733], Loss: 2.9890\n",
      "Epoch [37/300], Step [21500/27733], Loss: 3.3939\n",
      "Epoch [37/300], Step [21600/27733], Loss: 3.3117\n",
      "Epoch [37/300], Step [21700/27733], Loss: 2.1686\n",
      "Epoch [37/300], Step [21800/27733], Loss: 3.3543\n",
      "Epoch [37/300], Step [21900/27733], Loss: 3.3099\n",
      "Epoch [37/300], Step [22000/27733], Loss: 3.0379\n",
      "Epoch [37/300], Step [22100/27733], Loss: 2.9244\n",
      "Epoch [37/300], Step [22200/27733], Loss: 2.6878\n",
      "Epoch [37/300], Step [22300/27733], Loss: 2.8022\n",
      "Epoch [37/300], Step [22400/27733], Loss: 3.0507\n",
      "Epoch [37/300], Step [22500/27733], Loss: 3.3028\n",
      "Epoch [37/300], Step [22600/27733], Loss: 2.7597\n",
      "Epoch [37/300], Step [22700/27733], Loss: 3.1434\n",
      "Epoch [37/300], Step [22800/27733], Loss: 2.6046\n",
      "Epoch [37/300], Step [22900/27733], Loss: 2.6647\n",
      "Epoch [37/300], Step [23000/27733], Loss: 3.1865\n",
      "Epoch [37/300], Step [23100/27733], Loss: 2.9692\n",
      "Epoch [37/300], Step [23200/27733], Loss: 3.3131\n",
      "Epoch [37/300], Step [23300/27733], Loss: 2.6389\n",
      "Epoch [37/300], Step [23400/27733], Loss: 3.1157\n",
      "Epoch [37/300], Step [23500/27733], Loss: 2.7384\n",
      "Epoch [37/300], Step [23600/27733], Loss: 2.5102\n",
      "Epoch [37/300], Step [23700/27733], Loss: 2.3197\n",
      "Epoch [37/300], Step [23800/27733], Loss: 3.0689\n",
      "Epoch [37/300], Step [23900/27733], Loss: 2.8360\n",
      "Epoch [37/300], Step [24000/27733], Loss: 3.7902\n",
      "Epoch [37/300], Step [24100/27733], Loss: 3.2212\n",
      "Epoch [37/300], Step [24200/27733], Loss: 3.2515\n",
      "Epoch [37/300], Step [24300/27733], Loss: 1.9571\n",
      "Epoch [37/300], Step [24400/27733], Loss: 3.0933\n",
      "Epoch [37/300], Step [24500/27733], Loss: 1.6553\n",
      "Epoch [37/300], Step [24600/27733], Loss: 2.9883\n",
      "Epoch [37/300], Step [24700/27733], Loss: 3.7632\n",
      "Epoch [37/300], Step [24800/27733], Loss: 2.6054\n",
      "Epoch [37/300], Step [24900/27733], Loss: 3.6491\n",
      "Epoch [37/300], Step [25000/27733], Loss: 3.3679\n",
      "Epoch [37/300], Step [25100/27733], Loss: 3.0707\n",
      "Epoch [37/300], Step [25200/27733], Loss: 3.0612\n",
      "Epoch [37/300], Step [25300/27733], Loss: 2.1396\n",
      "Epoch [37/300], Step [25400/27733], Loss: 2.3789\n",
      "Epoch [37/300], Step [25500/27733], Loss: 3.1726\n",
      "Epoch [37/300], Step [25600/27733], Loss: 3.2412\n",
      "Epoch [37/300], Step [25700/27733], Loss: 3.1672\n",
      "Epoch [37/300], Step [25800/27733], Loss: 2.8380\n",
      "Epoch [37/300], Step [25900/27733], Loss: 3.0944\n",
      "Epoch [37/300], Step [26000/27733], Loss: 3.9954\n",
      "Epoch [37/300], Step [26100/27733], Loss: 2.8266\n",
      "Epoch [37/300], Step [26200/27733], Loss: 2.8716\n",
      "Epoch [37/300], Step [26300/27733], Loss: 3.6572\n",
      "Epoch [37/300], Step [26400/27733], Loss: 2.9577\n",
      "Epoch [37/300], Step [26500/27733], Loss: 2.8293\n",
      "Epoch [37/300], Step [26600/27733], Loss: 3.5252\n",
      "Epoch [37/300], Step [26700/27733], Loss: 2.9536\n",
      "Epoch [37/300], Step [26800/27733], Loss: 2.0945\n",
      "Epoch [37/300], Step [26900/27733], Loss: 3.8642\n",
      "Epoch [37/300], Step [27000/27733], Loss: 2.8560\n",
      "Epoch [37/300], Step [27100/27733], Loss: 3.4360\n",
      "Epoch [37/300], Step [27200/27733], Loss: 2.7680\n",
      "Epoch [37/300], Step [27300/27733], Loss: 2.7711\n",
      "Epoch [37/300], Step [27400/27733], Loss: 3.6794\n",
      "Epoch [37/300], Step [27500/27733], Loss: 3.5264\n",
      "Epoch [37/300], Step [27600/27733], Loss: 3.0808\n",
      "Epoch [37/300], Step [27700/27733], Loss: 3.1049\n",
      "Epoch [38/300], Step [100/27733], Loss: 1.8760\n",
      "Epoch [38/300], Step [200/27733], Loss: 2.8260\n",
      "Epoch [38/300], Step [300/27733], Loss: 1.7158\n",
      "Epoch [38/300], Step [400/27733], Loss: 2.5046\n",
      "Epoch [38/300], Step [500/27733], Loss: 2.3666\n",
      "Epoch [38/300], Step [600/27733], Loss: 2.5592\n",
      "Epoch [38/300], Step [700/27733], Loss: 2.0711\n",
      "Epoch [38/300], Step [800/27733], Loss: 1.8450\n",
      "Epoch [38/300], Step [900/27733], Loss: 2.0289\n",
      "Epoch [38/300], Step [1000/27733], Loss: 1.9474\n",
      "Epoch [38/300], Step [1100/27733], Loss: 1.9293\n",
      "Epoch [38/300], Step [1200/27733], Loss: 3.2356\n",
      "Epoch [38/300], Step [1300/27733], Loss: 2.7093\n",
      "Epoch [38/300], Step [1400/27733], Loss: 2.1808\n",
      "Epoch [38/300], Step [1500/27733], Loss: 2.2397\n",
      "Epoch [38/300], Step [1600/27733], Loss: 2.2854\n",
      "Epoch [38/300], Step [1700/27733], Loss: 3.0339\n",
      "Epoch [38/300], Step [1800/27733], Loss: 2.7311\n",
      "Epoch [38/300], Step [1900/27733], Loss: 2.0234\n",
      "Epoch [38/300], Step [2000/27733], Loss: 2.4102\n",
      "Epoch [38/300], Step [2100/27733], Loss: 1.5041\n",
      "Epoch [38/300], Step [2200/27733], Loss: 2.6799\n",
      "Epoch [38/300], Step [2300/27733], Loss: 2.2259\n",
      "Epoch [38/300], Step [2400/27733], Loss: 2.9825\n",
      "Epoch [38/300], Step [2500/27733], Loss: 2.6054\n",
      "Epoch [38/300], Step [2600/27733], Loss: 2.1439\n",
      "Epoch [38/300], Step [2700/27733], Loss: 1.9077\n",
      "Epoch [38/300], Step [2800/27733], Loss: 2.4880\n",
      "Epoch [38/300], Step [2900/27733], Loss: 2.8979\n",
      "Epoch [38/300], Step [3000/27733], Loss: 2.3520\n",
      "Epoch [38/300], Step [3100/27733], Loss: 1.9456\n",
      "Epoch [38/300], Step [3200/27733], Loss: 2.8332\n",
      "Epoch [38/300], Step [3300/27733], Loss: 2.7918\n",
      "Epoch [38/300], Step [3400/27733], Loss: 3.1563\n",
      "Epoch [38/300], Step [3500/27733], Loss: 2.7157\n",
      "Epoch [38/300], Step [3600/27733], Loss: 2.8786\n",
      "Epoch [38/300], Step [3700/27733], Loss: 2.7713\n",
      "Epoch [38/300], Step [3800/27733], Loss: 2.9637\n",
      "Epoch [38/300], Step [3900/27733], Loss: 1.9756\n",
      "Epoch [38/300], Step [4000/27733], Loss: 3.0310\n",
      "Epoch [38/300], Step [4100/27733], Loss: 3.1144\n",
      "Epoch [38/300], Step [4200/27733], Loss: 1.8758\n",
      "Epoch [38/300], Step [4300/27733], Loss: 2.1874\n",
      "Epoch [38/300], Step [4400/27733], Loss: 2.8032\n",
      "Epoch [38/300], Step [4500/27733], Loss: 2.2128\n",
      "Epoch [38/300], Step [4600/27733], Loss: 2.3346\n",
      "Epoch [38/300], Step [4700/27733], Loss: 2.0285\n",
      "Epoch [38/300], Step [4800/27733], Loss: 2.8456\n",
      "Epoch [38/300], Step [4900/27733], Loss: 2.9486\n",
      "Epoch [38/300], Step [5000/27733], Loss: 2.1417\n",
      "Epoch [38/300], Step [5100/27733], Loss: 2.6122\n",
      "Epoch [38/300], Step [5200/27733], Loss: 2.9681\n",
      "Epoch [38/300], Step [5300/27733], Loss: 1.6720\n",
      "Epoch [38/300], Step [5400/27733], Loss: 3.0378\n",
      "Epoch [38/300], Step [5500/27733], Loss: 2.9601\n",
      "Epoch [38/300], Step [5600/27733], Loss: 3.0263\n",
      "Epoch [38/300], Step [5700/27733], Loss: 2.4915\n",
      "Epoch [38/300], Step [5800/27733], Loss: 2.9410\n",
      "Epoch [38/300], Step [5900/27733], Loss: 2.6419\n",
      "Epoch [38/300], Step [6000/27733], Loss: 2.7458\n",
      "Epoch [38/300], Step [6100/27733], Loss: 3.0717\n",
      "Epoch [38/300], Step [6200/27733], Loss: 2.4683\n",
      "Epoch [38/300], Step [6300/27733], Loss: 2.4536\n",
      "Epoch [38/300], Step [6400/27733], Loss: 2.7594\n",
      "Epoch [38/300], Step [6500/27733], Loss: 3.1965\n",
      "Epoch [38/300], Step [6600/27733], Loss: 3.2409\n",
      "Epoch [38/300], Step [6700/27733], Loss: 3.1195\n",
      "Epoch [38/300], Step [6800/27733], Loss: 2.0685\n",
      "Epoch [38/300], Step [6900/27733], Loss: 2.1514\n",
      "Epoch [38/300], Step [7000/27733], Loss: 2.4678\n",
      "Epoch [38/300], Step [7100/27733], Loss: 2.8518\n",
      "Epoch [38/300], Step [7200/27733], Loss: 3.2732\n",
      "Epoch [38/300], Step [7300/27733], Loss: 2.6300\n",
      "Epoch [38/300], Step [7400/27733], Loss: 1.9393\n",
      "Epoch [38/300], Step [7500/27733], Loss: 2.9729\n",
      "Epoch [38/300], Step [7600/27733], Loss: 2.9032\n",
      "Epoch [38/300], Step [7700/27733], Loss: 2.3760\n",
      "Epoch [38/300], Step [7800/27733], Loss: 2.9750\n",
      "Epoch [38/300], Step [7900/27733], Loss: 2.8144\n",
      "Epoch [38/300], Step [8000/27733], Loss: 2.2911\n",
      "Epoch [38/300], Step [8100/27733], Loss: 3.0756\n",
      "Epoch [38/300], Step [8200/27733], Loss: 2.7270\n",
      "Epoch [38/300], Step [8300/27733], Loss: 2.8899\n",
      "Epoch [38/300], Step [8400/27733], Loss: 2.4430\n",
      "Epoch [38/300], Step [8500/27733], Loss: 2.5865\n",
      "Epoch [38/300], Step [8600/27733], Loss: 2.7139\n",
      "Epoch [38/300], Step [8700/27733], Loss: 2.7164\n",
      "Epoch [38/300], Step [8800/27733], Loss: 2.7498\n",
      "Epoch [38/300], Step [8900/27733], Loss: 2.5076\n",
      "Epoch [38/300], Step [9000/27733], Loss: 3.1066\n",
      "Epoch [38/300], Step [9100/27733], Loss: 2.9994\n",
      "Epoch [38/300], Step [9200/27733], Loss: 2.3068\n",
      "Epoch [38/300], Step [9300/27733], Loss: 2.4527\n",
      "Epoch [38/300], Step [9400/27733], Loss: 2.0980\n",
      "Epoch [38/300], Step [9500/27733], Loss: 2.8260\n",
      "Epoch [38/300], Step [9600/27733], Loss: 3.0983\n",
      "Epoch [38/300], Step [9700/27733], Loss: 2.4371\n",
      "Epoch [38/300], Step [9800/27733], Loss: 1.8578\n",
      "Epoch [38/300], Step [9900/27733], Loss: 2.6176\n",
      "Epoch [38/300], Step [10000/27733], Loss: 2.2942\n",
      "Epoch [38/300], Step [10100/27733], Loss: 2.7764\n",
      "Epoch [38/300], Step [10200/27733], Loss: 2.5657\n",
      "Epoch [38/300], Step [10300/27733], Loss: 1.8202\n",
      "Epoch [38/300], Step [10400/27733], Loss: 3.0209\n",
      "Epoch [38/300], Step [10500/27733], Loss: 2.7700\n",
      "Epoch [38/300], Step [10600/27733], Loss: 2.7178\n",
      "Epoch [38/300], Step [10700/27733], Loss: 3.1479\n",
      "Epoch [38/300], Step [10800/27733], Loss: 2.8865\n",
      "Epoch [38/300], Step [10900/27733], Loss: 2.9799\n",
      "Epoch [38/300], Step [11000/27733], Loss: 2.7687\n",
      "Epoch [38/300], Step [11100/27733], Loss: 2.6007\n",
      "Epoch [38/300], Step [11200/27733], Loss: 2.6163\n",
      "Epoch [38/300], Step [11300/27733], Loss: 3.2718\n",
      "Epoch [38/300], Step [11400/27733], Loss: 2.5545\n",
      "Epoch [38/300], Step [11500/27733], Loss: 2.3767\n",
      "Epoch [38/300], Step [11600/27733], Loss: 3.2004\n",
      "Epoch [38/300], Step [11700/27733], Loss: 2.6102\n",
      "Epoch [38/300], Step [11800/27733], Loss: 3.5643\n",
      "Epoch [38/300], Step [11900/27733], Loss: 3.2636\n",
      "Epoch [38/300], Step [12000/27733], Loss: 2.7398\n",
      "Epoch [38/300], Step [12100/27733], Loss: 2.6108\n",
      "Epoch [38/300], Step [12200/27733], Loss: 2.9910\n",
      "Epoch [38/300], Step [12300/27733], Loss: 3.6524\n",
      "Epoch [38/300], Step [12400/27733], Loss: 2.4366\n",
      "Epoch [38/300], Step [12500/27733], Loss: 2.4476\n",
      "Epoch [38/300], Step [12600/27733], Loss: 2.4384\n",
      "Epoch [38/300], Step [12700/27733], Loss: 2.9024\n",
      "Epoch [38/300], Step [12800/27733], Loss: 2.5065\n",
      "Epoch [38/300], Step [12900/27733], Loss: 2.4559\n",
      "Epoch [38/300], Step [13000/27733], Loss: 2.7201\n",
      "Epoch [38/300], Step [13100/27733], Loss: 2.6359\n",
      "Epoch [38/300], Step [13200/27733], Loss: 3.0430\n",
      "Epoch [38/300], Step [13300/27733], Loss: 2.4564\n",
      "Epoch [38/300], Step [13400/27733], Loss: 2.4856\n",
      "Epoch [38/300], Step [13500/27733], Loss: 2.4301\n",
      "Epoch [38/300], Step [13600/27733], Loss: 3.4789\n",
      "Epoch [38/300], Step [13700/27733], Loss: 2.4850\n",
      "Epoch [38/300], Step [13800/27733], Loss: 3.4668\n",
      "Epoch [38/300], Step [13900/27733], Loss: 2.6817\n",
      "Epoch [38/300], Step [14000/27733], Loss: 3.0887\n",
      "Epoch [38/300], Step [14100/27733], Loss: 3.4595\n",
      "Epoch [38/300], Step [14200/27733], Loss: 3.9891\n",
      "Epoch [38/300], Step [14300/27733], Loss: 2.4843\n",
      "Epoch [38/300], Step [14400/27733], Loss: 2.6240\n",
      "Epoch [38/300], Step [14500/27733], Loss: 2.7940\n",
      "Epoch [38/300], Step [14600/27733], Loss: 2.7662\n",
      "Epoch [38/300], Step [14700/27733], Loss: 2.8963\n",
      "Epoch [38/300], Step [14800/27733], Loss: 2.5528\n",
      "Epoch [38/300], Step [14900/27733], Loss: 3.0831\n",
      "Epoch [38/300], Step [15000/27733], Loss: 3.1620\n",
      "Epoch [38/300], Step [15100/27733], Loss: 2.6126\n",
      "Epoch [38/300], Step [15200/27733], Loss: 2.5196\n",
      "Epoch [38/300], Step [15300/27733], Loss: 1.6646\n",
      "Epoch [38/300], Step [15400/27733], Loss: 3.0381\n",
      "Epoch [38/300], Step [15500/27733], Loss: 2.4523\n",
      "Epoch [38/300], Step [15600/27733], Loss: 3.0595\n",
      "Epoch [38/300], Step [15700/27733], Loss: 2.5296\n",
      "Epoch [38/300], Step [15800/27733], Loss: 3.0016\n",
      "Epoch [38/300], Step [15900/27733], Loss: 3.1251\n",
      "Epoch [38/300], Step [16000/27733], Loss: 2.6235\n",
      "Epoch [38/300], Step [16100/27733], Loss: 2.7412\n",
      "Epoch [38/300], Step [16200/27733], Loss: 3.3079\n",
      "Epoch [38/300], Step [16300/27733], Loss: 2.6851\n",
      "Epoch [38/300], Step [16400/27733], Loss: 3.1184\n",
      "Epoch [38/300], Step [16500/27733], Loss: 2.3938\n",
      "Epoch [38/300], Step [16600/27733], Loss: 3.0437\n",
      "Epoch [38/300], Step [16700/27733], Loss: 3.1207\n",
      "Epoch [38/300], Step [16800/27733], Loss: 1.9995\n",
      "Epoch [38/300], Step [16900/27733], Loss: 2.5519\n",
      "Epoch [38/300], Step [17000/27733], Loss: 2.1471\n",
      "Epoch [38/300], Step [17100/27733], Loss: 2.3670\n",
      "Epoch [38/300], Step [17200/27733], Loss: 2.6674\n",
      "Epoch [38/300], Step [17300/27733], Loss: 3.1402\n",
      "Epoch [38/300], Step [17400/27733], Loss: 3.9227\n",
      "Epoch [38/300], Step [17500/27733], Loss: 2.6037\n",
      "Epoch [38/300], Step [17600/27733], Loss: 2.8745\n",
      "Epoch [38/300], Step [17700/27733], Loss: 3.6913\n",
      "Epoch [38/300], Step [17800/27733], Loss: 2.2301\n",
      "Epoch [38/300], Step [17900/27733], Loss: 2.6239\n",
      "Epoch [38/300], Step [18000/27733], Loss: 2.5970\n",
      "Epoch [38/300], Step [18100/27733], Loss: 2.5281\n",
      "Epoch [38/300], Step [18200/27733], Loss: 3.4630\n",
      "Epoch [38/300], Step [18300/27733], Loss: 2.8008\n",
      "Epoch [38/300], Step [18400/27733], Loss: 2.5097\n",
      "Epoch [38/300], Step [18500/27733], Loss: 3.4015\n",
      "Epoch [38/300], Step [18600/27733], Loss: 2.8646\n",
      "Epoch [38/300], Step [18700/27733], Loss: 3.1131\n",
      "Epoch [38/300], Step [18800/27733], Loss: 2.9483\n",
      "Epoch [38/300], Step [18900/27733], Loss: 2.9804\n",
      "Epoch [38/300], Step [19000/27733], Loss: 2.4138\n",
      "Epoch [38/300], Step [19100/27733], Loss: 2.5857\n",
      "Epoch [38/300], Step [19200/27733], Loss: 2.5163\n",
      "Epoch [38/300], Step [19300/27733], Loss: 3.2475\n",
      "Epoch [38/300], Step [19400/27733], Loss: 2.8004\n",
      "Epoch [38/300], Step [19500/27733], Loss: 3.0345\n",
      "Epoch [38/300], Step [19600/27733], Loss: 2.6129\n",
      "Epoch [38/300], Step [19700/27733], Loss: 3.2913\n",
      "Epoch [38/300], Step [19800/27733], Loss: 2.3624\n",
      "Epoch [38/300], Step [19900/27733], Loss: 2.6466\n",
      "Epoch [38/300], Step [20000/27733], Loss: 3.4553\n",
      "Epoch [38/300], Step [20100/27733], Loss: 3.3090\n",
      "Epoch [38/300], Step [20200/27733], Loss: 2.6889\n",
      "Epoch [38/300], Step [20300/27733], Loss: 2.4406\n",
      "Epoch [38/300], Step [20400/27733], Loss: 3.6035\n",
      "Epoch [38/300], Step [20500/27733], Loss: 3.7363\n",
      "Epoch [38/300], Step [20600/27733], Loss: 3.3291\n",
      "Epoch [38/300], Step [20700/27733], Loss: 2.4149\n",
      "Epoch [38/300], Step [20800/27733], Loss: 2.6252\n",
      "Epoch [38/300], Step [20900/27733], Loss: 3.1614\n",
      "Epoch [38/300], Step [21000/27733], Loss: 3.0344\n",
      "Epoch [38/300], Step [21100/27733], Loss: 3.2377\n",
      "Epoch [38/300], Step [21200/27733], Loss: 3.1180\n",
      "Epoch [38/300], Step [21300/27733], Loss: 2.5878\n",
      "Epoch [38/300], Step [21400/27733], Loss: 3.6386\n",
      "Epoch [38/300], Step [21500/27733], Loss: 2.4933\n",
      "Epoch [38/300], Step [21600/27733], Loss: 3.1434\n",
      "Epoch [38/300], Step [21700/27733], Loss: 2.1543\n",
      "Epoch [38/300], Step [21800/27733], Loss: 3.2013\n",
      "Epoch [38/300], Step [21900/27733], Loss: 2.9079\n",
      "Epoch [38/300], Step [22000/27733], Loss: 2.9006\n",
      "Epoch [38/300], Step [22100/27733], Loss: 3.8882\n",
      "Epoch [38/300], Step [22200/27733], Loss: 2.7947\n",
      "Epoch [38/300], Step [22300/27733], Loss: 2.5720\n",
      "Epoch [38/300], Step [22400/27733], Loss: 3.2814\n",
      "Epoch [38/300], Step [22500/27733], Loss: 2.4778\n",
      "Epoch [38/300], Step [22600/27733], Loss: 2.9245\n",
      "Epoch [38/300], Step [22700/27733], Loss: 2.1812\n",
      "Epoch [38/300], Step [22800/27733], Loss: 2.7081\n",
      "Epoch [38/300], Step [22900/27733], Loss: 2.3360\n",
      "Epoch [38/300], Step [23000/27733], Loss: 3.7674\n",
      "Epoch [38/300], Step [23100/27733], Loss: 3.4755\n",
      "Epoch [38/300], Step [23200/27733], Loss: 3.0395\n",
      "Epoch [38/300], Step [23300/27733], Loss: 2.2006\n",
      "Epoch [38/300], Step [23400/27733], Loss: 2.8139\n",
      "Epoch [38/300], Step [23500/27733], Loss: 3.5279\n",
      "Epoch [38/300], Step [23600/27733], Loss: 3.1067\n",
      "Epoch [38/300], Step [23700/27733], Loss: 2.9128\n",
      "Epoch [38/300], Step [23800/27733], Loss: 3.2405\n",
      "Epoch [38/300], Step [23900/27733], Loss: 2.6202\n",
      "Epoch [38/300], Step [24000/27733], Loss: 3.4643\n",
      "Epoch [38/300], Step [24100/27733], Loss: 3.4444\n",
      "Epoch [38/300], Step [24200/27733], Loss: 3.6593\n",
      "Epoch [38/300], Step [24300/27733], Loss: 3.4642\n",
      "Epoch [38/300], Step [24400/27733], Loss: 2.9833\n",
      "Epoch [38/300], Step [24500/27733], Loss: 2.9379\n",
      "Epoch [38/300], Step [24600/27733], Loss: 4.1003\n",
      "Epoch [38/300], Step [24700/27733], Loss: 3.3873\n",
      "Epoch [38/300], Step [24800/27733], Loss: 2.4711\n",
      "Epoch [38/300], Step [24900/27733], Loss: 2.7395\n",
      "Epoch [38/300], Step [25000/27733], Loss: 2.6593\n",
      "Epoch [38/300], Step [25100/27733], Loss: 2.8778\n",
      "Epoch [38/300], Step [25200/27733], Loss: 2.9810\n",
      "Epoch [38/300], Step [25300/27733], Loss: 3.2735\n",
      "Epoch [38/300], Step [25400/27733], Loss: 2.5229\n",
      "Epoch [38/300], Step [25500/27733], Loss: 2.3561\n",
      "Epoch [38/300], Step [25600/27733], Loss: 3.2199\n",
      "Epoch [38/300], Step [25700/27733], Loss: 2.9852\n",
      "Epoch [38/300], Step [25800/27733], Loss: 2.5071\n",
      "Epoch [38/300], Step [25900/27733], Loss: 2.2519\n",
      "Epoch [38/300], Step [26000/27733], Loss: 2.6614\n",
      "Epoch [38/300], Step [26100/27733], Loss: 3.1041\n",
      "Epoch [38/300], Step [26200/27733], Loss: 2.6626\n",
      "Epoch [38/300], Step [26300/27733], Loss: 2.6383\n",
      "Epoch [38/300], Step [26400/27733], Loss: 3.1215\n",
      "Epoch [38/300], Step [26500/27733], Loss: 2.8691\n",
      "Epoch [38/300], Step [26600/27733], Loss: 3.2653\n",
      "Epoch [38/300], Step [26700/27733], Loss: 2.5652\n",
      "Epoch [38/300], Step [26800/27733], Loss: 2.3459\n",
      "Epoch [38/300], Step [26900/27733], Loss: 4.3678\n",
      "Epoch [38/300], Step [27000/27733], Loss: 3.9745\n",
      "Epoch [38/300], Step [27100/27733], Loss: 3.8681\n",
      "Epoch [38/300], Step [27200/27733], Loss: 2.4441\n",
      "Epoch [38/300], Step [27300/27733], Loss: 3.2584\n",
      "Epoch [38/300], Step [27400/27733], Loss: 2.2923\n",
      "Epoch [38/300], Step [27500/27733], Loss: 3.3616\n",
      "Epoch [38/300], Step [27600/27733], Loss: 2.5815\n",
      "Epoch [38/300], Step [27700/27733], Loss: 3.4182\n",
      "Epoch [39/300], Step [100/27733], Loss: 2.6363\n",
      "Epoch [39/300], Step [200/27733], Loss: 2.2363\n",
      "Epoch [39/300], Step [300/27733], Loss: 1.5041\n",
      "Epoch [39/300], Step [400/27733], Loss: 2.4247\n",
      "Epoch [39/300], Step [500/27733], Loss: 3.1950\n",
      "Epoch [39/300], Step [600/27733], Loss: 2.4242\n",
      "Epoch [39/300], Step [700/27733], Loss: 2.0761\n",
      "Epoch [39/300], Step [800/27733], Loss: 2.6782\n",
      "Epoch [39/300], Step [900/27733], Loss: 1.8439\n",
      "Epoch [39/300], Step [1000/27733], Loss: 2.4448\n",
      "Epoch [39/300], Step [1100/27733], Loss: 2.3422\n",
      "Epoch [39/300], Step [1200/27733], Loss: 1.9946\n",
      "Epoch [39/300], Step [1300/27733], Loss: 2.3925\n",
      "Epoch [39/300], Step [1400/27733], Loss: 1.6805\n",
      "Epoch [39/300], Step [1500/27733], Loss: 2.2329\n",
      "Epoch [39/300], Step [1600/27733], Loss: 1.9262\n",
      "Epoch [39/300], Step [1700/27733], Loss: 1.8604\n",
      "Epoch [39/300], Step [1800/27733], Loss: 2.4258\n",
      "Epoch [39/300], Step [1900/27733], Loss: 2.8712\n",
      "Epoch [39/300], Step [2000/27733], Loss: 2.8657\n",
      "Epoch [39/300], Step [2100/27733], Loss: 3.2232\n",
      "Epoch [39/300], Step [2200/27733], Loss: 1.7553\n",
      "Epoch [39/300], Step [2300/27733], Loss: 2.6798\n",
      "Epoch [39/300], Step [2400/27733], Loss: 1.9028\n",
      "Epoch [39/300], Step [2500/27733], Loss: 2.8104\n",
      "Epoch [39/300], Step [2600/27733], Loss: 2.6537\n",
      "Epoch [39/300], Step [2700/27733], Loss: 2.0342\n",
      "Epoch [39/300], Step [2800/27733], Loss: 2.6690\n",
      "Epoch [39/300], Step [2900/27733], Loss: 1.9732\n",
      "Epoch [39/300], Step [3000/27733], Loss: 2.8260\n",
      "Epoch [39/300], Step [3100/27733], Loss: 2.5646\n",
      "Epoch [39/300], Step [3200/27733], Loss: 2.8305\n",
      "Epoch [39/300], Step [3300/27733], Loss: 2.3686\n",
      "Epoch [39/300], Step [3400/27733], Loss: 2.3242\n",
      "Epoch [39/300], Step [3500/27733], Loss: 2.2167\n",
      "Epoch [39/300], Step [3600/27733], Loss: 2.4198\n",
      "Epoch [39/300], Step [3700/27733], Loss: 2.8986\n",
      "Epoch [39/300], Step [3800/27733], Loss: 2.4405\n",
      "Epoch [39/300], Step [3900/27733], Loss: 2.8578\n",
      "Epoch [39/300], Step [4000/27733], Loss: 2.6267\n",
      "Epoch [39/300], Step [4100/27733], Loss: 1.8988\n",
      "Epoch [39/300], Step [4200/27733], Loss: 2.5440\n",
      "Epoch [39/300], Step [4300/27733], Loss: 2.3906\n",
      "Epoch [39/300], Step [4400/27733], Loss: 3.2246\n",
      "Epoch [39/300], Step [4500/27733], Loss: 2.4719\n",
      "Epoch [39/300], Step [4600/27733], Loss: 2.1989\n",
      "Epoch [39/300], Step [4700/27733], Loss: 2.4563\n",
      "Epoch [39/300], Step [4800/27733], Loss: 2.7441\n",
      "Epoch [39/300], Step [4900/27733], Loss: 3.3816\n",
      "Epoch [39/300], Step [5000/27733], Loss: 2.6234\n",
      "Epoch [39/300], Step [5100/27733], Loss: 2.6987\n",
      "Epoch [39/300], Step [5200/27733], Loss: 1.7783\n",
      "Epoch [39/300], Step [5300/27733], Loss: 2.4244\n",
      "Epoch [39/300], Step [5400/27733], Loss: 2.0464\n",
      "Epoch [39/300], Step [5500/27733], Loss: 2.6900\n",
      "Epoch [39/300], Step [5600/27733], Loss: 2.0057\n",
      "Epoch [39/300], Step [5700/27733], Loss: 2.8302\n",
      "Epoch [39/300], Step [5800/27733], Loss: 2.8960\n",
      "Epoch [39/300], Step [5900/27733], Loss: 2.7903\n",
      "Epoch [39/300], Step [6000/27733], Loss: 2.9432\n",
      "Epoch [39/300], Step [6100/27733], Loss: 2.4696\n",
      "Epoch [39/300], Step [6200/27733], Loss: 3.4237\n",
      "Epoch [39/300], Step [6300/27733], Loss: 3.0991\n",
      "Epoch [39/300], Step [6400/27733], Loss: 2.9777\n",
      "Epoch [39/300], Step [6500/27733], Loss: 3.2609\n",
      "Epoch [39/300], Step [6600/27733], Loss: 2.4146\n",
      "Epoch [39/300], Step [6700/27733], Loss: 2.1998\n",
      "Epoch [39/300], Step [6800/27733], Loss: 2.3703\n",
      "Epoch [39/300], Step [6900/27733], Loss: 2.6270\n",
      "Epoch [39/300], Step [7000/27733], Loss: 2.4969\n",
      "Epoch [39/300], Step [7100/27733], Loss: 2.1891\n",
      "Epoch [39/300], Step [7200/27733], Loss: 2.4891\n",
      "Epoch [39/300], Step [7300/27733], Loss: 3.6829\n",
      "Epoch [39/300], Step [7400/27733], Loss: 2.6065\n",
      "Epoch [39/300], Step [7500/27733], Loss: 2.7916\n",
      "Epoch [39/300], Step [7600/27733], Loss: 2.5369\n",
      "Epoch [39/300], Step [7700/27733], Loss: 3.5510\n",
      "Epoch [39/300], Step [7800/27733], Loss: 2.1300\n",
      "Epoch [39/300], Step [7900/27733], Loss: 2.6258\n",
      "Epoch [39/300], Step [8000/27733], Loss: 3.0389\n",
      "Epoch [39/300], Step [8100/27733], Loss: 3.3348\n",
      "Epoch [39/300], Step [8200/27733], Loss: 2.0254\n",
      "Epoch [39/300], Step [8300/27733], Loss: 3.0315\n",
      "Epoch [39/300], Step [8400/27733], Loss: 2.7834\n",
      "Epoch [39/300], Step [8500/27733], Loss: 3.4728\n",
      "Epoch [39/300], Step [8600/27733], Loss: 2.7179\n",
      "Epoch [39/300], Step [8700/27733], Loss: 3.1749\n",
      "Epoch [39/300], Step [8800/27733], Loss: 2.1552\n",
      "Epoch [39/300], Step [8900/27733], Loss: 2.3575\n",
      "Epoch [39/300], Step [9000/27733], Loss: 1.8445\n",
      "Epoch [39/300], Step [9100/27733], Loss: 2.3484\n",
      "Epoch [39/300], Step [9200/27733], Loss: 2.5107\n",
      "Epoch [39/300], Step [9300/27733], Loss: 2.9902\n",
      "Epoch [39/300], Step [9400/27733], Loss: 2.7979\n",
      "Epoch [39/300], Step [9500/27733], Loss: 2.3924\n",
      "Epoch [39/300], Step [9600/27733], Loss: 2.3821\n",
      "Epoch [39/300], Step [9700/27733], Loss: 2.5578\n",
      "Epoch [39/300], Step [9800/27733], Loss: 2.9512\n",
      "Epoch [39/300], Step [9900/27733], Loss: 2.2269\n",
      "Epoch [39/300], Step [10000/27733], Loss: 2.4715\n",
      "Epoch [39/300], Step [10100/27733], Loss: 2.2088\n",
      "Epoch [39/300], Step [10200/27733], Loss: 2.6025\n",
      "Epoch [39/300], Step [10300/27733], Loss: 2.7147\n",
      "Epoch [39/300], Step [10400/27733], Loss: 2.0122\n",
      "Epoch [39/300], Step [10500/27733], Loss: 3.0194\n",
      "Epoch [39/300], Step [10600/27733], Loss: 3.0922\n",
      "Epoch [39/300], Step [10700/27733], Loss: 2.9082\n",
      "Epoch [39/300], Step [10800/27733], Loss: 3.1115\n",
      "Epoch [39/300], Step [10900/27733], Loss: 2.0692\n",
      "Epoch [39/300], Step [11000/27733], Loss: 3.0928\n",
      "Epoch [39/300], Step [11100/27733], Loss: 2.3809\n",
      "Epoch [39/300], Step [11200/27733], Loss: 2.4143\n",
      "Epoch [39/300], Step [11300/27733], Loss: 2.8550\n",
      "Epoch [39/300], Step [11400/27733], Loss: 2.7671\n",
      "Epoch [39/300], Step [11500/27733], Loss: 3.3695\n",
      "Epoch [39/300], Step [11600/27733], Loss: 3.3813\n",
      "Epoch [39/300], Step [11700/27733], Loss: 2.3955\n",
      "Epoch [39/300], Step [11800/27733], Loss: 2.8111\n",
      "Epoch [39/300], Step [11900/27733], Loss: 3.5940\n",
      "Epoch [39/300], Step [12000/27733], Loss: 3.2498\n",
      "Epoch [39/300], Step [12100/27733], Loss: 3.1065\n",
      "Epoch [39/300], Step [12200/27733], Loss: 3.1127\n",
      "Epoch [39/300], Step [12300/27733], Loss: 3.2422\n",
      "Epoch [39/300], Step [12400/27733], Loss: 3.5153\n",
      "Epoch [39/300], Step [12500/27733], Loss: 2.8515\n",
      "Epoch [39/300], Step [12600/27733], Loss: 2.5341\n",
      "Epoch [39/300], Step [12700/27733], Loss: 2.4344\n",
      "Epoch [39/300], Step [12800/27733], Loss: 3.6676\n",
      "Epoch [39/300], Step [12900/27733], Loss: 2.8433\n",
      "Epoch [39/300], Step [13000/27733], Loss: 2.6107\n",
      "Epoch [39/300], Step [13100/27733], Loss: 2.3910\n",
      "Epoch [39/300], Step [13200/27733], Loss: 3.9633\n",
      "Epoch [39/300], Step [13300/27733], Loss: 3.1370\n",
      "Epoch [39/300], Step [13400/27733], Loss: 3.4390\n",
      "Epoch [39/300], Step [13500/27733], Loss: 2.9285\n",
      "Epoch [39/300], Step [13600/27733], Loss: 2.9112\n",
      "Epoch [39/300], Step [13700/27733], Loss: 3.3727\n",
      "Epoch [39/300], Step [13800/27733], Loss: 2.6099\n",
      "Epoch [39/300], Step [13900/27733], Loss: 2.6836\n",
      "Epoch [39/300], Step [14000/27733], Loss: 4.2160\n",
      "Epoch [39/300], Step [14100/27733], Loss: 3.0608\n",
      "Epoch [39/300], Step [14200/27733], Loss: 2.1528\n",
      "Epoch [39/300], Step [14300/27733], Loss: 2.5011\n",
      "Epoch [39/300], Step [14400/27733], Loss: 2.6788\n",
      "Epoch [39/300], Step [14500/27733], Loss: 2.8409\n",
      "Epoch [39/300], Step [14600/27733], Loss: 2.8159\n",
      "Epoch [39/300], Step [14700/27733], Loss: 3.0504\n",
      "Epoch [39/300], Step [14800/27733], Loss: 2.6998\n",
      "Epoch [39/300], Step [14900/27733], Loss: 2.5855\n",
      "Epoch [39/300], Step [15000/27733], Loss: 2.7117\n",
      "Epoch [39/300], Step [15100/27733], Loss: 2.6323\n",
      "Epoch [39/300], Step [15200/27733], Loss: 3.1009\n",
      "Epoch [39/300], Step [15300/27733], Loss: 2.7786\n",
      "Epoch [39/300], Step [15400/27733], Loss: 2.3864\n",
      "Epoch [39/300], Step [15500/27733], Loss: 2.6845\n",
      "Epoch [39/300], Step [15600/27733], Loss: 2.9127\n",
      "Epoch [39/300], Step [15700/27733], Loss: 3.1509\n",
      "Epoch [39/300], Step [15800/27733], Loss: 2.2815\n",
      "Epoch [39/300], Step [15900/27733], Loss: 2.4112\n",
      "Epoch [39/300], Step [16000/27733], Loss: 2.9975\n",
      "Epoch [39/300], Step [16100/27733], Loss: 2.7089\n",
      "Epoch [39/300], Step [16200/27733], Loss: 3.1128\n",
      "Epoch [39/300], Step [16300/27733], Loss: 2.3276\n",
      "Epoch [39/300], Step [16400/27733], Loss: 2.6474\n",
      "Epoch [39/300], Step [16500/27733], Loss: 3.3654\n",
      "Epoch [39/300], Step [16600/27733], Loss: 3.0512\n",
      "Epoch [39/300], Step [16700/27733], Loss: 2.9512\n",
      "Epoch [39/300], Step [16800/27733], Loss: 2.2446\n",
      "Epoch [39/300], Step [16900/27733], Loss: 2.7224\n",
      "Epoch [39/300], Step [17000/27733], Loss: 3.0783\n",
      "Epoch [39/300], Step [17100/27733], Loss: 2.7060\n",
      "Epoch [39/300], Step [17200/27733], Loss: 2.8779\n",
      "Epoch [39/300], Step [17300/27733], Loss: 3.2652\n",
      "Epoch [39/300], Step [17400/27733], Loss: 3.1926\n",
      "Epoch [39/300], Step [17500/27733], Loss: 1.6925\n",
      "Epoch [39/300], Step [17600/27733], Loss: 3.2028\n",
      "Epoch [39/300], Step [17700/27733], Loss: 2.5531\n",
      "Epoch [39/300], Step [17800/27733], Loss: 2.4082\n",
      "Epoch [39/300], Step [17900/27733], Loss: 2.6141\n",
      "Epoch [39/300], Step [18000/27733], Loss: 3.3058\n",
      "Epoch [39/300], Step [18100/27733], Loss: 3.0499\n",
      "Epoch [39/300], Step [18200/27733], Loss: 2.6102\n",
      "Epoch [39/300], Step [18300/27733], Loss: 2.7741\n",
      "Epoch [39/300], Step [18400/27733], Loss: 3.0164\n",
      "Epoch [39/300], Step [18500/27733], Loss: 2.8093\n",
      "Epoch [39/300], Step [18600/27733], Loss: 2.4981\n",
      "Epoch [39/300], Step [18700/27733], Loss: 2.9814\n",
      "Epoch [39/300], Step [18800/27733], Loss: 3.0343\n",
      "Epoch [39/300], Step [18900/27733], Loss: 3.5553\n",
      "Epoch [39/300], Step [19000/27733], Loss: 2.6644\n",
      "Epoch [39/300], Step [19100/27733], Loss: 2.5093\n",
      "Epoch [39/300], Step [19200/27733], Loss: 2.2998\n",
      "Epoch [39/300], Step [19300/27733], Loss: 2.5098\n",
      "Epoch [39/300], Step [19400/27733], Loss: 2.9834\n",
      "Epoch [39/300], Step [19500/27733], Loss: 2.9431\n",
      "Epoch [39/300], Step [19600/27733], Loss: 3.0909\n",
      "Epoch [39/300], Step [19700/27733], Loss: 3.4960\n",
      "Epoch [39/300], Step [19800/27733], Loss: 2.5401\n",
      "Epoch [39/300], Step [19900/27733], Loss: 2.8780\n",
      "Epoch [39/300], Step [20000/27733], Loss: 2.7748\n",
      "Epoch [39/300], Step [20100/27733], Loss: 2.3978\n",
      "Epoch [39/300], Step [20200/27733], Loss: 3.5665\n",
      "Epoch [39/300], Step [20300/27733], Loss: 3.3317\n",
      "Epoch [39/300], Step [20400/27733], Loss: 3.2448\n",
      "Epoch [39/300], Step [20500/27733], Loss: 3.5611\n",
      "Epoch [39/300], Step [20600/27733], Loss: 3.4738\n",
      "Epoch [39/300], Step [20700/27733], Loss: 2.3567\n",
      "Epoch [39/300], Step [20800/27733], Loss: 3.8242\n",
      "Epoch [39/300], Step [20900/27733], Loss: 2.4039\n",
      "Epoch [39/300], Step [21000/27733], Loss: 2.7933\n",
      "Epoch [39/300], Step [21100/27733], Loss: 2.4421\n",
      "Epoch [39/300], Step [21200/27733], Loss: 2.9462\n",
      "Epoch [39/300], Step [21300/27733], Loss: 2.4790\n",
      "Epoch [39/300], Step [21400/27733], Loss: 2.3201\n",
      "Epoch [39/300], Step [21500/27733], Loss: 3.3428\n",
      "Epoch [39/300], Step [21600/27733], Loss: 2.6290\n",
      "Epoch [39/300], Step [21700/27733], Loss: 3.4285\n",
      "Epoch [39/300], Step [21800/27733], Loss: 3.0242\n",
      "Epoch [39/300], Step [21900/27733], Loss: 3.1167\n",
      "Epoch [39/300], Step [22000/27733], Loss: 3.0588\n",
      "Epoch [39/300], Step [22100/27733], Loss: 2.7879\n",
      "Epoch [39/300], Step [22200/27733], Loss: 3.2007\n",
      "Epoch [39/300], Step [22300/27733], Loss: 2.3817\n",
      "Epoch [39/300], Step [22400/27733], Loss: 3.7104\n",
      "Epoch [39/300], Step [22500/27733], Loss: 3.0847\n",
      "Epoch [39/300], Step [22600/27733], Loss: 2.7947\n",
      "Epoch [39/300], Step [22700/27733], Loss: 3.0572\n",
      "Epoch [39/300], Step [22800/27733], Loss: 2.9522\n",
      "Epoch [39/300], Step [22900/27733], Loss: 2.7711\n",
      "Epoch [39/300], Step [23000/27733], Loss: 2.9254\n",
      "Epoch [39/300], Step [23100/27733], Loss: 3.0583\n",
      "Epoch [39/300], Step [23200/27733], Loss: 3.1486\n",
      "Epoch [39/300], Step [23300/27733], Loss: 3.7960\n",
      "Epoch [39/300], Step [23400/27733], Loss: 3.1662\n",
      "Epoch [39/300], Step [23500/27733], Loss: 2.2006\n",
      "Epoch [39/300], Step [23600/27733], Loss: 3.4372\n",
      "Epoch [39/300], Step [23700/27733], Loss: 2.5402\n",
      "Epoch [39/300], Step [23800/27733], Loss: 3.1879\n",
      "Epoch [39/300], Step [23900/27733], Loss: 3.8919\n",
      "Epoch [39/300], Step [24000/27733], Loss: 2.0020\n",
      "Epoch [39/300], Step [24100/27733], Loss: 2.5387\n",
      "Epoch [39/300], Step [24200/27733], Loss: 2.3852\n",
      "Epoch [39/300], Step [24300/27733], Loss: 3.1984\n",
      "Epoch [39/300], Step [24400/27733], Loss: 3.3296\n",
      "Epoch [39/300], Step [24500/27733], Loss: 3.2879\n",
      "Epoch [39/300], Step [24600/27733], Loss: 2.9014\n",
      "Epoch [39/300], Step [24700/27733], Loss: 2.3518\n",
      "Epoch [39/300], Step [24800/27733], Loss: 2.5689\n",
      "Epoch [39/300], Step [24900/27733], Loss: 3.2244\n",
      "Epoch [39/300], Step [25000/27733], Loss: 3.5536\n",
      "Epoch [39/300], Step [25100/27733], Loss: 2.1099\n",
      "Epoch [39/300], Step [25200/27733], Loss: 3.2249\n",
      "Epoch [39/300], Step [25300/27733], Loss: 3.1528\n",
      "Epoch [39/300], Step [25400/27733], Loss: 2.5094\n",
      "Epoch [39/300], Step [25500/27733], Loss: 3.1439\n",
      "Epoch [39/300], Step [25600/27733], Loss: 3.4404\n",
      "Epoch [39/300], Step [25700/27733], Loss: 3.1102\n",
      "Epoch [39/300], Step [25800/27733], Loss: 2.6372\n",
      "Epoch [39/300], Step [25900/27733], Loss: 2.5605\n",
      "Epoch [39/300], Step [26000/27733], Loss: 3.0219\n",
      "Epoch [39/300], Step [26100/27733], Loss: 3.1034\n",
      "Epoch [39/300], Step [26200/27733], Loss: 2.5336\n",
      "Epoch [39/300], Step [26300/27733], Loss: 3.1516\n",
      "Epoch [39/300], Step [26400/27733], Loss: 3.2840\n",
      "Epoch [39/300], Step [26500/27733], Loss: 2.9951\n",
      "Epoch [39/300], Step [26600/27733], Loss: 2.4816\n",
      "Epoch [39/300], Step [26700/27733], Loss: 3.4726\n",
      "Epoch [39/300], Step [26800/27733], Loss: 3.5590\n",
      "Epoch [39/300], Step [26900/27733], Loss: 2.1811\n",
      "Epoch [39/300], Step [27000/27733], Loss: 2.6711\n",
      "Epoch [39/300], Step [27100/27733], Loss: 3.3863\n",
      "Epoch [39/300], Step [27200/27733], Loss: 3.3167\n",
      "Epoch [39/300], Step [27300/27733], Loss: 2.5056\n",
      "Epoch [39/300], Step [27400/27733], Loss: 3.3618\n",
      "Epoch [39/300], Step [27500/27733], Loss: 2.9506\n",
      "Epoch [39/300], Step [27600/27733], Loss: 3.9170\n",
      "Epoch [39/300], Step [27700/27733], Loss: 3.3218\n",
      "Epoch [40/300], Step [100/27733], Loss: 1.9997\n",
      "Epoch [40/300], Step [200/27733], Loss: 2.5275\n",
      "Epoch [40/300], Step [300/27733], Loss: 1.9357\n",
      "Epoch [40/300], Step [400/27733], Loss: 2.3845\n",
      "Epoch [40/300], Step [500/27733], Loss: 2.3774\n",
      "Epoch [40/300], Step [600/27733], Loss: 1.6986\n",
      "Epoch [40/300], Step [700/27733], Loss: 2.2890\n",
      "Epoch [40/300], Step [800/27733], Loss: 3.1733\n",
      "Epoch [40/300], Step [900/27733], Loss: 2.3935\n",
      "Epoch [40/300], Step [1000/27733], Loss: 2.6088\n",
      "Epoch [40/300], Step [1100/27733], Loss: 2.1485\n",
      "Epoch [40/300], Step [1200/27733], Loss: 2.5627\n",
      "Epoch [40/300], Step [1300/27733], Loss: 2.4624\n",
      "Epoch [40/300], Step [1400/27733], Loss: 1.7609\n",
      "Epoch [40/300], Step [1500/27733], Loss: 2.3876\n",
      "Epoch [40/300], Step [1600/27733], Loss: 2.7893\n",
      "Epoch [40/300], Step [1700/27733], Loss: 2.7220\n",
      "Epoch [40/300], Step [1800/27733], Loss: 2.2178\n",
      "Epoch [40/300], Step [1900/27733], Loss: 2.2746\n",
      "Epoch [40/300], Step [2000/27733], Loss: 2.1103\n",
      "Epoch [40/300], Step [2100/27733], Loss: 1.5252\n",
      "Epoch [40/300], Step [2200/27733], Loss: 2.8636\n",
      "Epoch [40/300], Step [2300/27733], Loss: 2.5509\n",
      "Epoch [40/300], Step [2400/27733], Loss: 2.7428\n",
      "Epoch [40/300], Step [2500/27733], Loss: 2.5617\n",
      "Epoch [40/300], Step [2600/27733], Loss: 1.9235\n",
      "Epoch [40/300], Step [2700/27733], Loss: 3.2893\n",
      "Epoch [40/300], Step [2800/27733], Loss: 2.2569\n",
      "Epoch [40/300], Step [2900/27733], Loss: 2.7797\n",
      "Epoch [40/300], Step [3000/27733], Loss: 2.9626\n",
      "Epoch [40/300], Step [3100/27733], Loss: 2.4712\n",
      "Epoch [40/300], Step [3200/27733], Loss: 2.6166\n",
      "Epoch [40/300], Step [3300/27733], Loss: 3.4048\n",
      "Epoch [40/300], Step [3400/27733], Loss: 2.7526\n",
      "Epoch [40/300], Step [3500/27733], Loss: 2.7370\n",
      "Epoch [40/300], Step [3600/27733], Loss: 2.0696\n",
      "Epoch [40/300], Step [3700/27733], Loss: 2.6957\n",
      "Epoch [40/300], Step [3800/27733], Loss: 3.0042\n",
      "Epoch [40/300], Step [3900/27733], Loss: 2.5364\n",
      "Epoch [40/300], Step [4000/27733], Loss: 2.6956\n",
      "Epoch [40/300], Step [4100/27733], Loss: 2.5764\n",
      "Epoch [40/300], Step [4200/27733], Loss: 1.9201\n",
      "Epoch [40/300], Step [4300/27733], Loss: 2.9797\n",
      "Epoch [40/300], Step [4400/27733], Loss: 3.0179\n",
      "Epoch [40/300], Step [4500/27733], Loss: 2.5563\n",
      "Epoch [40/300], Step [4600/27733], Loss: 2.7361\n",
      "Epoch [40/300], Step [4700/27733], Loss: 2.7436\n",
      "Epoch [40/300], Step [4800/27733], Loss: 2.5884\n",
      "Epoch [40/300], Step [4900/27733], Loss: 2.1235\n",
      "Epoch [40/300], Step [5000/27733], Loss: 2.2103\n",
      "Epoch [40/300], Step [5100/27733], Loss: 1.8327\n",
      "Epoch [40/300], Step [5200/27733], Loss: 2.5035\n",
      "Epoch [40/300], Step [5300/27733], Loss: 1.9845\n",
      "Epoch [40/300], Step [5400/27733], Loss: 1.9655\n",
      "Epoch [40/300], Step [5500/27733], Loss: 2.2450\n",
      "Epoch [40/300], Step [5600/27733], Loss: 2.2955\n",
      "Epoch [40/300], Step [5700/27733], Loss: 3.1338\n",
      "Epoch [40/300], Step [5800/27733], Loss: 2.4375\n",
      "Epoch [40/300], Step [5900/27733], Loss: 2.5721\n",
      "Epoch [40/300], Step [6000/27733], Loss: 2.6553\n",
      "Epoch [40/300], Step [6100/27733], Loss: 2.3294\n",
      "Epoch [40/300], Step [6200/27733], Loss: 2.3548\n",
      "Epoch [40/300], Step [6300/27733], Loss: 3.0378\n",
      "Epoch [40/300], Step [6400/27733], Loss: 2.7585\n",
      "Epoch [40/300], Step [6500/27733], Loss: 2.3845\n",
      "Epoch [40/300], Step [6600/27733], Loss: 3.6547\n",
      "Epoch [40/300], Step [6700/27733], Loss: 2.8228\n",
      "Epoch [40/300], Step [6800/27733], Loss: 2.4261\n",
      "Epoch [40/300], Step [6900/27733], Loss: 2.3103\n",
      "Epoch [40/300], Step [7000/27733], Loss: 2.7821\n",
      "Epoch [40/300], Step [7100/27733], Loss: 3.4803\n",
      "Epoch [40/300], Step [7200/27733], Loss: 2.9213\n",
      "Epoch [40/300], Step [7300/27733], Loss: 2.4856\n",
      "Epoch [40/300], Step [7400/27733], Loss: 2.6259\n",
      "Epoch [40/300], Step [7500/27733], Loss: 2.0835\n",
      "Epoch [40/300], Step [7600/27733], Loss: 2.1204\n",
      "Epoch [40/300], Step [7700/27733], Loss: 2.9757\n",
      "Epoch [40/300], Step [7800/27733], Loss: 2.3743\n",
      "Epoch [40/300], Step [7900/27733], Loss: 1.9619\n",
      "Epoch [40/300], Step [8000/27733], Loss: 2.5208\n",
      "Epoch [40/300], Step [8100/27733], Loss: 2.7248\n",
      "Epoch [40/300], Step [8200/27733], Loss: 2.6997\n",
      "Epoch [40/300], Step [8300/27733], Loss: 2.8779\n",
      "Epoch [40/300], Step [8400/27733], Loss: 2.7613\n",
      "Epoch [40/300], Step [8500/27733], Loss: 2.7459\n",
      "Epoch [40/300], Step [8600/27733], Loss: 3.3366\n",
      "Epoch [40/300], Step [8700/27733], Loss: 2.4961\n",
      "Epoch [40/300], Step [8800/27733], Loss: 2.1484\n",
      "Epoch [40/300], Step [8900/27733], Loss: 2.7002\n",
      "Epoch [40/300], Step [9000/27733], Loss: 2.3460\n",
      "Epoch [40/300], Step [9100/27733], Loss: 2.2740\n",
      "Epoch [40/300], Step [9200/27733], Loss: 2.3108\n",
      "Epoch [40/300], Step [9300/27733], Loss: 2.7889\n",
      "Epoch [40/300], Step [9400/27733], Loss: 2.5096\n",
      "Epoch [40/300], Step [9500/27733], Loss: 2.8350\n",
      "Epoch [40/300], Step [9600/27733], Loss: 3.0326\n",
      "Epoch [40/300], Step [9700/27733], Loss: 3.5233\n",
      "Epoch [40/300], Step [9800/27733], Loss: 2.6839\n",
      "Epoch [40/300], Step [9900/27733], Loss: 2.6514\n",
      "Epoch [40/300], Step [10000/27733], Loss: 2.0362\n",
      "Epoch [40/300], Step [10100/27733], Loss: 3.5887\n",
      "Epoch [40/300], Step [10200/27733], Loss: 2.4246\n",
      "Epoch [40/300], Step [10300/27733], Loss: 2.7886\n",
      "Epoch [40/300], Step [10400/27733], Loss: 3.1895\n",
      "Epoch [40/300], Step [10500/27733], Loss: 2.6090\n",
      "Epoch [40/300], Step [10600/27733], Loss: 2.5967\n",
      "Epoch [40/300], Step [10700/27733], Loss: 2.4662\n",
      "Epoch [40/300], Step [10800/27733], Loss: 2.3472\n",
      "Epoch [40/300], Step [10900/27733], Loss: 2.9739\n",
      "Epoch [40/300], Step [11000/27733], Loss: 3.2975\n",
      "Epoch [40/300], Step [11100/27733], Loss: 2.5223\n",
      "Epoch [40/300], Step [11200/27733], Loss: 3.1256\n",
      "Epoch [40/300], Step [11300/27733], Loss: 2.0586\n",
      "Epoch [40/300], Step [11400/27733], Loss: 2.9266\n",
      "Epoch [40/300], Step [11500/27733], Loss: 2.1708\n",
      "Epoch [40/300], Step [11600/27733], Loss: 2.6574\n",
      "Epoch [40/300], Step [11700/27733], Loss: 2.3842\n",
      "Epoch [40/300], Step [11800/27733], Loss: 2.7665\n",
      "Epoch [40/300], Step [11900/27733], Loss: 2.5644\n",
      "Epoch [40/300], Step [12000/27733], Loss: 2.7001\n",
      "Epoch [40/300], Step [12100/27733], Loss: 3.3989\n",
      "Epoch [40/300], Step [12200/27733], Loss: 2.5082\n",
      "Epoch [40/300], Step [12300/27733], Loss: 1.7003\n",
      "Epoch [40/300], Step [12400/27733], Loss: 2.6269\n",
      "Epoch [40/300], Step [12500/27733], Loss: 3.2836\n",
      "Epoch [40/300], Step [12600/27733], Loss: 3.3287\n",
      "Epoch [40/300], Step [12700/27733], Loss: 1.6556\n",
      "Epoch [40/300], Step [12800/27733], Loss: 2.8869\n",
      "Epoch [40/300], Step [12900/27733], Loss: 2.7200\n",
      "Epoch [40/300], Step [13000/27733], Loss: 3.0366\n",
      "Epoch [40/300], Step [13100/27733], Loss: 2.1625\n",
      "Epoch [40/300], Step [13200/27733], Loss: 3.2333\n",
      "Epoch [40/300], Step [13300/27733], Loss: 3.0356\n",
      "Epoch [40/300], Step [13400/27733], Loss: 3.1443\n",
      "Epoch [40/300], Step [13500/27733], Loss: 3.5425\n",
      "Epoch [40/300], Step [13600/27733], Loss: 2.8514\n",
      "Epoch [40/300], Step [13700/27733], Loss: 3.3232\n",
      "Epoch [40/300], Step [13800/27733], Loss: 2.2501\n",
      "Epoch [40/300], Step [13900/27733], Loss: 2.9307\n",
      "Epoch [40/300], Step [14000/27733], Loss: 2.8466\n",
      "Epoch [40/300], Step [14100/27733], Loss: 3.3583\n",
      "Epoch [40/300], Step [14200/27733], Loss: 3.1102\n",
      "Epoch [40/300], Step [14300/27733], Loss: 2.3945\n",
      "Epoch [40/300], Step [14400/27733], Loss: 2.2365\n",
      "Epoch [40/300], Step [14500/27733], Loss: 2.6306\n",
      "Epoch [40/300], Step [14600/27733], Loss: 2.6358\n",
      "Epoch [40/300], Step [14700/27733], Loss: 2.3596\n",
      "Epoch [40/300], Step [14800/27733], Loss: 2.0685\n",
      "Epoch [40/300], Step [14900/27733], Loss: 2.7844\n",
      "Epoch [40/300], Step [15000/27733], Loss: 2.6719\n",
      "Epoch [40/300], Step [15100/27733], Loss: 3.4109\n",
      "Epoch [40/300], Step [15200/27733], Loss: 2.8673\n",
      "Epoch [40/300], Step [15300/27733], Loss: 2.9148\n",
      "Epoch [40/300], Step [15400/27733], Loss: 2.3744\n",
      "Epoch [40/300], Step [15500/27733], Loss: 2.5978\n",
      "Epoch [40/300], Step [15600/27733], Loss: 2.2310\n",
      "Epoch [40/300], Step [15700/27733], Loss: 3.3296\n",
      "Epoch [40/300], Step [15800/27733], Loss: 2.4243\n",
      "Epoch [40/300], Step [15900/27733], Loss: 2.6360\n",
      "Epoch [40/300], Step [16000/27733], Loss: 3.9244\n",
      "Epoch [40/300], Step [16100/27733], Loss: 3.2861\n",
      "Epoch [40/300], Step [16200/27733], Loss: 3.1147\n",
      "Epoch [40/300], Step [16300/27733], Loss: 2.3609\n",
      "Epoch [40/300], Step [16400/27733], Loss: 2.9929\n",
      "Epoch [40/300], Step [16500/27733], Loss: 2.4566\n",
      "Epoch [40/300], Step [16600/27733], Loss: 2.9123\n",
      "Epoch [40/300], Step [16700/27733], Loss: 3.1126\n",
      "Epoch [40/300], Step [16800/27733], Loss: 2.8485\n",
      "Epoch [40/300], Step [16900/27733], Loss: 3.2452\n",
      "Epoch [40/300], Step [17000/27733], Loss: 2.3040\n",
      "Epoch [40/300], Step [17100/27733], Loss: 2.9228\n",
      "Epoch [40/300], Step [17200/27733], Loss: 2.6913\n",
      "Epoch [40/300], Step [17300/27733], Loss: 2.4715\n",
      "Epoch [40/300], Step [17400/27733], Loss: 3.1777\n",
      "Epoch [40/300], Step [17500/27733], Loss: 3.1429\n",
      "Epoch [40/300], Step [17600/27733], Loss: 3.0408\n",
      "Epoch [40/300], Step [17700/27733], Loss: 1.9688\n",
      "Epoch [40/300], Step [17800/27733], Loss: 3.8774\n",
      "Epoch [40/300], Step [17900/27733], Loss: 2.6034\n",
      "Epoch [40/300], Step [18000/27733], Loss: 3.2570\n",
      "Epoch [40/300], Step [18100/27733], Loss: 2.9918\n",
      "Epoch [40/300], Step [18200/27733], Loss: 2.4541\n",
      "Epoch [40/300], Step [18300/27733], Loss: 3.3088\n",
      "Epoch [40/300], Step [18400/27733], Loss: 3.4354\n",
      "Epoch [40/300], Step [18500/27733], Loss: 2.7442\n",
      "Epoch [40/300], Step [18600/27733], Loss: 2.5094\n",
      "Epoch [40/300], Step [18700/27733], Loss: 3.0519\n",
      "Epoch [40/300], Step [18800/27733], Loss: 2.8789\n",
      "Epoch [40/300], Step [18900/27733], Loss: 3.2808\n",
      "Epoch [40/300], Step [19000/27733], Loss: 2.9665\n",
      "Epoch [40/300], Step [19100/27733], Loss: 3.1016\n",
      "Epoch [40/300], Step [19200/27733], Loss: 2.6668\n",
      "Epoch [40/300], Step [19300/27733], Loss: 3.0025\n",
      "Epoch [40/300], Step [19400/27733], Loss: 2.8513\n",
      "Epoch [40/300], Step [19500/27733], Loss: 3.1935\n",
      "Epoch [40/300], Step [19600/27733], Loss: 3.2478\n",
      "Epoch [40/300], Step [19700/27733], Loss: 2.6175\n",
      "Epoch [40/300], Step [19800/27733], Loss: 2.8009\n",
      "Epoch [40/300], Step [19900/27733], Loss: 2.9500\n",
      "Epoch [40/300], Step [20000/27733], Loss: 3.0365\n",
      "Epoch [40/300], Step [20100/27733], Loss: 2.5125\n",
      "Epoch [40/300], Step [20200/27733], Loss: 3.2151\n",
      "Epoch [40/300], Step [20300/27733], Loss: 3.7616\n",
      "Epoch [40/300], Step [20400/27733], Loss: 2.0861\n",
      "Epoch [40/300], Step [20500/27733], Loss: 2.7772\n",
      "Epoch [40/300], Step [20600/27733], Loss: 2.7287\n",
      "Epoch [40/300], Step [20700/27733], Loss: 2.3275\n",
      "Epoch [40/300], Step [20800/27733], Loss: 2.4874\n",
      "Epoch [40/300], Step [20900/27733], Loss: 3.0948\n",
      "Epoch [40/300], Step [21000/27733], Loss: 3.8586\n",
      "Epoch [40/300], Step [21100/27733], Loss: 2.5621\n",
      "Epoch [40/300], Step [21200/27733], Loss: 2.9092\n",
      "Epoch [40/300], Step [21300/27733], Loss: 2.1797\n",
      "Epoch [40/300], Step [21400/27733], Loss: 2.0066\n",
      "Epoch [40/300], Step [21500/27733], Loss: 2.6078\n",
      "Epoch [40/300], Step [21600/27733], Loss: 3.0580\n",
      "Epoch [40/300], Step [21700/27733], Loss: 2.3562\n",
      "Epoch [40/300], Step [21800/27733], Loss: 3.0912\n",
      "Epoch [40/300], Step [21900/27733], Loss: 2.2747\n",
      "Epoch [40/300], Step [22000/27733], Loss: 3.0623\n",
      "Epoch [40/300], Step [22100/27733], Loss: 3.0837\n",
      "Epoch [40/300], Step [22200/27733], Loss: 3.0854\n",
      "Epoch [40/300], Step [22300/27733], Loss: 2.7469\n",
      "Epoch [40/300], Step [22400/27733], Loss: 3.2086\n",
      "Epoch [40/300], Step [22500/27733], Loss: 2.8784\n",
      "Epoch [40/300], Step [22600/27733], Loss: 3.1619\n",
      "Epoch [40/300], Step [22700/27733], Loss: 2.7991\n",
      "Epoch [40/300], Step [22800/27733], Loss: 3.3107\n",
      "Epoch [40/300], Step [22900/27733], Loss: 2.9962\n",
      "Epoch [40/300], Step [23000/27733], Loss: 2.6745\n",
      "Epoch [40/300], Step [23100/27733], Loss: 2.7546\n",
      "Epoch [40/300], Step [23200/27733], Loss: 3.7436\n",
      "Epoch [40/300], Step [23300/27733], Loss: 2.7845\n",
      "Epoch [40/300], Step [23400/27733], Loss: 3.6517\n",
      "Epoch [40/300], Step [23500/27733], Loss: 2.9527\n",
      "Epoch [40/300], Step [23600/27733], Loss: 2.5045\n",
      "Epoch [40/300], Step [23700/27733], Loss: 2.3989\n",
      "Epoch [40/300], Step [23800/27733], Loss: 2.6682\n",
      "Epoch [40/300], Step [23900/27733], Loss: 3.1930\n",
      "Epoch [40/300], Step [24000/27733], Loss: 2.9536\n",
      "Epoch [40/300], Step [24100/27733], Loss: 2.4771\n",
      "Epoch [40/300], Step [24200/27733], Loss: 3.6137\n",
      "Epoch [40/300], Step [24300/27733], Loss: 2.6752\n",
      "Epoch [40/300], Step [24400/27733], Loss: 3.1743\n",
      "Epoch [40/300], Step [24500/27733], Loss: 3.2750\n",
      "Epoch [40/300], Step [24600/27733], Loss: 2.7881\n",
      "Epoch [40/300], Step [24700/27733], Loss: 2.5138\n",
      "Epoch [40/300], Step [24800/27733], Loss: 1.8211\n",
      "Epoch [40/300], Step [24900/27733], Loss: 2.8446\n",
      "Epoch [40/300], Step [25000/27733], Loss: 3.4051\n",
      "Epoch [40/300], Step [25100/27733], Loss: 3.1576\n",
      "Epoch [40/300], Step [25200/27733], Loss: 2.2287\n",
      "Epoch [40/300], Step [25300/27733], Loss: 2.8782\n",
      "Epoch [40/300], Step [25400/27733], Loss: 2.9028\n",
      "Epoch [40/300], Step [25500/27733], Loss: 2.8692\n",
      "Epoch [40/300], Step [25600/27733], Loss: 3.1691\n",
      "Epoch [40/300], Step [25700/27733], Loss: 2.7035\n",
      "Epoch [40/300], Step [25800/27733], Loss: 3.1364\n",
      "Epoch [40/300], Step [25900/27733], Loss: 3.8290\n",
      "Epoch [40/300], Step [26000/27733], Loss: 2.8855\n",
      "Epoch [40/300], Step [26100/27733], Loss: 3.0280\n",
      "Epoch [40/300], Step [26200/27733], Loss: 2.5277\n",
      "Epoch [40/300], Step [26300/27733], Loss: 2.7563\n",
      "Epoch [40/300], Step [26400/27733], Loss: 3.3134\n",
      "Epoch [40/300], Step [26500/27733], Loss: 3.7903\n",
      "Epoch [40/300], Step [26600/27733], Loss: 2.9026\n",
      "Epoch [40/300], Step [26700/27733], Loss: 2.4408\n",
      "Epoch [40/300], Step [26800/27733], Loss: 3.1540\n",
      "Epoch [40/300], Step [26900/27733], Loss: 3.5189\n",
      "Epoch [40/300], Step [27000/27733], Loss: 3.5794\n",
      "Epoch [40/300], Step [27100/27733], Loss: 3.5164\n",
      "Epoch [40/300], Step [27200/27733], Loss: 4.2420\n",
      "Epoch [40/300], Step [27300/27733], Loss: 2.9163\n",
      "Epoch [40/300], Step [27400/27733], Loss: 3.1438\n",
      "Epoch [40/300], Step [27500/27733], Loss: 3.0601\n",
      "Epoch [40/300], Step [27600/27733], Loss: 2.4267\n",
      "Epoch [40/300], Step [27700/27733], Loss: 3.1394\n",
      "Epoch [41/300], Step [100/27733], Loss: 2.3042\n",
      "Epoch [41/300], Step [200/27733], Loss: 2.5307\n",
      "Epoch [41/300], Step [300/27733], Loss: 3.0518\n",
      "Epoch [41/300], Step [400/27733], Loss: 2.2369\n",
      "Epoch [41/300], Step [500/27733], Loss: 2.0190\n",
      "Epoch [41/300], Step [600/27733], Loss: 2.6452\n",
      "Epoch [41/300], Step [700/27733], Loss: 1.8358\n",
      "Epoch [41/300], Step [800/27733], Loss: 2.2863\n",
      "Epoch [41/300], Step [900/27733], Loss: 2.3204\n",
      "Epoch [41/300], Step [1000/27733], Loss: 3.0590\n",
      "Epoch [41/300], Step [1100/27733], Loss: 2.6058\n",
      "Epoch [41/300], Step [1200/27733], Loss: 2.3367\n",
      "Epoch [41/300], Step [1300/27733], Loss: 2.2780\n",
      "Epoch [41/300], Step [1400/27733], Loss: 2.6891\n",
      "Epoch [41/300], Step [1500/27733], Loss: 2.4786\n",
      "Epoch [41/300], Step [1600/27733], Loss: 3.2207\n",
      "Epoch [41/300], Step [1700/27733], Loss: 2.1907\n",
      "Epoch [41/300], Step [1800/27733], Loss: 1.5919\n",
      "Epoch [41/300], Step [1900/27733], Loss: 3.0437\n",
      "Epoch [41/300], Step [2000/27733], Loss: 2.3891\n",
      "Epoch [41/300], Step [2100/27733], Loss: 2.2149\n",
      "Epoch [41/300], Step [2200/27733], Loss: 2.0962\n",
      "Epoch [41/300], Step [2300/27733], Loss: 3.0365\n",
      "Epoch [41/300], Step [2400/27733], Loss: 2.3675\n",
      "Epoch [41/300], Step [2500/27733], Loss: 2.6470\n",
      "Epoch [41/300], Step [2600/27733], Loss: 2.2998\n",
      "Epoch [41/300], Step [2700/27733], Loss: 1.5605\n",
      "Epoch [41/300], Step [2800/27733], Loss: 2.3533\n",
      "Epoch [41/300], Step [2900/27733], Loss: 2.4480\n",
      "Epoch [41/300], Step [3000/27733], Loss: 2.1324\n",
      "Epoch [41/300], Step [3100/27733], Loss: 2.4947\n",
      "Epoch [41/300], Step [3200/27733], Loss: 1.8360\n",
      "Epoch [41/300], Step [3300/27733], Loss: 2.5772\n",
      "Epoch [41/300], Step [3400/27733], Loss: 2.7460\n",
      "Epoch [41/300], Step [3500/27733], Loss: 2.9165\n",
      "Epoch [41/300], Step [3600/27733], Loss: 1.6160\n",
      "Epoch [41/300], Step [3700/27733], Loss: 2.4451\n",
      "Epoch [41/300], Step [3800/27733], Loss: 1.8814\n",
      "Epoch [41/300], Step [3900/27733], Loss: 3.2050\n",
      "Epoch [41/300], Step [4000/27733], Loss: 2.3252\n",
      "Epoch [41/300], Step [4100/27733], Loss: 2.7986\n",
      "Epoch [41/300], Step [4200/27733], Loss: 2.3880\n",
      "Epoch [41/300], Step [4300/27733], Loss: 2.5017\n",
      "Epoch [41/300], Step [4400/27733], Loss: 2.5459\n",
      "Epoch [41/300], Step [4500/27733], Loss: 2.7369\n",
      "Epoch [41/300], Step [4600/27733], Loss: 2.2065\n",
      "Epoch [41/300], Step [4700/27733], Loss: 2.8132\n",
      "Epoch [41/300], Step [4800/27733], Loss: 2.6692\n",
      "Epoch [41/300], Step [4900/27733], Loss: 2.4415\n",
      "Epoch [41/300], Step [5000/27733], Loss: 1.8454\n",
      "Epoch [41/300], Step [5100/27733], Loss: 2.3252\n",
      "Epoch [41/300], Step [5200/27733], Loss: 2.7412\n",
      "Epoch [41/300], Step [5300/27733], Loss: 2.3232\n",
      "Epoch [41/300], Step [5400/27733], Loss: 2.8223\n",
      "Epoch [41/300], Step [5500/27733], Loss: 2.4602\n",
      "Epoch [41/300], Step [5600/27733], Loss: 2.3601\n",
      "Epoch [41/300], Step [5700/27733], Loss: 2.5932\n",
      "Epoch [41/300], Step [5800/27733], Loss: 2.4226\n",
      "Epoch [41/300], Step [5900/27733], Loss: 2.8496\n",
      "Epoch [41/300], Step [6000/27733], Loss: 2.4883\n",
      "Epoch [41/300], Step [6100/27733], Loss: 2.2496\n",
      "Epoch [41/300], Step [6200/27733], Loss: 2.9769\n",
      "Epoch [41/300], Step [6300/27733], Loss: 2.7716\n",
      "Epoch [41/300], Step [6400/27733], Loss: 2.9240\n",
      "Epoch [41/300], Step [6500/27733], Loss: 2.6707\n",
      "Epoch [41/300], Step [6600/27733], Loss: 2.3079\n",
      "Epoch [41/300], Step [6700/27733], Loss: 2.8941\n",
      "Epoch [41/300], Step [6800/27733], Loss: 3.0736\n",
      "Epoch [41/300], Step [6900/27733], Loss: 2.1136\n",
      "Epoch [41/300], Step [7000/27733], Loss: 2.2648\n",
      "Epoch [41/300], Step [7100/27733], Loss: 1.9100\n",
      "Epoch [41/300], Step [7200/27733], Loss: 2.6705\n",
      "Epoch [41/300], Step [7300/27733], Loss: 2.5964\n",
      "Epoch [41/300], Step [7400/27733], Loss: 2.4734\n",
      "Epoch [41/300], Step [7500/27733], Loss: 2.7418\n",
      "Epoch [41/300], Step [7600/27733], Loss: 2.4121\n",
      "Epoch [41/300], Step [7700/27733], Loss: 3.0144\n",
      "Epoch [41/300], Step [7800/27733], Loss: 2.0643\n",
      "Epoch [41/300], Step [7900/27733], Loss: 2.5684\n",
      "Epoch [41/300], Step [8000/27733], Loss: 2.0903\n",
      "Epoch [41/300], Step [8100/27733], Loss: 3.4961\n",
      "Epoch [41/300], Step [8200/27733], Loss: 2.3679\n",
      "Epoch [41/300], Step [8300/27733], Loss: 2.3594\n",
      "Epoch [41/300], Step [8400/27733], Loss: 2.8749\n",
      "Epoch [41/300], Step [8500/27733], Loss: 1.7300\n",
      "Epoch [41/300], Step [8600/27733], Loss: 2.6389\n",
      "Epoch [41/300], Step [8700/27733], Loss: 2.3637\n",
      "Epoch [41/300], Step [8800/27733], Loss: 2.5081\n",
      "Epoch [41/300], Step [8900/27733], Loss: 3.2587\n",
      "Epoch [41/300], Step [9000/27733], Loss: 3.8261\n",
      "Epoch [41/300], Step [9100/27733], Loss: 3.2299\n",
      "Epoch [41/300], Step [9200/27733], Loss: 3.3783\n",
      "Epoch [41/300], Step [9300/27733], Loss: 2.1630\n",
      "Epoch [41/300], Step [9400/27733], Loss: 3.3989\n",
      "Epoch [41/300], Step [9500/27733], Loss: 2.1080\n",
      "Epoch [41/300], Step [9600/27733], Loss: 2.6851\n",
      "Epoch [41/300], Step [9700/27733], Loss: 3.4053\n",
      "Epoch [41/300], Step [9800/27733], Loss: 2.9196\n",
      "Epoch [41/300], Step [9900/27733], Loss: 2.3885\n",
      "Epoch [41/300], Step [10000/27733], Loss: 2.2238\n",
      "Epoch [41/300], Step [10100/27733], Loss: 2.4766\n",
      "Epoch [41/300], Step [10200/27733], Loss: 2.0941\n",
      "Epoch [41/300], Step [10300/27733], Loss: 2.7853\n",
      "Epoch [41/300], Step [10400/27733], Loss: 3.0859\n",
      "Epoch [41/300], Step [10500/27733], Loss: 3.3111\n",
      "Epoch [41/300], Step [10600/27733], Loss: 2.6542\n",
      "Epoch [41/300], Step [10700/27733], Loss: 2.7279\n",
      "Epoch [41/300], Step [10800/27733], Loss: 3.0026\n",
      "Epoch [41/300], Step [10900/27733], Loss: 2.4288\n",
      "Epoch [41/300], Step [11000/27733], Loss: 2.5600\n",
      "Epoch [41/300], Step [11100/27733], Loss: 2.4544\n",
      "Epoch [41/300], Step [11200/27733], Loss: 2.9308\n",
      "Epoch [41/300], Step [11300/27733], Loss: 3.3093\n",
      "Epoch [41/300], Step [11400/27733], Loss: 2.8918\n",
      "Epoch [41/300], Step [11500/27733], Loss: 2.4799\n",
      "Epoch [41/300], Step [11600/27733], Loss: 2.6853\n",
      "Epoch [41/300], Step [11700/27733], Loss: 2.9892\n",
      "Epoch [41/300], Step [11800/27733], Loss: 2.7264\n",
      "Epoch [41/300], Step [11900/27733], Loss: 2.4450\n",
      "Epoch [41/300], Step [12000/27733], Loss: 2.9064\n",
      "Epoch [41/300], Step [12100/27733], Loss: 2.6064\n",
      "Epoch [41/300], Step [12200/27733], Loss: 2.1802\n",
      "Epoch [41/300], Step [12300/27733], Loss: 3.2640\n",
      "Epoch [41/300], Step [12400/27733], Loss: 2.7296\n",
      "Epoch [41/300], Step [12500/27733], Loss: 2.9405\n",
      "Epoch [41/300], Step [12600/27733], Loss: 3.2976\n",
      "Epoch [41/300], Step [12700/27733], Loss: 2.5877\n",
      "Epoch [41/300], Step [12800/27733], Loss: 1.7788\n",
      "Epoch [41/300], Step [12900/27733], Loss: 3.2697\n",
      "Epoch [41/300], Step [13000/27733], Loss: 2.5148\n",
      "Epoch [41/300], Step [13100/27733], Loss: 3.7807\n",
      "Epoch [41/300], Step [13200/27733], Loss: 2.5727\n",
      "Epoch [41/300], Step [13300/27733], Loss: 2.2514\n",
      "Epoch [41/300], Step [13400/27733], Loss: 2.8651\n",
      "Epoch [41/300], Step [13500/27733], Loss: 3.0198\n",
      "Epoch [41/300], Step [13600/27733], Loss: 3.6800\n",
      "Epoch [41/300], Step [13700/27733], Loss: 3.4629\n",
      "Epoch [41/300], Step [13800/27733], Loss: 2.7767\n",
      "Epoch [41/300], Step [13900/27733], Loss: 3.2024\n",
      "Epoch [41/300], Step [14000/27733], Loss: 3.2505\n",
      "Epoch [41/300], Step [14100/27733], Loss: 2.6130\n",
      "Epoch [41/300], Step [14200/27733], Loss: 2.6444\n",
      "Epoch [41/300], Step [14300/27733], Loss: 3.4715\n",
      "Epoch [41/300], Step [14400/27733], Loss: 3.1731\n",
      "Epoch [41/300], Step [14500/27733], Loss: 2.8351\n",
      "Epoch [41/300], Step [14600/27733], Loss: 2.8152\n",
      "Epoch [41/300], Step [14700/27733], Loss: 2.8291\n",
      "Epoch [41/300], Step [14800/27733], Loss: 2.4912\n",
      "Epoch [41/300], Step [14900/27733], Loss: 3.2325\n",
      "Epoch [41/300], Step [15000/27733], Loss: 2.1900\n",
      "Epoch [41/300], Step [15100/27733], Loss: 3.4930\n",
      "Epoch [41/300], Step [15200/27733], Loss: 2.1625\n",
      "Epoch [41/300], Step [15300/27733], Loss: 3.5291\n",
      "Epoch [41/300], Step [15400/27733], Loss: 3.5971\n",
      "Epoch [41/300], Step [15500/27733], Loss: 2.5830\n",
      "Epoch [41/300], Step [15600/27733], Loss: 3.3268\n",
      "Epoch [41/300], Step [15700/27733], Loss: 3.1651\n",
      "Epoch [41/300], Step [15800/27733], Loss: 2.0362\n",
      "Epoch [41/300], Step [15900/27733], Loss: 2.9564\n",
      "Epoch [41/300], Step [16000/27733], Loss: 3.6576\n",
      "Epoch [41/300], Step [16100/27733], Loss: 2.4070\n",
      "Epoch [41/300], Step [16200/27733], Loss: 2.8570\n",
      "Epoch [41/300], Step [16300/27733], Loss: 2.6450\n",
      "Epoch [41/300], Step [16400/27733], Loss: 3.0191\n",
      "Epoch [41/300], Step [16500/27733], Loss: 2.6911\n",
      "Epoch [41/300], Step [16600/27733], Loss: 2.6485\n",
      "Epoch [41/300], Step [16700/27733], Loss: 2.3911\n",
      "Epoch [41/300], Step [16800/27733], Loss: 2.8955\n",
      "Epoch [41/300], Step [16900/27733], Loss: 2.8320\n",
      "Epoch [41/300], Step [17000/27733], Loss: 2.5868\n",
      "Epoch [41/300], Step [17100/27733], Loss: 3.8359\n",
      "Epoch [41/300], Step [17200/27733], Loss: 2.8891\n",
      "Epoch [41/300], Step [17300/27733], Loss: 3.4999\n",
      "Epoch [41/300], Step [17400/27733], Loss: 2.7241\n",
      "Epoch [41/300], Step [17500/27733], Loss: 2.8782\n",
      "Epoch [41/300], Step [17600/27733], Loss: 2.4797\n",
      "Epoch [41/300], Step [17700/27733], Loss: 3.2285\n",
      "Epoch [41/300], Step [17800/27733], Loss: 3.8881\n",
      "Epoch [41/300], Step [17900/27733], Loss: 2.3235\n",
      "Epoch [41/300], Step [18000/27733], Loss: 2.8206\n",
      "Epoch [41/300], Step [18100/27733], Loss: 2.7781\n",
      "Epoch [41/300], Step [18200/27733], Loss: 3.0908\n",
      "Epoch [41/300], Step [18300/27733], Loss: 2.1693\n",
      "Epoch [41/300], Step [18400/27733], Loss: 2.8734\n",
      "Epoch [41/300], Step [18500/27733], Loss: 2.7731\n",
      "Epoch [41/300], Step [18600/27733], Loss: 3.2796\n",
      "Epoch [41/300], Step [18700/27733], Loss: 2.5636\n",
      "Epoch [41/300], Step [18800/27733], Loss: 3.0252\n",
      "Epoch [41/300], Step [18900/27733], Loss: 3.0380\n",
      "Epoch [41/300], Step [19000/27733], Loss: 2.7502\n",
      "Epoch [41/300], Step [19100/27733], Loss: 3.1278\n",
      "Epoch [41/300], Step [19200/27733], Loss: 3.2008\n",
      "Epoch [41/300], Step [19300/27733], Loss: 3.6985\n",
      "Epoch [41/300], Step [19400/27733], Loss: 3.1809\n",
      "Epoch [41/300], Step [19500/27733], Loss: 3.0716\n",
      "Epoch [41/300], Step [19600/27733], Loss: 2.4290\n",
      "Epoch [41/300], Step [19700/27733], Loss: 2.6018\n",
      "Epoch [41/300], Step [19800/27733], Loss: 2.8186\n",
      "Epoch [41/300], Step [19900/27733], Loss: 2.5142\n",
      "Epoch [41/300], Step [20000/27733], Loss: 3.5814\n",
      "Epoch [41/300], Step [20100/27733], Loss: 2.6009\n",
      "Epoch [41/300], Step [20200/27733], Loss: 2.6054\n",
      "Epoch [41/300], Step [20300/27733], Loss: 4.0478\n",
      "Epoch [41/300], Step [20400/27733], Loss: 2.6929\n",
      "Epoch [41/300], Step [20500/27733], Loss: 2.7396\n",
      "Epoch [41/300], Step [20600/27733], Loss: 3.2588\n",
      "Epoch [41/300], Step [20700/27733], Loss: 2.9744\n",
      "Epoch [41/300], Step [20800/27733], Loss: 3.2732\n",
      "Epoch [41/300], Step [20900/27733], Loss: 2.3283\n",
      "Epoch [41/300], Step [21000/27733], Loss: 3.3775\n",
      "Epoch [41/300], Step [21100/27733], Loss: 2.4073\n",
      "Epoch [41/300], Step [21200/27733], Loss: 3.1600\n",
      "Epoch [41/300], Step [21300/27733], Loss: 2.2819\n",
      "Epoch [41/300], Step [21400/27733], Loss: 3.2301\n",
      "Epoch [41/300], Step [21500/27733], Loss: 3.2731\n",
      "Epoch [41/300], Step [21600/27733], Loss: 2.6314\n",
      "Epoch [41/300], Step [21700/27733], Loss: 3.1786\n",
      "Epoch [41/300], Step [21800/27733], Loss: 2.8490\n",
      "Epoch [41/300], Step [21900/27733], Loss: 3.3891\n",
      "Epoch [41/300], Step [22000/27733], Loss: 3.6560\n",
      "Epoch [41/300], Step [22100/27733], Loss: 2.8526\n",
      "Epoch [41/300], Step [22200/27733], Loss: 3.1537\n",
      "Epoch [41/300], Step [22300/27733], Loss: 3.4328\n",
      "Epoch [41/300], Step [22400/27733], Loss: 3.1492\n",
      "Epoch [41/300], Step [22500/27733], Loss: 2.3682\n",
      "Epoch [41/300], Step [22600/27733], Loss: 3.8821\n",
      "Epoch [41/300], Step [22700/27733], Loss: 3.7745\n",
      "Epoch [41/300], Step [22800/27733], Loss: 3.1326\n",
      "Epoch [41/300], Step [22900/27733], Loss: 2.5546\n",
      "Epoch [41/300], Step [23000/27733], Loss: 3.4262\n",
      "Epoch [41/300], Step [23100/27733], Loss: 2.6854\n",
      "Epoch [41/300], Step [23200/27733], Loss: 3.5270\n",
      "Epoch [41/300], Step [23300/27733], Loss: 2.6389\n",
      "Epoch [41/300], Step [23400/27733], Loss: 3.1568\n",
      "Epoch [41/300], Step [23500/27733], Loss: 2.9507\n",
      "Epoch [41/300], Step [23600/27733], Loss: 2.8751\n",
      "Epoch [41/300], Step [23700/27733], Loss: 2.5195\n",
      "Epoch [41/300], Step [23800/27733], Loss: 3.3528\n",
      "Epoch [41/300], Step [23900/27733], Loss: 2.9239\n",
      "Epoch [41/300], Step [24000/27733], Loss: 2.9595\n",
      "Epoch [41/300], Step [24100/27733], Loss: 3.1131\n",
      "Epoch [41/300], Step [24200/27733], Loss: 2.4417\n",
      "Epoch [41/300], Step [24300/27733], Loss: 2.5558\n",
      "Epoch [41/300], Step [24400/27733], Loss: 3.3337\n",
      "Epoch [41/300], Step [24500/27733], Loss: 2.4600\n",
      "Epoch [41/300], Step [24600/27733], Loss: 2.3670\n",
      "Epoch [41/300], Step [24700/27733], Loss: 2.7368\n",
      "Epoch [41/300], Step [24800/27733], Loss: 2.8226\n",
      "Epoch [41/300], Step [24900/27733], Loss: 2.5843\n",
      "Epoch [41/300], Step [25000/27733], Loss: 2.8995\n",
      "Epoch [41/300], Step [25100/27733], Loss: 2.5148\n",
      "Epoch [41/300], Step [25200/27733], Loss: 2.7284\n",
      "Epoch [41/300], Step [25300/27733], Loss: 3.4759\n",
      "Epoch [41/300], Step [25400/27733], Loss: 3.2924\n",
      "Epoch [41/300], Step [25500/27733], Loss: 2.7807\n",
      "Epoch [41/300], Step [25600/27733], Loss: 3.0155\n",
      "Epoch [41/300], Step [25700/27733], Loss: 2.3821\n",
      "Epoch [41/300], Step [25800/27733], Loss: 2.4277\n",
      "Epoch [41/300], Step [25900/27733], Loss: 2.8444\n",
      "Epoch [41/300], Step [26000/27733], Loss: 2.6325\n",
      "Epoch [41/300], Step [26100/27733], Loss: 3.5666\n",
      "Epoch [41/300], Step [26200/27733], Loss: 3.7367\n",
      "Epoch [41/300], Step [26300/27733], Loss: 2.4019\n",
      "Epoch [41/300], Step [26400/27733], Loss: 2.5769\n",
      "Epoch [41/300], Step [26500/27733], Loss: 2.6585\n",
      "Epoch [41/300], Step [26600/27733], Loss: 3.5833\n",
      "Epoch [41/300], Step [26700/27733], Loss: 3.9279\n",
      "Epoch [41/300], Step [26800/27733], Loss: 3.2247\n",
      "Epoch [41/300], Step [26900/27733], Loss: 3.0354\n",
      "Epoch [41/300], Step [27000/27733], Loss: 3.4555\n",
      "Epoch [41/300], Step [27100/27733], Loss: 3.0271\n",
      "Epoch [41/300], Step [27200/27733], Loss: 2.1599\n",
      "Epoch [41/300], Step [27300/27733], Loss: 3.1358\n",
      "Epoch [41/300], Step [27400/27733], Loss: 3.5796\n",
      "Epoch [41/300], Step [27500/27733], Loss: 2.6967\n",
      "Epoch [41/300], Step [27600/27733], Loss: 2.9282\n",
      "Epoch [41/300], Step [27700/27733], Loss: 3.2641\n",
      "Epoch [42/300], Step [100/27733], Loss: 2.3584\n",
      "Epoch [42/300], Step [200/27733], Loss: 2.3974\n",
      "Epoch [42/300], Step [300/27733], Loss: 2.2396\n",
      "Epoch [42/300], Step [400/27733], Loss: 1.8508\n",
      "Epoch [42/300], Step [500/27733], Loss: 2.3698\n",
      "Epoch [42/300], Step [600/27733], Loss: 2.9666\n",
      "Epoch [42/300], Step [700/27733], Loss: 2.5163\n",
      "Epoch [42/300], Step [800/27733], Loss: 2.5156\n",
      "Epoch [42/300], Step [900/27733], Loss: 2.4493\n",
      "Epoch [42/300], Step [1000/27733], Loss: 2.9045\n",
      "Epoch [42/300], Step [1100/27733], Loss: 2.9982\n",
      "Epoch [42/300], Step [1200/27733], Loss: 3.0912\n",
      "Epoch [42/300], Step [1300/27733], Loss: 2.5928\n",
      "Epoch [42/300], Step [1400/27733], Loss: 2.2223\n",
      "Epoch [42/300], Step [1500/27733], Loss: 2.0752\n",
      "Epoch [42/300], Step [1600/27733], Loss: 2.1953\n",
      "Epoch [42/300], Step [1700/27733], Loss: 2.1995\n",
      "Epoch [42/300], Step [1800/27733], Loss: 3.4673\n",
      "Epoch [42/300], Step [1900/27733], Loss: 2.7956\n",
      "Epoch [42/300], Step [2000/27733], Loss: 2.3418\n",
      "Epoch [42/300], Step [2100/27733], Loss: 2.2422\n",
      "Epoch [42/300], Step [2200/27733], Loss: 2.4652\n",
      "Epoch [42/300], Step [2300/27733], Loss: 2.5607\n",
      "Epoch [42/300], Step [2400/27733], Loss: 3.4180\n",
      "Epoch [42/300], Step [2500/27733], Loss: 2.5411\n",
      "Epoch [42/300], Step [2600/27733], Loss: 2.0923\n",
      "Epoch [42/300], Step [2700/27733], Loss: 2.4227\n",
      "Epoch [42/300], Step [2800/27733], Loss: 2.8805\n",
      "Epoch [42/300], Step [2900/27733], Loss: 2.2676\n",
      "Epoch [42/300], Step [3000/27733], Loss: 2.3532\n",
      "Epoch [42/300], Step [3100/27733], Loss: 2.3748\n",
      "Epoch [42/300], Step [3200/27733], Loss: 2.6375\n",
      "Epoch [42/300], Step [3300/27733], Loss: 1.9692\n",
      "Epoch [42/300], Step [3400/27733], Loss: 2.6185\n",
      "Epoch [42/300], Step [3500/27733], Loss: 2.3418\n",
      "Epoch [42/300], Step [3600/27733], Loss: 1.7096\n",
      "Epoch [42/300], Step [3700/27733], Loss: 2.5560\n",
      "Epoch [42/300], Step [3800/27733], Loss: 2.8041\n",
      "Epoch [42/300], Step [3900/27733], Loss: 2.9074\n",
      "Epoch [42/300], Step [4000/27733], Loss: 2.2710\n",
      "Epoch [42/300], Step [4100/27733], Loss: 2.7626\n",
      "Epoch [42/300], Step [4200/27733], Loss: 3.3175\n",
      "Epoch [42/300], Step [4300/27733], Loss: 2.4470\n",
      "Epoch [42/300], Step [4400/27733], Loss: 2.1300\n",
      "Epoch [42/300], Step [4500/27733], Loss: 2.8071\n",
      "Epoch [42/300], Step [4600/27733], Loss: 3.3179\n",
      "Epoch [42/300], Step [4700/27733], Loss: 2.9664\n",
      "Epoch [42/300], Step [4800/27733], Loss: 2.4791\n",
      "Epoch [42/300], Step [4900/27733], Loss: 2.0719\n",
      "Epoch [42/300], Step [5000/27733], Loss: 2.5230\n",
      "Epoch [42/300], Step [5100/27733], Loss: 2.8865\n",
      "Epoch [42/300], Step [5200/27733], Loss: 2.1716\n",
      "Epoch [42/300], Step [5300/27733], Loss: 2.2168\n",
      "Epoch [42/300], Step [5400/27733], Loss: 2.5564\n",
      "Epoch [42/300], Step [5500/27733], Loss: 1.8825\n",
      "Epoch [42/300], Step [5600/27733], Loss: 2.3244\n",
      "Epoch [42/300], Step [5700/27733], Loss: 2.4294\n",
      "Epoch [42/300], Step [5800/27733], Loss: 3.2694\n",
      "Epoch [42/300], Step [5900/27733], Loss: 2.9466\n",
      "Epoch [42/300], Step [6000/27733], Loss: 2.6864\n",
      "Epoch [42/300], Step [6100/27733], Loss: 2.9483\n",
      "Epoch [42/300], Step [6200/27733], Loss: 2.5280\n",
      "Epoch [42/300], Step [6300/27733], Loss: 2.6280\n",
      "Epoch [42/300], Step [6400/27733], Loss: 2.2879\n",
      "Epoch [42/300], Step [6500/27733], Loss: 2.2707\n",
      "Epoch [42/300], Step [6600/27733], Loss: 2.2310\n",
      "Epoch [42/300], Step [6700/27733], Loss: 2.1886\n",
      "Epoch [42/300], Step [6800/27733], Loss: 2.9217\n",
      "Epoch [42/300], Step [6900/27733], Loss: 2.6596\n",
      "Epoch [42/300], Step [7000/27733], Loss: 3.0916\n",
      "Epoch [42/300], Step [7100/27733], Loss: 2.1541\n",
      "Epoch [42/300], Step [7200/27733], Loss: 1.9692\n",
      "Epoch [42/300], Step [7300/27733], Loss: 2.7921\n",
      "Epoch [42/300], Step [7400/27733], Loss: 2.8618\n",
      "Epoch [42/300], Step [7500/27733], Loss: 2.5465\n",
      "Epoch [42/300], Step [7600/27733], Loss: 1.9236\n",
      "Epoch [42/300], Step [7700/27733], Loss: 2.8699\n",
      "Epoch [42/300], Step [7800/27733], Loss: 2.3960\n",
      "Epoch [42/300], Step [7900/27733], Loss: 2.6493\n",
      "Epoch [42/300], Step [8000/27733], Loss: 3.3880\n",
      "Epoch [42/300], Step [8100/27733], Loss: 2.0635\n",
      "Epoch [42/300], Step [8200/27733], Loss: 2.8545\n",
      "Epoch [42/300], Step [8300/27733], Loss: 2.6320\n",
      "Epoch [42/300], Step [8400/27733], Loss: 2.5053\n",
      "Epoch [42/300], Step [8500/27733], Loss: 2.4923\n",
      "Epoch [42/300], Step [8600/27733], Loss: 3.0905\n",
      "Epoch [42/300], Step [8700/27733], Loss: 2.2903\n",
      "Epoch [42/300], Step [8800/27733], Loss: 2.4828\n",
      "Epoch [42/300], Step [8900/27733], Loss: 2.4828\n",
      "Epoch [42/300], Step [9000/27733], Loss: 3.4266\n",
      "Epoch [42/300], Step [9100/27733], Loss: 2.0585\n",
      "Epoch [42/300], Step [9200/27733], Loss: 2.5438\n",
      "Epoch [42/300], Step [9300/27733], Loss: 2.2897\n",
      "Epoch [42/300], Step [9400/27733], Loss: 3.3753\n",
      "Epoch [42/300], Step [9500/27733], Loss: 2.9582\n",
      "Epoch [42/300], Step [9600/27733], Loss: 3.4086\n",
      "Epoch [42/300], Step [9700/27733], Loss: 3.2122\n",
      "Epoch [42/300], Step [9800/27733], Loss: 3.2792\n",
      "Epoch [42/300], Step [9900/27733], Loss: 2.4659\n",
      "Epoch [42/300], Step [10000/27733], Loss: 2.8400\n",
      "Epoch [42/300], Step [10100/27733], Loss: 2.5876\n",
      "Epoch [42/300], Step [10200/27733], Loss: 2.6795\n",
      "Epoch [42/300], Step [10300/27733], Loss: 2.8626\n",
      "Epoch [42/300], Step [10400/27733], Loss: 1.8929\n",
      "Epoch [42/300], Step [10500/27733], Loss: 2.5840\n",
      "Epoch [42/300], Step [10600/27733], Loss: 3.0904\n",
      "Epoch [42/300], Step [10700/27733], Loss: 2.8801\n",
      "Epoch [42/300], Step [10800/27733], Loss: 2.6712\n",
      "Epoch [42/300], Step [10900/27733], Loss: 2.3461\n",
      "Epoch [42/300], Step [11000/27733], Loss: 3.4297\n",
      "Epoch [42/300], Step [11100/27733], Loss: 2.4974\n",
      "Epoch [42/300], Step [11200/27733], Loss: 2.5976\n",
      "Epoch [42/300], Step [11300/27733], Loss: 3.4570\n",
      "Epoch [42/300], Step [11400/27733], Loss: 2.0503\n",
      "Epoch [42/300], Step [11500/27733], Loss: 2.1222\n",
      "Epoch [42/300], Step [11600/27733], Loss: 2.5615\n",
      "Epoch [42/300], Step [11700/27733], Loss: 2.5899\n",
      "Epoch [42/300], Step [11800/27733], Loss: 2.2498\n",
      "Epoch [42/300], Step [11900/27733], Loss: 3.4561\n",
      "Epoch [42/300], Step [12000/27733], Loss: 4.0827\n",
      "Epoch [42/300], Step [12100/27733], Loss: 3.1612\n",
      "Epoch [42/300], Step [12200/27733], Loss: 3.0778\n",
      "Epoch [42/300], Step [12300/27733], Loss: 2.5147\n",
      "Epoch [42/300], Step [12400/27733], Loss: 2.9374\n",
      "Epoch [42/300], Step [12500/27733], Loss: 2.7773\n",
      "Epoch [42/300], Step [12600/27733], Loss: 2.6586\n",
      "Epoch [42/300], Step [12700/27733], Loss: 2.2102\n",
      "Epoch [42/300], Step [12800/27733], Loss: 2.9579\n",
      "Epoch [42/300], Step [12900/27733], Loss: 2.6564\n",
      "Epoch [42/300], Step [13000/27733], Loss: 2.4669\n",
      "Epoch [42/300], Step [13100/27733], Loss: 3.4019\n",
      "Epoch [42/300], Step [13200/27733], Loss: 2.6965\n",
      "Epoch [42/300], Step [13300/27733], Loss: 4.0210\n",
      "Epoch [42/300], Step [13400/27733], Loss: 2.3395\n",
      "Epoch [42/300], Step [13500/27733], Loss: 4.4936\n",
      "Epoch [42/300], Step [13600/27733], Loss: 2.2792\n",
      "Epoch [42/300], Step [13700/27733], Loss: 2.9539\n",
      "Epoch [42/300], Step [13800/27733], Loss: 2.9925\n",
      "Epoch [42/300], Step [13900/27733], Loss: 2.4264\n",
      "Epoch [42/300], Step [14000/27733], Loss: 2.4021\n",
      "Epoch [42/300], Step [14100/27733], Loss: 2.4199\n",
      "Epoch [42/300], Step [14200/27733], Loss: 3.0045\n",
      "Epoch [42/300], Step [14300/27733], Loss: 2.6478\n",
      "Epoch [42/300], Step [14400/27733], Loss: 2.9459\n",
      "Epoch [42/300], Step [14500/27733], Loss: 2.0893\n",
      "Epoch [42/300], Step [14600/27733], Loss: 2.5364\n",
      "Epoch [42/300], Step [14700/27733], Loss: 2.7092\n",
      "Epoch [42/300], Step [14800/27733], Loss: 2.7473\n",
      "Epoch [42/300], Step [14900/27733], Loss: 1.9918\n",
      "Epoch [42/300], Step [15000/27733], Loss: 3.1198\n",
      "Epoch [42/300], Step [15100/27733], Loss: 2.3619\n",
      "Epoch [42/300], Step [15200/27733], Loss: 4.2253\n",
      "Epoch [42/300], Step [15300/27733], Loss: 2.4818\n",
      "Epoch [42/300], Step [15400/27733], Loss: 2.0651\n",
      "Epoch [42/300], Step [15500/27733], Loss: 2.8823\n",
      "Epoch [42/300], Step [15600/27733], Loss: 2.7534\n",
      "Epoch [42/300], Step [15700/27733], Loss: 2.8213\n",
      "Epoch [42/300], Step [15800/27733], Loss: 2.4235\n",
      "Epoch [42/300], Step [15900/27733], Loss: 3.1748\n",
      "Epoch [42/300], Step [16000/27733], Loss: 3.2380\n",
      "Epoch [42/300], Step [16100/27733], Loss: 3.0235\n",
      "Epoch [42/300], Step [16200/27733], Loss: 3.4015\n",
      "Epoch [42/300], Step [16300/27733], Loss: 3.2641\n",
      "Epoch [42/300], Step [16400/27733], Loss: 3.4781\n",
      "Epoch [42/300], Step [16500/27733], Loss: 2.2623\n",
      "Epoch [42/300], Step [16600/27733], Loss: 2.7826\n",
      "Epoch [42/300], Step [16700/27733], Loss: 2.8303\n",
      "Epoch [42/300], Step [16800/27733], Loss: 2.8237\n",
      "Epoch [42/300], Step [16900/27733], Loss: 2.6692\n",
      "Epoch [42/300], Step [17000/27733], Loss: 2.4147\n",
      "Epoch [42/300], Step [17100/27733], Loss: 3.1517\n",
      "Epoch [42/300], Step [17200/27733], Loss: 4.2357\n",
      "Epoch [42/300], Step [17300/27733], Loss: 2.7025\n",
      "Epoch [42/300], Step [17400/27733], Loss: 2.4263\n",
      "Epoch [42/300], Step [17500/27733], Loss: 2.9048\n",
      "Epoch [42/300], Step [17600/27733], Loss: 2.1885\n",
      "Epoch [42/300], Step [17700/27733], Loss: 2.5618\n",
      "Epoch [42/300], Step [17800/27733], Loss: 3.3210\n",
      "Epoch [42/300], Step [17900/27733], Loss: 2.6786\n",
      "Epoch [42/300], Step [18000/27733], Loss: 3.0644\n",
      "Epoch [42/300], Step [18100/27733], Loss: 2.2290\n",
      "Epoch [42/300], Step [18200/27733], Loss: 2.8968\n",
      "Epoch [42/300], Step [18300/27733], Loss: 3.2334\n",
      "Epoch [42/300], Step [18400/27733], Loss: 2.3834\n",
      "Epoch [42/300], Step [18500/27733], Loss: 2.1957\n",
      "Epoch [42/300], Step [18600/27733], Loss: 2.7172\n",
      "Epoch [42/300], Step [18700/27733], Loss: 2.8073\n",
      "Epoch [42/300], Step [18800/27733], Loss: 3.0538\n",
      "Epoch [42/300], Step [18900/27733], Loss: 2.5917\n",
      "Epoch [42/300], Step [19000/27733], Loss: 3.3156\n",
      "Epoch [42/300], Step [19100/27733], Loss: 2.9046\n",
      "Epoch [42/300], Step [19200/27733], Loss: 2.9448\n",
      "Epoch [42/300], Step [19300/27733], Loss: 3.0372\n",
      "Epoch [42/300], Step [19400/27733], Loss: 2.8239\n",
      "Epoch [42/300], Step [19500/27733], Loss: 3.5980\n",
      "Epoch [42/300], Step [19600/27733], Loss: 2.9480\n",
      "Epoch [42/300], Step [19700/27733], Loss: 2.4472\n",
      "Epoch [42/300], Step [19800/27733], Loss: 3.0378\n",
      "Epoch [42/300], Step [19900/27733], Loss: 3.1833\n",
      "Epoch [42/300], Step [20000/27733], Loss: 3.3270\n",
      "Epoch [42/300], Step [20100/27733], Loss: 2.3530\n",
      "Epoch [42/300], Step [20200/27733], Loss: 2.7626\n",
      "Epoch [42/300], Step [20300/27733], Loss: 4.2620\n",
      "Epoch [42/300], Step [20400/27733], Loss: 2.2043\n",
      "Epoch [42/300], Step [20500/27733], Loss: 2.6826\n",
      "Epoch [42/300], Step [20600/27733], Loss: 2.5496\n",
      "Epoch [42/300], Step [20700/27733], Loss: 3.2192\n",
      "Epoch [42/300], Step [20800/27733], Loss: 2.9432\n",
      "Epoch [42/300], Step [20900/27733], Loss: 3.7632\n",
      "Epoch [42/300], Step [21000/27733], Loss: 3.6560\n",
      "Epoch [42/300], Step [21100/27733], Loss: 3.2922\n",
      "Epoch [42/300], Step [21200/27733], Loss: 4.2085\n",
      "Epoch [42/300], Step [21300/27733], Loss: 2.8652\n",
      "Epoch [42/300], Step [21400/27733], Loss: 3.5471\n",
      "Epoch [42/300], Step [21500/27733], Loss: 2.5773\n",
      "Epoch [42/300], Step [21600/27733], Loss: 2.9087\n",
      "Epoch [42/300], Step [21700/27733], Loss: 3.6624\n",
      "Epoch [42/300], Step [21800/27733], Loss: 3.1570\n",
      "Epoch [42/300], Step [21900/27733], Loss: 2.4535\n",
      "Epoch [42/300], Step [22000/27733], Loss: 2.6002\n",
      "Epoch [42/300], Step [22100/27733], Loss: 1.8235\n",
      "Epoch [42/300], Step [22200/27733], Loss: 2.0912\n",
      "Epoch [42/300], Step [22300/27733], Loss: 3.4734\n",
      "Epoch [42/300], Step [22400/27733], Loss: 3.4072\n",
      "Epoch [42/300], Step [22500/27733], Loss: 2.2602\n",
      "Epoch [42/300], Step [22600/27733], Loss: 2.4003\n",
      "Epoch [42/300], Step [22700/27733], Loss: 2.7248\n",
      "Epoch [42/300], Step [22800/27733], Loss: 2.9687\n",
      "Epoch [42/300], Step [22900/27733], Loss: 2.9727\n",
      "Epoch [42/300], Step [23000/27733], Loss: 3.0592\n",
      "Epoch [42/300], Step [23100/27733], Loss: 3.1913\n",
      "Epoch [42/300], Step [23200/27733], Loss: 2.3960\n",
      "Epoch [42/300], Step [23300/27733], Loss: 2.8884\n",
      "Epoch [42/300], Step [23400/27733], Loss: 3.5194\n",
      "Epoch [42/300], Step [23500/27733], Loss: 2.8970\n",
      "Epoch [42/300], Step [23600/27733], Loss: 2.6245\n",
      "Epoch [42/300], Step [23700/27733], Loss: 2.7874\n",
      "Epoch [42/300], Step [23800/27733], Loss: 2.8931\n",
      "Epoch [42/300], Step [23900/27733], Loss: 2.5833\n",
      "Epoch [42/300], Step [24000/27733], Loss: 3.1345\n",
      "Epoch [42/300], Step [24100/27733], Loss: 3.3089\n",
      "Epoch [42/300], Step [24200/27733], Loss: 2.9952\n",
      "Epoch [42/300], Step [24300/27733], Loss: 3.3999\n",
      "Epoch [42/300], Step [24400/27733], Loss: 2.2005\n",
      "Epoch [42/300], Step [24500/27733], Loss: 3.0802\n",
      "Epoch [42/300], Step [24600/27733], Loss: 2.6777\n",
      "Epoch [42/300], Step [24700/27733], Loss: 2.8984\n",
      "Epoch [42/300], Step [24800/27733], Loss: 3.2314\n",
      "Epoch [42/300], Step [24900/27733], Loss: 3.2950\n",
      "Epoch [42/300], Step [25000/27733], Loss: 3.9438\n",
      "Epoch [42/300], Step [25100/27733], Loss: 2.9942\n",
      "Epoch [42/300], Step [25200/27733], Loss: 3.5922\n",
      "Epoch [42/300], Step [25300/27733], Loss: 2.8816\n",
      "Epoch [42/300], Step [25400/27733], Loss: 2.8354\n",
      "Epoch [42/300], Step [25500/27733], Loss: 3.1543\n",
      "Epoch [42/300], Step [25600/27733], Loss: 2.8968\n",
      "Epoch [42/300], Step [25700/27733], Loss: 2.4531\n",
      "Epoch [42/300], Step [25800/27733], Loss: 2.7507\n",
      "Epoch [42/300], Step [25900/27733], Loss: 3.7473\n",
      "Epoch [42/300], Step [26000/27733], Loss: 2.8065\n",
      "Epoch [42/300], Step [26100/27733], Loss: 2.5766\n",
      "Epoch [42/300], Step [26200/27733], Loss: 2.7832\n",
      "Epoch [42/300], Step [26300/27733], Loss: 3.2535\n",
      "Epoch [42/300], Step [26400/27733], Loss: 2.5927\n",
      "Epoch [42/300], Step [26500/27733], Loss: 2.7808\n",
      "Epoch [42/300], Step [26600/27733], Loss: 3.3510\n",
      "Epoch [42/300], Step [26700/27733], Loss: 2.6013\n",
      "Epoch [42/300], Step [26800/27733], Loss: 2.8292\n",
      "Epoch [42/300], Step [26900/27733], Loss: 3.5742\n",
      "Epoch [42/300], Step [27000/27733], Loss: 2.3896\n",
      "Epoch [42/300], Step [27100/27733], Loss: 3.7588\n",
      "Epoch [42/300], Step [27200/27733], Loss: 2.7210\n",
      "Epoch [42/300], Step [27300/27733], Loss: 3.5446\n",
      "Epoch [42/300], Step [27400/27733], Loss: 2.8568\n",
      "Epoch [42/300], Step [27500/27733], Loss: 3.1403\n",
      "Epoch [42/300], Step [27600/27733], Loss: 3.6075\n",
      "Epoch [42/300], Step [27700/27733], Loss: 2.5424\n",
      "Epoch [43/300], Step [100/27733], Loss: 2.2795\n",
      "Epoch [43/300], Step [200/27733], Loss: 2.1651\n",
      "Epoch [43/300], Step [300/27733], Loss: 2.3462\n",
      "Epoch [43/300], Step [400/27733], Loss: 1.9451\n",
      "Epoch [43/300], Step [500/27733], Loss: 2.0749\n",
      "Epoch [43/300], Step [600/27733], Loss: 2.8907\n",
      "Epoch [43/300], Step [700/27733], Loss: 2.6299\n",
      "Epoch [43/300], Step [800/27733], Loss: 1.7776\n",
      "Epoch [43/300], Step [900/27733], Loss: 2.1376\n",
      "Epoch [43/300], Step [1000/27733], Loss: 2.7495\n",
      "Epoch [43/300], Step [1100/27733], Loss: 2.1570\n",
      "Epoch [43/300], Step [1200/27733], Loss: 2.5344\n",
      "Epoch [43/300], Step [1300/27733], Loss: 2.1920\n",
      "Epoch [43/300], Step [1400/27733], Loss: 2.4904\n",
      "Epoch [43/300], Step [1500/27733], Loss: 3.0415\n",
      "Epoch [43/300], Step [1600/27733], Loss: 2.6038\n",
      "Epoch [43/300], Step [1700/27733], Loss: 3.1771\n",
      "Epoch [43/300], Step [1800/27733], Loss: 2.1803\n",
      "Epoch [43/300], Step [1900/27733], Loss: 2.4778\n",
      "Epoch [43/300], Step [2000/27733], Loss: 1.4580\n",
      "Epoch [43/300], Step [2100/27733], Loss: 2.6168\n",
      "Epoch [43/300], Step [2200/27733], Loss: 2.3290\n",
      "Epoch [43/300], Step [2300/27733], Loss: 2.7701\n",
      "Epoch [43/300], Step [2400/27733], Loss: 2.3850\n",
      "Epoch [43/300], Step [2500/27733], Loss: 2.4952\n",
      "Epoch [43/300], Step [2600/27733], Loss: 2.4218\n",
      "Epoch [43/300], Step [2700/27733], Loss: 2.3000\n",
      "Epoch [43/300], Step [2800/27733], Loss: 2.7414\n",
      "Epoch [43/300], Step [2900/27733], Loss: 2.7382\n",
      "Epoch [43/300], Step [3000/27733], Loss: 2.9822\n",
      "Epoch [43/300], Step [3100/27733], Loss: 2.5887\n",
      "Epoch [43/300], Step [3200/27733], Loss: 3.2689\n",
      "Epoch [43/300], Step [3300/27733], Loss: 2.5688\n",
      "Epoch [43/300], Step [3400/27733], Loss: 2.3606\n",
      "Epoch [43/300], Step [3500/27733], Loss: 2.4181\n",
      "Epoch [43/300], Step [3600/27733], Loss: 2.3285\n",
      "Epoch [43/300], Step [3700/27733], Loss: 2.7734\n",
      "Epoch [43/300], Step [3800/27733], Loss: 2.7616\n",
      "Epoch [43/300], Step [3900/27733], Loss: 2.6147\n",
      "Epoch [43/300], Step [4000/27733], Loss: 3.2469\n",
      "Epoch [43/300], Step [4100/27733], Loss: 2.4629\n",
      "Epoch [43/300], Step [4200/27733], Loss: 2.3053\n",
      "Epoch [43/300], Step [4300/27733], Loss: 2.2800\n",
      "Epoch [43/300], Step [4400/27733], Loss: 2.1385\n",
      "Epoch [43/300], Step [4500/27733], Loss: 2.2145\n",
      "Epoch [43/300], Step [4600/27733], Loss: 1.8418\n",
      "Epoch [43/300], Step [4700/27733], Loss: 2.8652\n",
      "Epoch [43/300], Step [4800/27733], Loss: 2.5325\n",
      "Epoch [43/300], Step [4900/27733], Loss: 2.2109\n",
      "Epoch [43/300], Step [5000/27733], Loss: 3.2042\n",
      "Epoch [43/300], Step [5100/27733], Loss: 2.1897\n",
      "Epoch [43/300], Step [5200/27733], Loss: 3.1517\n",
      "Epoch [43/300], Step [5300/27733], Loss: 2.2128\n",
      "Epoch [43/300], Step [5400/27733], Loss: 2.9245\n",
      "Epoch [43/300], Step [5500/27733], Loss: 2.0783\n",
      "Epoch [43/300], Step [5600/27733], Loss: 3.3297\n",
      "Epoch [43/300], Step [5700/27733], Loss: 2.2078\n",
      "Epoch [43/300], Step [5800/27733], Loss: 2.7846\n",
      "Epoch [43/300], Step [5900/27733], Loss: 2.2333\n",
      "Epoch [43/300], Step [6000/27733], Loss: 2.1064\n",
      "Epoch [43/300], Step [6100/27733], Loss: 2.2883\n",
      "Epoch [43/300], Step [6200/27733], Loss: 2.3399\n",
      "Epoch [43/300], Step [6300/27733], Loss: 2.2319\n",
      "Epoch [43/300], Step [6400/27733], Loss: 2.7765\n",
      "Epoch [43/300], Step [6500/27733], Loss: 2.5255\n",
      "Epoch [43/300], Step [6600/27733], Loss: 2.5107\n",
      "Epoch [43/300], Step [6700/27733], Loss: 2.5881\n",
      "Epoch [43/300], Step [6800/27733], Loss: 1.6987\n",
      "Epoch [43/300], Step [6900/27733], Loss: 2.5081\n",
      "Epoch [43/300], Step [7000/27733], Loss: 2.9633\n",
      "Epoch [43/300], Step [7100/27733], Loss: 3.0670\n",
      "Epoch [43/300], Step [7200/27733], Loss: 2.4077\n",
      "Epoch [43/300], Step [7300/27733], Loss: 3.2905\n",
      "Epoch [43/300], Step [7400/27733], Loss: 3.4448\n",
      "Epoch [43/300], Step [7500/27733], Loss: 2.5033\n",
      "Epoch [43/300], Step [7600/27733], Loss: 2.1822\n",
      "Epoch [43/300], Step [7700/27733], Loss: 3.1210\n",
      "Epoch [43/300], Step [7800/27733], Loss: 2.4845\n",
      "Epoch [43/300], Step [7900/27733], Loss: 2.6900\n",
      "Epoch [43/300], Step [8000/27733], Loss: 3.1654\n",
      "Epoch [43/300], Step [8100/27733], Loss: 2.8550\n",
      "Epoch [43/300], Step [8200/27733], Loss: 1.9329\n",
      "Epoch [43/300], Step [8300/27733], Loss: 2.2323\n",
      "Epoch [43/300], Step [8400/27733], Loss: 2.4340\n",
      "Epoch [43/300], Step [8500/27733], Loss: 2.8458\n",
      "Epoch [43/300], Step [8600/27733], Loss: 2.8878\n",
      "Epoch [43/300], Step [8700/27733], Loss: 2.7299\n",
      "Epoch [43/300], Step [8800/27733], Loss: 2.7804\n",
      "Epoch [43/300], Step [8900/27733], Loss: 2.7186\n",
      "Epoch [43/300], Step [9000/27733], Loss: 2.3030\n",
      "Epoch [43/300], Step [9100/27733], Loss: 3.2139\n",
      "Epoch [43/300], Step [9200/27733], Loss: 2.4029\n",
      "Epoch [43/300], Step [9300/27733], Loss: 3.5944\n",
      "Epoch [43/300], Step [9400/27733], Loss: 2.4991\n",
      "Epoch [43/300], Step [9500/27733], Loss: 3.0326\n",
      "Epoch [43/300], Step [9600/27733], Loss: 2.8737\n",
      "Epoch [43/300], Step [9700/27733], Loss: 2.4845\n",
      "Epoch [43/300], Step [9800/27733], Loss: 2.2443\n",
      "Epoch [43/300], Step [9900/27733], Loss: 2.7508\n",
      "Epoch [43/300], Step [10000/27733], Loss: 2.3420\n",
      "Epoch [43/300], Step [10100/27733], Loss: 2.5728\n",
      "Epoch [43/300], Step [10200/27733], Loss: 2.3727\n",
      "Epoch [43/300], Step [10300/27733], Loss: 2.5934\n",
      "Epoch [43/300], Step [10400/27733], Loss: 1.8045\n",
      "Epoch [43/300], Step [10500/27733], Loss: 2.8900\n",
      "Epoch [43/300], Step [10600/27733], Loss: 2.6144\n",
      "Epoch [43/300], Step [10700/27733], Loss: 2.6611\n",
      "Epoch [43/300], Step [10800/27733], Loss: 2.4671\n",
      "Epoch [43/300], Step [10900/27733], Loss: 2.7241\n",
      "Epoch [43/300], Step [11000/27733], Loss: 2.5571\n",
      "Epoch [43/300], Step [11100/27733], Loss: 2.5206\n",
      "Epoch [43/300], Step [11200/27733], Loss: 2.7729\n",
      "Epoch [43/300], Step [11300/27733], Loss: 2.8848\n",
      "Epoch [43/300], Step [11400/27733], Loss: 2.4978\n",
      "Epoch [43/300], Step [11500/27733], Loss: 2.8180\n",
      "Epoch [43/300], Step [11600/27733], Loss: 3.5645\n",
      "Epoch [43/300], Step [11700/27733], Loss: 2.6631\n",
      "Epoch [43/300], Step [11800/27733], Loss: 2.8980\n",
      "Epoch [43/300], Step [11900/27733], Loss: 2.9350\n",
      "Epoch [43/300], Step [12000/27733], Loss: 3.0145\n",
      "Epoch [43/300], Step [12100/27733], Loss: 2.9834\n",
      "Epoch [43/300], Step [12200/27733], Loss: 2.4085\n",
      "Epoch [43/300], Step [12300/27733], Loss: 2.9952\n",
      "Epoch [43/300], Step [12400/27733], Loss: 2.4581\n",
      "Epoch [43/300], Step [12500/27733], Loss: 2.9715\n",
      "Epoch [43/300], Step [12600/27733], Loss: 3.2150\n",
      "Epoch [43/300], Step [12700/27733], Loss: 2.9070\n",
      "Epoch [43/300], Step [12800/27733], Loss: 2.4036\n",
      "Epoch [43/300], Step [12900/27733], Loss: 3.1585\n",
      "Epoch [43/300], Step [13000/27733], Loss: 3.8841\n",
      "Epoch [43/300], Step [13100/27733], Loss: 3.4862\n",
      "Epoch [43/300], Step [13200/27733], Loss: 1.9967\n",
      "Epoch [43/300], Step [13300/27733], Loss: 2.5396\n",
      "Epoch [43/300], Step [13400/27733], Loss: 1.9967\n",
      "Epoch [43/300], Step [13500/27733], Loss: 3.6450\n",
      "Epoch [43/300], Step [13600/27733], Loss: 2.9281\n",
      "Epoch [43/300], Step [13700/27733], Loss: 3.7130\n",
      "Epoch [43/300], Step [13800/27733], Loss: 2.5518\n",
      "Epoch [43/300], Step [13900/27733], Loss: 1.6062\n",
      "Epoch [43/300], Step [14000/27733], Loss: 3.5795\n",
      "Epoch [43/300], Step [14100/27733], Loss: 3.5632\n",
      "Epoch [43/300], Step [14200/27733], Loss: 2.1303\n",
      "Epoch [43/300], Step [14300/27733], Loss: 2.3441\n",
      "Epoch [43/300], Step [14400/27733], Loss: 2.5608\n",
      "Epoch [43/300], Step [14500/27733], Loss: 2.0528\n",
      "Epoch [43/300], Step [14600/27733], Loss: 2.4648\n",
      "Epoch [43/300], Step [14700/27733], Loss: 2.8935\n",
      "Epoch [43/300], Step [14800/27733], Loss: 2.9146\n",
      "Epoch [43/300], Step [14900/27733], Loss: 3.1762\n",
      "Epoch [43/300], Step [15000/27733], Loss: 2.9648\n",
      "Epoch [43/300], Step [15100/27733], Loss: 2.3179\n",
      "Epoch [43/300], Step [15200/27733], Loss: 2.5837\n",
      "Epoch [43/300], Step [15300/27733], Loss: 2.6109\n",
      "Epoch [43/300], Step [15400/27733], Loss: 2.7114\n",
      "Epoch [43/300], Step [15500/27733], Loss: 2.4687\n",
      "Epoch [43/300], Step [15600/27733], Loss: 2.4212\n",
      "Epoch [43/300], Step [15700/27733], Loss: 3.8025\n",
      "Epoch [43/300], Step [15800/27733], Loss: 3.0476\n",
      "Epoch [43/300], Step [15900/27733], Loss: 2.7414\n",
      "Epoch [43/300], Step [16000/27733], Loss: 3.0054\n",
      "Epoch [43/300], Step [16100/27733], Loss: 2.7557\n",
      "Epoch [43/300], Step [16200/27733], Loss: 3.0383\n",
      "Epoch [43/300], Step [16300/27733], Loss: 2.9812\n",
      "Epoch [43/300], Step [16400/27733], Loss: 3.4398\n",
      "Epoch [43/300], Step [16500/27733], Loss: 3.2898\n",
      "Epoch [43/300], Step [16600/27733], Loss: 3.3284\n",
      "Epoch [43/300], Step [16700/27733], Loss: 3.3303\n",
      "Epoch [43/300], Step [16800/27733], Loss: 2.7707\n",
      "Epoch [43/300], Step [16900/27733], Loss: 2.3149\n",
      "Epoch [43/300], Step [17000/27733], Loss: 3.0696\n",
      "Epoch [43/300], Step [17100/27733], Loss: 2.4908\n",
      "Epoch [43/300], Step [17200/27733], Loss: 3.4470\n",
      "Epoch [43/300], Step [17300/27733], Loss: 2.9082\n",
      "Epoch [43/300], Step [17400/27733], Loss: 3.5109\n",
      "Epoch [43/300], Step [17500/27733], Loss: 2.8992\n",
      "Epoch [43/300], Step [17600/27733], Loss: 2.8031\n",
      "Epoch [43/300], Step [17700/27733], Loss: 3.7487\n",
      "Epoch [43/300], Step [17800/27733], Loss: 3.1519\n",
      "Epoch [43/300], Step [17900/27733], Loss: 3.2971\n",
      "Epoch [43/300], Step [18000/27733], Loss: 3.4000\n",
      "Epoch [43/300], Step [18100/27733], Loss: 2.4292\n",
      "Epoch [43/300], Step [18200/27733], Loss: 2.5384\n",
      "Epoch [43/300], Step [18300/27733], Loss: 3.2774\n",
      "Epoch [43/300], Step [18400/27733], Loss: 2.4233\n",
      "Epoch [43/300], Step [18500/27733], Loss: 2.7152\n",
      "Epoch [43/300], Step [18600/27733], Loss: 2.8103\n",
      "Epoch [43/300], Step [18700/27733], Loss: 2.6214\n",
      "Epoch [43/300], Step [18800/27733], Loss: 2.5554\n",
      "Epoch [43/300], Step [18900/27733], Loss: 2.9052\n",
      "Epoch [43/300], Step [19000/27733], Loss: 2.1919\n",
      "Epoch [43/300], Step [19100/27733], Loss: 2.6967\n",
      "Epoch [43/300], Step [19200/27733], Loss: 3.3102\n",
      "Epoch [43/300], Step [19300/27733], Loss: 2.8210\n",
      "Epoch [43/300], Step [19400/27733], Loss: 2.2236\n",
      "Epoch [43/300], Step [19500/27733], Loss: 2.8555\n",
      "Epoch [43/300], Step [19600/27733], Loss: 3.1147\n",
      "Epoch [43/300], Step [19700/27733], Loss: 2.5538\n",
      "Epoch [43/300], Step [19800/27733], Loss: 2.5426\n",
      "Epoch [43/300], Step [19900/27733], Loss: 3.2450\n",
      "Epoch [43/300], Step [20000/27733], Loss: 3.7076\n",
      "Epoch [43/300], Step [20100/27733], Loss: 3.0655\n",
      "Epoch [43/300], Step [20200/27733], Loss: 2.8416\n",
      "Epoch [43/300], Step [20300/27733], Loss: 2.8723\n",
      "Epoch [43/300], Step [20400/27733], Loss: 3.2823\n",
      "Epoch [43/300], Step [20500/27733], Loss: 3.7376\n",
      "Epoch [43/300], Step [20600/27733], Loss: 3.2908\n",
      "Epoch [43/300], Step [20700/27733], Loss: 2.2780\n",
      "Epoch [43/300], Step [20800/27733], Loss: 2.6017\n",
      "Epoch [43/300], Step [20900/27733], Loss: 2.6770\n",
      "Epoch [43/300], Step [21000/27733], Loss: 2.9016\n",
      "Epoch [43/300], Step [21100/27733], Loss: 3.2441\n",
      "Epoch [43/300], Step [21200/27733], Loss: 2.9360\n",
      "Epoch [43/300], Step [21300/27733], Loss: 3.3423\n",
      "Epoch [43/300], Step [21400/27733], Loss: 2.6154\n",
      "Epoch [43/300], Step [21500/27733], Loss: 2.2884\n",
      "Epoch [43/300], Step [21600/27733], Loss: 3.3784\n",
      "Epoch [43/300], Step [21700/27733], Loss: 3.3636\n",
      "Epoch [43/300], Step [21800/27733], Loss: 3.1665\n",
      "Epoch [43/300], Step [21900/27733], Loss: 3.4832\n",
      "Epoch [43/300], Step [22000/27733], Loss: 2.7874\n",
      "Epoch [43/300], Step [22100/27733], Loss: 3.1776\n",
      "Epoch [43/300], Step [22200/27733], Loss: 3.9311\n",
      "Epoch [43/300], Step [22300/27733], Loss: 3.5235\n",
      "Epoch [43/300], Step [22400/27733], Loss: 2.1389\n",
      "Epoch [43/300], Step [22500/27733], Loss: 2.5115\n",
      "Epoch [43/300], Step [22600/27733], Loss: 3.5525\n",
      "Epoch [43/300], Step [22700/27733], Loss: 3.3738\n",
      "Epoch [43/300], Step [22800/27733], Loss: 3.0422\n",
      "Epoch [43/300], Step [22900/27733], Loss: 3.3099\n",
      "Epoch [43/300], Step [23000/27733], Loss: 3.0627\n",
      "Epoch [43/300], Step [23100/27733], Loss: 1.9581\n",
      "Epoch [43/300], Step [23200/27733], Loss: 3.2763\n",
      "Epoch [43/300], Step [23300/27733], Loss: 2.1381\n",
      "Epoch [43/300], Step [23400/27733], Loss: 2.7261\n",
      "Epoch [43/300], Step [23500/27733], Loss: 3.3656\n",
      "Epoch [43/300], Step [23600/27733], Loss: 3.2732\n",
      "Epoch [43/300], Step [23700/27733], Loss: 3.5305\n",
      "Epoch [43/300], Step [23800/27733], Loss: 2.1564\n",
      "Epoch [43/300], Step [23900/27733], Loss: 2.1115\n",
      "Epoch [43/300], Step [24000/27733], Loss: 2.6107\n",
      "Epoch [43/300], Step [24100/27733], Loss: 2.4062\n",
      "Epoch [43/300], Step [24200/27733], Loss: 3.0200\n",
      "Epoch [43/300], Step [24300/27733], Loss: 2.6620\n",
      "Epoch [43/300], Step [24400/27733], Loss: 3.6337\n",
      "Epoch [43/300], Step [24500/27733], Loss: 3.2132\n",
      "Epoch [43/300], Step [24600/27733], Loss: 3.2913\n",
      "Epoch [43/300], Step [24700/27733], Loss: 2.3755\n",
      "Epoch [43/300], Step [24800/27733], Loss: 2.9416\n",
      "Epoch [43/300], Step [24900/27733], Loss: 2.9695\n",
      "Epoch [43/300], Step [25000/27733], Loss: 2.4211\n",
      "Epoch [43/300], Step [25100/27733], Loss: 2.2818\n",
      "Epoch [43/300], Step [25200/27733], Loss: 2.5877\n",
      "Epoch [43/300], Step [25300/27733], Loss: 2.8402\n",
      "Epoch [43/300], Step [25400/27733], Loss: 2.6933\n",
      "Epoch [43/300], Step [25500/27733], Loss: 3.5588\n",
      "Epoch [43/300], Step [25600/27733], Loss: 3.0249\n",
      "Epoch [43/300], Step [25700/27733], Loss: 3.1724\n",
      "Epoch [43/300], Step [25800/27733], Loss: 2.0874\n",
      "Epoch [43/300], Step [25900/27733], Loss: 2.7664\n",
      "Epoch [43/300], Step [26000/27733], Loss: 2.4366\n",
      "Epoch [43/300], Step [26100/27733], Loss: 3.0524\n",
      "Epoch [43/300], Step [26200/27733], Loss: 2.7346\n",
      "Epoch [43/300], Step [26300/27733], Loss: 3.3148\n",
      "Epoch [43/300], Step [26400/27733], Loss: 3.1108\n",
      "Epoch [43/300], Step [26500/27733], Loss: 3.0977\n",
      "Epoch [43/300], Step [26600/27733], Loss: 2.3755\n",
      "Epoch [43/300], Step [26700/27733], Loss: 2.8439\n",
      "Epoch [43/300], Step [26800/27733], Loss: 2.0314\n",
      "Epoch [43/300], Step [26900/27733], Loss: 3.8719\n",
      "Epoch [43/300], Step [27000/27733], Loss: 3.1962\n",
      "Epoch [43/300], Step [27100/27733], Loss: 3.0481\n",
      "Epoch [43/300], Step [27200/27733], Loss: 2.8859\n",
      "Epoch [43/300], Step [27300/27733], Loss: 2.8140\n",
      "Epoch [43/300], Step [27400/27733], Loss: 2.4988\n",
      "Epoch [43/300], Step [27500/27733], Loss: 2.9318\n",
      "Epoch [43/300], Step [27600/27733], Loss: 2.7127\n",
      "Epoch [43/300], Step [27700/27733], Loss: 3.5197\n",
      "Epoch [44/300], Step [100/27733], Loss: 2.8568\n",
      "Epoch [44/300], Step [200/27733], Loss: 2.2371\n",
      "Epoch [44/300], Step [300/27733], Loss: 1.8706\n",
      "Epoch [44/300], Step [400/27733], Loss: 1.5389\n",
      "Epoch [44/300], Step [500/27733], Loss: 2.9297\n",
      "Epoch [44/300], Step [600/27733], Loss: 2.6865\n",
      "Epoch [44/300], Step [700/27733], Loss: 2.2722\n",
      "Epoch [44/300], Step [800/27733], Loss: 2.1950\n",
      "Epoch [44/300], Step [900/27733], Loss: 1.9461\n",
      "Epoch [44/300], Step [1000/27733], Loss: 2.7706\n",
      "Epoch [44/300], Step [1100/27733], Loss: 1.9352\n",
      "Epoch [44/300], Step [1200/27733], Loss: 1.9905\n",
      "Epoch [44/300], Step [1300/27733], Loss: 1.6867\n",
      "Epoch [44/300], Step [1400/27733], Loss: 3.1384\n",
      "Epoch [44/300], Step [1500/27733], Loss: 1.8684\n",
      "Epoch [44/300], Step [1600/27733], Loss: 2.1691\n",
      "Epoch [44/300], Step [1700/27733], Loss: 2.2176\n",
      "Epoch [44/300], Step [1800/27733], Loss: 2.6284\n",
      "Epoch [44/300], Step [1900/27733], Loss: 2.2279\n",
      "Epoch [44/300], Step [2000/27733], Loss: 2.7537\n",
      "Epoch [44/300], Step [2100/27733], Loss: 2.1666\n",
      "Epoch [44/300], Step [2200/27733], Loss: 2.3692\n",
      "Epoch [44/300], Step [2300/27733], Loss: 1.9707\n",
      "Epoch [44/300], Step [2400/27733], Loss: 2.9966\n",
      "Epoch [44/300], Step [2500/27733], Loss: 3.2601\n",
      "Epoch [44/300], Step [2600/27733], Loss: 2.9447\n",
      "Epoch [44/300], Step [2700/27733], Loss: 2.3809\n",
      "Epoch [44/300], Step [2800/27733], Loss: 3.0110\n",
      "Epoch [44/300], Step [2900/27733], Loss: 1.7252\n",
      "Epoch [44/300], Step [3000/27733], Loss: 2.2927\n",
      "Epoch [44/300], Step [3100/27733], Loss: 2.6844\n",
      "Epoch [44/300], Step [3200/27733], Loss: 2.3374\n",
      "Epoch [44/300], Step [3300/27733], Loss: 3.1749\n",
      "Epoch [44/300], Step [3400/27733], Loss: 2.1859\n",
      "Epoch [44/300], Step [3500/27733], Loss: 2.2996\n",
      "Epoch [44/300], Step [3600/27733], Loss: 3.0262\n",
      "Epoch [44/300], Step [3700/27733], Loss: 2.6443\n",
      "Epoch [44/300], Step [3800/27733], Loss: 2.4878\n",
      "Epoch [44/300], Step [3900/27733], Loss: 2.3232\n",
      "Epoch [44/300], Step [4000/27733], Loss: 2.7954\n",
      "Epoch [44/300], Step [4100/27733], Loss: 2.5591\n",
      "Epoch [44/300], Step [4200/27733], Loss: 1.5152\n",
      "Epoch [44/300], Step [4300/27733], Loss: 2.5646\n",
      "Epoch [44/300], Step [4400/27733], Loss: 2.8753\n",
      "Epoch [44/300], Step [4500/27733], Loss: 2.0283\n",
      "Epoch [44/300], Step [4600/27733], Loss: 2.3505\n",
      "Epoch [44/300], Step [4700/27733], Loss: 2.0077\n",
      "Epoch [44/300], Step [4800/27733], Loss: 2.2956\n",
      "Epoch [44/300], Step [4900/27733], Loss: 2.9064\n",
      "Epoch [44/300], Step [5000/27733], Loss: 2.8043\n",
      "Epoch [44/300], Step [5100/27733], Loss: 2.5356\n",
      "Epoch [44/300], Step [5200/27733], Loss: 2.1088\n",
      "Epoch [44/300], Step [5300/27733], Loss: 2.8887\n",
      "Epoch [44/300], Step [5400/27733], Loss: 2.0186\n",
      "Epoch [44/300], Step [5500/27733], Loss: 3.1146\n",
      "Epoch [44/300], Step [5600/27733], Loss: 2.5062\n",
      "Epoch [44/300], Step [5700/27733], Loss: 2.9872\n",
      "Epoch [44/300], Step [5800/27733], Loss: 3.0186\n",
      "Epoch [44/300], Step [5900/27733], Loss: 2.4646\n",
      "Epoch [44/300], Step [6000/27733], Loss: 2.8777\n",
      "Epoch [44/300], Step [6100/27733], Loss: 2.1194\n",
      "Epoch [44/300], Step [6200/27733], Loss: 2.8937\n",
      "Epoch [44/300], Step [6300/27733], Loss: 2.3356\n",
      "Epoch [44/300], Step [6400/27733], Loss: 2.3024\n",
      "Epoch [44/300], Step [6500/27733], Loss: 2.9145\n",
      "Epoch [44/300], Step [6600/27733], Loss: 2.6932\n",
      "Epoch [44/300], Step [6700/27733], Loss: 2.5106\n",
      "Epoch [44/300], Step [6800/27733], Loss: 2.1536\n",
      "Epoch [44/300], Step [6900/27733], Loss: 2.5176\n",
      "Epoch [44/300], Step [7000/27733], Loss: 2.3275\n",
      "Epoch [44/300], Step [7100/27733], Loss: 2.6070\n",
      "Epoch [44/300], Step [7200/27733], Loss: 2.3027\n",
      "Epoch [44/300], Step [7300/27733], Loss: 2.5720\n",
      "Epoch [44/300], Step [7400/27733], Loss: 3.6206\n",
      "Epoch [44/300], Step [7500/27733], Loss: 2.8160\n",
      "Epoch [44/300], Step [7600/27733], Loss: 2.9963\n",
      "Epoch [44/300], Step [7700/27733], Loss: 2.5869\n",
      "Epoch [44/300], Step [7800/27733], Loss: 1.7816\n",
      "Epoch [44/300], Step [7900/27733], Loss: 2.4240\n",
      "Epoch [44/300], Step [8000/27733], Loss: 2.4301\n",
      "Epoch [44/300], Step [8100/27733], Loss: 2.9371\n",
      "Epoch [44/300], Step [8200/27733], Loss: 2.6907\n",
      "Epoch [44/300], Step [8300/27733], Loss: 2.9095\n",
      "Epoch [44/300], Step [8400/27733], Loss: 2.1701\n",
      "Epoch [44/300], Step [8500/27733], Loss: 2.6953\n",
      "Epoch [44/300], Step [8600/27733], Loss: 2.3388\n",
      "Epoch [44/300], Step [8700/27733], Loss: 2.7499\n",
      "Epoch [44/300], Step [8800/27733], Loss: 1.9973\n",
      "Epoch [44/300], Step [8900/27733], Loss: 2.9098\n",
      "Epoch [44/300], Step [9000/27733], Loss: 2.9853\n",
      "Epoch [44/300], Step [9100/27733], Loss: 3.3007\n",
      "Epoch [44/300], Step [9200/27733], Loss: 2.8593\n",
      "Epoch [44/300], Step [9300/27733], Loss: 3.1905\n",
      "Epoch [44/300], Step [9400/27733], Loss: 3.3434\n",
      "Epoch [44/300], Step [9500/27733], Loss: 2.9251\n",
      "Epoch [44/300], Step [9600/27733], Loss: 2.2176\n",
      "Epoch [44/300], Step [9700/27733], Loss: 3.8215\n",
      "Epoch [44/300], Step [9800/27733], Loss: 2.5463\n",
      "Epoch [44/300], Step [9900/27733], Loss: 2.6368\n",
      "Epoch [44/300], Step [10000/27733], Loss: 2.3032\n",
      "Epoch [44/300], Step [10100/27733], Loss: 2.3173\n",
      "Epoch [44/300], Step [10200/27733], Loss: 3.1645\n",
      "Epoch [44/300], Step [10300/27733], Loss: 2.1077\n",
      "Epoch [44/300], Step [10400/27733], Loss: 2.7733\n",
      "Epoch [44/300], Step [10500/27733], Loss: 2.5776\n",
      "Epoch [44/300], Step [10600/27733], Loss: 2.2647\n",
      "Epoch [44/300], Step [10700/27733], Loss: 2.8040\n",
      "Epoch [44/300], Step [10800/27733], Loss: 2.6021\n",
      "Epoch [44/300], Step [10900/27733], Loss: 2.8463\n",
      "Epoch [44/300], Step [11000/27733], Loss: 2.6283\n",
      "Epoch [44/300], Step [11100/27733], Loss: 3.5206\n",
      "Epoch [44/300], Step [11200/27733], Loss: 2.1541\n",
      "Epoch [44/300], Step [11300/27733], Loss: 2.5656\n",
      "Epoch [44/300], Step [11400/27733], Loss: 2.3666\n",
      "Epoch [44/300], Step [11500/27733], Loss: 2.3378\n",
      "Epoch [44/300], Step [11600/27733], Loss: 2.4906\n",
      "Epoch [44/300], Step [11700/27733], Loss: 2.0050\n",
      "Epoch [44/300], Step [11800/27733], Loss: 3.0255\n",
      "Epoch [44/300], Step [11900/27733], Loss: 2.0525\n",
      "Epoch [44/300], Step [12000/27733], Loss: 3.3214\n",
      "Epoch [44/300], Step [12100/27733], Loss: 2.5431\n",
      "Epoch [44/300], Step [12200/27733], Loss: 3.0841\n",
      "Epoch [44/300], Step [12300/27733], Loss: 2.5484\n",
      "Epoch [44/300], Step [12400/27733], Loss: 2.0172\n",
      "Epoch [44/300], Step [12500/27733], Loss: 2.7968\n",
      "Epoch [44/300], Step [12600/27733], Loss: 3.7089\n",
      "Epoch [44/300], Step [12700/27733], Loss: 3.4973\n",
      "Epoch [44/300], Step [12800/27733], Loss: 3.0388\n",
      "Epoch [44/300], Step [12900/27733], Loss: 2.0591\n",
      "Epoch [44/300], Step [13000/27733], Loss: 2.7061\n",
      "Epoch [44/300], Step [13100/27733], Loss: 3.4788\n",
      "Epoch [44/300], Step [13200/27733], Loss: 3.2080\n",
      "Epoch [44/300], Step [13300/27733], Loss: 3.6565\n",
      "Epoch [44/300], Step [13400/27733], Loss: 3.0404\n",
      "Epoch [44/300], Step [13500/27733], Loss: 2.2861\n",
      "Epoch [44/300], Step [13600/27733], Loss: 2.8315\n",
      "Epoch [44/300], Step [13700/27733], Loss: 2.9811\n",
      "Epoch [44/300], Step [13800/27733], Loss: 2.9581\n",
      "Epoch [44/300], Step [13900/27733], Loss: 2.7024\n",
      "Epoch [44/300], Step [14000/27733], Loss: 3.0148\n",
      "Epoch [44/300], Step [14100/27733], Loss: 2.3618\n",
      "Epoch [44/300], Step [14200/27733], Loss: 3.1203\n",
      "Epoch [44/300], Step [14300/27733], Loss: 1.8782\n",
      "Epoch [44/300], Step [14400/27733], Loss: 2.3218\n",
      "Epoch [44/300], Step [14500/27733], Loss: 2.7707\n",
      "Epoch [44/300], Step [14600/27733], Loss: 2.4035\n",
      "Epoch [44/300], Step [14700/27733], Loss: 2.2005\n",
      "Epoch [44/300], Step [14800/27733], Loss: 3.2136\n",
      "Epoch [44/300], Step [14900/27733], Loss: 3.4771\n",
      "Epoch [44/300], Step [15000/27733], Loss: 2.3783\n",
      "Epoch [44/300], Step [15100/27733], Loss: 2.7898\n",
      "Epoch [44/300], Step [15200/27733], Loss: 2.4719\n",
      "Epoch [44/300], Step [15300/27733], Loss: 3.8961\n",
      "Epoch [44/300], Step [15400/27733], Loss: 2.7117\n",
      "Epoch [44/300], Step [15500/27733], Loss: 2.7403\n",
      "Epoch [44/300], Step [15600/27733], Loss: 3.1595\n",
      "Epoch [44/300], Step [15700/27733], Loss: 3.0199\n",
      "Epoch [44/300], Step [15800/27733], Loss: 3.2206\n",
      "Epoch [44/300], Step [15900/27733], Loss: 2.9552\n",
      "Epoch [44/300], Step [16000/27733], Loss: 3.4646\n",
      "Epoch [44/300], Step [16100/27733], Loss: 2.5034\n",
      "Epoch [44/300], Step [16200/27733], Loss: 2.2208\n",
      "Epoch [44/300], Step [16300/27733], Loss: 2.7180\n",
      "Epoch [44/300], Step [16400/27733], Loss: 2.6968\n",
      "Epoch [44/300], Step [16500/27733], Loss: 2.7670\n",
      "Epoch [44/300], Step [16600/27733], Loss: 2.3373\n",
      "Epoch [44/300], Step [16700/27733], Loss: 2.8874\n",
      "Epoch [44/300], Step [16800/27733], Loss: 2.6078\n",
      "Epoch [44/300], Step [16900/27733], Loss: 3.1280\n",
      "Epoch [44/300], Step [17000/27733], Loss: 2.9491\n",
      "Epoch [44/300], Step [17100/27733], Loss: 2.4169\n",
      "Epoch [44/300], Step [17200/27733], Loss: 2.6427\n",
      "Epoch [44/300], Step [17300/27733], Loss: 4.1782\n",
      "Epoch [44/300], Step [17400/27733], Loss: 2.7162\n",
      "Epoch [44/300], Step [17500/27733], Loss: 2.3475\n",
      "Epoch [44/300], Step [17600/27733], Loss: 2.2219\n",
      "Epoch [44/300], Step [17700/27733], Loss: 2.7145\n",
      "Epoch [44/300], Step [17800/27733], Loss: 2.3022\n",
      "Epoch [44/300], Step [17900/27733], Loss: 2.7464\n",
      "Epoch [44/300], Step [18000/27733], Loss: 2.8940\n",
      "Epoch [44/300], Step [18100/27733], Loss: 2.5557\n",
      "Epoch [44/300], Step [18200/27733], Loss: 3.0991\n",
      "Epoch [44/300], Step [18300/27733], Loss: 2.6036\n",
      "Epoch [44/300], Step [18400/27733], Loss: 2.4818\n",
      "Epoch [44/300], Step [18500/27733], Loss: 3.2431\n",
      "Epoch [44/300], Step [18600/27733], Loss: 2.0925\n",
      "Epoch [44/300], Step [18700/27733], Loss: 2.6483\n",
      "Epoch [44/300], Step [18800/27733], Loss: 2.9128\n",
      "Epoch [44/300], Step [18900/27733], Loss: 2.9985\n",
      "Epoch [44/300], Step [19000/27733], Loss: 3.0257\n",
      "Epoch [44/300], Step [19100/27733], Loss: 2.5369\n",
      "Epoch [44/300], Step [19200/27733], Loss: 2.4129\n",
      "Epoch [44/300], Step [19300/27733], Loss: 2.7040\n",
      "Epoch [44/300], Step [19400/27733], Loss: 3.3507\n",
      "Epoch [44/300], Step [19500/27733], Loss: 2.7563\n",
      "Epoch [44/300], Step [19600/27733], Loss: 3.2677\n",
      "Epoch [44/300], Step [19700/27733], Loss: 2.4948\n",
      "Epoch [44/300], Step [19800/27733], Loss: 2.7281\n",
      "Epoch [44/300], Step [19900/27733], Loss: 3.0604\n",
      "Epoch [44/300], Step [20000/27733], Loss: 3.5686\n",
      "Epoch [44/300], Step [20100/27733], Loss: 2.6213\n",
      "Epoch [44/300], Step [20200/27733], Loss: 2.9532\n",
      "Epoch [44/300], Step [20300/27733], Loss: 2.4232\n",
      "Epoch [44/300], Step [20400/27733], Loss: 2.7411\n",
      "Epoch [44/300], Step [20500/27733], Loss: 1.9179\n",
      "Epoch [44/300], Step [20600/27733], Loss: 2.5191\n",
      "Epoch [44/300], Step [20700/27733], Loss: 2.7847\n",
      "Epoch [44/300], Step [20800/27733], Loss: 3.1037\n",
      "Epoch [44/300], Step [20900/27733], Loss: 4.3550\n",
      "Epoch [44/300], Step [21000/27733], Loss: 3.0109\n",
      "Epoch [44/300], Step [21100/27733], Loss: 2.6393\n",
      "Epoch [44/300], Step [21200/27733], Loss: 2.9474\n",
      "Epoch [44/300], Step [21300/27733], Loss: 2.3842\n",
      "Epoch [44/300], Step [21400/27733], Loss: 2.4065\n",
      "Epoch [44/300], Step [21500/27733], Loss: 3.1749\n",
      "Epoch [44/300], Step [21600/27733], Loss: 3.7060\n",
      "Epoch [44/300], Step [21700/27733], Loss: 3.2037\n",
      "Epoch [44/300], Step [21800/27733], Loss: 3.2877\n",
      "Epoch [44/300], Step [21900/27733], Loss: 2.9729\n",
      "Epoch [44/300], Step [22000/27733], Loss: 2.5389\n",
      "Epoch [44/300], Step [22100/27733], Loss: 2.5124\n",
      "Epoch [44/300], Step [22200/27733], Loss: 3.2462\n",
      "Epoch [44/300], Step [22300/27733], Loss: 3.3152\n",
      "Epoch [44/300], Step [22400/27733], Loss: 2.8318\n",
      "Epoch [44/300], Step [22500/27733], Loss: 2.5772\n",
      "Epoch [44/300], Step [22600/27733], Loss: 2.8755\n",
      "Epoch [44/300], Step [22700/27733], Loss: 2.9117\n",
      "Epoch [44/300], Step [22800/27733], Loss: 2.4774\n",
      "Epoch [44/300], Step [22900/27733], Loss: 2.3007\n",
      "Epoch [44/300], Step [23000/27733], Loss: 3.5922\n",
      "Epoch [44/300], Step [23100/27733], Loss: 3.2362\n",
      "Epoch [44/300], Step [23200/27733], Loss: 2.5840\n",
      "Epoch [44/300], Step [23300/27733], Loss: 2.6173\n",
      "Epoch [44/300], Step [23400/27733], Loss: 2.8604\n",
      "Epoch [44/300], Step [23500/27733], Loss: 3.0176\n",
      "Epoch [44/300], Step [23600/27733], Loss: 2.2492\n",
      "Epoch [44/300], Step [23700/27733], Loss: 3.2192\n",
      "Epoch [44/300], Step [23800/27733], Loss: 2.6226\n",
      "Epoch [44/300], Step [23900/27733], Loss: 2.7431\n",
      "Epoch [44/300], Step [24000/27733], Loss: 2.8008\n",
      "Epoch [44/300], Step [24100/27733], Loss: 3.7308\n",
      "Epoch [44/300], Step [24200/27733], Loss: 3.0816\n",
      "Epoch [44/300], Step [24300/27733], Loss: 3.0966\n",
      "Epoch [44/300], Step [24400/27733], Loss: 2.6490\n",
      "Epoch [44/300], Step [24500/27733], Loss: 3.4171\n",
      "Epoch [44/300], Step [24600/27733], Loss: 2.9514\n",
      "Epoch [44/300], Step [24700/27733], Loss: 3.1949\n",
      "Epoch [44/300], Step [24800/27733], Loss: 2.6523\n",
      "Epoch [44/300], Step [24900/27733], Loss: 2.6217\n",
      "Epoch [44/300], Step [25000/27733], Loss: 3.6168\n",
      "Epoch [44/300], Step [25100/27733], Loss: 3.5842\n",
      "Epoch [44/300], Step [25200/27733], Loss: 2.7118\n",
      "Epoch [44/300], Step [25300/27733], Loss: 2.7525\n",
      "Epoch [44/300], Step [25400/27733], Loss: 2.7114\n",
      "Epoch [44/300], Step [25500/27733], Loss: 3.5607\n",
      "Epoch [44/300], Step [25600/27733], Loss: 3.1548\n",
      "Epoch [44/300], Step [25700/27733], Loss: 3.4629\n",
      "Epoch [44/300], Step [25800/27733], Loss: 2.9596\n",
      "Epoch [44/300], Step [25900/27733], Loss: 3.0593\n",
      "Epoch [44/300], Step [26000/27733], Loss: 2.7602\n",
      "Epoch [44/300], Step [26100/27733], Loss: 2.2166\n",
      "Epoch [44/300], Step [26200/27733], Loss: 2.4313\n",
      "Epoch [44/300], Step [26300/27733], Loss: 3.0724\n",
      "Epoch [44/300], Step [26400/27733], Loss: 3.1218\n",
      "Epoch [44/300], Step [26500/27733], Loss: 2.9209\n",
      "Epoch [44/300], Step [26600/27733], Loss: 2.5517\n",
      "Epoch [44/300], Step [26700/27733], Loss: 2.7782\n",
      "Epoch [44/300], Step [26800/27733], Loss: 3.0278\n",
      "Epoch [44/300], Step [26900/27733], Loss: 3.2656\n",
      "Epoch [44/300], Step [27000/27733], Loss: 2.3177\n",
      "Epoch [44/300], Step [27100/27733], Loss: 2.9650\n",
      "Epoch [44/300], Step [27200/27733], Loss: 3.0496\n",
      "Epoch [44/300], Step [27300/27733], Loss: 2.3671\n",
      "Epoch [44/300], Step [27400/27733], Loss: 3.2922\n",
      "Epoch [44/300], Step [27500/27733], Loss: 3.6747\n",
      "Epoch [44/300], Step [27600/27733], Loss: 2.9680\n",
      "Epoch [44/300], Step [27700/27733], Loss: 2.7020\n",
      "Epoch [45/300], Step [100/27733], Loss: 2.3230\n",
      "Epoch [45/300], Step [200/27733], Loss: 2.2009\n",
      "Epoch [45/300], Step [300/27733], Loss: 2.3171\n",
      "Epoch [45/300], Step [400/27733], Loss: 2.7433\n",
      "Epoch [45/300], Step [500/27733], Loss: 2.3503\n",
      "Epoch [45/300], Step [600/27733], Loss: 2.0033\n",
      "Epoch [45/300], Step [700/27733], Loss: 2.3612\n",
      "Epoch [45/300], Step [800/27733], Loss: 1.8786\n",
      "Epoch [45/300], Step [900/27733], Loss: 2.6710\n",
      "Epoch [45/300], Step [1000/27733], Loss: 1.9697\n",
      "Epoch [45/300], Step [1100/27733], Loss: 2.3034\n",
      "Epoch [45/300], Step [1200/27733], Loss: 2.3546\n",
      "Epoch [45/300], Step [1300/27733], Loss: 3.1743\n",
      "Epoch [45/300], Step [1400/27733], Loss: 2.7204\n",
      "Epoch [45/300], Step [1500/27733], Loss: 1.7750\n",
      "Epoch [45/300], Step [1600/27733], Loss: 2.4322\n",
      "Epoch [45/300], Step [1700/27733], Loss: 2.6672\n",
      "Epoch [45/300], Step [1800/27733], Loss: 2.9019\n",
      "Epoch [45/300], Step [1900/27733], Loss: 2.6476\n",
      "Epoch [45/300], Step [2000/27733], Loss: 2.6630\n",
      "Epoch [45/300], Step [2100/27733], Loss: 2.1344\n",
      "Epoch [45/300], Step [2200/27733], Loss: 1.9043\n",
      "Epoch [45/300], Step [2300/27733], Loss: 2.7598\n",
      "Epoch [45/300], Step [2400/27733], Loss: 1.9483\n",
      "Epoch [45/300], Step [2500/27733], Loss: 2.0822\n",
      "Epoch [45/300], Step [2600/27733], Loss: 2.2283\n",
      "Epoch [45/300], Step [2700/27733], Loss: 2.4063\n",
      "Epoch [45/300], Step [2800/27733], Loss: 1.7588\n",
      "Epoch [45/300], Step [2900/27733], Loss: 3.3837\n",
      "Epoch [45/300], Step [3000/27733], Loss: 2.1092\n",
      "Epoch [45/300], Step [3100/27733], Loss: 2.6450\n",
      "Epoch [45/300], Step [3200/27733], Loss: 2.6042\n",
      "Epoch [45/300], Step [3300/27733], Loss: 2.5897\n",
      "Epoch [45/300], Step [3400/27733], Loss: 2.3192\n",
      "Epoch [45/300], Step [3500/27733], Loss: 2.4658\n",
      "Epoch [45/300], Step [3600/27733], Loss: 2.4296\n",
      "Epoch [45/300], Step [3700/27733], Loss: 2.5512\n",
      "Epoch [45/300], Step [3800/27733], Loss: 2.4746\n",
      "Epoch [45/300], Step [3900/27733], Loss: 2.4424\n",
      "Epoch [45/300], Step [4000/27733], Loss: 2.2154\n",
      "Epoch [45/300], Step [4100/27733], Loss: 2.6401\n",
      "Epoch [45/300], Step [4200/27733], Loss: 1.9886\n",
      "Epoch [45/300], Step [4300/27733], Loss: 2.2491\n",
      "Epoch [45/300], Step [4400/27733], Loss: 1.9689\n",
      "Epoch [45/300], Step [4500/27733], Loss: 2.4084\n",
      "Epoch [45/300], Step [4600/27733], Loss: 3.1871\n",
      "Epoch [45/300], Step [4700/27733], Loss: 2.8311\n",
      "Epoch [45/300], Step [4800/27733], Loss: 1.9149\n",
      "Epoch [45/300], Step [4900/27733], Loss: 2.7271\n",
      "Epoch [45/300], Step [5000/27733], Loss: 2.6732\n",
      "Epoch [45/300], Step [5100/27733], Loss: 2.8726\n",
      "Epoch [45/300], Step [5200/27733], Loss: 1.6580\n",
      "Epoch [45/300], Step [5300/27733], Loss: 2.9090\n",
      "Epoch [45/300], Step [5400/27733], Loss: 2.5518\n",
      "Epoch [45/300], Step [5500/27733], Loss: 2.1640\n",
      "Epoch [45/300], Step [5600/27733], Loss: 2.3429\n",
      "Epoch [45/300], Step [5700/27733], Loss: 2.7049\n",
      "Epoch [45/300], Step [5800/27733], Loss: 2.5229\n",
      "Epoch [45/300], Step [5900/27733], Loss: 2.1697\n",
      "Epoch [45/300], Step [6000/27733], Loss: 2.5186\n",
      "Epoch [45/300], Step [6100/27733], Loss: 2.6272\n",
      "Epoch [45/300], Step [6200/27733], Loss: 3.0228\n",
      "Epoch [45/300], Step [6300/27733], Loss: 2.2002\n",
      "Epoch [45/300], Step [6400/27733], Loss: 2.9451\n",
      "Epoch [45/300], Step [6500/27733], Loss: 2.1269\n",
      "Epoch [45/300], Step [6600/27733], Loss: 3.0036\n",
      "Epoch [45/300], Step [6700/27733], Loss: 3.4868\n",
      "Epoch [45/300], Step [6800/27733], Loss: 2.5645\n",
      "Epoch [45/300], Step [6900/27733], Loss: 2.6993\n",
      "Epoch [45/300], Step [7000/27733], Loss: 3.3667\n",
      "Epoch [45/300], Step [7100/27733], Loss: 2.2195\n",
      "Epoch [45/300], Step [7200/27733], Loss: 3.4145\n",
      "Epoch [45/300], Step [7300/27733], Loss: 2.1738\n",
      "Epoch [45/300], Step [7400/27733], Loss: 2.8412\n",
      "Epoch [45/300], Step [7500/27733], Loss: 2.6849\n",
      "Epoch [45/300], Step [7600/27733], Loss: 2.6425\n",
      "Epoch [45/300], Step [7700/27733], Loss: 2.4828\n",
      "Epoch [45/300], Step [7800/27733], Loss: 2.3893\n",
      "Epoch [45/300], Step [7900/27733], Loss: 3.0520\n",
      "Epoch [45/300], Step [8000/27733], Loss: 2.7289\n",
      "Epoch [45/300], Step [8100/27733], Loss: 2.8330\n",
      "Epoch [45/300], Step [8200/27733], Loss: 2.4535\n",
      "Epoch [45/300], Step [8300/27733], Loss: 2.5223\n",
      "Epoch [45/300], Step [8400/27733], Loss: 2.6679\n",
      "Epoch [45/300], Step [8500/27733], Loss: 2.3728\n",
      "Epoch [45/300], Step [8600/27733], Loss: 2.7770\n",
      "Epoch [45/300], Step [8700/27733], Loss: 3.3272\n",
      "Epoch [45/300], Step [8800/27733], Loss: 2.4807\n",
      "Epoch [45/300], Step [8900/27733], Loss: 3.0559\n",
      "Epoch [45/300], Step [9000/27733], Loss: 2.5754\n",
      "Epoch [45/300], Step [9100/27733], Loss: 3.5161\n",
      "Epoch [45/300], Step [9200/27733], Loss: 2.6717\n",
      "Epoch [45/300], Step [9300/27733], Loss: 2.7679\n",
      "Epoch [45/300], Step [9400/27733], Loss: 3.5062\n",
      "Epoch [45/300], Step [9500/27733], Loss: 2.6639\n",
      "Epoch [45/300], Step [9600/27733], Loss: 2.7053\n",
      "Epoch [45/300], Step [9700/27733], Loss: 2.4362\n",
      "Epoch [45/300], Step [9800/27733], Loss: 2.7891\n",
      "Epoch [45/300], Step [9900/27733], Loss: 3.0000\n",
      "Epoch [45/300], Step [10000/27733], Loss: 3.4564\n",
      "Epoch [45/300], Step [10100/27733], Loss: 3.0442\n",
      "Epoch [45/300], Step [10200/27733], Loss: 2.0817\n",
      "Epoch [45/300], Step [10300/27733], Loss: 2.9107\n",
      "Epoch [45/300], Step [10400/27733], Loss: 3.0240\n",
      "Epoch [45/300], Step [10500/27733], Loss: 3.7613\n",
      "Epoch [45/300], Step [10600/27733], Loss: 2.9945\n",
      "Epoch [45/300], Step [10700/27733], Loss: 3.1080\n",
      "Epoch [45/300], Step [10800/27733], Loss: 2.6763\n",
      "Epoch [45/300], Step [10900/27733], Loss: 2.7223\n",
      "Epoch [45/300], Step [11000/27733], Loss: 2.8769\n",
      "Epoch [45/300], Step [11100/27733], Loss: 2.0309\n",
      "Epoch [45/300], Step [11200/27733], Loss: 2.6758\n",
      "Epoch [45/300], Step [11300/27733], Loss: 2.9130\n",
      "Epoch [45/300], Step [11400/27733], Loss: 2.8377\n",
      "Epoch [45/300], Step [11500/27733], Loss: 2.1560\n",
      "Epoch [45/300], Step [11600/27733], Loss: 2.4034\n",
      "Epoch [45/300], Step [11700/27733], Loss: 2.4947\n",
      "Epoch [45/300], Step [11800/27733], Loss: 2.2793\n",
      "Epoch [45/300], Step [11900/27733], Loss: 3.1967\n",
      "Epoch [45/300], Step [12000/27733], Loss: 2.5044\n",
      "Epoch [45/300], Step [12100/27733], Loss: 3.2792\n",
      "Epoch [45/300], Step [12200/27733], Loss: 3.1669\n",
      "Epoch [45/300], Step [12300/27733], Loss: 2.7466\n",
      "Epoch [45/300], Step [12400/27733], Loss: 2.9493\n",
      "Epoch [45/300], Step [12500/27733], Loss: 2.7462\n",
      "Epoch [45/300], Step [12600/27733], Loss: 2.8241\n",
      "Epoch [45/300], Step [12700/27733], Loss: 3.1951\n",
      "Epoch [45/300], Step [12800/27733], Loss: 3.3053\n",
      "Epoch [45/300], Step [12900/27733], Loss: 2.4824\n",
      "Epoch [45/300], Step [13000/27733], Loss: 2.9962\n",
      "Epoch [45/300], Step [13100/27733], Loss: 2.6855\n",
      "Epoch [45/300], Step [13200/27733], Loss: 2.6665\n",
      "Epoch [45/300], Step [13300/27733], Loss: 2.5182\n",
      "Epoch [45/300], Step [13400/27733], Loss: 2.8100\n",
      "Epoch [45/300], Step [13500/27733], Loss: 1.9710\n",
      "Epoch [45/300], Step [13600/27733], Loss: 2.8494\n",
      "Epoch [45/300], Step [13700/27733], Loss: 2.8354\n",
      "Epoch [45/300], Step [13800/27733], Loss: 3.8347\n",
      "Epoch [45/300], Step [13900/27733], Loss: 2.3680\n",
      "Epoch [45/300], Step [14000/27733], Loss: 2.1173\n",
      "Epoch [45/300], Step [14100/27733], Loss: 3.2688\n",
      "Epoch [45/300], Step [14200/27733], Loss: 3.1229\n",
      "Epoch [45/300], Step [14300/27733], Loss: 3.1315\n",
      "Epoch [45/300], Step [14400/27733], Loss: 2.5309\n",
      "Epoch [45/300], Step [14500/27733], Loss: 3.1588\n",
      "Epoch [45/300], Step [14600/27733], Loss: 2.6385\n",
      "Epoch [45/300], Step [14700/27733], Loss: 2.0938\n",
      "Epoch [45/300], Step [14800/27733], Loss: 3.0363\n",
      "Epoch [45/300], Step [14900/27733], Loss: 2.6319\n",
      "Epoch [45/300], Step [15000/27733], Loss: 2.6616\n",
      "Epoch [45/300], Step [15100/27733], Loss: 2.3458\n",
      "Epoch [45/300], Step [15200/27733], Loss: 2.6765\n",
      "Epoch [45/300], Step [15300/27733], Loss: 2.6613\n",
      "Epoch [45/300], Step [15400/27733], Loss: 3.3429\n",
      "Epoch [45/300], Step [15500/27733], Loss: 2.8554\n",
      "Epoch [45/300], Step [15600/27733], Loss: 2.7491\n",
      "Epoch [45/300], Step [15700/27733], Loss: 2.4850\n",
      "Epoch [45/300], Step [15800/27733], Loss: 2.6284\n",
      "Epoch [45/300], Step [15900/27733], Loss: 2.3157\n",
      "Epoch [45/300], Step [16000/27733], Loss: 2.3024\n",
      "Epoch [45/300], Step [16100/27733], Loss: 2.5283\n",
      "Epoch [45/300], Step [16200/27733], Loss: 2.8792\n",
      "Epoch [45/300], Step [16300/27733], Loss: 2.5833\n",
      "Epoch [45/300], Step [16400/27733], Loss: 2.9822\n",
      "Epoch [45/300], Step [16500/27733], Loss: 2.0277\n",
      "Epoch [45/300], Step [16600/27733], Loss: 2.1175\n",
      "Epoch [45/300], Step [16700/27733], Loss: 2.4423\n",
      "Epoch [45/300], Step [16800/27733], Loss: 2.5856\n",
      "Epoch [45/300], Step [16900/27733], Loss: 2.2997\n",
      "Epoch [45/300], Step [17000/27733], Loss: 2.6741\n",
      "Epoch [45/300], Step [17100/27733], Loss: 2.9169\n",
      "Epoch [45/300], Step [17200/27733], Loss: 2.7086\n",
      "Epoch [45/300], Step [17300/27733], Loss: 3.5516\n",
      "Epoch [45/300], Step [17400/27733], Loss: 2.8607\n",
      "Epoch [45/300], Step [17500/27733], Loss: 2.2774\n",
      "Epoch [45/300], Step [17600/27733], Loss: 3.1264\n",
      "Epoch [45/300], Step [17700/27733], Loss: 2.6441\n",
      "Epoch [45/300], Step [17800/27733], Loss: 2.7413\n",
      "Epoch [45/300], Step [17900/27733], Loss: 2.3405\n",
      "Epoch [45/300], Step [18000/27733], Loss: 3.0698\n",
      "Epoch [45/300], Step [18100/27733], Loss: 3.0328\n",
      "Epoch [45/300], Step [18200/27733], Loss: 3.2909\n",
      "Epoch [45/300], Step [18300/27733], Loss: 2.9091\n",
      "Epoch [45/300], Step [18400/27733], Loss: 3.8348\n",
      "Epoch [45/300], Step [18500/27733], Loss: 2.7417\n",
      "Epoch [45/300], Step [18600/27733], Loss: 2.4182\n",
      "Epoch [45/300], Step [18700/27733], Loss: 2.6879\n",
      "Epoch [45/300], Step [18800/27733], Loss: 3.2252\n",
      "Epoch [45/300], Step [18900/27733], Loss: 2.3927\n",
      "Epoch [45/300], Step [19000/27733], Loss: 2.6652\n",
      "Epoch [45/300], Step [19100/27733], Loss: 2.7954\n",
      "Epoch [45/300], Step [19200/27733], Loss: 3.1430\n",
      "Epoch [45/300], Step [19300/27733], Loss: 3.3556\n",
      "Epoch [45/300], Step [19400/27733], Loss: 2.7664\n",
      "Epoch [45/300], Step [19500/27733], Loss: 3.1067\n",
      "Epoch [45/300], Step [19600/27733], Loss: 2.5729\n",
      "Epoch [45/300], Step [19700/27733], Loss: 3.8012\n",
      "Epoch [45/300], Step [19800/27733], Loss: 3.2733\n",
      "Epoch [45/300], Step [19900/27733], Loss: 2.5498\n",
      "Epoch [45/300], Step [20000/27733], Loss: 2.7272\n",
      "Epoch [45/300], Step [20100/27733], Loss: 3.3054\n",
      "Epoch [45/300], Step [20200/27733], Loss: 2.5528\n",
      "Epoch [45/300], Step [20300/27733], Loss: 2.3035\n",
      "Epoch [45/300], Step [20400/27733], Loss: 2.4612\n",
      "Epoch [45/300], Step [20500/27733], Loss: 3.0301\n",
      "Epoch [45/300], Step [20600/27733], Loss: 3.1533\n",
      "Epoch [45/300], Step [20700/27733], Loss: 3.6945\n",
      "Epoch [45/300], Step [20800/27733], Loss: 2.6245\n",
      "Epoch [45/300], Step [20900/27733], Loss: 3.0329\n",
      "Epoch [45/300], Step [21000/27733], Loss: 1.8192\n",
      "Epoch [45/300], Step [21100/27733], Loss: 2.7420\n",
      "Epoch [45/300], Step [21200/27733], Loss: 3.0110\n",
      "Epoch [45/300], Step [21300/27733], Loss: 2.5798\n",
      "Epoch [45/300], Step [21400/27733], Loss: 2.8316\n",
      "Epoch [45/300], Step [21500/27733], Loss: 3.3120\n",
      "Epoch [45/300], Step [21600/27733], Loss: 3.3648\n",
      "Epoch [45/300], Step [21700/27733], Loss: 2.3813\n",
      "Epoch [45/300], Step [21800/27733], Loss: 3.0424\n",
      "Epoch [45/300], Step [21900/27733], Loss: 3.1639\n",
      "Epoch [45/300], Step [22000/27733], Loss: 2.9937\n",
      "Epoch [45/300], Step [22100/27733], Loss: 2.9285\n",
      "Epoch [45/300], Step [22200/27733], Loss: 2.8406\n",
      "Epoch [45/300], Step [22300/27733], Loss: 2.2523\n",
      "Epoch [45/300], Step [22400/27733], Loss: 2.7165\n",
      "Epoch [45/300], Step [22500/27733], Loss: 2.8550\n",
      "Epoch [45/300], Step [22600/27733], Loss: 3.3998\n",
      "Epoch [45/300], Step [22700/27733], Loss: 4.1002\n",
      "Epoch [45/300], Step [22800/27733], Loss: 3.3461\n",
      "Epoch [45/300], Step [22900/27733], Loss: 2.6853\n",
      "Epoch [45/300], Step [23000/27733], Loss: 3.2272\n",
      "Epoch [45/300], Step [23100/27733], Loss: 2.5855\n",
      "Epoch [45/300], Step [23200/27733], Loss: 2.9253\n",
      "Epoch [45/300], Step [23300/27733], Loss: 2.8040\n",
      "Epoch [45/300], Step [23400/27733], Loss: 2.5608\n",
      "Epoch [45/300], Step [23500/27733], Loss: 2.9196\n",
      "Epoch [45/300], Step [23600/27733], Loss: 2.5755\n",
      "Epoch [45/300], Step [23700/27733], Loss: 2.1103\n",
      "Epoch [45/300], Step [23800/27733], Loss: 2.5142\n",
      "Epoch [45/300], Step [23900/27733], Loss: 2.8754\n",
      "Epoch [45/300], Step [24000/27733], Loss: 2.4187\n",
      "Epoch [45/300], Step [24100/27733], Loss: 3.1769\n",
      "Epoch [45/300], Step [24200/27733], Loss: 3.8479\n",
      "Epoch [45/300], Step [24300/27733], Loss: 3.6993\n",
      "Epoch [45/300], Step [24400/27733], Loss: 2.6206\n",
      "Epoch [45/300], Step [24500/27733], Loss: 2.2699\n",
      "Epoch [45/300], Step [24600/27733], Loss: 3.1038\n",
      "Epoch [45/300], Step [24700/27733], Loss: 4.2021\n",
      "Epoch [45/300], Step [24800/27733], Loss: 1.9133\n",
      "Epoch [45/300], Step [24900/27733], Loss: 2.8203\n",
      "Epoch [45/300], Step [25000/27733], Loss: 2.8701\n",
      "Epoch [45/300], Step [25100/27733], Loss: 2.6545\n",
      "Epoch [45/300], Step [25200/27733], Loss: 2.7738\n",
      "Epoch [45/300], Step [25300/27733], Loss: 2.9540\n",
      "Epoch [45/300], Step [25400/27733], Loss: 2.3182\n",
      "Epoch [45/300], Step [25500/27733], Loss: 2.4519\n",
      "Epoch [45/300], Step [25600/27733], Loss: 3.7725\n",
      "Epoch [45/300], Step [25700/27733], Loss: 3.2728\n",
      "Epoch [45/300], Step [25800/27733], Loss: 2.1534\n",
      "Epoch [45/300], Step [25900/27733], Loss: 2.4268\n",
      "Epoch [45/300], Step [26000/27733], Loss: 2.6473\n",
      "Epoch [45/300], Step [26100/27733], Loss: 2.4890\n",
      "Epoch [45/300], Step [26200/27733], Loss: 2.7785\n",
      "Epoch [45/300], Step [26300/27733], Loss: 2.2867\n",
      "Epoch [45/300], Step [26400/27733], Loss: 2.1102\n",
      "Epoch [45/300], Step [26500/27733], Loss: 2.9335\n",
      "Epoch [45/300], Step [26600/27733], Loss: 2.8279\n",
      "Epoch [45/300], Step [26700/27733], Loss: 3.1822\n",
      "Epoch [45/300], Step [26800/27733], Loss: 2.9727\n",
      "Epoch [45/300], Step [26900/27733], Loss: 3.3326\n",
      "Epoch [45/300], Step [27000/27733], Loss: 3.5835\n",
      "Epoch [45/300], Step [27100/27733], Loss: 2.5552\n",
      "Epoch [45/300], Step [27200/27733], Loss: 2.7087\n",
      "Epoch [45/300], Step [27300/27733], Loss: 3.2203\n",
      "Epoch [45/300], Step [27400/27733], Loss: 2.4527\n",
      "Epoch [45/300], Step [27500/27733], Loss: 2.7813\n",
      "Epoch [45/300], Step [27600/27733], Loss: 3.4501\n",
      "Epoch [45/300], Step [27700/27733], Loss: 3.2731\n",
      "Epoch [46/300], Step [100/27733], Loss: 1.8410\n",
      "Epoch [46/300], Step [200/27733], Loss: 2.3199\n",
      "Epoch [46/300], Step [300/27733], Loss: 2.3475\n",
      "Epoch [46/300], Step [400/27733], Loss: 1.8644\n",
      "Epoch [46/300], Step [500/27733], Loss: 2.3820\n",
      "Epoch [46/300], Step [600/27733], Loss: 1.7971\n",
      "Epoch [46/300], Step [700/27733], Loss: 1.9063\n",
      "Epoch [46/300], Step [800/27733], Loss: 2.1487\n",
      "Epoch [46/300], Step [900/27733], Loss: 2.9785\n",
      "Epoch [46/300], Step [1000/27733], Loss: 2.4276\n",
      "Epoch [46/300], Step [1100/27733], Loss: 1.8606\n",
      "Epoch [46/300], Step [1200/27733], Loss: 3.3920\n",
      "Epoch [46/300], Step [1300/27733], Loss: 3.0884\n",
      "Epoch [46/300], Step [1400/27733], Loss: 2.6111\n",
      "Epoch [46/300], Step [1500/27733], Loss: 2.3180\n",
      "Epoch [46/300], Step [1600/27733], Loss: 2.0139\n",
      "Epoch [46/300], Step [1700/27733], Loss: 2.0402\n",
      "Epoch [46/300], Step [1800/27733], Loss: 2.1036\n",
      "Epoch [46/300], Step [1900/27733], Loss: 2.2583\n",
      "Epoch [46/300], Step [2000/27733], Loss: 2.1166\n",
      "Epoch [46/300], Step [2100/27733], Loss: 2.3357\n",
      "Epoch [46/300], Step [2200/27733], Loss: 2.3402\n",
      "Epoch [46/300], Step [2300/27733], Loss: 1.9653\n",
      "Epoch [46/300], Step [2400/27733], Loss: 2.4217\n",
      "Epoch [46/300], Step [2500/27733], Loss: 2.5032\n",
      "Epoch [46/300], Step [2600/27733], Loss: 2.2083\n",
      "Epoch [46/300], Step [2700/27733], Loss: 1.9160\n",
      "Epoch [46/300], Step [2800/27733], Loss: 2.4651\n",
      "Epoch [46/300], Step [2900/27733], Loss: 2.7321\n",
      "Epoch [46/300], Step [3000/27733], Loss: 2.2604\n",
      "Epoch [46/300], Step [3100/27733], Loss: 3.9545\n",
      "Epoch [46/300], Step [3200/27733], Loss: 2.1042\n",
      "Epoch [46/300], Step [3300/27733], Loss: 2.7238\n",
      "Epoch [46/300], Step [3400/27733], Loss: 2.5440\n",
      "Epoch [46/300], Step [3500/27733], Loss: 2.0595\n",
      "Epoch [46/300], Step [3600/27733], Loss: 1.8060\n",
      "Epoch [46/300], Step [3700/27733], Loss: 2.8622\n",
      "Epoch [46/300], Step [3800/27733], Loss: 2.2184\n",
      "Epoch [46/300], Step [3900/27733], Loss: 3.0351\n",
      "Epoch [46/300], Step [4000/27733], Loss: 2.5973\n",
      "Epoch [46/300], Step [4100/27733], Loss: 2.2202\n",
      "Epoch [46/300], Step [4200/27733], Loss: 2.2145\n",
      "Epoch [46/300], Step [4300/27733], Loss: 2.2319\n",
      "Epoch [46/300], Step [4400/27733], Loss: 2.4627\n",
      "Epoch [46/300], Step [4500/27733], Loss: 3.0798\n",
      "Epoch [46/300], Step [4600/27733], Loss: 2.8629\n",
      "Epoch [46/300], Step [4700/27733], Loss: 3.1029\n",
      "Epoch [46/300], Step [4800/27733], Loss: 1.8900\n",
      "Epoch [46/300], Step [4900/27733], Loss: 2.8384\n",
      "Epoch [46/300], Step [5000/27733], Loss: 2.5807\n",
      "Epoch [46/300], Step [5100/27733], Loss: 2.7867\n",
      "Epoch [46/300], Step [5200/27733], Loss: 2.2872\n",
      "Epoch [46/300], Step [5300/27733], Loss: 2.2655\n",
      "Epoch [46/300], Step [5400/27733], Loss: 2.5539\n",
      "Epoch [46/300], Step [5500/27733], Loss: 3.1062\n",
      "Epoch [46/300], Step [5600/27733], Loss: 2.9417\n",
      "Epoch [46/300], Step [5700/27733], Loss: 2.8838\n",
      "Epoch [46/300], Step [5800/27733], Loss: 1.9645\n",
      "Epoch [46/300], Step [5900/27733], Loss: 2.8324\n",
      "Epoch [46/300], Step [6000/27733], Loss: 2.6796\n",
      "Epoch [46/300], Step [6100/27733], Loss: 2.2676\n",
      "Epoch [46/300], Step [6200/27733], Loss: 2.1960\n",
      "Epoch [46/300], Step [6300/27733], Loss: 2.0496\n",
      "Epoch [46/300], Step [6400/27733], Loss: 2.4273\n",
      "Epoch [46/300], Step [6500/27733], Loss: 2.8184\n",
      "Epoch [46/300], Step [6600/27733], Loss: 3.8188\n",
      "Epoch [46/300], Step [6700/27733], Loss: 2.9340\n",
      "Epoch [46/300], Step [6800/27733], Loss: 2.2346\n",
      "Epoch [46/300], Step [6900/27733], Loss: 2.4041\n",
      "Epoch [46/300], Step [7000/27733], Loss: 2.2339\n",
      "Epoch [46/300], Step [7100/27733], Loss: 1.8211\n",
      "Epoch [46/300], Step [7200/27733], Loss: 3.1116\n",
      "Epoch [46/300], Step [7300/27733], Loss: 3.7825\n",
      "Epoch [46/300], Step [7400/27733], Loss: 3.4277\n",
      "Epoch [46/300], Step [7500/27733], Loss: 2.7080\n",
      "Epoch [46/300], Step [7600/27733], Loss: 3.1982\n",
      "Epoch [46/300], Step [7700/27733], Loss: 2.4996\n",
      "Epoch [46/300], Step [7800/27733], Loss: 2.6654\n",
      "Epoch [46/300], Step [7900/27733], Loss: 2.8681\n",
      "Epoch [46/300], Step [8000/27733], Loss: 3.2210\n",
      "Epoch [46/300], Step [8100/27733], Loss: 3.2795\n",
      "Epoch [46/300], Step [8200/27733], Loss: 2.2251\n",
      "Epoch [46/300], Step [8300/27733], Loss: 2.4101\n",
      "Epoch [46/300], Step [8400/27733], Loss: 2.6630\n",
      "Epoch [46/300], Step [8500/27733], Loss: 1.9075\n",
      "Epoch [46/300], Step [8600/27733], Loss: 2.0971\n",
      "Epoch [46/300], Step [8700/27733], Loss: 2.6105\n",
      "Epoch [46/300], Step [8800/27733], Loss: 2.7555\n",
      "Epoch [46/300], Step [8900/27733], Loss: 2.7408\n",
      "Epoch [46/300], Step [9000/27733], Loss: 2.6406\n",
      "Epoch [46/300], Step [9100/27733], Loss: 2.6921\n",
      "Epoch [46/300], Step [9200/27733], Loss: 2.4756\n",
      "Epoch [46/300], Step [9300/27733], Loss: 2.9205\n",
      "Epoch [46/300], Step [9400/27733], Loss: 2.7447\n",
      "Epoch [46/300], Step [9500/27733], Loss: 2.9993\n",
      "Epoch [46/300], Step [9600/27733], Loss: 3.2911\n",
      "Epoch [46/300], Step [9700/27733], Loss: 1.8832\n",
      "Epoch [46/300], Step [9800/27733], Loss: 2.4319\n",
      "Epoch [46/300], Step [9900/27733], Loss: 2.5528\n",
      "Epoch [46/300], Step [10000/27733], Loss: 2.7075\n",
      "Epoch [46/300], Step [10100/27733], Loss: 2.3799\n",
      "Epoch [46/300], Step [10200/27733], Loss: 2.4934\n",
      "Epoch [46/300], Step [10300/27733], Loss: 3.2632\n",
      "Epoch [46/300], Step [10400/27733], Loss: 2.5381\n",
      "Epoch [46/300], Step [10500/27733], Loss: 2.8425\n",
      "Epoch [46/300], Step [10600/27733], Loss: 1.8706\n",
      "Epoch [46/300], Step [10700/27733], Loss: 2.2117\n",
      "Epoch [46/300], Step [10800/27733], Loss: 2.6427\n",
      "Epoch [46/300], Step [10900/27733], Loss: 3.4163\n",
      "Epoch [46/300], Step [11000/27733], Loss: 2.5216\n",
      "Epoch [46/300], Step [11100/27733], Loss: 1.9271\n",
      "Epoch [46/300], Step [11200/27733], Loss: 3.0670\n",
      "Epoch [46/300], Step [11300/27733], Loss: 2.6910\n",
      "Epoch [46/300], Step [11400/27733], Loss: 2.1853\n",
      "Epoch [46/300], Step [11500/27733], Loss: 3.2007\n",
      "Epoch [46/300], Step [11600/27733], Loss: 2.5561\n",
      "Epoch [46/300], Step [11700/27733], Loss: 2.2249\n",
      "Epoch [46/300], Step [11800/27733], Loss: 2.5141\n",
      "Epoch [46/300], Step [11900/27733], Loss: 2.8190\n",
      "Epoch [46/300], Step [12000/27733], Loss: 2.0990\n",
      "Epoch [46/300], Step [12100/27733], Loss: 3.5561\n",
      "Epoch [46/300], Step [12200/27733], Loss: 2.5001\n",
      "Epoch [46/300], Step [12300/27733], Loss: 2.6331\n",
      "Epoch [46/300], Step [12400/27733], Loss: 2.4154\n",
      "Epoch [46/300], Step [12500/27733], Loss: 2.0887\n",
      "Epoch [46/300], Step [12600/27733], Loss: 3.4998\n",
      "Epoch [46/300], Step [12700/27733], Loss: 3.1296\n",
      "Epoch [46/300], Step [12800/27733], Loss: 3.5557\n",
      "Epoch [46/300], Step [12900/27733], Loss: 2.3259\n",
      "Epoch [46/300], Step [13000/27733], Loss: 2.3024\n",
      "Epoch [46/300], Step [13100/27733], Loss: 3.1159\n",
      "Epoch [46/300], Step [13200/27733], Loss: 2.1813\n",
      "Epoch [46/300], Step [13300/27733], Loss: 3.3064\n",
      "Epoch [46/300], Step [13400/27733], Loss: 2.7992\n",
      "Epoch [46/300], Step [13500/27733], Loss: 2.8083\n",
      "Epoch [46/300], Step [13600/27733], Loss: 3.0431\n",
      "Epoch [46/300], Step [13700/27733], Loss: 2.4398\n",
      "Epoch [46/300], Step [13800/27733], Loss: 2.7028\n",
      "Epoch [46/300], Step [13900/27733], Loss: 2.7305\n",
      "Epoch [46/300], Step [14000/27733], Loss: 3.0673\n",
      "Epoch [46/300], Step [14100/27733], Loss: 3.0372\n",
      "Epoch [46/300], Step [14200/27733], Loss: 2.6514\n",
      "Epoch [46/300], Step [14300/27733], Loss: 3.2479\n",
      "Epoch [46/300], Step [14400/27733], Loss: 3.5612\n",
      "Epoch [46/300], Step [14500/27733], Loss: 2.3851\n",
      "Epoch [46/300], Step [14600/27733], Loss: 2.3963\n",
      "Epoch [46/300], Step [14700/27733], Loss: 2.9683\n",
      "Epoch [46/300], Step [14800/27733], Loss: 2.7885\n",
      "Epoch [46/300], Step [14900/27733], Loss: 2.8812\n",
      "Epoch [46/300], Step [15000/27733], Loss: 2.4656\n",
      "Epoch [46/300], Step [15100/27733], Loss: 2.6455\n",
      "Epoch [46/300], Step [15200/27733], Loss: 2.9307\n",
      "Epoch [46/300], Step [15300/27733], Loss: 2.5407\n",
      "Epoch [46/300], Step [15400/27733], Loss: 2.8496\n",
      "Epoch [46/300], Step [15500/27733], Loss: 2.2975\n",
      "Epoch [46/300], Step [15600/27733], Loss: 2.9224\n",
      "Epoch [46/300], Step [15700/27733], Loss: 3.0036\n",
      "Epoch [46/300], Step [15800/27733], Loss: 1.8284\n",
      "Epoch [46/300], Step [15900/27733], Loss: 3.1172\n",
      "Epoch [46/300], Step [16000/27733], Loss: 2.5761\n",
      "Epoch [46/300], Step [16100/27733], Loss: 3.0755\n",
      "Epoch [46/300], Step [16200/27733], Loss: 3.0190\n",
      "Epoch [46/300], Step [16300/27733], Loss: 2.1932\n",
      "Epoch [46/300], Step [16400/27733], Loss: 2.0335\n",
      "Epoch [46/300], Step [16500/27733], Loss: 2.1818\n",
      "Epoch [46/300], Step [16600/27733], Loss: 2.8579\n",
      "Epoch [46/300], Step [16700/27733], Loss: 2.3227\n",
      "Epoch [46/300], Step [16800/27733], Loss: 3.0151\n",
      "Epoch [46/300], Step [16900/27733], Loss: 3.2596\n",
      "Epoch [46/300], Step [17000/27733], Loss: 1.6606\n",
      "Epoch [46/300], Step [17100/27733], Loss: 2.5808\n",
      "Epoch [46/300], Step [17200/27733], Loss: 3.1739\n",
      "Epoch [46/300], Step [17300/27733], Loss: 2.8407\n",
      "Epoch [46/300], Step [17400/27733], Loss: 3.3563\n",
      "Epoch [46/300], Step [17500/27733], Loss: 2.1296\n",
      "Epoch [46/300], Step [17600/27733], Loss: 2.4406\n",
      "Epoch [46/300], Step [17700/27733], Loss: 2.8698\n",
      "Epoch [46/300], Step [17800/27733], Loss: 3.0269\n",
      "Epoch [46/300], Step [17900/27733], Loss: 2.9559\n",
      "Epoch [46/300], Step [18000/27733], Loss: 2.2594\n",
      "Epoch [46/300], Step [18100/27733], Loss: 2.4931\n",
      "Epoch [46/300], Step [18200/27733], Loss: 3.2287\n",
      "Epoch [46/300], Step [18300/27733], Loss: 2.3167\n",
      "Epoch [46/300], Step [18400/27733], Loss: 2.5549\n",
      "Epoch [46/300], Step [18500/27733], Loss: 2.5695\n",
      "Epoch [46/300], Step [18600/27733], Loss: 2.3232\n",
      "Epoch [46/300], Step [18700/27733], Loss: 2.6029\n",
      "Epoch [46/300], Step [18800/27733], Loss: 2.9918\n",
      "Epoch [46/300], Step [18900/27733], Loss: 2.2602\n",
      "Epoch [46/300], Step [19000/27733], Loss: 2.2443\n",
      "Epoch [46/300], Step [19100/27733], Loss: 3.7564\n",
      "Epoch [46/300], Step [19200/27733], Loss: 2.6997\n",
      "Epoch [46/300], Step [19300/27733], Loss: 3.7986\n",
      "Epoch [46/300], Step [19400/27733], Loss: 2.3894\n",
      "Epoch [46/300], Step [19500/27733], Loss: 2.5045\n",
      "Epoch [46/300], Step [19600/27733], Loss: 2.5130\n",
      "Epoch [46/300], Step [19700/27733], Loss: 3.2755\n",
      "Epoch [46/300], Step [19800/27733], Loss: 3.5398\n",
      "Epoch [46/300], Step [19900/27733], Loss: 3.2344\n",
      "Epoch [46/300], Step [20000/27733], Loss: 3.1139\n",
      "Epoch [46/300], Step [20100/27733], Loss: 2.9883\n",
      "Epoch [46/300], Step [20200/27733], Loss: 2.1089\n",
      "Epoch [46/300], Step [20300/27733], Loss: 2.7165\n",
      "Epoch [46/300], Step [20400/27733], Loss: 3.7214\n",
      "Epoch [46/300], Step [20500/27733], Loss: 2.8310\n",
      "Epoch [46/300], Step [20600/27733], Loss: 2.3445\n",
      "Epoch [46/300], Step [20700/27733], Loss: 2.8246\n",
      "Epoch [46/300], Step [20800/27733], Loss: 2.7593\n",
      "Epoch [46/300], Step [20900/27733], Loss: 3.1585\n",
      "Epoch [46/300], Step [21000/27733], Loss: 3.0875\n",
      "Epoch [46/300], Step [21100/27733], Loss: 3.0485\n",
      "Epoch [46/300], Step [21200/27733], Loss: 2.8489\n",
      "Epoch [46/300], Step [21300/27733], Loss: 2.6950\n",
      "Epoch [46/300], Step [21400/27733], Loss: 2.7917\n",
      "Epoch [46/300], Step [21500/27733], Loss: 3.4533\n",
      "Epoch [46/300], Step [21600/27733], Loss: 2.8203\n",
      "Epoch [46/300], Step [21700/27733], Loss: 2.9277\n",
      "Epoch [46/300], Step [21800/27733], Loss: 2.5920\n",
      "Epoch [46/300], Step [21900/27733], Loss: 2.6667\n",
      "Epoch [46/300], Step [22000/27733], Loss: 3.3075\n",
      "Epoch [46/300], Step [22100/27733], Loss: 2.4987\n",
      "Epoch [46/300], Step [22200/27733], Loss: 3.4386\n",
      "Epoch [46/300], Step [22300/27733], Loss: 3.0657\n",
      "Epoch [46/300], Step [22400/27733], Loss: 3.0344\n",
      "Epoch [46/300], Step [22500/27733], Loss: 2.3812\n",
      "Epoch [46/300], Step [22600/27733], Loss: 2.8633\n",
      "Epoch [46/300], Step [22700/27733], Loss: 2.9936\n",
      "Epoch [46/300], Step [22800/27733], Loss: 2.8413\n",
      "Epoch [46/300], Step [22900/27733], Loss: 3.3381\n",
      "Epoch [46/300], Step [23000/27733], Loss: 2.7145\n",
      "Epoch [46/300], Step [23100/27733], Loss: 3.2624\n",
      "Epoch [46/300], Step [23200/27733], Loss: 2.6722\n",
      "Epoch [46/300], Step [23300/27733], Loss: 2.5245\n",
      "Epoch [46/300], Step [23400/27733], Loss: 2.5764\n",
      "Epoch [46/300], Step [23500/27733], Loss: 2.9232\n",
      "Epoch [46/300], Step [23600/27733], Loss: 3.0561\n",
      "Epoch [46/300], Step [23700/27733], Loss: 2.6911\n",
      "Epoch [46/300], Step [23800/27733], Loss: 3.2635\n",
      "Epoch [46/300], Step [23900/27733], Loss: 2.1423\n",
      "Epoch [46/300], Step [24000/27733], Loss: 2.6054\n",
      "Epoch [46/300], Step [24100/27733], Loss: 2.7979\n",
      "Epoch [46/300], Step [24200/27733], Loss: 3.2923\n",
      "Epoch [46/300], Step [24300/27733], Loss: 2.8352\n",
      "Epoch [46/300], Step [24400/27733], Loss: 2.4166\n",
      "Epoch [46/300], Step [24500/27733], Loss: 2.6959\n",
      "Epoch [46/300], Step [24600/27733], Loss: 2.6730\n",
      "Epoch [46/300], Step [24700/27733], Loss: 2.9447\n",
      "Epoch [46/300], Step [24800/27733], Loss: 2.8815\n",
      "Epoch [46/300], Step [24900/27733], Loss: 2.2169\n",
      "Epoch [46/300], Step [25000/27733], Loss: 2.3676\n",
      "Epoch [46/300], Step [25100/27733], Loss: 2.6414\n",
      "Epoch [46/300], Step [25200/27733], Loss: 2.3004\n",
      "Epoch [46/300], Step [25300/27733], Loss: 2.9662\n",
      "Epoch [46/300], Step [25400/27733], Loss: 3.7105\n",
      "Epoch [46/300], Step [25500/27733], Loss: 2.9852\n",
      "Epoch [46/300], Step [25600/27733], Loss: 3.1690\n",
      "Epoch [46/300], Step [25700/27733], Loss: 3.2778\n",
      "Epoch [46/300], Step [25800/27733], Loss: 2.8857\n",
      "Epoch [46/300], Step [25900/27733], Loss: 4.2358\n",
      "Epoch [46/300], Step [26000/27733], Loss: 3.4734\n",
      "Epoch [46/300], Step [26100/27733], Loss: 2.7571\n",
      "Epoch [46/300], Step [26200/27733], Loss: 2.7072\n",
      "Epoch [46/300], Step [26300/27733], Loss: 2.8789\n",
      "Epoch [46/300], Step [26400/27733], Loss: 2.3924\n",
      "Epoch [46/300], Step [26500/27733], Loss: 2.5310\n",
      "Epoch [46/300], Step [26600/27733], Loss: 2.8934\n",
      "Epoch [46/300], Step [26700/27733], Loss: 2.7074\n",
      "Epoch [46/300], Step [26800/27733], Loss: 2.9517\n",
      "Epoch [46/300], Step [26900/27733], Loss: 2.8616\n",
      "Epoch [46/300], Step [27000/27733], Loss: 2.2361\n",
      "Epoch [46/300], Step [27100/27733], Loss: 2.8371\n",
      "Epoch [46/300], Step [27200/27733], Loss: 3.0071\n",
      "Epoch [46/300], Step [27300/27733], Loss: 3.3465\n",
      "Epoch [46/300], Step [27400/27733], Loss: 2.6223\n",
      "Epoch [46/300], Step [27500/27733], Loss: 2.5066\n",
      "Epoch [46/300], Step [27600/27733], Loss: 2.6993\n",
      "Epoch [46/300], Step [27700/27733], Loss: 2.7395\n",
      "Epoch [47/300], Step [100/27733], Loss: 2.9795\n",
      "Epoch [47/300], Step [200/27733], Loss: 2.6370\n",
      "Epoch [47/300], Step [300/27733], Loss: 2.4502\n",
      "Epoch [47/300], Step [400/27733], Loss: 2.4977\n",
      "Epoch [47/300], Step [500/27733], Loss: 2.2544\n",
      "Epoch [47/300], Step [600/27733], Loss: 2.5480\n",
      "Epoch [47/300], Step [700/27733], Loss: 3.4222\n",
      "Epoch [47/300], Step [800/27733], Loss: 2.2028\n",
      "Epoch [47/300], Step [900/27733], Loss: 2.8523\n",
      "Epoch [47/300], Step [1000/27733], Loss: 1.7056\n",
      "Epoch [47/300], Step [1100/27733], Loss: 3.1103\n",
      "Epoch [47/300], Step [1200/27733], Loss: 2.5557\n",
      "Epoch [47/300], Step [1300/27733], Loss: 2.0888\n",
      "Epoch [47/300], Step [1400/27733], Loss: 2.0354\n",
      "Epoch [47/300], Step [1500/27733], Loss: 2.0713\n",
      "Epoch [47/300], Step [1600/27733], Loss: 2.2968\n",
      "Epoch [47/300], Step [1700/27733], Loss: 2.7740\n",
      "Epoch [47/300], Step [1800/27733], Loss: 2.5693\n",
      "Epoch [47/300], Step [1900/27733], Loss: 2.0256\n",
      "Epoch [47/300], Step [2000/27733], Loss: 2.3797\n",
      "Epoch [47/300], Step [2100/27733], Loss: 1.8928\n",
      "Epoch [47/300], Step [2200/27733], Loss: 1.9086\n",
      "Epoch [47/300], Step [2300/27733], Loss: 2.2451\n",
      "Epoch [47/300], Step [2400/27733], Loss: 2.7181\n",
      "Epoch [47/300], Step [2500/27733], Loss: 1.8937\n",
      "Epoch [47/300], Step [2600/27733], Loss: 2.9054\n",
      "Epoch [47/300], Step [2700/27733], Loss: 1.6325\n",
      "Epoch [47/300], Step [2800/27733], Loss: 2.7762\n",
      "Epoch [47/300], Step [2900/27733], Loss: 2.8949\n",
      "Epoch [47/300], Step [3000/27733], Loss: 2.9978\n",
      "Epoch [47/300], Step [3100/27733], Loss: 3.1189\n",
      "Epoch [47/300], Step [3200/27733], Loss: 3.0057\n",
      "Epoch [47/300], Step [3300/27733], Loss: 3.0029\n",
      "Epoch [47/300], Step [3400/27733], Loss: 2.7276\n",
      "Epoch [47/300], Step [3500/27733], Loss: 2.4789\n",
      "Epoch [47/300], Step [3600/27733], Loss: 3.0259\n",
      "Epoch [47/300], Step [3700/27733], Loss: 2.1675\n",
      "Epoch [47/300], Step [3800/27733], Loss: 1.7305\n",
      "Epoch [47/300], Step [3900/27733], Loss: 2.5311\n",
      "Epoch [47/300], Step [4000/27733], Loss: 1.8344\n",
      "Epoch [47/300], Step [4100/27733], Loss: 1.8271\n",
      "Epoch [47/300], Step [4200/27733], Loss: 2.9052\n",
      "Epoch [47/300], Step [4300/27733], Loss: 2.5704\n",
      "Epoch [47/300], Step [4400/27733], Loss: 2.2391\n",
      "Epoch [47/300], Step [4500/27733], Loss: 2.5419\n",
      "Epoch [47/300], Step [4600/27733], Loss: 2.5291\n",
      "Epoch [47/300], Step [4700/27733], Loss: 2.1758\n",
      "Epoch [47/300], Step [4800/27733], Loss: 2.7920\n",
      "Epoch [47/300], Step [4900/27733], Loss: 2.1350\n",
      "Epoch [47/300], Step [5000/27733], Loss: 1.9406\n",
      "Epoch [47/300], Step [5100/27733], Loss: 2.8004\n",
      "Epoch [47/300], Step [5200/27733], Loss: 2.0555\n",
      "Epoch [47/300], Step [5300/27733], Loss: 2.2179\n",
      "Epoch [47/300], Step [5400/27733], Loss: 1.8433\n",
      "Epoch [47/300], Step [5500/27733], Loss: 1.8872\n",
      "Epoch [47/300], Step [5600/27733], Loss: 2.5105\n",
      "Epoch [47/300], Step [5700/27733], Loss: 2.8191\n",
      "Epoch [47/300], Step [5800/27733], Loss: 2.0155\n",
      "Epoch [47/300], Step [5900/27733], Loss: 2.1957\n",
      "Epoch [47/300], Step [6000/27733], Loss: 2.0215\n",
      "Epoch [47/300], Step [6100/27733], Loss: 2.7421\n",
      "Epoch [47/300], Step [6200/27733], Loss: 1.8636\n",
      "Epoch [47/300], Step [6300/27733], Loss: 2.3847\n",
      "Epoch [47/300], Step [6400/27733], Loss: 2.9540\n",
      "Epoch [47/300], Step [6500/27733], Loss: 2.4247\n",
      "Epoch [47/300], Step [6600/27733], Loss: 2.6962\n",
      "Epoch [47/300], Step [6700/27733], Loss: 2.1805\n",
      "Epoch [47/300], Step [6800/27733], Loss: 2.5801\n",
      "Epoch [47/300], Step [6900/27733], Loss: 2.3748\n",
      "Epoch [47/300], Step [7000/27733], Loss: 2.9012\n",
      "Epoch [47/300], Step [7100/27733], Loss: 2.6490\n",
      "Epoch [47/300], Step [7200/27733], Loss: 1.6146\n",
      "Epoch [47/300], Step [7300/27733], Loss: 2.4928\n",
      "Epoch [47/300], Step [7400/27733], Loss: 2.0259\n",
      "Epoch [47/300], Step [7500/27733], Loss: 2.5782\n",
      "Epoch [47/300], Step [7600/27733], Loss: 2.5696\n",
      "Epoch [47/300], Step [7700/27733], Loss: 2.8857\n",
      "Epoch [47/300], Step [7800/27733], Loss: 2.4975\n",
      "Epoch [47/300], Step [7900/27733], Loss: 2.5724\n",
      "Epoch [47/300], Step [8000/27733], Loss: 2.5919\n",
      "Epoch [47/300], Step [8100/27733], Loss: 2.1226\n",
      "Epoch [47/300], Step [8200/27733], Loss: 2.8009\n",
      "Epoch [47/300], Step [8300/27733], Loss: 3.0160\n",
      "Epoch [47/300], Step [8400/27733], Loss: 2.6141\n",
      "Epoch [47/300], Step [8500/27733], Loss: 2.4878\n",
      "Epoch [47/300], Step [8600/27733], Loss: 2.2957\n",
      "Epoch [47/300], Step [8700/27733], Loss: 2.4591\n",
      "Epoch [47/300], Step [8800/27733], Loss: 2.8410\n",
      "Epoch [47/300], Step [8900/27733], Loss: 2.4872\n",
      "Epoch [47/300], Step [9000/27733], Loss: 2.3832\n",
      "Epoch [47/300], Step [9100/27733], Loss: 2.6495\n",
      "Epoch [47/300], Step [9200/27733], Loss: 2.3457\n",
      "Epoch [47/300], Step [9300/27733], Loss: 3.0398\n",
      "Epoch [47/300], Step [9400/27733], Loss: 2.5647\n",
      "Epoch [47/300], Step [9500/27733], Loss: 2.5536\n",
      "Epoch [47/300], Step [9600/27733], Loss: 3.2942\n",
      "Epoch [47/300], Step [9700/27733], Loss: 2.9727\n",
      "Epoch [47/300], Step [9800/27733], Loss: 3.4665\n",
      "Epoch [47/300], Step [9900/27733], Loss: 3.6309\n",
      "Epoch [47/300], Step [10000/27733], Loss: 2.4375\n",
      "Epoch [47/300], Step [10100/27733], Loss: 2.1761\n",
      "Epoch [47/300], Step [10200/27733], Loss: 2.7128\n",
      "Epoch [47/300], Step [10300/27733], Loss: 2.6517\n",
      "Epoch [47/300], Step [10400/27733], Loss: 2.4967\n",
      "Epoch [47/300], Step [10500/27733], Loss: 2.9399\n",
      "Epoch [47/300], Step [10600/27733], Loss: 2.6745\n",
      "Epoch [47/300], Step [10700/27733], Loss: 2.7005\n",
      "Epoch [47/300], Step [10800/27733], Loss: 2.5881\n",
      "Epoch [47/300], Step [10900/27733], Loss: 3.1093\n",
      "Epoch [47/300], Step [11000/27733], Loss: 2.3464\n",
      "Epoch [47/300], Step [11100/27733], Loss: 2.2063\n",
      "Epoch [47/300], Step [11200/27733], Loss: 2.8694\n",
      "Epoch [47/300], Step [11300/27733], Loss: 2.6374\n",
      "Epoch [47/300], Step [11400/27733], Loss: 2.6332\n",
      "Epoch [47/300], Step [11500/27733], Loss: 3.0754\n",
      "Epoch [47/300], Step [11600/27733], Loss: 2.9754\n",
      "Epoch [47/300], Step [11700/27733], Loss: 2.5839\n",
      "Epoch [47/300], Step [11800/27733], Loss: 2.2789\n",
      "Epoch [47/300], Step [11900/27733], Loss: 3.3088\n",
      "Epoch [47/300], Step [12000/27733], Loss: 2.8834\n",
      "Epoch [47/300], Step [12100/27733], Loss: 2.2790\n",
      "Epoch [47/300], Step [12200/27733], Loss: 2.6633\n",
      "Epoch [47/300], Step [12300/27733], Loss: 2.1584\n",
      "Epoch [47/300], Step [12400/27733], Loss: 2.7740\n",
      "Epoch [47/300], Step [12500/27733], Loss: 2.2915\n",
      "Epoch [47/300], Step [12600/27733], Loss: 2.2015\n",
      "Epoch [47/300], Step [12700/27733], Loss: 3.2118\n",
      "Epoch [47/300], Step [12800/27733], Loss: 2.8213\n",
      "Epoch [47/300], Step [12900/27733], Loss: 2.3965\n",
      "Epoch [47/300], Step [13000/27733], Loss: 2.9770\n",
      "Epoch [47/300], Step [13100/27733], Loss: 2.4240\n",
      "Epoch [47/300], Step [13200/27733], Loss: 2.6252\n",
      "Epoch [47/300], Step [13300/27733], Loss: 3.2575\n",
      "Epoch [47/300], Step [13400/27733], Loss: 2.2813\n",
      "Epoch [47/300], Step [13500/27733], Loss: 2.9089\n",
      "Epoch [47/300], Step [13600/27733], Loss: 2.2772\n",
      "Epoch [47/300], Step [13700/27733], Loss: 2.3998\n",
      "Epoch [47/300], Step [13800/27733], Loss: 2.2785\n",
      "Epoch [47/300], Step [13900/27733], Loss: 2.4571\n",
      "Epoch [47/300], Step [14000/27733], Loss: 2.1476\n",
      "Epoch [47/300], Step [14100/27733], Loss: 2.5133\n",
      "Epoch [47/300], Step [14200/27733], Loss: 3.1015\n",
      "Epoch [47/300], Step [14300/27733], Loss: 3.0268\n",
      "Epoch [47/300], Step [14400/27733], Loss: 2.5319\n",
      "Epoch [47/300], Step [14500/27733], Loss: 3.2050\n",
      "Epoch [47/300], Step [14600/27733], Loss: 2.7851\n",
      "Epoch [47/300], Step [14700/27733], Loss: 2.4066\n",
      "Epoch [47/300], Step [14800/27733], Loss: 2.6267\n",
      "Epoch [47/300], Step [14900/27733], Loss: 2.9089\n",
      "Epoch [47/300], Step [15000/27733], Loss: 3.2587\n",
      "Epoch [47/300], Step [15100/27733], Loss: 2.4755\n",
      "Epoch [47/300], Step [15200/27733], Loss: 2.7054\n",
      "Epoch [47/300], Step [15300/27733], Loss: 2.0588\n",
      "Epoch [47/300], Step [15400/27733], Loss: 3.1184\n",
      "Epoch [47/300], Step [15500/27733], Loss: 2.6891\n",
      "Epoch [47/300], Step [15600/27733], Loss: 1.8279\n",
      "Epoch [47/300], Step [15700/27733], Loss: 2.6492\n",
      "Epoch [47/300], Step [15800/27733], Loss: 3.1943\n",
      "Epoch [47/300], Step [15900/27733], Loss: 2.3003\n",
      "Epoch [47/300], Step [16000/27733], Loss: 3.3020\n",
      "Epoch [47/300], Step [16100/27733], Loss: 3.1050\n",
      "Epoch [47/300], Step [16200/27733], Loss: 2.3365\n",
      "Epoch [47/300], Step [16300/27733], Loss: 1.8870\n",
      "Epoch [47/300], Step [16400/27733], Loss: 2.5025\n",
      "Epoch [47/300], Step [16500/27733], Loss: 2.9877\n",
      "Epoch [47/300], Step [16600/27733], Loss: 2.0807\n",
      "Epoch [47/300], Step [16700/27733], Loss: 2.2909\n",
      "Epoch [47/300], Step [16800/27733], Loss: 2.8263\n",
      "Epoch [47/300], Step [16900/27733], Loss: 2.5809\n",
      "Epoch [47/300], Step [17000/27733], Loss: 2.7462\n",
      "Epoch [47/300], Step [17100/27733], Loss: 2.8956\n",
      "Epoch [47/300], Step [17200/27733], Loss: 3.4854\n",
      "Epoch [47/300], Step [17300/27733], Loss: 3.1477\n",
      "Epoch [47/300], Step [17400/27733], Loss: 3.5691\n",
      "Epoch [47/300], Step [17500/27733], Loss: 2.9969\n",
      "Epoch [47/300], Step [17600/27733], Loss: 2.3433\n",
      "Epoch [47/300], Step [17700/27733], Loss: 2.6865\n",
      "Epoch [47/300], Step [17800/27733], Loss: 2.6496\n",
      "Epoch [47/300], Step [17900/27733], Loss: 3.5372\n",
      "Epoch [47/300], Step [18000/27733], Loss: 2.2733\n",
      "Epoch [47/300], Step [18100/27733], Loss: 2.4120\n",
      "Epoch [47/300], Step [18200/27733], Loss: 2.8201\n",
      "Epoch [47/300], Step [18300/27733], Loss: 3.5686\n",
      "Epoch [47/300], Step [18400/27733], Loss: 2.3351\n",
      "Epoch [47/300], Step [18500/27733], Loss: 3.0705\n",
      "Epoch [47/300], Step [18600/27733], Loss: 2.5090\n",
      "Epoch [47/300], Step [18700/27733], Loss: 2.6703\n",
      "Epoch [47/300], Step [18800/27733], Loss: 2.5863\n",
      "Epoch [47/300], Step [18900/27733], Loss: 2.5783\n",
      "Epoch [47/300], Step [19000/27733], Loss: 2.8716\n",
      "Epoch [47/300], Step [19100/27733], Loss: 2.2911\n",
      "Epoch [47/300], Step [19200/27733], Loss: 2.9505\n",
      "Epoch [47/300], Step [19300/27733], Loss: 2.9311\n",
      "Epoch [47/300], Step [19400/27733], Loss: 2.8130\n",
      "Epoch [47/300], Step [19500/27733], Loss: 2.7219\n",
      "Epoch [47/300], Step [19600/27733], Loss: 2.4996\n",
      "Epoch [47/300], Step [19700/27733], Loss: 2.4420\n",
      "Epoch [47/300], Step [19800/27733], Loss: 3.2438\n",
      "Epoch [47/300], Step [19900/27733], Loss: 2.7936\n",
      "Epoch [47/300], Step [20000/27733], Loss: 1.9159\n",
      "Epoch [47/300], Step [20100/27733], Loss: 3.0458\n",
      "Epoch [47/300], Step [20200/27733], Loss: 3.7254\n",
      "Epoch [47/300], Step [20300/27733], Loss: 2.9184\n",
      "Epoch [47/300], Step [20400/27733], Loss: 3.3250\n",
      "Epoch [47/300], Step [20500/27733], Loss: 2.2086\n",
      "Epoch [47/300], Step [20600/27733], Loss: 2.9348\n",
      "Epoch [47/300], Step [20700/27733], Loss: 2.7270\n",
      "Epoch [47/300], Step [20800/27733], Loss: 2.8863\n",
      "Epoch [47/300], Step [20900/27733], Loss: 2.9212\n",
      "Epoch [47/300], Step [21000/27733], Loss: 2.5683\n",
      "Epoch [47/300], Step [21100/27733], Loss: 3.7989\n",
      "Epoch [47/300], Step [21200/27733], Loss: 2.6490\n",
      "Epoch [47/300], Step [21300/27733], Loss: 2.7337\n",
      "Epoch [47/300], Step [21400/27733], Loss: 2.6086\n",
      "Epoch [47/300], Step [21500/27733], Loss: 3.7302\n",
      "Epoch [47/300], Step [21600/27733], Loss: 2.3279\n",
      "Epoch [47/300], Step [21700/27733], Loss: 3.7678\n",
      "Epoch [47/300], Step [21800/27733], Loss: 2.9069\n",
      "Epoch [47/300], Step [21900/27733], Loss: 2.3909\n",
      "Epoch [47/300], Step [22000/27733], Loss: 2.8131\n",
      "Epoch [47/300], Step [22100/27733], Loss: 3.1387\n",
      "Epoch [47/300], Step [22200/27733], Loss: 3.2809\n",
      "Epoch [47/300], Step [22300/27733], Loss: 2.8621\n",
      "Epoch [47/300], Step [22400/27733], Loss: 3.0846\n",
      "Epoch [47/300], Step [22500/27733], Loss: 2.8122\n",
      "Epoch [47/300], Step [22600/27733], Loss: 3.2495\n",
      "Epoch [47/300], Step [22700/27733], Loss: 2.8722\n",
      "Epoch [47/300], Step [22800/27733], Loss: 2.8262\n",
      "Epoch [47/300], Step [22900/27733], Loss: 3.9930\n",
      "Epoch [47/300], Step [23000/27733], Loss: 2.8422\n",
      "Epoch [47/300], Step [23100/27733], Loss: 1.9311\n",
      "Epoch [47/300], Step [23200/27733], Loss: 3.2298\n",
      "Epoch [47/300], Step [23300/27733], Loss: 2.6814\n",
      "Epoch [47/300], Step [23400/27733], Loss: 3.2042\n",
      "Epoch [47/300], Step [23500/27733], Loss: 3.7161\n",
      "Epoch [47/300], Step [23600/27733], Loss: 2.8757\n",
      "Epoch [47/300], Step [23700/27733], Loss: 3.5074\n",
      "Epoch [47/300], Step [23800/27733], Loss: 3.2030\n",
      "Epoch [47/300], Step [23900/27733], Loss: 3.0556\n",
      "Epoch [47/300], Step [24000/27733], Loss: 3.0183\n",
      "Epoch [47/300], Step [24100/27733], Loss: 3.4304\n",
      "Epoch [47/300], Step [24200/27733], Loss: 3.2433\n",
      "Epoch [47/300], Step [24300/27733], Loss: 2.8722\n",
      "Epoch [47/300], Step [24400/27733], Loss: 2.9509\n",
      "Epoch [47/300], Step [24500/27733], Loss: 3.1584\n",
      "Epoch [47/300], Step [24600/27733], Loss: 2.6693\n",
      "Epoch [47/300], Step [24700/27733], Loss: 3.5129\n",
      "Epoch [47/300], Step [24800/27733], Loss: 2.3610\n",
      "Epoch [47/300], Step [24900/27733], Loss: 3.2027\n",
      "Epoch [47/300], Step [25000/27733], Loss: 2.8325\n",
      "Epoch [47/300], Step [25100/27733], Loss: 4.0022\n",
      "Epoch [47/300], Step [25200/27733], Loss: 3.2003\n",
      "Epoch [47/300], Step [25300/27733], Loss: 2.7413\n",
      "Epoch [47/300], Step [25400/27733], Loss: 3.1422\n",
      "Epoch [47/300], Step [25500/27733], Loss: 2.5948\n",
      "Epoch [47/300], Step [25600/27733], Loss: 3.0933\n",
      "Epoch [47/300], Step [25700/27733], Loss: 3.1141\n",
      "Epoch [47/300], Step [25800/27733], Loss: 3.8529\n",
      "Epoch [47/300], Step [25900/27733], Loss: 2.1905\n",
      "Epoch [47/300], Step [26000/27733], Loss: 2.8553\n",
      "Epoch [47/300], Step [26100/27733], Loss: 3.2429\n",
      "Epoch [47/300], Step [26200/27733], Loss: 4.0506\n",
      "Epoch [47/300], Step [26300/27733], Loss: 2.8535\n",
      "Epoch [47/300], Step [26400/27733], Loss: 2.6427\n",
      "Epoch [47/300], Step [26500/27733], Loss: 2.7470\n",
      "Epoch [47/300], Step [26600/27733], Loss: 3.2472\n",
      "Epoch [47/300], Step [26700/27733], Loss: 3.3128\n",
      "Epoch [47/300], Step [26800/27733], Loss: 3.0581\n",
      "Epoch [47/300], Step [26900/27733], Loss: 3.0400\n",
      "Epoch [47/300], Step [27000/27733], Loss: 3.6763\n",
      "Epoch [47/300], Step [27100/27733], Loss: 3.4812\n",
      "Epoch [47/300], Step [27200/27733], Loss: 3.5488\n",
      "Epoch [47/300], Step [27300/27733], Loss: 3.3501\n",
      "Epoch [47/300], Step [27400/27733], Loss: 2.4321\n",
      "Epoch [47/300], Step [27500/27733], Loss: 3.2754\n",
      "Epoch [47/300], Step [27600/27733], Loss: 2.1674\n",
      "Epoch [47/300], Step [27700/27733], Loss: 3.3651\n",
      "Epoch [48/300], Step [100/27733], Loss: 2.2179\n",
      "Epoch [48/300], Step [200/27733], Loss: 2.1577\n",
      "Epoch [48/300], Step [300/27733], Loss: 2.5875\n",
      "Epoch [48/300], Step [400/27733], Loss: 2.9784\n",
      "Epoch [48/300], Step [500/27733], Loss: 2.0460\n",
      "Epoch [48/300], Step [600/27733], Loss: 2.8385\n",
      "Epoch [48/300], Step [700/27733], Loss: 2.2722\n",
      "Epoch [48/300], Step [800/27733], Loss: 1.4335\n",
      "Epoch [48/300], Step [900/27733], Loss: 2.4459\n",
      "Epoch [48/300], Step [1000/27733], Loss: 2.4542\n",
      "Epoch [48/300], Step [1100/27733], Loss: 2.2143\n",
      "Epoch [48/300], Step [1200/27733], Loss: 2.2391\n",
      "Epoch [48/300], Step [1300/27733], Loss: 2.5647\n",
      "Epoch [48/300], Step [1400/27733], Loss: 2.2009\n",
      "Epoch [48/300], Step [1500/27733], Loss: 2.5185\n",
      "Epoch [48/300], Step [1600/27733], Loss: 2.7754\n",
      "Epoch [48/300], Step [1700/27733], Loss: 2.6368\n",
      "Epoch [48/300], Step [1800/27733], Loss: 2.8102\n",
      "Epoch [48/300], Step [1900/27733], Loss: 2.4130\n",
      "Epoch [48/300], Step [2000/27733], Loss: 2.5712\n",
      "Epoch [48/300], Step [2100/27733], Loss: 2.8571\n",
      "Epoch [48/300], Step [2200/27733], Loss: 2.1412\n",
      "Epoch [48/300], Step [2300/27733], Loss: 2.2089\n",
      "Epoch [48/300], Step [2400/27733], Loss: 1.6575\n",
      "Epoch [48/300], Step [2500/27733], Loss: 3.3364\n",
      "Epoch [48/300], Step [2600/27733], Loss: 2.7759\n",
      "Epoch [48/300], Step [2700/27733], Loss: 2.8498\n",
      "Epoch [48/300], Step [2800/27733], Loss: 2.4997\n",
      "Epoch [48/300], Step [2900/27733], Loss: 2.5059\n",
      "Epoch [48/300], Step [3000/27733], Loss: 3.0577\n",
      "Epoch [48/300], Step [3100/27733], Loss: 3.3798\n",
      "Epoch [48/300], Step [3200/27733], Loss: 2.7147\n",
      "Epoch [48/300], Step [3300/27733], Loss: 2.2441\n",
      "Epoch [48/300], Step [3400/27733], Loss: 2.3447\n",
      "Epoch [48/300], Step [3500/27733], Loss: 2.1207\n",
      "Epoch [48/300], Step [3600/27733], Loss: 2.9940\n",
      "Epoch [48/300], Step [3700/27733], Loss: 2.6034\n",
      "Epoch [48/300], Step [3800/27733], Loss: 2.1450\n",
      "Epoch [48/300], Step [3900/27733], Loss: 2.2500\n",
      "Epoch [48/300], Step [4000/27733], Loss: 2.8188\n",
      "Epoch [48/300], Step [4100/27733], Loss: 2.0889\n",
      "Epoch [48/300], Step [4200/27733], Loss: 1.7173\n",
      "Epoch [48/300], Step [4300/27733], Loss: 2.5851\n",
      "Epoch [48/300], Step [4400/27733], Loss: 2.7864\n",
      "Epoch [48/300], Step [4500/27733], Loss: 2.8865\n",
      "Epoch [48/300], Step [4600/27733], Loss: 2.8783\n",
      "Epoch [48/300], Step [4700/27733], Loss: 2.6962\n",
      "Epoch [48/300], Step [4800/27733], Loss: 2.9153\n",
      "Epoch [48/300], Step [4900/27733], Loss: 1.9771\n",
      "Epoch [48/300], Step [5000/27733], Loss: 2.7220\n",
      "Epoch [48/300], Step [5100/27733], Loss: 2.7321\n",
      "Epoch [48/300], Step [5200/27733], Loss: 2.4039\n",
      "Epoch [48/300], Step [5300/27733], Loss: 2.1361\n",
      "Epoch [48/300], Step [5400/27733], Loss: 2.1870\n",
      "Epoch [48/300], Step [5500/27733], Loss: 2.3949\n",
      "Epoch [48/300], Step [5600/27733], Loss: 2.1811\n",
      "Epoch [48/300], Step [5700/27733], Loss: 1.9582\n",
      "Epoch [48/300], Step [5800/27733], Loss: 2.2094\n",
      "Epoch [48/300], Step [5900/27733], Loss: 3.2766\n",
      "Epoch [48/300], Step [6000/27733], Loss: 2.8796\n",
      "Epoch [48/300], Step [6100/27733], Loss: 3.0286\n",
      "Epoch [48/300], Step [6200/27733], Loss: 2.3458\n",
      "Epoch [48/300], Step [6300/27733], Loss: 3.1455\n",
      "Epoch [48/300], Step [6400/27733], Loss: 2.3224\n",
      "Epoch [48/300], Step [6500/27733], Loss: 2.6877\n",
      "Epoch [48/300], Step [6600/27733], Loss: 2.8670\n",
      "Epoch [48/300], Step [6700/27733], Loss: 2.6193\n",
      "Epoch [48/300], Step [6800/27733], Loss: 3.3939\n",
      "Epoch [48/300], Step [6900/27733], Loss: 2.7602\n",
      "Epoch [48/300], Step [7000/27733], Loss: 2.3050\n",
      "Epoch [48/300], Step [7100/27733], Loss: 1.9211\n",
      "Epoch [48/300], Step [7200/27733], Loss: 2.6772\n",
      "Epoch [48/300], Step [7300/27733], Loss: 2.6435\n",
      "Epoch [48/300], Step [7400/27733], Loss: 1.8831\n",
      "Epoch [48/300], Step [7500/27733], Loss: 2.9873\n",
      "Epoch [48/300], Step [7600/27733], Loss: 2.3448\n",
      "Epoch [48/300], Step [7700/27733], Loss: 2.9216\n",
      "Epoch [48/300], Step [7800/27733], Loss: 2.1109\n",
      "Epoch [48/300], Step [7900/27733], Loss: 1.9625\n",
      "Epoch [48/300], Step [8000/27733], Loss: 2.5620\n",
      "Epoch [48/300], Step [8100/27733], Loss: 2.8795\n",
      "Epoch [48/300], Step [8200/27733], Loss: 2.7970\n",
      "Epoch [48/300], Step [8300/27733], Loss: 1.5826\n",
      "Epoch [48/300], Step [8400/27733], Loss: 2.8722\n",
      "Epoch [48/300], Step [8500/27733], Loss: 2.4482\n",
      "Epoch [48/300], Step [8600/27733], Loss: 2.7636\n",
      "Epoch [48/300], Step [8700/27733], Loss: 2.8604\n",
      "Epoch [48/300], Step [8800/27733], Loss: 2.8125\n",
      "Epoch [48/300], Step [8900/27733], Loss: 2.5264\n",
      "Epoch [48/300], Step [9000/27733], Loss: 2.6258\n",
      "Epoch [48/300], Step [9100/27733], Loss: 2.7069\n",
      "Epoch [48/300], Step [9200/27733], Loss: 3.2095\n",
      "Epoch [48/300], Step [9300/27733], Loss: 2.1442\n",
      "Epoch [48/300], Step [9400/27733], Loss: 2.4880\n",
      "Epoch [48/300], Step [9500/27733], Loss: 2.9839\n",
      "Epoch [48/300], Step [9600/27733], Loss: 3.0373\n",
      "Epoch [48/300], Step [9700/27733], Loss: 2.1667\n",
      "Epoch [48/300], Step [9800/27733], Loss: 2.7050\n",
      "Epoch [48/300], Step [9900/27733], Loss: 2.9387\n",
      "Epoch [48/300], Step [10000/27733], Loss: 2.8058\n",
      "Epoch [48/300], Step [10100/27733], Loss: 1.9939\n",
      "Epoch [48/300], Step [10200/27733], Loss: 2.5302\n",
      "Epoch [48/300], Step [10300/27733], Loss: 2.4823\n",
      "Epoch [48/300], Step [10400/27733], Loss: 3.1459\n",
      "Epoch [48/300], Step [10500/27733], Loss: 2.8082\n",
      "Epoch [48/300], Step [10600/27733], Loss: 2.5187\n",
      "Epoch [48/300], Step [10700/27733], Loss: 3.4343\n",
      "Epoch [48/300], Step [10800/27733], Loss: 2.3673\n",
      "Epoch [48/300], Step [10900/27733], Loss: 2.9728\n",
      "Epoch [48/300], Step [11000/27733], Loss: 2.5556\n",
      "Epoch [48/300], Step [11100/27733], Loss: 4.0930\n",
      "Epoch [48/300], Step [11200/27733], Loss: 3.3126\n",
      "Epoch [48/300], Step [11300/27733], Loss: 2.4472\n",
      "Epoch [48/300], Step [11400/27733], Loss: 2.8686\n",
      "Epoch [48/300], Step [11500/27733], Loss: 2.5074\n",
      "Epoch [48/300], Step [11600/27733], Loss: 3.6334\n",
      "Epoch [48/300], Step [11700/27733], Loss: 2.6773\n",
      "Epoch [48/300], Step [11800/27733], Loss: 3.0170\n",
      "Epoch [48/300], Step [11900/27733], Loss: 2.3905\n",
      "Epoch [48/300], Step [12000/27733], Loss: 2.7785\n",
      "Epoch [48/300], Step [12100/27733], Loss: 2.6706\n",
      "Epoch [48/300], Step [12200/27733], Loss: 1.9188\n",
      "Epoch [48/300], Step [12300/27733], Loss: 2.7696\n",
      "Epoch [48/300], Step [12400/27733], Loss: 3.5469\n",
      "Epoch [48/300], Step [12500/27733], Loss: 2.1061\n",
      "Epoch [48/300], Step [12600/27733], Loss: 2.4421\n",
      "Epoch [48/300], Step [12700/27733], Loss: 3.4479\n",
      "Epoch [48/300], Step [12800/27733], Loss: 2.9507\n",
      "Epoch [48/300], Step [12900/27733], Loss: 2.0053\n",
      "Epoch [48/300], Step [13000/27733], Loss: 2.5346\n",
      "Epoch [48/300], Step [13100/27733], Loss: 2.6836\n",
      "Epoch [48/300], Step [13200/27733], Loss: 2.3192\n",
      "Epoch [48/300], Step [13300/27733], Loss: 3.2513\n",
      "Epoch [48/300], Step [13400/27733], Loss: 2.8542\n",
      "Epoch [48/300], Step [13500/27733], Loss: 3.1673\n",
      "Epoch [48/300], Step [13600/27733], Loss: 2.4992\n",
      "Epoch [48/300], Step [13700/27733], Loss: 3.1435\n",
      "Epoch [48/300], Step [13800/27733], Loss: 2.3019\n",
      "Epoch [48/300], Step [13900/27733], Loss: 2.4524\n",
      "Epoch [48/300], Step [14000/27733], Loss: 2.9196\n",
      "Epoch [48/300], Step [14100/27733], Loss: 2.4940\n",
      "Epoch [48/300], Step [14200/27733], Loss: 2.5127\n",
      "Epoch [48/300], Step [14300/27733], Loss: 2.9154\n",
      "Epoch [48/300], Step [14400/27733], Loss: 2.7741\n",
      "Epoch [48/300], Step [14500/27733], Loss: 2.6652\n",
      "Epoch [48/300], Step [14600/27733], Loss: 3.3286\n",
      "Epoch [48/300], Step [14700/27733], Loss: 2.7035\n",
      "Epoch [48/300], Step [14800/27733], Loss: 2.0059\n",
      "Epoch [48/300], Step [14900/27733], Loss: 2.4552\n",
      "Epoch [48/300], Step [15000/27733], Loss: 3.6119\n",
      "Epoch [48/300], Step [15100/27733], Loss: 2.9223\n",
      "Epoch [48/300], Step [15200/27733], Loss: 2.9671\n",
      "Epoch [48/300], Step [15300/27733], Loss: 3.4374\n",
      "Epoch [48/300], Step [15400/27733], Loss: 2.2704\n",
      "Epoch [48/300], Step [15500/27733], Loss: 2.8600\n",
      "Epoch [48/300], Step [15600/27733], Loss: 2.0855\n",
      "Epoch [48/300], Step [15700/27733], Loss: 3.1844\n",
      "Epoch [48/300], Step [15800/27733], Loss: 3.3831\n",
      "Epoch [48/300], Step [15900/27733], Loss: 2.6303\n",
      "Epoch [48/300], Step [16000/27733], Loss: 2.5695\n",
      "Epoch [48/300], Step [16100/27733], Loss: 3.2842\n",
      "Epoch [48/300], Step [16200/27733], Loss: 2.7368\n",
      "Epoch [48/300], Step [16300/27733], Loss: 2.2834\n",
      "Epoch [48/300], Step [16400/27733], Loss: 2.2219\n",
      "Epoch [48/300], Step [16500/27733], Loss: 3.2145\n",
      "Epoch [48/300], Step [16600/27733], Loss: 2.2958\n",
      "Epoch [48/300], Step [16700/27733], Loss: 2.6249\n",
      "Epoch [48/300], Step [16800/27733], Loss: 2.5395\n",
      "Epoch [48/300], Step [16900/27733], Loss: 3.1017\n",
      "Epoch [48/300], Step [17000/27733], Loss: 2.8257\n",
      "Epoch [48/300], Step [17100/27733], Loss: 2.4832\n",
      "Epoch [48/300], Step [17200/27733], Loss: 2.8428\n",
      "Epoch [48/300], Step [17300/27733], Loss: 3.4230\n",
      "Epoch [48/300], Step [17400/27733], Loss: 3.2240\n",
      "Epoch [48/300], Step [17500/27733], Loss: 2.6321\n",
      "Epoch [48/300], Step [17600/27733], Loss: 3.5373\n",
      "Epoch [48/300], Step [17700/27733], Loss: 2.6138\n",
      "Epoch [48/300], Step [17800/27733], Loss: 2.9702\n",
      "Epoch [48/300], Step [17900/27733], Loss: 3.0058\n",
      "Epoch [48/300], Step [18000/27733], Loss: 2.8943\n",
      "Epoch [48/300], Step [18100/27733], Loss: 3.1940\n",
      "Epoch [48/300], Step [18200/27733], Loss: 2.4646\n",
      "Epoch [48/300], Step [18300/27733], Loss: 3.1420\n",
      "Epoch [48/300], Step [18400/27733], Loss: 2.4054\n",
      "Epoch [48/300], Step [18500/27733], Loss: 2.9475\n",
      "Epoch [48/300], Step [18600/27733], Loss: 3.7503\n",
      "Epoch [48/300], Step [18700/27733], Loss: 2.8351\n",
      "Epoch [48/300], Step [18800/27733], Loss: 2.8780\n",
      "Epoch [48/300], Step [18900/27733], Loss: 3.0912\n",
      "Epoch [48/300], Step [19000/27733], Loss: 2.9924\n",
      "Epoch [48/300], Step [19100/27733], Loss: 2.9895\n",
      "Epoch [48/300], Step [19200/27733], Loss: 2.8492\n",
      "Epoch [48/300], Step [19300/27733], Loss: 2.7098\n",
      "Epoch [48/300], Step [19400/27733], Loss: 3.4479\n",
      "Epoch [48/300], Step [19500/27733], Loss: 3.4310\n",
      "Epoch [48/300], Step [19600/27733], Loss: 3.2748\n",
      "Epoch [48/300], Step [19700/27733], Loss: 2.3634\n",
      "Epoch [48/300], Step [19800/27733], Loss: 3.1198\n",
      "Epoch [48/300], Step [19900/27733], Loss: 3.0980\n",
      "Epoch [48/300], Step [20000/27733], Loss: 2.8951\n",
      "Epoch [48/300], Step [20100/27733], Loss: 3.3759\n",
      "Epoch [48/300], Step [20200/27733], Loss: 2.6650\n",
      "Epoch [48/300], Step [20300/27733], Loss: 2.6326\n",
      "Epoch [48/300], Step [20400/27733], Loss: 3.2875\n",
      "Epoch [48/300], Step [20500/27733], Loss: 2.4669\n",
      "Epoch [48/300], Step [20600/27733], Loss: 2.5210\n",
      "Epoch [48/300], Step [20700/27733], Loss: 2.4983\n",
      "Epoch [48/300], Step [20800/27733], Loss: 2.9626\n",
      "Epoch [48/300], Step [20900/27733], Loss: 3.1198\n",
      "Epoch [48/300], Step [21000/27733], Loss: 2.6718\n",
      "Epoch [48/300], Step [21100/27733], Loss: 3.2204\n",
      "Epoch [48/300], Step [21200/27733], Loss: 3.2262\n",
      "Epoch [48/300], Step [21300/27733], Loss: 3.0362\n",
      "Epoch [48/300], Step [21400/27733], Loss: 3.3030\n",
      "Epoch [48/300], Step [21500/27733], Loss: 2.9714\n",
      "Epoch [48/300], Step [21600/27733], Loss: 2.6766\n",
      "Epoch [48/300], Step [21700/27733], Loss: 3.4442\n",
      "Epoch [48/300], Step [21800/27733], Loss: 2.3907\n",
      "Epoch [48/300], Step [21900/27733], Loss: 2.5270\n",
      "Epoch [48/300], Step [22000/27733], Loss: 2.0815\n",
      "Epoch [48/300], Step [22100/27733], Loss: 2.5876\n",
      "Epoch [48/300], Step [22200/27733], Loss: 2.9324\n",
      "Epoch [48/300], Step [22300/27733], Loss: 2.2364\n",
      "Epoch [48/300], Step [22400/27733], Loss: 2.3616\n",
      "Epoch [48/300], Step [22500/27733], Loss: 3.4632\n",
      "Epoch [48/300], Step [22600/27733], Loss: 2.6305\n",
      "Epoch [48/300], Step [22700/27733], Loss: 2.7248\n",
      "Epoch [48/300], Step [22800/27733], Loss: 2.7743\n",
      "Epoch [48/300], Step [22900/27733], Loss: 3.4496\n",
      "Epoch [48/300], Step [23000/27733], Loss: 2.1107\n",
      "Epoch [48/300], Step [23100/27733], Loss: 3.2699\n",
      "Epoch [48/300], Step [23200/27733], Loss: 2.8087\n",
      "Epoch [48/300], Step [23300/27733], Loss: 3.0805\n",
      "Epoch [48/300], Step [23400/27733], Loss: 2.5339\n",
      "Epoch [48/300], Step [23500/27733], Loss: 2.8864\n",
      "Epoch [48/300], Step [23600/27733], Loss: 2.9274\n",
      "Epoch [48/300], Step [23700/27733], Loss: 2.6139\n",
      "Epoch [48/300], Step [23800/27733], Loss: 2.8987\n",
      "Epoch [48/300], Step [23900/27733], Loss: 3.2586\n",
      "Epoch [48/300], Step [24000/27733], Loss: 3.3648\n",
      "Epoch [48/300], Step [24100/27733], Loss: 3.1410\n",
      "Epoch [48/300], Step [24200/27733], Loss: 3.0268\n",
      "Epoch [48/300], Step [24300/27733], Loss: 3.2983\n",
      "Epoch [48/300], Step [24400/27733], Loss: 2.6963\n",
      "Epoch [48/300], Step [24500/27733], Loss: 2.7123\n",
      "Epoch [48/300], Step [24600/27733], Loss: 2.5584\n",
      "Epoch [48/300], Step [24700/27733], Loss: 2.9539\n",
      "Epoch [48/300], Step [24800/27733], Loss: 2.7482\n",
      "Epoch [48/300], Step [24900/27733], Loss: 2.2529\n",
      "Epoch [48/300], Step [25000/27733], Loss: 2.7661\n",
      "Epoch [48/300], Step [25100/27733], Loss: 3.6628\n",
      "Epoch [48/300], Step [25200/27733], Loss: 2.2046\n",
      "Epoch [48/300], Step [25300/27733], Loss: 2.9318\n",
      "Epoch [48/300], Step [25400/27733], Loss: 2.6263\n",
      "Epoch [48/300], Step [25500/27733], Loss: 3.0286\n",
      "Epoch [48/300], Step [25600/27733], Loss: 3.2605\n",
      "Epoch [48/300], Step [25700/27733], Loss: 2.8934\n",
      "Epoch [48/300], Step [25800/27733], Loss: 3.4526\n",
      "Epoch [48/300], Step [25900/27733], Loss: 2.7545\n",
      "Epoch [48/300], Step [26000/27733], Loss: 2.5714\n",
      "Epoch [48/300], Step [26100/27733], Loss: 3.5945\n",
      "Epoch [48/300], Step [26200/27733], Loss: 3.1344\n",
      "Epoch [48/300], Step [26300/27733], Loss: 2.9413\n",
      "Epoch [48/300], Step [26400/27733], Loss: 2.7348\n",
      "Epoch [48/300], Step [26500/27733], Loss: 3.6743\n",
      "Epoch [48/300], Step [26600/27733], Loss: 3.4281\n",
      "Epoch [48/300], Step [26700/27733], Loss: 2.4770\n",
      "Epoch [48/300], Step [26800/27733], Loss: 2.7381\n",
      "Epoch [48/300], Step [26900/27733], Loss: 3.1430\n",
      "Epoch [48/300], Step [27000/27733], Loss: 2.8556\n",
      "Epoch [48/300], Step [27100/27733], Loss: 3.2070\n",
      "Epoch [48/300], Step [27200/27733], Loss: 2.5486\n",
      "Epoch [48/300], Step [27300/27733], Loss: 3.9990\n",
      "Epoch [48/300], Step [27400/27733], Loss: 3.5112\n",
      "Epoch [48/300], Step [27500/27733], Loss: 2.4922\n",
      "Epoch [48/300], Step [27600/27733], Loss: 3.6105\n",
      "Epoch [48/300], Step [27700/27733], Loss: 1.7361\n",
      "Epoch [49/300], Step [100/27733], Loss: 2.5414\n",
      "Epoch [49/300], Step [200/27733], Loss: 2.2879\n",
      "Epoch [49/300], Step [300/27733], Loss: 2.6717\n",
      "Epoch [49/300], Step [400/27733], Loss: 2.8456\n",
      "Epoch [49/300], Step [500/27733], Loss: 2.4614\n",
      "Epoch [49/300], Step [600/27733], Loss: 2.4738\n",
      "Epoch [49/300], Step [700/27733], Loss: 2.3596\n",
      "Epoch [49/300], Step [800/27733], Loss: 2.8582\n",
      "Epoch [49/300], Step [900/27733], Loss: 2.1474\n",
      "Epoch [49/300], Step [1000/27733], Loss: 2.1143\n",
      "Epoch [49/300], Step [1100/27733], Loss: 1.4492\n",
      "Epoch [49/300], Step [1200/27733], Loss: 2.1518\n",
      "Epoch [49/300], Step [1300/27733], Loss: 2.3270\n",
      "Epoch [49/300], Step [1400/27733], Loss: 2.1668\n",
      "Epoch [49/300], Step [1500/27733], Loss: 2.2248\n",
      "Epoch [49/300], Step [1600/27733], Loss: 2.1312\n",
      "Epoch [49/300], Step [1700/27733], Loss: 2.2943\n",
      "Epoch [49/300], Step [1800/27733], Loss: 2.2442\n",
      "Epoch [49/300], Step [1900/27733], Loss: 1.8387\n",
      "Epoch [49/300], Step [2000/27733], Loss: 2.7866\n",
      "Epoch [49/300], Step [2100/27733], Loss: 1.9268\n",
      "Epoch [49/300], Step [2200/27733], Loss: 2.1603\n",
      "Epoch [49/300], Step [2300/27733], Loss: 2.6073\n",
      "Epoch [49/300], Step [2400/27733], Loss: 2.2293\n",
      "Epoch [49/300], Step [2500/27733], Loss: 2.6370\n",
      "Epoch [49/300], Step [2600/27733], Loss: 2.6467\n",
      "Epoch [49/300], Step [2700/27733], Loss: 2.4961\n",
      "Epoch [49/300], Step [2800/27733], Loss: 2.4389\n",
      "Epoch [49/300], Step [2900/27733], Loss: 2.8536\n",
      "Epoch [49/300], Step [3000/27733], Loss: 2.6332\n",
      "Epoch [49/300], Step [3100/27733], Loss: 2.6712\n",
      "Epoch [49/300], Step [3200/27733], Loss: 1.8113\n",
      "Epoch [49/300], Step [3300/27733], Loss: 2.1211\n",
      "Epoch [49/300], Step [3400/27733], Loss: 1.5670\n",
      "Epoch [49/300], Step [3500/27733], Loss: 2.4755\n",
      "Epoch [49/300], Step [3600/27733], Loss: 3.3947\n",
      "Epoch [49/300], Step [3700/27733], Loss: 2.3194\n",
      "Epoch [49/300], Step [3800/27733], Loss: 1.6384\n",
      "Epoch [49/300], Step [3900/27733], Loss: 2.3952\n",
      "Epoch [49/300], Step [4000/27733], Loss: 3.2332\n",
      "Epoch [49/300], Step [4100/27733], Loss: 1.7243\n",
      "Epoch [49/300], Step [4200/27733], Loss: 2.9253\n",
      "Epoch [49/300], Step [4300/27733], Loss: 3.4719\n",
      "Epoch [49/300], Step [4400/27733], Loss: 1.7015\n",
      "Epoch [49/300], Step [4500/27733], Loss: 2.8352\n",
      "Epoch [49/300], Step [4600/27733], Loss: 2.0959\n",
      "Epoch [49/300], Step [4700/27733], Loss: 2.3640\n",
      "Epoch [49/300], Step [4800/27733], Loss: 2.2554\n",
      "Epoch [49/300], Step [4900/27733], Loss: 2.4490\n",
      "Epoch [49/300], Step [5000/27733], Loss: 1.9382\n",
      "Epoch [49/300], Step [5100/27733], Loss: 2.2286\n",
      "Epoch [49/300], Step [5200/27733], Loss: 3.2064\n",
      "Epoch [49/300], Step [5300/27733], Loss: 3.0820\n",
      "Epoch [49/300], Step [5400/27733], Loss: 2.5918\n",
      "Epoch [49/300], Step [5500/27733], Loss: 2.8795\n",
      "Epoch [49/300], Step [5600/27733], Loss: 2.7335\n",
      "Epoch [49/300], Step [5700/27733], Loss: 2.2671\n",
      "Epoch [49/300], Step [5800/27733], Loss: 2.2302\n",
      "Epoch [49/300], Step [5900/27733], Loss: 2.6205\n",
      "Epoch [49/300], Step [6000/27733], Loss: 2.3126\n",
      "Epoch [49/300], Step [6100/27733], Loss: 2.4664\n",
      "Epoch [49/300], Step [6200/27733], Loss: 2.9893\n",
      "Epoch [49/300], Step [6300/27733], Loss: 2.4589\n",
      "Epoch [49/300], Step [6400/27733], Loss: 2.3306\n",
      "Epoch [49/300], Step [6500/27733], Loss: 2.1631\n",
      "Epoch [49/300], Step [6600/27733], Loss: 2.3091\n",
      "Epoch [49/300], Step [6700/27733], Loss: 2.3754\n",
      "Epoch [49/300], Step [6800/27733], Loss: 3.5362\n",
      "Epoch [49/300], Step [6900/27733], Loss: 2.2519\n",
      "Epoch [49/300], Step [7000/27733], Loss: 2.9614\n",
      "Epoch [49/300], Step [7100/27733], Loss: 2.2330\n",
      "Epoch [49/300], Step [7200/27733], Loss: 2.2120\n",
      "Epoch [49/300], Step [7300/27733], Loss: 3.1722\n",
      "Epoch [49/300], Step [7400/27733], Loss: 2.3840\n",
      "Epoch [49/300], Step [7500/27733], Loss: 2.6616\n",
      "Epoch [49/300], Step [7600/27733], Loss: 2.4521\n",
      "Epoch [49/300], Step [7700/27733], Loss: 2.6189\n",
      "Epoch [49/300], Step [7800/27733], Loss: 3.4395\n",
      "Epoch [49/300], Step [7900/27733], Loss: 2.7859\n",
      "Epoch [49/300], Step [8000/27733], Loss: 2.0208\n",
      "Epoch [49/300], Step [8100/27733], Loss: 2.0333\n",
      "Epoch [49/300], Step [8200/27733], Loss: 3.1096\n",
      "Epoch [49/300], Step [8300/27733], Loss: 2.4487\n",
      "Epoch [49/300], Step [8400/27733], Loss: 3.4095\n",
      "Epoch [49/300], Step [8500/27733], Loss: 2.4686\n",
      "Epoch [49/300], Step [8600/27733], Loss: 3.5412\n",
      "Epoch [49/300], Step [8700/27733], Loss: 2.7841\n",
      "Epoch [49/300], Step [8800/27733], Loss: 3.0119\n",
      "Epoch [49/300], Step [8900/27733], Loss: 2.9981\n",
      "Epoch [49/300], Step [9000/27733], Loss: 3.0326\n",
      "Epoch [49/300], Step [9100/27733], Loss: 2.2208\n",
      "Epoch [49/300], Step [9200/27733], Loss: 3.0869\n",
      "Epoch [49/300], Step [9300/27733], Loss: 2.7068\n",
      "Epoch [49/300], Step [9400/27733], Loss: 2.5591\n",
      "Epoch [49/300], Step [9500/27733], Loss: 2.7856\n",
      "Epoch [49/300], Step [9600/27733], Loss: 2.3732\n",
      "Epoch [49/300], Step [9700/27733], Loss: 2.4828\n",
      "Epoch [49/300], Step [9800/27733], Loss: 2.3223\n",
      "Epoch [49/300], Step [9900/27733], Loss: 2.4348\n",
      "Epoch [49/300], Step [10000/27733], Loss: 2.2393\n",
      "Epoch [49/300], Step [10100/27733], Loss: 1.9876\n",
      "Epoch [49/300], Step [10200/27733], Loss: 2.5618\n",
      "Epoch [49/300], Step [10300/27733], Loss: 3.2201\n",
      "Epoch [49/300], Step [10400/27733], Loss: 3.1706\n",
      "Epoch [49/300], Step [10500/27733], Loss: 2.7597\n",
      "Epoch [49/300], Step [10600/27733], Loss: 2.8272\n",
      "Epoch [49/300], Step [10700/27733], Loss: 2.8240\n",
      "Epoch [49/300], Step [10800/27733], Loss: 3.3001\n",
      "Epoch [49/300], Step [10900/27733], Loss: 2.4654\n",
      "Epoch [49/300], Step [11000/27733], Loss: 2.8882\n",
      "Epoch [49/300], Step [11100/27733], Loss: 3.5486\n",
      "Epoch [49/300], Step [11200/27733], Loss: 2.2875\n",
      "Epoch [49/300], Step [11300/27733], Loss: 2.1404\n",
      "Epoch [49/300], Step [11400/27733], Loss: 2.2216\n",
      "Epoch [49/300], Step [11500/27733], Loss: 3.0618\n",
      "Epoch [49/300], Step [11600/27733], Loss: 2.3393\n",
      "Epoch [49/300], Step [11700/27733], Loss: 2.7632\n",
      "Epoch [49/300], Step [11800/27733], Loss: 2.1912\n",
      "Epoch [49/300], Step [11900/27733], Loss: 2.4360\n",
      "Epoch [49/300], Step [12000/27733], Loss: 3.0402\n",
      "Epoch [49/300], Step [12100/27733], Loss: 3.7706\n",
      "Epoch [49/300], Step [12200/27733], Loss: 2.3732\n",
      "Epoch [49/300], Step [12300/27733], Loss: 3.2890\n",
      "Epoch [49/300], Step [12400/27733], Loss: 3.3597\n",
      "Epoch [49/300], Step [12500/27733], Loss: 1.6597\n",
      "Epoch [49/300], Step [12600/27733], Loss: 3.0592\n",
      "Epoch [49/300], Step [12700/27733], Loss: 2.3164\n",
      "Epoch [49/300], Step [12800/27733], Loss: 3.6244\n",
      "Epoch [49/300], Step [12900/27733], Loss: 3.4804\n",
      "Epoch [49/300], Step [13000/27733], Loss: 2.3194\n",
      "Epoch [49/300], Step [13100/27733], Loss: 2.4729\n",
      "Epoch [49/300], Step [13200/27733], Loss: 3.4186\n",
      "Epoch [49/300], Step [13300/27733], Loss: 2.1138\n",
      "Epoch [49/300], Step [13400/27733], Loss: 3.2644\n",
      "Epoch [49/300], Step [13500/27733], Loss: 3.6730\n",
      "Epoch [49/300], Step [13600/27733], Loss: 2.0734\n",
      "Epoch [49/300], Step [13700/27733], Loss: 2.6025\n",
      "Epoch [49/300], Step [13800/27733], Loss: 3.3516\n",
      "Epoch [49/300], Step [13900/27733], Loss: 2.3204\n",
      "Epoch [49/300], Step [14000/27733], Loss: 2.3091\n",
      "Epoch [49/300], Step [14100/27733], Loss: 2.4706\n",
      "Epoch [49/300], Step [14200/27733], Loss: 2.4322\n",
      "Epoch [49/300], Step [14300/27733], Loss: 2.3087\n",
      "Epoch [49/300], Step [14400/27733], Loss: 2.3586\n",
      "Epoch [49/300], Step [14500/27733], Loss: 2.1078\n",
      "Epoch [49/300], Step [14600/27733], Loss: 2.7652\n",
      "Epoch [49/300], Step [14700/27733], Loss: 2.5093\n",
      "Epoch [49/300], Step [14800/27733], Loss: 2.6852\n",
      "Epoch [49/300], Step [14900/27733], Loss: 2.5900\n",
      "Epoch [49/300], Step [15000/27733], Loss: 2.8077\n",
      "Epoch [49/300], Step [15100/27733], Loss: 3.0957\n",
      "Epoch [49/300], Step [15200/27733], Loss: 2.6926\n",
      "Epoch [49/300], Step [15300/27733], Loss: 2.4256\n",
      "Epoch [49/300], Step [15400/27733], Loss: 2.8927\n",
      "Epoch [49/300], Step [15500/27733], Loss: 2.5460\n",
      "Epoch [49/300], Step [15600/27733], Loss: 1.8405\n",
      "Epoch [49/300], Step [15700/27733], Loss: 2.4967\n",
      "Epoch [49/300], Step [15800/27733], Loss: 2.5331\n",
      "Epoch [49/300], Step [15900/27733], Loss: 3.3326\n",
      "Epoch [49/300], Step [16000/27733], Loss: 2.7052\n",
      "Epoch [49/300], Step [16100/27733], Loss: 2.2101\n",
      "Epoch [49/300], Step [16200/27733], Loss: 2.7823\n",
      "Epoch [49/300], Step [16300/27733], Loss: 2.0943\n",
      "Epoch [49/300], Step [16400/27733], Loss: 2.9715\n",
      "Epoch [49/300], Step [16500/27733], Loss: 2.3167\n",
      "Epoch [49/300], Step [16600/27733], Loss: 2.3364\n",
      "Epoch [49/300], Step [16700/27733], Loss: 2.7484\n",
      "Epoch [49/300], Step [16800/27733], Loss: 3.1520\n",
      "Epoch [49/300], Step [16900/27733], Loss: 3.2593\n",
      "Epoch [49/300], Step [17000/27733], Loss: 2.4524\n",
      "Epoch [49/300], Step [17100/27733], Loss: 3.2596\n",
      "Epoch [49/300], Step [17200/27733], Loss: 2.6094\n",
      "Epoch [49/300], Step [17300/27733], Loss: 2.6349\n",
      "Epoch [49/300], Step [17400/27733], Loss: 3.4856\n",
      "Epoch [49/300], Step [17500/27733], Loss: 2.8224\n",
      "Epoch [49/300], Step [17600/27733], Loss: 2.3840\n",
      "Epoch [49/300], Step [17700/27733], Loss: 1.8423\n",
      "Epoch [49/300], Step [17800/27733], Loss: 2.5728\n",
      "Epoch [49/300], Step [17900/27733], Loss: 2.7321\n",
      "Epoch [49/300], Step [18000/27733], Loss: 3.1696\n",
      "Epoch [49/300], Step [18100/27733], Loss: 3.1256\n",
      "Epoch [49/300], Step [18200/27733], Loss: 3.0664\n",
      "Epoch [49/300], Step [18300/27733], Loss: 2.6993\n",
      "Epoch [49/300], Step [18400/27733], Loss: 2.7174\n",
      "Epoch [49/300], Step [18500/27733], Loss: 2.7395\n",
      "Epoch [49/300], Step [18600/27733], Loss: 1.9888\n",
      "Epoch [49/300], Step [18700/27733], Loss: 2.3975\n",
      "Epoch [49/300], Step [18800/27733], Loss: 2.3847\n",
      "Epoch [49/300], Step [18900/27733], Loss: 3.0683\n",
      "Epoch [49/300], Step [19000/27733], Loss: 2.7433\n",
      "Epoch [49/300], Step [19100/27733], Loss: 3.4327\n",
      "Epoch [49/300], Step [19200/27733], Loss: 3.1842\n",
      "Epoch [49/300], Step [19300/27733], Loss: 2.6321\n",
      "Epoch [49/300], Step [19400/27733], Loss: 2.8187\n",
      "Epoch [49/300], Step [19500/27733], Loss: 2.9744\n",
      "Epoch [49/300], Step [19600/27733], Loss: 2.7083\n",
      "Epoch [49/300], Step [19700/27733], Loss: 2.5093\n",
      "Epoch [49/300], Step [19800/27733], Loss: 3.4839\n",
      "Epoch [49/300], Step [19900/27733], Loss: 3.2268\n",
      "Epoch [49/300], Step [20000/27733], Loss: 2.6777\n",
      "Epoch [49/300], Step [20100/27733], Loss: 2.6056\n",
      "Epoch [49/300], Step [20200/27733], Loss: 3.0759\n",
      "Epoch [49/300], Step [20300/27733], Loss: 2.9065\n",
      "Epoch [49/300], Step [20400/27733], Loss: 2.9122\n",
      "Epoch [49/300], Step [20500/27733], Loss: 2.6216\n",
      "Epoch [49/300], Step [20600/27733], Loss: 3.1698\n",
      "Epoch [49/300], Step [20700/27733], Loss: 2.5655\n",
      "Epoch [49/300], Step [20800/27733], Loss: 2.4784\n",
      "Epoch [49/300], Step [20900/27733], Loss: 2.9187\n",
      "Epoch [49/300], Step [21000/27733], Loss: 3.4289\n",
      "Epoch [49/300], Step [21100/27733], Loss: 2.8577\n",
      "Epoch [49/300], Step [21200/27733], Loss: 3.3920\n",
      "Epoch [49/300], Step [21300/27733], Loss: 3.0175\n",
      "Epoch [49/300], Step [21400/27733], Loss: 3.2815\n",
      "Epoch [49/300], Step [21500/27733], Loss: 3.2115\n",
      "Epoch [49/300], Step [21600/27733], Loss: 2.8487\n",
      "Epoch [49/300], Step [21700/27733], Loss: 3.1506\n",
      "Epoch [49/300], Step [21800/27733], Loss: 3.1806\n",
      "Epoch [49/300], Step [21900/27733], Loss: 2.0513\n",
      "Epoch [49/300], Step [22000/27733], Loss: 3.7660\n",
      "Epoch [49/300], Step [22100/27733], Loss: 2.6065\n",
      "Epoch [49/300], Step [22200/27733], Loss: 2.6072\n",
      "Epoch [49/300], Step [22300/27733], Loss: 3.3800\n",
      "Epoch [49/300], Step [22400/27733], Loss: 2.8502\n",
      "Epoch [49/300], Step [22500/27733], Loss: 3.0708\n",
      "Epoch [49/300], Step [22600/27733], Loss: 2.5138\n",
      "Epoch [49/300], Step [22700/27733], Loss: 3.5848\n",
      "Epoch [49/300], Step [22800/27733], Loss: 2.3281\n",
      "Epoch [49/300], Step [22900/27733], Loss: 3.7994\n",
      "Epoch [49/300], Step [23000/27733], Loss: 2.4556\n",
      "Epoch [49/300], Step [23100/27733], Loss: 2.7430\n",
      "Epoch [49/300], Step [23200/27733], Loss: 2.7484\n",
      "Epoch [49/300], Step [23300/27733], Loss: 2.3190\n",
      "Epoch [49/300], Step [23400/27733], Loss: 2.9356\n",
      "Epoch [49/300], Step [23500/27733], Loss: 3.0836\n",
      "Epoch [49/300], Step [23600/27733], Loss: 3.6133\n",
      "Epoch [49/300], Step [23700/27733], Loss: 3.1656\n",
      "Epoch [49/300], Step [23800/27733], Loss: 3.4545\n",
      "Epoch [49/300], Step [23900/27733], Loss: 2.7503\n",
      "Epoch [49/300], Step [24000/27733], Loss: 2.8581\n",
      "Epoch [49/300], Step [24100/27733], Loss: 2.7591\n",
      "Epoch [49/300], Step [24200/27733], Loss: 3.1777\n",
      "Epoch [49/300], Step [24300/27733], Loss: 2.3614\n",
      "Epoch [49/300], Step [24400/27733], Loss: 2.5812\n",
      "Epoch [49/300], Step [24500/27733], Loss: 3.1490\n",
      "Epoch [49/300], Step [24600/27733], Loss: 2.9262\n",
      "Epoch [49/300], Step [24700/27733], Loss: 2.9809\n",
      "Epoch [49/300], Step [24800/27733], Loss: 3.7667\n",
      "Epoch [49/300], Step [24900/27733], Loss: 3.0121\n",
      "Epoch [49/300], Step [25000/27733], Loss: 2.7178\n",
      "Epoch [49/300], Step [25100/27733], Loss: 2.6546\n",
      "Epoch [49/300], Step [25200/27733], Loss: 2.2627\n",
      "Epoch [49/300], Step [25300/27733], Loss: 2.4621\n",
      "Epoch [49/300], Step [25400/27733], Loss: 2.8316\n",
      "Epoch [49/300], Step [25500/27733], Loss: 2.7190\n",
      "Epoch [49/300], Step [25600/27733], Loss: 2.0982\n",
      "Epoch [49/300], Step [25700/27733], Loss: 2.8882\n",
      "Epoch [49/300], Step [25800/27733], Loss: 2.5182\n",
      "Epoch [49/300], Step [25900/27733], Loss: 2.5065\n",
      "Epoch [49/300], Step [26000/27733], Loss: 2.6111\n",
      "Epoch [49/300], Step [26100/27733], Loss: 3.0686\n",
      "Epoch [49/300], Step [26200/27733], Loss: 4.0232\n",
      "Epoch [49/300], Step [26300/27733], Loss: 3.2638\n",
      "Epoch [49/300], Step [26400/27733], Loss: 3.1387\n",
      "Epoch [49/300], Step [26500/27733], Loss: 2.9879\n",
      "Epoch [49/300], Step [26600/27733], Loss: 2.3560\n",
      "Epoch [49/300], Step [26700/27733], Loss: 2.8970\n",
      "Epoch [49/300], Step [26800/27733], Loss: 3.4408\n",
      "Epoch [49/300], Step [26900/27733], Loss: 2.8349\n",
      "Epoch [49/300], Step [27000/27733], Loss: 2.7541\n",
      "Epoch [49/300], Step [27100/27733], Loss: 3.2098\n",
      "Epoch [49/300], Step [27200/27733], Loss: 3.1013\n",
      "Epoch [49/300], Step [27300/27733], Loss: 2.8016\n",
      "Epoch [49/300], Step [27400/27733], Loss: 3.3920\n",
      "Epoch [49/300], Step [27500/27733], Loss: 2.5143\n",
      "Epoch [49/300], Step [27600/27733], Loss: 2.9277\n",
      "Epoch [49/300], Step [27700/27733], Loss: 2.8602\n",
      "Epoch [50/300], Step [100/27733], Loss: 2.6688\n",
      "Epoch [50/300], Step [200/27733], Loss: 2.6271\n",
      "Epoch [50/300], Step [300/27733], Loss: 1.8739\n",
      "Epoch [50/300], Step [400/27733], Loss: 2.1244\n",
      "Epoch [50/300], Step [500/27733], Loss: 2.9657\n",
      "Epoch [50/300], Step [600/27733], Loss: 3.8166\n",
      "Epoch [50/300], Step [700/27733], Loss: 2.4409\n",
      "Epoch [50/300], Step [800/27733], Loss: 1.9170\n",
      "Epoch [50/300], Step [900/27733], Loss: 2.6766\n",
      "Epoch [50/300], Step [1000/27733], Loss: 1.8954\n",
      "Epoch [50/300], Step [1100/27733], Loss: 3.5158\n",
      "Epoch [50/300], Step [1200/27733], Loss: 1.7530\n",
      "Epoch [50/300], Step [1300/27733], Loss: 2.5241\n",
      "Epoch [50/300], Step [1400/27733], Loss: 2.1876\n",
      "Epoch [50/300], Step [1500/27733], Loss: 2.2143\n",
      "Epoch [50/300], Step [1600/27733], Loss: 2.0107\n",
      "Epoch [50/300], Step [1700/27733], Loss: 2.6624\n",
      "Epoch [50/300], Step [1800/27733], Loss: 1.9680\n",
      "Epoch [50/300], Step [1900/27733], Loss: 2.4708\n",
      "Epoch [50/300], Step [2000/27733], Loss: 2.2039\n",
      "Epoch [50/300], Step [2100/27733], Loss: 2.4817\n",
      "Epoch [50/300], Step [2200/27733], Loss: 2.7653\n",
      "Epoch [50/300], Step [2300/27733], Loss: 2.3214\n",
      "Epoch [50/300], Step [2400/27733], Loss: 2.1371\n",
      "Epoch [50/300], Step [2500/27733], Loss: 2.3012\n",
      "Epoch [50/300], Step [2600/27733], Loss: 2.2924\n",
      "Epoch [50/300], Step [2700/27733], Loss: 3.2437\n",
      "Epoch [50/300], Step [2800/27733], Loss: 1.9961\n",
      "Epoch [50/300], Step [2900/27733], Loss: 2.5079\n",
      "Epoch [50/300], Step [3000/27733], Loss: 2.7267\n",
      "Epoch [50/300], Step [3100/27733], Loss: 1.7404\n",
      "Epoch [50/300], Step [3200/27733], Loss: 2.6209\n",
      "Epoch [50/300], Step [3300/27733], Loss: 2.0808\n",
      "Epoch [50/300], Step [3400/27733], Loss: 2.5434\n",
      "Epoch [50/300], Step [3500/27733], Loss: 2.2216\n",
      "Epoch [50/300], Step [3600/27733], Loss: 2.6141\n",
      "Epoch [50/300], Step [3700/27733], Loss: 2.9107\n",
      "Epoch [50/300], Step [3800/27733], Loss: 2.4454\n",
      "Epoch [50/300], Step [3900/27733], Loss: 3.4128\n",
      "Epoch [50/300], Step [4000/27733], Loss: 2.6967\n",
      "Epoch [50/300], Step [4100/27733], Loss: 2.8843\n",
      "Epoch [50/300], Step [4200/27733], Loss: 1.9062\n",
      "Epoch [50/300], Step [4300/27733], Loss: 2.2813\n",
      "Epoch [50/300], Step [4400/27733], Loss: 1.9096\n",
      "Epoch [50/300], Step [4500/27733], Loss: 2.0587\n",
      "Epoch [50/300], Step [4600/27733], Loss: 2.8446\n",
      "Epoch [50/300], Step [4700/27733], Loss: 3.0084\n",
      "Epoch [50/300], Step [4800/27733], Loss: 1.7958\n",
      "Epoch [50/300], Step [4900/27733], Loss: 2.8531\n",
      "Epoch [50/300], Step [5000/27733], Loss: 1.4552\n",
      "Epoch [50/300], Step [5100/27733], Loss: 2.9259\n",
      "Epoch [50/300], Step [5200/27733], Loss: 2.5519\n",
      "Epoch [50/300], Step [5300/27733], Loss: 2.5705\n",
      "Epoch [50/300], Step [5400/27733], Loss: 2.5077\n",
      "Epoch [50/300], Step [5500/27733], Loss: 3.1376\n",
      "Epoch [50/300], Step [5600/27733], Loss: 2.3738\n",
      "Epoch [50/300], Step [5700/27733], Loss: 3.0085\n",
      "Epoch [50/300], Step [5800/27733], Loss: 2.0635\n",
      "Epoch [50/300], Step [5900/27733], Loss: 3.2787\n",
      "Epoch [50/300], Step [6000/27733], Loss: 1.9590\n",
      "Epoch [50/300], Step [6100/27733], Loss: 2.2240\n",
      "Epoch [50/300], Step [6200/27733], Loss: 2.8001\n",
      "Epoch [50/300], Step [6300/27733], Loss: 2.3179\n",
      "Epoch [50/300], Step [6400/27733], Loss: 2.7058\n",
      "Epoch [50/300], Step [6500/27733], Loss: 2.6969\n",
      "Epoch [50/300], Step [6600/27733], Loss: 2.9817\n",
      "Epoch [50/300], Step [6700/27733], Loss: 2.5166\n",
      "Epoch [50/300], Step [6800/27733], Loss: 2.3432\n",
      "Epoch [50/300], Step [6900/27733], Loss: 2.9111\n",
      "Epoch [50/300], Step [7000/27733], Loss: 3.3755\n",
      "Epoch [50/300], Step [7100/27733], Loss: 2.7882\n",
      "Epoch [50/300], Step [7200/27733], Loss: 2.2926\n",
      "Epoch [50/300], Step [7300/27733], Loss: 3.0456\n",
      "Epoch [50/300], Step [7400/27733], Loss: 2.5285\n",
      "Epoch [50/300], Step [7500/27733], Loss: 3.5709\n",
      "Epoch [50/300], Step [7600/27733], Loss: 2.8157\n",
      "Epoch [50/300], Step [7700/27733], Loss: 2.2730\n",
      "Epoch [50/300], Step [7800/27733], Loss: 1.8708\n",
      "Epoch [50/300], Step [7900/27733], Loss: 3.6995\n",
      "Epoch [50/300], Step [8000/27733], Loss: 2.5411\n",
      "Epoch [50/300], Step [8100/27733], Loss: 1.9865\n",
      "Epoch [50/300], Step [8200/27733], Loss: 2.1897\n",
      "Epoch [50/300], Step [8300/27733], Loss: 3.0555\n",
      "Epoch [50/300], Step [8400/27733], Loss: 1.9478\n",
      "Epoch [50/300], Step [8500/27733], Loss: 2.2614\n",
      "Epoch [50/300], Step [8600/27733], Loss: 2.8155\n",
      "Epoch [50/300], Step [8700/27733], Loss: 2.7992\n",
      "Epoch [50/300], Step [8800/27733], Loss: 2.6238\n",
      "Epoch [50/300], Step [8900/27733], Loss: 2.2352\n",
      "Epoch [50/300], Step [9000/27733], Loss: 2.4307\n",
      "Epoch [50/300], Step [9100/27733], Loss: 2.9291\n",
      "Epoch [50/300], Step [9200/27733], Loss: 2.9505\n",
      "Epoch [50/300], Step [9300/27733], Loss: 2.8805\n",
      "Epoch [50/300], Step [9400/27733], Loss: 2.4305\n",
      "Epoch [50/300], Step [9500/27733], Loss: 2.8846\n",
      "Epoch [50/300], Step [9600/27733], Loss: 2.7092\n",
      "Epoch [50/300], Step [9700/27733], Loss: 2.9943\n",
      "Epoch [50/300], Step [9800/27733], Loss: 2.8517\n",
      "Epoch [50/300], Step [9900/27733], Loss: 2.5882\n",
      "Epoch [50/300], Step [10000/27733], Loss: 2.8836\n",
      "Epoch [50/300], Step [10100/27733], Loss: 2.3243\n",
      "Epoch [50/300], Step [10200/27733], Loss: 3.2000\n",
      "Epoch [50/300], Step [10300/27733], Loss: 2.6648\n",
      "Epoch [50/300], Step [10400/27733], Loss: 3.2797\n",
      "Epoch [50/300], Step [10500/27733], Loss: 2.9855\n",
      "Epoch [50/300], Step [10600/27733], Loss: 2.3110\n",
      "Epoch [50/300], Step [10700/27733], Loss: 3.1720\n",
      "Epoch [50/300], Step [10800/27733], Loss: 2.7968\n",
      "Epoch [50/300], Step [10900/27733], Loss: 2.3685\n",
      "Epoch [50/300], Step [11000/27733], Loss: 2.3769\n",
      "Epoch [50/300], Step [11100/27733], Loss: 1.8517\n",
      "Epoch [50/300], Step [11200/27733], Loss: 2.7368\n",
      "Epoch [50/300], Step [11300/27733], Loss: 2.1255\n",
      "Epoch [50/300], Step [11400/27733], Loss: 2.6538\n",
      "Epoch [50/300], Step [11500/27733], Loss: 3.3479\n",
      "Epoch [50/300], Step [11600/27733], Loss: 2.3364\n",
      "Epoch [50/300], Step [11700/27733], Loss: 2.3370\n",
      "Epoch [50/300], Step [11800/27733], Loss: 2.6186\n",
      "Epoch [50/300], Step [11900/27733], Loss: 2.5314\n",
      "Epoch [50/300], Step [12000/27733], Loss: 2.3018\n",
      "Epoch [50/300], Step [12100/27733], Loss: 2.0372\n",
      "Epoch [50/300], Step [12200/27733], Loss: 2.5665\n",
      "Epoch [50/300], Step [12300/27733], Loss: 2.9439\n",
      "Epoch [50/300], Step [12400/27733], Loss: 2.7043\n",
      "Epoch [50/300], Step [12500/27733], Loss: 2.8568\n",
      "Epoch [50/300], Step [12600/27733], Loss: 1.6322\n",
      "Epoch [50/300], Step [12700/27733], Loss: 2.1347\n",
      "Epoch [50/300], Step [12800/27733], Loss: 3.4551\n",
      "Epoch [50/300], Step [12900/27733], Loss: 2.0912\n",
      "Epoch [50/300], Step [13000/27733], Loss: 2.7920\n",
      "Epoch [50/300], Step [13100/27733], Loss: 2.8120\n",
      "Epoch [50/300], Step [13200/27733], Loss: 3.1695\n",
      "Epoch [50/300], Step [13300/27733], Loss: 2.0330\n",
      "Epoch [50/300], Step [13400/27733], Loss: 2.4891\n",
      "Epoch [50/300], Step [13500/27733], Loss: 1.7280\n",
      "Epoch [50/300], Step [13600/27733], Loss: 3.0677\n",
      "Epoch [50/300], Step [13700/27733], Loss: 3.0844\n",
      "Epoch [50/300], Step [13800/27733], Loss: 2.7801\n",
      "Epoch [50/300], Step [13900/27733], Loss: 2.4045\n",
      "Epoch [50/300], Step [14000/27733], Loss: 3.0040\n",
      "Epoch [50/300], Step [14100/27733], Loss: 2.9197\n",
      "Epoch [50/300], Step [14200/27733], Loss: 2.1472\n",
      "Epoch [50/300], Step [14300/27733], Loss: 2.5407\n",
      "Epoch [50/300], Step [14400/27733], Loss: 2.7233\n",
      "Epoch [50/300], Step [14500/27733], Loss: 3.0914\n",
      "Epoch [50/300], Step [14600/27733], Loss: 2.5310\n",
      "Epoch [50/300], Step [14700/27733], Loss: 3.5949\n",
      "Epoch [50/300], Step [14800/27733], Loss: 3.1754\n",
      "Epoch [50/300], Step [14900/27733], Loss: 1.8830\n",
      "Epoch [50/300], Step [15000/27733], Loss: 3.7863\n",
      "Epoch [50/300], Step [15100/27733], Loss: 3.1186\n",
      "Epoch [50/300], Step [15200/27733], Loss: 2.6151\n",
      "Epoch [50/300], Step [15300/27733], Loss: 2.7570\n",
      "Epoch [50/300], Step [15400/27733], Loss: 3.1398\n",
      "Epoch [50/300], Step [15500/27733], Loss: 2.9143\n",
      "Epoch [50/300], Step [15600/27733], Loss: 2.1678\n",
      "Epoch [50/300], Step [15700/27733], Loss: 3.0175\n",
      "Epoch [50/300], Step [15800/27733], Loss: 3.2157\n",
      "Epoch [50/300], Step [15900/27733], Loss: 2.8833\n",
      "Epoch [50/300], Step [16000/27733], Loss: 2.6034\n",
      "Epoch [50/300], Step [16100/27733], Loss: 2.2237\n",
      "Epoch [50/300], Step [16200/27733], Loss: 2.7420\n",
      "Epoch [50/300], Step [16300/27733], Loss: 2.7531\n",
      "Epoch [50/300], Step [16400/27733], Loss: 2.9538\n",
      "Epoch [50/300], Step [16500/27733], Loss: 2.1896\n",
      "Epoch [50/300], Step [16600/27733], Loss: 2.7066\n",
      "Epoch [50/300], Step [16700/27733], Loss: 2.1965\n",
      "Epoch [50/300], Step [16800/27733], Loss: 2.2833\n",
      "Epoch [50/300], Step [16900/27733], Loss: 2.4107\n",
      "Epoch [50/300], Step [17000/27733], Loss: 2.9209\n",
      "Epoch [50/300], Step [17100/27733], Loss: 3.4904\n",
      "Epoch [50/300], Step [17200/27733], Loss: 2.6275\n",
      "Epoch [50/300], Step [17300/27733], Loss: 3.6676\n",
      "Epoch [50/300], Step [17400/27733], Loss: 2.7967\n",
      "Epoch [50/300], Step [17500/27733], Loss: 3.0276\n",
      "Epoch [50/300], Step [17600/27733], Loss: 2.9965\n",
      "Epoch [50/300], Step [17700/27733], Loss: 3.1600\n",
      "Epoch [50/300], Step [17800/27733], Loss: 2.8098\n",
      "Epoch [50/300], Step [17900/27733], Loss: 3.5244\n",
      "Epoch [50/300], Step [18000/27733], Loss: 2.3381\n",
      "Epoch [50/300], Step [18100/27733], Loss: 2.0442\n",
      "Epoch [50/300], Step [18200/27733], Loss: 2.4639\n",
      "Epoch [50/300], Step [18300/27733], Loss: 2.5271\n",
      "Epoch [50/300], Step [18400/27733], Loss: 2.4701\n",
      "Epoch [50/300], Step [18500/27733], Loss: 2.7867\n",
      "Epoch [50/300], Step [18600/27733], Loss: 1.9711\n",
      "Epoch [50/300], Step [18700/27733], Loss: 2.3833\n",
      "Epoch [50/300], Step [18800/27733], Loss: 2.5124\n",
      "Epoch [50/300], Step [18900/27733], Loss: 3.2802\n",
      "Epoch [50/300], Step [19000/27733], Loss: 3.6292\n",
      "Epoch [50/300], Step [19100/27733], Loss: 2.8879\n",
      "Epoch [50/300], Step [19200/27733], Loss: 2.9802\n",
      "Epoch [50/300], Step [19300/27733], Loss: 2.9433\n",
      "Epoch [50/300], Step [19400/27733], Loss: 3.0781\n",
      "Epoch [50/300], Step [19500/27733], Loss: 3.1188\n",
      "Epoch [50/300], Step [19600/27733], Loss: 2.5797\n",
      "Epoch [50/300], Step [19700/27733], Loss: 2.2520\n",
      "Epoch [50/300], Step [19800/27733], Loss: 2.8615\n",
      "Epoch [50/300], Step [19900/27733], Loss: 3.2079\n",
      "Epoch [50/300], Step [20000/27733], Loss: 3.0604\n",
      "Epoch [50/300], Step [20100/27733], Loss: 3.0059\n",
      "Epoch [50/300], Step [20200/27733], Loss: 2.2371\n",
      "Epoch [50/300], Step [20300/27733], Loss: 2.7246\n",
      "Epoch [50/300], Step [20400/27733], Loss: 2.5975\n",
      "Epoch [50/300], Step [20500/27733], Loss: 2.6215\n",
      "Epoch [50/300], Step [20600/27733], Loss: 2.8445\n",
      "Epoch [50/300], Step [20700/27733], Loss: 2.9440\n",
      "Epoch [50/300], Step [20800/27733], Loss: 2.4938\n",
      "Epoch [50/300], Step [20900/27733], Loss: 2.6384\n",
      "Epoch [50/300], Step [21000/27733], Loss: 3.8745\n",
      "Epoch [50/300], Step [21100/27733], Loss: 2.7111\n",
      "Epoch [50/300], Step [21200/27733], Loss: 2.7140\n",
      "Epoch [50/300], Step [21300/27733], Loss: 2.8243\n",
      "Epoch [50/300], Step [21400/27733], Loss: 2.6267\n",
      "Epoch [50/300], Step [21500/27733], Loss: 2.1953\n",
      "Epoch [50/300], Step [21600/27733], Loss: 3.5294\n",
      "Epoch [50/300], Step [21700/27733], Loss: 2.4827\n",
      "Epoch [50/300], Step [21800/27733], Loss: 2.8919\n",
      "Epoch [50/300], Step [21900/27733], Loss: 3.6751\n",
      "Epoch [50/300], Step [22000/27733], Loss: 2.4677\n",
      "Epoch [50/300], Step [22100/27733], Loss: 2.7130\n",
      "Epoch [50/300], Step [22200/27733], Loss: 3.3876\n",
      "Epoch [50/300], Step [22300/27733], Loss: 3.3621\n",
      "Epoch [50/300], Step [22400/27733], Loss: 2.6630\n",
      "Epoch [50/300], Step [22500/27733], Loss: 2.7556\n",
      "Epoch [50/300], Step [22600/27733], Loss: 2.6505\n",
      "Epoch [50/300], Step [22700/27733], Loss: 2.8279\n",
      "Epoch [50/300], Step [22800/27733], Loss: 3.3476\n",
      "Epoch [50/300], Step [22900/27733], Loss: 2.2901\n",
      "Epoch [50/300], Step [23000/27733], Loss: 3.5031\n",
      "Epoch [50/300], Step [23100/27733], Loss: 2.4808\n",
      "Epoch [50/300], Step [23200/27733], Loss: 3.2127\n",
      "Epoch [50/300], Step [23300/27733], Loss: 2.7842\n",
      "Epoch [50/300], Step [23400/27733], Loss: 2.9913\n",
      "Epoch [50/300], Step [23500/27733], Loss: 3.3235\n",
      "Epoch [50/300], Step [23600/27733], Loss: 3.0098\n",
      "Epoch [50/300], Step [23700/27733], Loss: 2.9054\n",
      "Epoch [50/300], Step [23800/27733], Loss: 2.9562\n",
      "Epoch [50/300], Step [23900/27733], Loss: 2.2565\n",
      "Epoch [50/300], Step [24000/27733], Loss: 2.3565\n",
      "Epoch [50/300], Step [24100/27733], Loss: 3.1141\n",
      "Epoch [50/300], Step [24200/27733], Loss: 3.1468\n",
      "Epoch [50/300], Step [24300/27733], Loss: 3.1366\n",
      "Epoch [50/300], Step [24400/27733], Loss: 3.5144\n",
      "Epoch [50/300], Step [24500/27733], Loss: 4.0234\n",
      "Epoch [50/300], Step [24600/27733], Loss: 2.6408\n",
      "Epoch [50/300], Step [24700/27733], Loss: 2.8401\n",
      "Epoch [50/300], Step [24800/27733], Loss: 2.6234\n",
      "Epoch [50/300], Step [24900/27733], Loss: 2.8702\n",
      "Epoch [50/300], Step [25000/27733], Loss: 3.1075\n",
      "Epoch [50/300], Step [25100/27733], Loss: 2.8359\n",
      "Epoch [50/300], Step [25200/27733], Loss: 3.4870\n",
      "Epoch [50/300], Step [25300/27733], Loss: 2.9268\n",
      "Epoch [50/300], Step [25400/27733], Loss: 2.5032\n",
      "Epoch [50/300], Step [25500/27733], Loss: 3.1548\n",
      "Epoch [50/300], Step [25600/27733], Loss: 3.3902\n",
      "Epoch [50/300], Step [25700/27733], Loss: 3.2641\n",
      "Epoch [50/300], Step [25800/27733], Loss: 3.1977\n",
      "Epoch [50/300], Step [25900/27733], Loss: 3.4919\n",
      "Epoch [50/300], Step [26000/27733], Loss: 3.2787\n",
      "Epoch [50/300], Step [26100/27733], Loss: 2.8483\n",
      "Epoch [50/300], Step [26200/27733], Loss: 3.0173\n",
      "Epoch [50/300], Step [26300/27733], Loss: 2.5096\n",
      "Epoch [50/300], Step [26400/27733], Loss: 3.0359\n",
      "Epoch [50/300], Step [26500/27733], Loss: 2.8505\n",
      "Epoch [50/300], Step [26600/27733], Loss: 3.7765\n",
      "Epoch [50/300], Step [26700/27733], Loss: 2.8470\n",
      "Epoch [50/300], Step [26800/27733], Loss: 3.0766\n",
      "Epoch [50/300], Step [26900/27733], Loss: 2.7831\n",
      "Epoch [50/300], Step [27000/27733], Loss: 2.1096\n",
      "Epoch [50/300], Step [27100/27733], Loss: 2.7290\n",
      "Epoch [50/300], Step [27200/27733], Loss: 2.4315\n",
      "Epoch [50/300], Step [27300/27733], Loss: 2.9307\n",
      "Epoch [50/300], Step [27400/27733], Loss: 2.1128\n",
      "Epoch [50/300], Step [27500/27733], Loss: 3.2715\n",
      "Epoch [50/300], Step [27600/27733], Loss: 2.0990\n",
      "Epoch [50/300], Step [27700/27733], Loss: 3.1122\n",
      "Epoch [51/300], Step [100/27733], Loss: 3.1849\n",
      "Epoch [51/300], Step [200/27733], Loss: 2.2989\n",
      "Epoch [51/300], Step [300/27733], Loss: 2.8012\n",
      "Epoch [51/300], Step [400/27733], Loss: 2.1166\n",
      "Epoch [51/300], Step [500/27733], Loss: 2.2575\n",
      "Epoch [51/300], Step [600/27733], Loss: 2.4982\n",
      "Epoch [51/300], Step [700/27733], Loss: 3.0298\n",
      "Epoch [51/300], Step [800/27733], Loss: 1.9583\n",
      "Epoch [51/300], Step [900/27733], Loss: 2.3409\n",
      "Epoch [51/300], Step [1000/27733], Loss: 2.1212\n",
      "Epoch [51/300], Step [1100/27733], Loss: 2.1457\n",
      "Epoch [51/300], Step [1200/27733], Loss: 2.9401\n",
      "Epoch [51/300], Step [1300/27733], Loss: 2.0105\n",
      "Epoch [51/300], Step [1400/27733], Loss: 2.2850\n",
      "Epoch [51/300], Step [1500/27733], Loss: 2.4117\n",
      "Epoch [51/300], Step [1600/27733], Loss: 2.4764\n",
      "Epoch [51/300], Step [1700/27733], Loss: 2.1800\n",
      "Epoch [51/300], Step [1800/27733], Loss: 2.6365\n",
      "Epoch [51/300], Step [1900/27733], Loss: 2.7123\n",
      "Epoch [51/300], Step [2000/27733], Loss: 1.8442\n",
      "Epoch [51/300], Step [2100/27733], Loss: 1.9472\n",
      "Epoch [51/300], Step [2200/27733], Loss: 2.9770\n",
      "Epoch [51/300], Step [2300/27733], Loss: 2.2371\n",
      "Epoch [51/300], Step [2400/27733], Loss: 2.3724\n",
      "Epoch [51/300], Step [2500/27733], Loss: 2.4979\n",
      "Epoch [51/300], Step [2600/27733], Loss: 2.3124\n",
      "Epoch [51/300], Step [2700/27733], Loss: 1.5859\n",
      "Epoch [51/300], Step [2800/27733], Loss: 1.8975\n",
      "Epoch [51/300], Step [2900/27733], Loss: 2.6628\n",
      "Epoch [51/300], Step [3000/27733], Loss: 1.8082\n",
      "Epoch [51/300], Step [3100/27733], Loss: 2.0952\n",
      "Epoch [51/300], Step [3200/27733], Loss: 2.3347\n",
      "Epoch [51/300], Step [3300/27733], Loss: 2.1354\n",
      "Epoch [51/300], Step [3400/27733], Loss: 2.3281\n",
      "Epoch [51/300], Step [3500/27733], Loss: 2.0908\n",
      "Epoch [51/300], Step [3600/27733], Loss: 2.1616\n",
      "Epoch [51/300], Step [3700/27733], Loss: 3.2397\n",
      "Epoch [51/300], Step [3800/27733], Loss: 1.8658\n",
      "Epoch [51/300], Step [3900/27733], Loss: 2.1616\n",
      "Epoch [51/300], Step [4000/27733], Loss: 2.5259\n",
      "Epoch [51/300], Step [4100/27733], Loss: 2.7163\n",
      "Epoch [51/300], Step [4200/27733], Loss: 2.4951\n",
      "Epoch [51/300], Step [4300/27733], Loss: 2.8562\n",
      "Epoch [51/300], Step [4400/27733], Loss: 2.1529\n",
      "Epoch [51/300], Step [4500/27733], Loss: 2.5610\n",
      "Epoch [51/300], Step [4600/27733], Loss: 2.6521\n",
      "Epoch [51/300], Step [4700/27733], Loss: 2.7217\n",
      "Epoch [51/300], Step [4800/27733], Loss: 2.2120\n",
      "Epoch [51/300], Step [4900/27733], Loss: 3.3585\n",
      "Epoch [51/300], Step [5000/27733], Loss: 3.2346\n",
      "Epoch [51/300], Step [5100/27733], Loss: 2.3476\n",
      "Epoch [51/300], Step [5200/27733], Loss: 2.4890\n",
      "Epoch [51/300], Step [5300/27733], Loss: 3.1561\n",
      "Epoch [51/300], Step [5400/27733], Loss: 2.4605\n",
      "Epoch [51/300], Step [5500/27733], Loss: 2.4817\n",
      "Epoch [51/300], Step [5600/27733], Loss: 3.0032\n",
      "Epoch [51/300], Step [5700/27733], Loss: 2.7376\n",
      "Epoch [51/300], Step [5800/27733], Loss: 2.0915\n",
      "Epoch [51/300], Step [5900/27733], Loss: 1.8101\n",
      "Epoch [51/300], Step [6000/27733], Loss: 2.8896\n",
      "Epoch [51/300], Step [6100/27733], Loss: 2.3600\n",
      "Epoch [51/300], Step [6200/27733], Loss: 2.8940\n",
      "Epoch [51/300], Step [6300/27733], Loss: 2.8779\n",
      "Epoch [51/300], Step [6400/27733], Loss: 2.3581\n",
      "Epoch [51/300], Step [6500/27733], Loss: 1.9678\n",
      "Epoch [51/300], Step [6600/27733], Loss: 1.7563\n",
      "Epoch [51/300], Step [6700/27733], Loss: 2.6710\n",
      "Epoch [51/300], Step [6800/27733], Loss: 2.8324\n",
      "Epoch [51/300], Step [6900/27733], Loss: 2.1636\n",
      "Epoch [51/300], Step [7000/27733], Loss: 1.9986\n",
      "Epoch [51/300], Step [7100/27733], Loss: 2.4845\n",
      "Epoch [51/300], Step [7200/27733], Loss: 2.9741\n",
      "Epoch [51/300], Step [7300/27733], Loss: 2.2228\n",
      "Epoch [51/300], Step [7400/27733], Loss: 2.7640\n",
      "Epoch [51/300], Step [7500/27733], Loss: 1.9622\n",
      "Epoch [51/300], Step [7600/27733], Loss: 2.1271\n",
      "Epoch [51/300], Step [7700/27733], Loss: 3.3278\n",
      "Epoch [51/300], Step [7800/27733], Loss: 3.4302\n",
      "Epoch [51/300], Step [7900/27733], Loss: 2.6316\n",
      "Epoch [51/300], Step [8000/27733], Loss: 2.6174\n",
      "Epoch [51/300], Step [8100/27733], Loss: 2.0725\n",
      "Epoch [51/300], Step [8200/27733], Loss: 2.6492\n",
      "Epoch [51/300], Step [8300/27733], Loss: 2.7458\n",
      "Epoch [51/300], Step [8400/27733], Loss: 3.0411\n",
      "Epoch [51/300], Step [8500/27733], Loss: 2.9941\n",
      "Epoch [51/300], Step [8600/27733], Loss: 2.1654\n",
      "Epoch [51/300], Step [8700/27733], Loss: 2.4663\n",
      "Epoch [51/300], Step [8800/27733], Loss: 2.5532\n",
      "Epoch [51/300], Step [8900/27733], Loss: 2.6948\n",
      "Epoch [51/300], Step [9000/27733], Loss: 2.4032\n",
      "Epoch [51/300], Step [9100/27733], Loss: 2.1144\n",
      "Epoch [51/300], Step [9200/27733], Loss: 3.4529\n",
      "Epoch [51/300], Step [9300/27733], Loss: 2.4821\n",
      "Epoch [51/300], Step [9400/27733], Loss: 1.7996\n",
      "Epoch [51/300], Step [9500/27733], Loss: 2.4178\n",
      "Epoch [51/300], Step [9600/27733], Loss: 3.0963\n",
      "Epoch [51/300], Step [9700/27733], Loss: 2.1101\n",
      "Epoch [51/300], Step [9800/27733], Loss: 2.9121\n",
      "Epoch [51/300], Step [9900/27733], Loss: 3.2723\n",
      "Epoch [51/300], Step [10000/27733], Loss: 2.2410\n",
      "Epoch [51/300], Step [10100/27733], Loss: 2.1720\n",
      "Epoch [51/300], Step [10200/27733], Loss: 2.9449\n",
      "Epoch [51/300], Step [10300/27733], Loss: 2.9181\n",
      "Epoch [51/300], Step [10400/27733], Loss: 2.5479\n",
      "Epoch [51/300], Step [10500/27733], Loss: 2.4270\n",
      "Epoch [51/300], Step [10600/27733], Loss: 2.7664\n",
      "Epoch [51/300], Step [10700/27733], Loss: 2.7700\n",
      "Epoch [51/300], Step [10800/27733], Loss: 2.8096\n",
      "Epoch [51/300], Step [10900/27733], Loss: 2.6913\n",
      "Epoch [51/300], Step [11000/27733], Loss: 2.2641\n",
      "Epoch [51/300], Step [11100/27733], Loss: 1.9262\n",
      "Epoch [51/300], Step [11200/27733], Loss: 2.9722\n",
      "Epoch [51/300], Step [11300/27733], Loss: 3.4480\n",
      "Epoch [51/300], Step [11400/27733], Loss: 2.1475\n",
      "Epoch [51/300], Step [11500/27733], Loss: 2.8923\n",
      "Epoch [51/300], Step [11600/27733], Loss: 2.0584\n",
      "Epoch [51/300], Step [11700/27733], Loss: 2.7482\n",
      "Epoch [51/300], Step [11800/27733], Loss: 3.3130\n",
      "Epoch [51/300], Step [11900/27733], Loss: 2.4634\n",
      "Epoch [51/300], Step [12000/27733], Loss: 3.3094\n",
      "Epoch [51/300], Step [12100/27733], Loss: 3.1355\n",
      "Epoch [51/300], Step [12200/27733], Loss: 2.5229\n",
      "Epoch [51/300], Step [12300/27733], Loss: 3.1797\n",
      "Epoch [51/300], Step [12400/27733], Loss: 3.2785\n",
      "Epoch [51/300], Step [12500/27733], Loss: 2.6590\n",
      "Epoch [51/300], Step [12600/27733], Loss: 2.9894\n",
      "Epoch [51/300], Step [12700/27733], Loss: 2.8179\n",
      "Epoch [51/300], Step [12800/27733], Loss: 2.9479\n",
      "Epoch [51/300], Step [12900/27733], Loss: 2.6229\n",
      "Epoch [51/300], Step [13000/27733], Loss: 4.0594\n",
      "Epoch [51/300], Step [13100/27733], Loss: 2.5394\n",
      "Epoch [51/300], Step [13200/27733], Loss: 1.9038\n",
      "Epoch [51/300], Step [13300/27733], Loss: 2.7932\n",
      "Epoch [51/300], Step [13400/27733], Loss: 2.2644\n",
      "Epoch [51/300], Step [13500/27733], Loss: 2.2728\n",
      "Epoch [51/300], Step [13600/27733], Loss: 2.6790\n",
      "Epoch [51/300], Step [13700/27733], Loss: 2.2464\n",
      "Epoch [51/300], Step [13800/27733], Loss: 2.8726\n",
      "Epoch [51/300], Step [13900/27733], Loss: 2.6404\n",
      "Epoch [51/300], Step [14000/27733], Loss: 3.3184\n",
      "Epoch [51/300], Step [14100/27733], Loss: 2.3443\n",
      "Epoch [51/300], Step [14200/27733], Loss: 2.3993\n",
      "Epoch [51/300], Step [14300/27733], Loss: 3.0707\n",
      "Epoch [51/300], Step [14400/27733], Loss: 3.4295\n",
      "Epoch [51/300], Step [14500/27733], Loss: 2.2885\n",
      "Epoch [51/300], Step [14600/27733], Loss: 2.5832\n",
      "Epoch [51/300], Step [14700/27733], Loss: 2.1124\n",
      "Epoch [51/300], Step [14800/27733], Loss: 2.3262\n",
      "Epoch [51/300], Step [14900/27733], Loss: 2.2053\n",
      "Epoch [51/300], Step [15000/27733], Loss: 2.8957\n",
      "Epoch [51/300], Step [15100/27733], Loss: 2.7156\n",
      "Epoch [51/300], Step [15200/27733], Loss: 2.6900\n",
      "Epoch [51/300], Step [15300/27733], Loss: 3.7279\n",
      "Epoch [51/300], Step [15400/27733], Loss: 3.0907\n",
      "Epoch [51/300], Step [15500/27733], Loss: 2.6706\n",
      "Epoch [51/300], Step [15600/27733], Loss: 2.2218\n",
      "Epoch [51/300], Step [15700/27733], Loss: 2.7582\n",
      "Epoch [51/300], Step [15800/27733], Loss: 2.6800\n",
      "Epoch [51/300], Step [15900/27733], Loss: 2.0106\n",
      "Epoch [51/300], Step [16000/27733], Loss: 3.3867\n",
      "Epoch [51/300], Step [16100/27733], Loss: 2.1033\n",
      "Epoch [51/300], Step [16200/27733], Loss: 2.2757\n",
      "Epoch [51/300], Step [16300/27733], Loss: 3.3311\n",
      "Epoch [51/300], Step [16400/27733], Loss: 2.8425\n",
      "Epoch [51/300], Step [16500/27733], Loss: 3.1143\n",
      "Epoch [51/300], Step [16600/27733], Loss: 2.9522\n",
      "Epoch [51/300], Step [16700/27733], Loss: 2.7242\n",
      "Epoch [51/300], Step [16800/27733], Loss: 2.9587\n",
      "Epoch [51/300], Step [16900/27733], Loss: 1.9436\n",
      "Epoch [51/300], Step [17000/27733], Loss: 2.6851\n",
      "Epoch [51/300], Step [17100/27733], Loss: 2.6381\n",
      "Epoch [51/300], Step [17200/27733], Loss: 2.1911\n",
      "Epoch [51/300], Step [17300/27733], Loss: 2.5022\n",
      "Epoch [51/300], Step [17400/27733], Loss: 3.3007\n",
      "Epoch [51/300], Step [17500/27733], Loss: 2.2320\n",
      "Epoch [51/300], Step [17600/27733], Loss: 3.3090\n",
      "Epoch [51/300], Step [17700/27733], Loss: 3.4310\n",
      "Epoch [51/300], Step [17800/27733], Loss: 3.1657\n",
      "Epoch [51/300], Step [17900/27733], Loss: 3.2407\n",
      "Epoch [51/300], Step [18000/27733], Loss: 3.0509\n",
      "Epoch [51/300], Step [18100/27733], Loss: 2.9029\n",
      "Epoch [51/300], Step [18200/27733], Loss: 3.2771\n",
      "Epoch [51/300], Step [18300/27733], Loss: 3.2068\n",
      "Epoch [51/300], Step [18400/27733], Loss: 3.3992\n",
      "Epoch [51/300], Step [18500/27733], Loss: 2.4693\n",
      "Epoch [51/300], Step [18600/27733], Loss: 3.5410\n",
      "Epoch [51/300], Step [18700/27733], Loss: 3.0809\n",
      "Epoch [51/300], Step [18800/27733], Loss: 2.9185\n",
      "Epoch [51/300], Step [18900/27733], Loss: 3.0523\n",
      "Epoch [51/300], Step [19000/27733], Loss: 2.8659\n",
      "Epoch [51/300], Step [19100/27733], Loss: 2.7528\n",
      "Epoch [51/300], Step [19200/27733], Loss: 2.5505\n",
      "Epoch [51/300], Step [19300/27733], Loss: 3.4383\n",
      "Epoch [51/300], Step [19400/27733], Loss: 1.9384\n",
      "Epoch [51/300], Step [19500/27733], Loss: 3.8957\n",
      "Epoch [51/300], Step [19600/27733], Loss: 3.0738\n",
      "Epoch [51/300], Step [19700/27733], Loss: 2.5546\n",
      "Epoch [51/300], Step [19800/27733], Loss: 2.5989\n",
      "Epoch [51/300], Step [19900/27733], Loss: 3.1919\n",
      "Epoch [51/300], Step [20000/27733], Loss: 3.1396\n",
      "Epoch [51/300], Step [20100/27733], Loss: 3.2422\n",
      "Epoch [51/300], Step [20200/27733], Loss: 3.0601\n",
      "Epoch [51/300], Step [20300/27733], Loss: 2.8376\n",
      "Epoch [51/300], Step [20400/27733], Loss: 2.0283\n",
      "Epoch [51/300], Step [20500/27733], Loss: 2.7178\n",
      "Epoch [51/300], Step [20600/27733], Loss: 2.9394\n",
      "Epoch [51/300], Step [20700/27733], Loss: 3.9498\n",
      "Epoch [51/300], Step [20800/27733], Loss: 3.8692\n",
      "Epoch [51/300], Step [20900/27733], Loss: 2.7166\n",
      "Epoch [51/300], Step [21000/27733], Loss: 2.4109\n",
      "Epoch [51/300], Step [21100/27733], Loss: 2.5221\n",
      "Epoch [51/300], Step [21200/27733], Loss: 1.8748\n",
      "Epoch [51/300], Step [21300/27733], Loss: 2.3203\n",
      "Epoch [51/300], Step [21400/27733], Loss: 3.5313\n",
      "Epoch [51/300], Step [21500/27733], Loss: 4.0455\n",
      "Epoch [51/300], Step [21600/27733], Loss: 2.6381\n",
      "Epoch [51/300], Step [21700/27733], Loss: 2.4432\n",
      "Epoch [51/300], Step [21800/27733], Loss: 2.3853\n",
      "Epoch [51/300], Step [21900/27733], Loss: 2.3335\n",
      "Epoch [51/300], Step [22000/27733], Loss: 3.3907\n",
      "Epoch [51/300], Step [22100/27733], Loss: 2.1822\n",
      "Epoch [51/300], Step [22200/27733], Loss: 2.6868\n",
      "Epoch [51/300], Step [22300/27733], Loss: 2.8657\n",
      "Epoch [51/300], Step [22400/27733], Loss: 1.9406\n",
      "Epoch [51/300], Step [22500/27733], Loss: 2.4465\n",
      "Epoch [51/300], Step [22600/27733], Loss: 3.4252\n",
      "Epoch [51/300], Step [22700/27733], Loss: 1.9706\n",
      "Epoch [51/300], Step [22800/27733], Loss: 1.9196\n",
      "Epoch [51/300], Step [22900/27733], Loss: 2.6828\n",
      "Epoch [51/300], Step [23000/27733], Loss: 2.8261\n",
      "Epoch [51/300], Step [23100/27733], Loss: 2.6611\n",
      "Epoch [51/300], Step [23200/27733], Loss: 2.5557\n",
      "Epoch [51/300], Step [23300/27733], Loss: 2.0557\n",
      "Epoch [51/300], Step [23400/27733], Loss: 2.5621\n",
      "Epoch [51/300], Step [23500/27733], Loss: 2.7503\n",
      "Epoch [51/300], Step [23600/27733], Loss: 2.9100\n",
      "Epoch [51/300], Step [23700/27733], Loss: 3.1613\n",
      "Epoch [51/300], Step [23800/27733], Loss: 3.1745\n",
      "Epoch [51/300], Step [23900/27733], Loss: 2.7129\n",
      "Epoch [51/300], Step [24000/27733], Loss: 4.0259\n",
      "Epoch [51/300], Step [24100/27733], Loss: 3.2425\n",
      "Epoch [51/300], Step [24200/27733], Loss: 2.7047\n",
      "Epoch [51/300], Step [24300/27733], Loss: 3.7970\n",
      "Epoch [51/300], Step [24400/27733], Loss: 2.6441\n",
      "Epoch [51/300], Step [24500/27733], Loss: 3.9883\n",
      "Epoch [51/300], Step [24600/27733], Loss: 2.6184\n",
      "Epoch [51/300], Step [24700/27733], Loss: 1.9373\n",
      "Epoch [51/300], Step [24800/27733], Loss: 2.4783\n",
      "Epoch [51/300], Step [24900/27733], Loss: 2.8726\n",
      "Epoch [51/300], Step [25000/27733], Loss: 3.6988\n",
      "Epoch [51/300], Step [25100/27733], Loss: 3.3390\n",
      "Epoch [51/300], Step [25200/27733], Loss: 2.8338\n",
      "Epoch [51/300], Step [25300/27733], Loss: 2.9001\n",
      "Epoch [51/300], Step [25400/27733], Loss: 2.7751\n",
      "Epoch [51/300], Step [25500/27733], Loss: 2.9810\n",
      "Epoch [51/300], Step [25600/27733], Loss: 2.8723\n",
      "Epoch [51/300], Step [25700/27733], Loss: 2.4716\n",
      "Epoch [51/300], Step [25800/27733], Loss: 2.5549\n",
      "Epoch [51/300], Step [25900/27733], Loss: 2.9890\n",
      "Epoch [51/300], Step [26000/27733], Loss: 2.6915\n",
      "Epoch [51/300], Step [26100/27733], Loss: 2.7890\n",
      "Epoch [51/300], Step [26200/27733], Loss: 3.6721\n",
      "Epoch [51/300], Step [26300/27733], Loss: 3.0323\n",
      "Epoch [51/300], Step [26400/27733], Loss: 2.9440\n",
      "Epoch [51/300], Step [26500/27733], Loss: 2.7499\n",
      "Epoch [51/300], Step [26600/27733], Loss: 2.7225\n",
      "Epoch [51/300], Step [26700/27733], Loss: 2.9762\n",
      "Epoch [51/300], Step [26800/27733], Loss: 3.2360\n",
      "Epoch [51/300], Step [26900/27733], Loss: 3.1958\n",
      "Epoch [51/300], Step [27000/27733], Loss: 3.1926\n",
      "Epoch [51/300], Step [27100/27733], Loss: 2.3484\n",
      "Epoch [51/300], Step [27200/27733], Loss: 2.9081\n",
      "Epoch [51/300], Step [27300/27733], Loss: 3.4312\n",
      "Epoch [51/300], Step [27400/27733], Loss: 4.3225\n",
      "Epoch [51/300], Step [27500/27733], Loss: 2.3834\n",
      "Epoch [51/300], Step [27600/27733], Loss: 2.3306\n",
      "Epoch [51/300], Step [27700/27733], Loss: 3.1025\n",
      "Epoch [52/300], Step [100/27733], Loss: 2.7124\n",
      "Epoch [52/300], Step [200/27733], Loss: 2.3496\n",
      "Epoch [52/300], Step [300/27733], Loss: 2.4301\n",
      "Epoch [52/300], Step [400/27733], Loss: 2.8116\n",
      "Epoch [52/300], Step [500/27733], Loss: 2.7635\n",
      "Epoch [52/300], Step [600/27733], Loss: 2.4891\n",
      "Epoch [52/300], Step [700/27733], Loss: 2.0133\n",
      "Epoch [52/300], Step [800/27733], Loss: 1.7481\n",
      "Epoch [52/300], Step [900/27733], Loss: 2.7984\n",
      "Epoch [52/300], Step [1000/27733], Loss: 2.4812\n",
      "Epoch [52/300], Step [1100/27733], Loss: 1.6608\n",
      "Epoch [52/300], Step [1200/27733], Loss: 2.2510\n",
      "Epoch [52/300], Step [1300/27733], Loss: 2.2826\n",
      "Epoch [52/300], Step [1400/27733], Loss: 2.0349\n",
      "Epoch [52/300], Step [1500/27733], Loss: 2.2291\n",
      "Epoch [52/300], Step [1600/27733], Loss: 1.3703\n",
      "Epoch [52/300], Step [1700/27733], Loss: 3.3500\n",
      "Epoch [52/300], Step [1800/27733], Loss: 3.1689\n",
      "Epoch [52/300], Step [1900/27733], Loss: 2.9079\n",
      "Epoch [52/300], Step [2000/27733], Loss: 2.0028\n",
      "Epoch [52/300], Step [2100/27733], Loss: 3.1660\n",
      "Epoch [52/300], Step [2200/27733], Loss: 2.3290\n",
      "Epoch [52/300], Step [2300/27733], Loss: 2.9781\n",
      "Epoch [52/300], Step [2400/27733], Loss: 1.4073\n",
      "Epoch [52/300], Step [2500/27733], Loss: 1.9012\n",
      "Epoch [52/300], Step [2600/27733], Loss: 2.0530\n",
      "Epoch [52/300], Step [2700/27733], Loss: 2.4655\n",
      "Epoch [52/300], Step [2800/27733], Loss: 2.0726\n",
      "Epoch [52/300], Step [2900/27733], Loss: 2.0904\n",
      "Epoch [52/300], Step [3000/27733], Loss: 2.3354\n",
      "Epoch [52/300], Step [3100/27733], Loss: 2.5436\n",
      "Epoch [52/300], Step [3200/27733], Loss: 1.5833\n",
      "Epoch [52/300], Step [3300/27733], Loss: 3.1611\n",
      "Epoch [52/300], Step [3400/27733], Loss: 2.2022\n",
      "Epoch [52/300], Step [3500/27733], Loss: 1.9331\n",
      "Epoch [52/300], Step [3600/27733], Loss: 2.2480\n",
      "Epoch [52/300], Step [3700/27733], Loss: 1.6583\n",
      "Epoch [52/300], Step [3800/27733], Loss: 1.7707\n",
      "Epoch [52/300], Step [3900/27733], Loss: 2.2720\n",
      "Epoch [52/300], Step [4000/27733], Loss: 1.9013\n",
      "Epoch [52/300], Step [4100/27733], Loss: 2.6842\n",
      "Epoch [52/300], Step [4200/27733], Loss: 2.5953\n",
      "Epoch [52/300], Step [4300/27733], Loss: 1.6197\n",
      "Epoch [52/300], Step [4400/27733], Loss: 2.2947\n",
      "Epoch [52/300], Step [4500/27733], Loss: 2.3863\n",
      "Epoch [52/300], Step [4600/27733], Loss: 2.2277\n",
      "Epoch [52/300], Step [4700/27733], Loss: 2.3115\n",
      "Epoch [52/300], Step [4800/27733], Loss: 2.2232\n",
      "Epoch [52/300], Step [4900/27733], Loss: 2.9047\n",
      "Epoch [52/300], Step [5000/27733], Loss: 2.0099\n",
      "Epoch [52/300], Step [5100/27733], Loss: 2.5804\n",
      "Epoch [52/300], Step [5200/27733], Loss: 2.5265\n",
      "Epoch [52/300], Step [5300/27733], Loss: 2.3373\n",
      "Epoch [52/300], Step [5400/27733], Loss: 1.8853\n",
      "Epoch [52/300], Step [5500/27733], Loss: 2.3016\n",
      "Epoch [52/300], Step [5600/27733], Loss: 2.6470\n",
      "Epoch [52/300], Step [5700/27733], Loss: 2.3367\n",
      "Epoch [52/300], Step [5800/27733], Loss: 2.5796\n",
      "Epoch [52/300], Step [5900/27733], Loss: 2.0206\n",
      "Epoch [52/300], Step [6000/27733], Loss: 1.9459\n",
      "Epoch [52/300], Step [6100/27733], Loss: 2.3370\n",
      "Epoch [52/300], Step [6200/27733], Loss: 2.1743\n",
      "Epoch [52/300], Step [6300/27733], Loss: 2.2397\n",
      "Epoch [52/300], Step [6400/27733], Loss: 2.5886\n",
      "Epoch [52/300], Step [6500/27733], Loss: 2.6400\n",
      "Epoch [52/300], Step [6600/27733], Loss: 2.6079\n",
      "Epoch [52/300], Step [6700/27733], Loss: 2.1608\n",
      "Epoch [52/300], Step [6800/27733], Loss: 2.6758\n",
      "Epoch [52/300], Step [6900/27733], Loss: 2.8287\n",
      "Epoch [52/300], Step [7000/27733], Loss: 2.2668\n",
      "Epoch [52/300], Step [7100/27733], Loss: 2.1161\n",
      "Epoch [52/300], Step [7200/27733], Loss: 2.5012\n",
      "Epoch [52/300], Step [7300/27733], Loss: 1.6031\n",
      "Epoch [52/300], Step [7400/27733], Loss: 2.9567\n",
      "Epoch [52/300], Step [7500/27733], Loss: 2.4613\n",
      "Epoch [52/300], Step [7600/27733], Loss: 2.6515\n",
      "Epoch [52/300], Step [7700/27733], Loss: 2.9750\n",
      "Epoch [52/300], Step [7800/27733], Loss: 2.2578\n",
      "Epoch [52/300], Step [7900/27733], Loss: 2.7880\n",
      "Epoch [52/300], Step [8000/27733], Loss: 2.7568\n",
      "Epoch [52/300], Step [8100/27733], Loss: 2.3262\n",
      "Epoch [52/300], Step [8200/27733], Loss: 2.8493\n",
      "Epoch [52/300], Step [8300/27733], Loss: 2.4497\n",
      "Epoch [52/300], Step [8400/27733], Loss: 2.9281\n",
      "Epoch [52/300], Step [8500/27733], Loss: 3.0492\n",
      "Epoch [52/300], Step [8600/27733], Loss: 2.2663\n",
      "Epoch [52/300], Step [8700/27733], Loss: 2.1843\n",
      "Epoch [52/300], Step [8800/27733], Loss: 2.0437\n",
      "Epoch [52/300], Step [8900/27733], Loss: 2.0013\n",
      "Epoch [52/300], Step [9000/27733], Loss: 2.6911\n",
      "Epoch [52/300], Step [9100/27733], Loss: 2.4531\n",
      "Epoch [52/300], Step [9200/27733], Loss: 2.1679\n",
      "Epoch [52/300], Step [9300/27733], Loss: 2.4770\n",
      "Epoch [52/300], Step [9400/27733], Loss: 2.0858\n",
      "Epoch [52/300], Step [9500/27733], Loss: 2.3998\n",
      "Epoch [52/300], Step [9600/27733], Loss: 2.6130\n",
      "Epoch [52/300], Step [9700/27733], Loss: 3.1855\n",
      "Epoch [52/300], Step [9800/27733], Loss: 2.7602\n",
      "Epoch [52/300], Step [9900/27733], Loss: 2.7584\n",
      "Epoch [52/300], Step [10000/27733], Loss: 2.4502\n",
      "Epoch [52/300], Step [10100/27733], Loss: 3.1252\n",
      "Epoch [52/300], Step [10200/27733], Loss: 2.2268\n",
      "Epoch [52/300], Step [10300/27733], Loss: 2.8329\n",
      "Epoch [52/300], Step [10400/27733], Loss: 1.8639\n",
      "Epoch [52/300], Step [10500/27733], Loss: 2.1726\n",
      "Epoch [52/300], Step [10600/27733], Loss: 2.5898\n",
      "Epoch [52/300], Step [10700/27733], Loss: 2.3131\n",
      "Epoch [52/300], Step [10800/27733], Loss: 3.1072\n",
      "Epoch [52/300], Step [10900/27733], Loss: 2.8207\n",
      "Epoch [52/300], Step [11000/27733], Loss: 3.6635\n",
      "Epoch [52/300], Step [11100/27733], Loss: 2.6221\n",
      "Epoch [52/300], Step [11200/27733], Loss: 2.7927\n",
      "Epoch [52/300], Step [11300/27733], Loss: 2.6586\n",
      "Epoch [52/300], Step [11400/27733], Loss: 2.9848\n",
      "Epoch [52/300], Step [11500/27733], Loss: 3.3719\n",
      "Epoch [52/300], Step [11600/27733], Loss: 2.8731\n",
      "Epoch [52/300], Step [11700/27733], Loss: 3.0781\n",
      "Epoch [52/300], Step [11800/27733], Loss: 3.3141\n",
      "Epoch [52/300], Step [11900/27733], Loss: 2.7397\n",
      "Epoch [52/300], Step [12000/27733], Loss: 2.6616\n",
      "Epoch [52/300], Step [12100/27733], Loss: 2.5684\n",
      "Epoch [52/300], Step [12200/27733], Loss: 2.4202\n",
      "Epoch [52/300], Step [12300/27733], Loss: 2.5795\n",
      "Epoch [52/300], Step [12400/27733], Loss: 2.2537\n",
      "Epoch [52/300], Step [12500/27733], Loss: 2.4440\n",
      "Epoch [52/300], Step [12600/27733], Loss: 2.6995\n",
      "Epoch [52/300], Step [12700/27733], Loss: 2.9833\n",
      "Epoch [52/300], Step [12800/27733], Loss: 2.8669\n",
      "Epoch [52/300], Step [12900/27733], Loss: 3.5143\n",
      "Epoch [52/300], Step [13000/27733], Loss: 2.8588\n",
      "Epoch [52/300], Step [13100/27733], Loss: 3.0064\n",
      "Epoch [52/300], Step [13200/27733], Loss: 3.5048\n",
      "Epoch [52/300], Step [13300/27733], Loss: 2.9993\n",
      "Epoch [52/300], Step [13400/27733], Loss: 2.7307\n",
      "Epoch [52/300], Step [13500/27733], Loss: 3.9556\n",
      "Epoch [52/300], Step [13600/27733], Loss: 1.9147\n",
      "Epoch [52/300], Step [13700/27733], Loss: 3.0102\n",
      "Epoch [52/300], Step [13800/27733], Loss: 2.5986\n",
      "Epoch [52/300], Step [13900/27733], Loss: 2.9102\n",
      "Epoch [52/300], Step [14000/27733], Loss: 2.8695\n",
      "Epoch [52/300], Step [14100/27733], Loss: 2.6363\n",
      "Epoch [52/300], Step [14200/27733], Loss: 3.3987\n",
      "Epoch [52/300], Step [14300/27733], Loss: 3.3771\n",
      "Epoch [52/300], Step [14400/27733], Loss: 2.8897\n",
      "Epoch [52/300], Step [14500/27733], Loss: 2.5568\n",
      "Epoch [52/300], Step [14600/27733], Loss: 2.7824\n",
      "Epoch [52/300], Step [14700/27733], Loss: 2.2567\n",
      "Epoch [52/300], Step [14800/27733], Loss: 2.4576\n",
      "Epoch [52/300], Step [14900/27733], Loss: 2.7984\n",
      "Epoch [52/300], Step [15000/27733], Loss: 3.3470\n",
      "Epoch [52/300], Step [15100/27733], Loss: 3.5307\n",
      "Epoch [52/300], Step [15200/27733], Loss: 2.6969\n",
      "Epoch [52/300], Step [15300/27733], Loss: 3.7444\n",
      "Epoch [52/300], Step [15400/27733], Loss: 2.6001\n",
      "Epoch [52/300], Step [15500/27733], Loss: 2.9434\n",
      "Epoch [52/300], Step [15600/27733], Loss: 2.8218\n",
      "Epoch [52/300], Step [15700/27733], Loss: 2.7196\n",
      "Epoch [52/300], Step [15800/27733], Loss: 3.0085\n",
      "Epoch [52/300], Step [15900/27733], Loss: 1.8126\n",
      "Epoch [52/300], Step [16000/27733], Loss: 2.3978\n",
      "Epoch [52/300], Step [16100/27733], Loss: 3.3680\n",
      "Epoch [52/300], Step [16200/27733], Loss: 3.5298\n",
      "Epoch [52/300], Step [16300/27733], Loss: 2.5303\n",
      "Epoch [52/300], Step [16400/27733], Loss: 2.6039\n",
      "Epoch [52/300], Step [16500/27733], Loss: 2.6118\n",
      "Epoch [52/300], Step [16600/27733], Loss: 2.2601\n",
      "Epoch [52/300], Step [16700/27733], Loss: 2.4946\n",
      "Epoch [52/300], Step [16800/27733], Loss: 2.7783\n",
      "Epoch [52/300], Step [16900/27733], Loss: 2.2354\n",
      "Epoch [52/300], Step [17000/27733], Loss: 2.2684\n",
      "Epoch [52/300], Step [17100/27733], Loss: 2.4389\n",
      "Epoch [52/300], Step [17200/27733], Loss: 2.4817\n",
      "Epoch [52/300], Step [17300/27733], Loss: 2.1050\n",
      "Epoch [52/300], Step [17400/27733], Loss: 2.3186\n",
      "Epoch [52/300], Step [17500/27733], Loss: 2.7134\n",
      "Epoch [52/300], Step [17600/27733], Loss: 2.7008\n",
      "Epoch [52/300], Step [17700/27733], Loss: 3.3362\n",
      "Epoch [52/300], Step [17800/27733], Loss: 3.6230\n",
      "Epoch [52/300], Step [17900/27733], Loss: 3.2773\n",
      "Epoch [52/300], Step [18000/27733], Loss: 2.8314\n",
      "Epoch [52/300], Step [18100/27733], Loss: 2.9889\n",
      "Epoch [52/300], Step [18200/27733], Loss: 2.9193\n",
      "Epoch [52/300], Step [18300/27733], Loss: 2.1654\n",
      "Epoch [52/300], Step [18400/27733], Loss: 3.5188\n",
      "Epoch [52/300], Step [18500/27733], Loss: 2.8296\n",
      "Epoch [52/300], Step [18600/27733], Loss: 3.2758\n",
      "Epoch [52/300], Step [18700/27733], Loss: 3.0562\n",
      "Epoch [52/300], Step [18800/27733], Loss: 2.5598\n",
      "Epoch [52/300], Step [18900/27733], Loss: 2.6100\n",
      "Epoch [52/300], Step [19000/27733], Loss: 2.8792\n",
      "Epoch [52/300], Step [19100/27733], Loss: 2.3940\n",
      "Epoch [52/300], Step [19200/27733], Loss: 3.3498\n",
      "Epoch [52/300], Step [19300/27733], Loss: 2.6428\n",
      "Epoch [52/300], Step [19400/27733], Loss: 2.8143\n",
      "Epoch [52/300], Step [19500/27733], Loss: 2.0960\n",
      "Epoch [52/300], Step [19600/27733], Loss: 3.1629\n",
      "Epoch [52/300], Step [19700/27733], Loss: 1.8972\n",
      "Epoch [52/300], Step [19800/27733], Loss: 2.5501\n",
      "Epoch [52/300], Step [19900/27733], Loss: 2.4328\n",
      "Epoch [52/300], Step [20000/27733], Loss: 2.8994\n",
      "Epoch [52/300], Step [20100/27733], Loss: 2.5564\n",
      "Epoch [52/300], Step [20200/27733], Loss: 2.5676\n",
      "Epoch [52/300], Step [20300/27733], Loss: 3.8783\n",
      "Epoch [52/300], Step [20400/27733], Loss: 2.6861\n",
      "Epoch [52/300], Step [20500/27733], Loss: 3.0119\n",
      "Epoch [52/300], Step [20600/27733], Loss: 2.6684\n",
      "Epoch [52/300], Step [20700/27733], Loss: 2.9071\n",
      "Epoch [52/300], Step [20800/27733], Loss: 2.2340\n",
      "Epoch [52/300], Step [20900/27733], Loss: 3.4070\n",
      "Epoch [52/300], Step [21000/27733], Loss: 2.6976\n",
      "Epoch [52/300], Step [21100/27733], Loss: 2.9545\n",
      "Epoch [52/300], Step [21200/27733], Loss: 3.6815\n",
      "Epoch [52/300], Step [21300/27733], Loss: 3.2914\n",
      "Epoch [52/300], Step [21400/27733], Loss: 2.6801\n",
      "Epoch [52/300], Step [21500/27733], Loss: 2.8263\n",
      "Epoch [52/300], Step [21600/27733], Loss: 2.8432\n",
      "Epoch [52/300], Step [21700/27733], Loss: 2.9332\n",
      "Epoch [52/300], Step [21800/27733], Loss: 2.7623\n",
      "Epoch [52/300], Step [21900/27733], Loss: 3.3265\n",
      "Epoch [52/300], Step [22000/27733], Loss: 3.3850\n",
      "Epoch [52/300], Step [22100/27733], Loss: 3.3383\n",
      "Epoch [52/300], Step [22200/27733], Loss: 2.9458\n",
      "Epoch [52/300], Step [22300/27733], Loss: 2.9334\n",
      "Epoch [52/300], Step [22400/27733], Loss: 3.0543\n",
      "Epoch [52/300], Step [22500/27733], Loss: 2.7513\n",
      "Epoch [52/300], Step [22600/27733], Loss: 2.6791\n",
      "Epoch [52/300], Step [22700/27733], Loss: 2.8428\n",
      "Epoch [52/300], Step [22800/27733], Loss: 2.8679\n",
      "Epoch [52/300], Step [22900/27733], Loss: 3.0617\n",
      "Epoch [52/300], Step [23000/27733], Loss: 2.2941\n",
      "Epoch [52/300], Step [23100/27733], Loss: 2.8155\n",
      "Epoch [52/300], Step [23200/27733], Loss: 3.1361\n",
      "Epoch [52/300], Step [23300/27733], Loss: 2.2262\n",
      "Epoch [52/300], Step [23400/27733], Loss: 3.2918\n",
      "Epoch [52/300], Step [23500/27733], Loss: 2.3438\n",
      "Epoch [52/300], Step [23600/27733], Loss: 2.5769\n",
      "Epoch [52/300], Step [23700/27733], Loss: 2.8812\n",
      "Epoch [52/300], Step [23800/27733], Loss: 1.8855\n",
      "Epoch [52/300], Step [23900/27733], Loss: 2.0199\n",
      "Epoch [52/300], Step [24000/27733], Loss: 3.3081\n",
      "Epoch [52/300], Step [24100/27733], Loss: 2.9075\n",
      "Epoch [52/300], Step [24200/27733], Loss: 3.2655\n",
      "Epoch [52/300], Step [24300/27733], Loss: 3.3738\n",
      "Epoch [52/300], Step [24400/27733], Loss: 2.1085\n",
      "Epoch [52/300], Step [24500/27733], Loss: 3.3727\n",
      "Epoch [52/300], Step [24600/27733], Loss: 2.9194\n",
      "Epoch [52/300], Step [24700/27733], Loss: 3.6053\n",
      "Epoch [52/300], Step [24800/27733], Loss: 2.7842\n",
      "Epoch [52/300], Step [24900/27733], Loss: 3.2692\n",
      "Epoch [52/300], Step [25000/27733], Loss: 2.6466\n",
      "Epoch [52/300], Step [25100/27733], Loss: 2.4077\n",
      "Epoch [52/300], Step [25200/27733], Loss: 2.2727\n",
      "Epoch [52/300], Step [25300/27733], Loss: 2.7997\n",
      "Epoch [52/300], Step [25400/27733], Loss: 2.3106\n",
      "Epoch [52/300], Step [25500/27733], Loss: 3.2326\n",
      "Epoch [52/300], Step [25600/27733], Loss: 3.1146\n",
      "Epoch [52/300], Step [25700/27733], Loss: 2.1951\n",
      "Epoch [52/300], Step [25800/27733], Loss: 3.1716\n",
      "Epoch [52/300], Step [25900/27733], Loss: 2.9274\n",
      "Epoch [52/300], Step [26000/27733], Loss: 2.3459\n",
      "Epoch [52/300], Step [26100/27733], Loss: 4.6069\n",
      "Epoch [52/300], Step [26200/27733], Loss: 3.0139\n",
      "Epoch [52/300], Step [26300/27733], Loss: 3.3321\n",
      "Epoch [52/300], Step [26400/27733], Loss: 3.0232\n",
      "Epoch [52/300], Step [26500/27733], Loss: 3.3903\n",
      "Epoch [52/300], Step [26600/27733], Loss: 3.5203\n",
      "Epoch [52/300], Step [26700/27733], Loss: 3.2802\n",
      "Epoch [52/300], Step [26800/27733], Loss: 2.8145\n",
      "Epoch [52/300], Step [26900/27733], Loss: 3.8127\n",
      "Epoch [52/300], Step [27000/27733], Loss: 3.1299\n",
      "Epoch [52/300], Step [27100/27733], Loss: 2.9698\n",
      "Epoch [52/300], Step [27200/27733], Loss: 3.0916\n",
      "Epoch [52/300], Step [27300/27733], Loss: 2.9096\n",
      "Epoch [52/300], Step [27400/27733], Loss: 2.1904\n",
      "Epoch [52/300], Step [27500/27733], Loss: 3.6675\n",
      "Epoch [52/300], Step [27600/27733], Loss: 2.6496\n",
      "Epoch [52/300], Step [27700/27733], Loss: 2.4259\n",
      "Epoch [53/300], Step [100/27733], Loss: 2.5849\n",
      "Epoch [53/300], Step [200/27733], Loss: 2.6080\n",
      "Epoch [53/300], Step [300/27733], Loss: 2.3489\n",
      "Epoch [53/300], Step [400/27733], Loss: 1.6377\n",
      "Epoch [53/300], Step [500/27733], Loss: 1.5844\n",
      "Epoch [53/300], Step [600/27733], Loss: 2.4694\n",
      "Epoch [53/300], Step [700/27733], Loss: 1.6037\n",
      "Epoch [53/300], Step [800/27733], Loss: 2.1528\n",
      "Epoch [53/300], Step [900/27733], Loss: 2.4448\n",
      "Epoch [53/300], Step [1000/27733], Loss: 2.3382\n",
      "Epoch [53/300], Step [1100/27733], Loss: 2.3889\n",
      "Epoch [53/300], Step [1200/27733], Loss: 2.5959\n",
      "Epoch [53/300], Step [1300/27733], Loss: 2.6152\n",
      "Epoch [53/300], Step [1400/27733], Loss: 2.0289\n",
      "Epoch [53/300], Step [1500/27733], Loss: 2.0541\n",
      "Epoch [53/300], Step [1600/27733], Loss: 2.4588\n",
      "Epoch [53/300], Step [1700/27733], Loss: 3.8320\n",
      "Epoch [53/300], Step [1800/27733], Loss: 2.1981\n",
      "Epoch [53/300], Step [1900/27733], Loss: 2.8552\n",
      "Epoch [53/300], Step [2000/27733], Loss: 2.0856\n",
      "Epoch [53/300], Step [2100/27733], Loss: 2.2473\n",
      "Epoch [53/300], Step [2200/27733], Loss: 3.0855\n",
      "Epoch [53/300], Step [2300/27733], Loss: 2.5509\n",
      "Epoch [53/300], Step [2400/27733], Loss: 2.5196\n",
      "Epoch [53/300], Step [2500/27733], Loss: 2.6353\n",
      "Epoch [53/300], Step [2600/27733], Loss: 2.8307\n",
      "Epoch [53/300], Step [2700/27733], Loss: 2.0315\n",
      "Epoch [53/300], Step [2800/27733], Loss: 2.1809\n",
      "Epoch [53/300], Step [2900/27733], Loss: 2.9409\n",
      "Epoch [53/300], Step [3000/27733], Loss: 2.3530\n",
      "Epoch [53/300], Step [3100/27733], Loss: 1.7780\n",
      "Epoch [53/300], Step [3200/27733], Loss: 2.6097\n",
      "Epoch [53/300], Step [3300/27733], Loss: 2.9297\n",
      "Epoch [53/300], Step [3400/27733], Loss: 2.7246\n",
      "Epoch [53/300], Step [3500/27733], Loss: 2.6137\n",
      "Epoch [53/300], Step [3600/27733], Loss: 2.6691\n",
      "Epoch [53/300], Step [3700/27733], Loss: 3.1807\n",
      "Epoch [53/300], Step [3800/27733], Loss: 2.1284\n",
      "Epoch [53/300], Step [3900/27733], Loss: 2.3466\n",
      "Epoch [53/300], Step [4000/27733], Loss: 1.7057\n",
      "Epoch [53/300], Step [4100/27733], Loss: 2.5692\n",
      "Epoch [53/300], Step [4200/27733], Loss: 1.5937\n",
      "Epoch [53/300], Step [4300/27733], Loss: 2.2405\n",
      "Epoch [53/300], Step [4400/27733], Loss: 2.9174\n",
      "Epoch [53/300], Step [4500/27733], Loss: 3.1108\n",
      "Epoch [53/300], Step [4600/27733], Loss: 2.6299\n",
      "Epoch [53/300], Step [4700/27733], Loss: 2.6640\n",
      "Epoch [53/300], Step [4800/27733], Loss: 2.3917\n",
      "Epoch [53/300], Step [4900/27733], Loss: 2.9990\n",
      "Epoch [53/300], Step [5000/27733], Loss: 2.3616\n",
      "Epoch [53/300], Step [5100/27733], Loss: 1.9042\n",
      "Epoch [53/300], Step [5200/27733], Loss: 2.2663\n",
      "Epoch [53/300], Step [5300/27733], Loss: 3.0044\n",
      "Epoch [53/300], Step [5400/27733], Loss: 2.7942\n",
      "Epoch [53/300], Step [5500/27733], Loss: 2.5371\n",
      "Epoch [53/300], Step [5600/27733], Loss: 2.5123\n",
      "Epoch [53/300], Step [5700/27733], Loss: 3.3099\n",
      "Epoch [53/300], Step [5800/27733], Loss: 2.1995\n",
      "Epoch [53/300], Step [5900/27733], Loss: 2.2211\n",
      "Epoch [53/300], Step [6000/27733], Loss: 2.3218\n",
      "Epoch [53/300], Step [6100/27733], Loss: 2.8842\n",
      "Epoch [53/300], Step [6200/27733], Loss: 1.9694\n",
      "Epoch [53/300], Step [6300/27733], Loss: 2.8779\n",
      "Epoch [53/300], Step [6400/27733], Loss: 2.3599\n",
      "Epoch [53/300], Step [6500/27733], Loss: 2.6976\n",
      "Epoch [53/300], Step [6600/27733], Loss: 2.3136\n",
      "Epoch [53/300], Step [6700/27733], Loss: 2.8480\n",
      "Epoch [53/300], Step [6800/27733], Loss: 2.5137\n",
      "Epoch [53/300], Step [6900/27733], Loss: 3.1100\n",
      "Epoch [53/300], Step [7000/27733], Loss: 2.4275\n",
      "Epoch [53/300], Step [7100/27733], Loss: 2.8162\n",
      "Epoch [53/300], Step [7200/27733], Loss: 3.6614\n",
      "Epoch [53/300], Step [7300/27733], Loss: 3.0544\n",
      "Epoch [53/300], Step [7400/27733], Loss: 2.1034\n",
      "Epoch [53/300], Step [7500/27733], Loss: 2.6982\n",
      "Epoch [53/300], Step [7600/27733], Loss: 2.2680\n",
      "Epoch [53/300], Step [7700/27733], Loss: 3.1002\n",
      "Epoch [53/300], Step [7800/27733], Loss: 2.3564\n",
      "Epoch [53/300], Step [7900/27733], Loss: 2.3022\n",
      "Epoch [53/300], Step [8000/27733], Loss: 3.0617\n",
      "Epoch [53/300], Step [8100/27733], Loss: 3.2433\n",
      "Epoch [53/300], Step [8200/27733], Loss: 2.4421\n",
      "Epoch [53/300], Step [8300/27733], Loss: 3.3890\n",
      "Epoch [53/300], Step [8400/27733], Loss: 2.6165\n",
      "Epoch [53/300], Step [8500/27733], Loss: 2.4805\n",
      "Epoch [53/300], Step [8600/27733], Loss: 3.1662\n",
      "Epoch [53/300], Step [8700/27733], Loss: 2.5466\n",
      "Epoch [53/300], Step [8800/27733], Loss: 2.4507\n",
      "Epoch [53/300], Step [8900/27733], Loss: 2.8466\n",
      "Epoch [53/300], Step [9000/27733], Loss: 2.4896\n",
      "Epoch [53/300], Step [9100/27733], Loss: 2.7637\n",
      "Epoch [53/300], Step [9200/27733], Loss: 3.0893\n",
      "Epoch [53/300], Step [9300/27733], Loss: 2.4753\n",
      "Epoch [53/300], Step [9400/27733], Loss: 2.3281\n",
      "Epoch [53/300], Step [9500/27733], Loss: 2.7699\n",
      "Epoch [53/300], Step [9600/27733], Loss: 2.0254\n",
      "Epoch [53/300], Step [9700/27733], Loss: 2.4353\n",
      "Epoch [53/300], Step [9800/27733], Loss: 2.7191\n",
      "Epoch [53/300], Step [9900/27733], Loss: 3.6161\n",
      "Epoch [53/300], Step [10000/27733], Loss: 2.5860\n",
      "Epoch [53/300], Step [10100/27733], Loss: 3.2063\n",
      "Epoch [53/300], Step [10200/27733], Loss: 2.5520\n",
      "Epoch [53/300], Step [10300/27733], Loss: 2.1658\n",
      "Epoch [53/300], Step [10400/27733], Loss: 2.1942\n",
      "Epoch [53/300], Step [10500/27733], Loss: 2.7062\n",
      "Epoch [53/300], Step [10600/27733], Loss: 2.4481\n",
      "Epoch [53/300], Step [10700/27733], Loss: 2.5038\n",
      "Epoch [53/300], Step [10800/27733], Loss: 2.3092\n",
      "Epoch [53/300], Step [10900/27733], Loss: 2.5552\n",
      "Epoch [53/300], Step [11000/27733], Loss: 3.0449\n",
      "Epoch [53/300], Step [11100/27733], Loss: 3.1219\n",
      "Epoch [53/300], Step [11200/27733], Loss: 3.5943\n",
      "Epoch [53/300], Step [11300/27733], Loss: 2.2477\n",
      "Epoch [53/300], Step [11400/27733], Loss: 3.3924\n",
      "Epoch [53/300], Step [11500/27733], Loss: 2.5022\n",
      "Epoch [53/300], Step [11600/27733], Loss: 2.1080\n",
      "Epoch [53/300], Step [11700/27733], Loss: 2.8517\n",
      "Epoch [53/300], Step [11800/27733], Loss: 2.5370\n",
      "Epoch [53/300], Step [11900/27733], Loss: 1.9095\n",
      "Epoch [53/300], Step [12000/27733], Loss: 3.1320\n",
      "Epoch [53/300], Step [12100/27733], Loss: 2.4399\n",
      "Epoch [53/300], Step [12200/27733], Loss: 2.9419\n",
      "Epoch [53/300], Step [12300/27733], Loss: 2.5674\n",
      "Epoch [53/300], Step [12400/27733], Loss: 2.9903\n",
      "Epoch [53/300], Step [12500/27733], Loss: 2.5124\n",
      "Epoch [53/300], Step [12600/27733], Loss: 2.2465\n",
      "Epoch [53/300], Step [12700/27733], Loss: 2.4951\n",
      "Epoch [53/300], Step [12800/27733], Loss: 3.6036\n",
      "Epoch [53/300], Step [12900/27733], Loss: 2.2718\n",
      "Epoch [53/300], Step [13000/27733], Loss: 2.8408\n",
      "Epoch [53/300], Step [13100/27733], Loss: 3.0706\n",
      "Epoch [53/300], Step [13200/27733], Loss: 2.2187\n",
      "Epoch [53/300], Step [13300/27733], Loss: 3.0937\n",
      "Epoch [53/300], Step [13400/27733], Loss: 3.2095\n",
      "Epoch [53/300], Step [13500/27733], Loss: 2.5755\n",
      "Epoch [53/300], Step [13600/27733], Loss: 2.7233\n",
      "Epoch [53/300], Step [13700/27733], Loss: 2.4662\n",
      "Epoch [53/300], Step [13800/27733], Loss: 3.3658\n",
      "Epoch [53/300], Step [13900/27733], Loss: 3.6151\n",
      "Epoch [53/300], Step [14000/27733], Loss: 3.3585\n",
      "Epoch [53/300], Step [14100/27733], Loss: 2.6330\n",
      "Epoch [53/300], Step [14200/27733], Loss: 2.5721\n",
      "Epoch [53/300], Step [14300/27733], Loss: 2.7469\n",
      "Epoch [53/300], Step [14400/27733], Loss: 1.9351\n",
      "Epoch [53/300], Step [14500/27733], Loss: 3.6206\n",
      "Epoch [53/300], Step [14600/27733], Loss: 2.9409\n",
      "Epoch [53/300], Step [14700/27733], Loss: 2.4278\n",
      "Epoch [53/300], Step [14800/27733], Loss: 2.2904\n",
      "Epoch [53/300], Step [14900/27733], Loss: 1.7348\n",
      "Epoch [53/300], Step [15000/27733], Loss: 2.7433\n",
      "Epoch [53/300], Step [15100/27733], Loss: 2.3695\n",
      "Epoch [53/300], Step [15200/27733], Loss: 2.5380\n",
      "Epoch [53/300], Step [15300/27733], Loss: 2.9093\n",
      "Epoch [53/300], Step [15400/27733], Loss: 2.5937\n",
      "Epoch [53/300], Step [15500/27733], Loss: 2.8011\n",
      "Epoch [53/300], Step [15600/27733], Loss: 2.1869\n",
      "Epoch [53/300], Step [15700/27733], Loss: 2.8768\n",
      "Epoch [53/300], Step [15800/27733], Loss: 2.7096\n",
      "Epoch [53/300], Step [15900/27733], Loss: 2.0561\n",
      "Epoch [53/300], Step [16000/27733], Loss: 3.0446\n",
      "Epoch [53/300], Step [16100/27733], Loss: 2.3313\n",
      "Epoch [53/300], Step [16200/27733], Loss: 2.6605\n",
      "Epoch [53/300], Step [16300/27733], Loss: 3.2876\n",
      "Epoch [53/300], Step [16400/27733], Loss: 3.2410\n",
      "Epoch [53/300], Step [16500/27733], Loss: 2.3318\n",
      "Epoch [53/300], Step [16600/27733], Loss: 3.1930\n",
      "Epoch [53/300], Step [16700/27733], Loss: 2.0753\n",
      "Epoch [53/300], Step [16800/27733], Loss: 2.6265\n",
      "Epoch [53/300], Step [16900/27733], Loss: 2.8277\n",
      "Epoch [53/300], Step [17000/27733], Loss: 2.8461\n",
      "Epoch [53/300], Step [17100/27733], Loss: 2.9208\n",
      "Epoch [53/300], Step [17200/27733], Loss: 2.8088\n",
      "Epoch [53/300], Step [17300/27733], Loss: 3.0284\n",
      "Epoch [53/300], Step [17400/27733], Loss: 2.0877\n",
      "Epoch [53/300], Step [17500/27733], Loss: 2.4547\n",
      "Epoch [53/300], Step [17600/27733], Loss: 2.2159\n",
      "Epoch [53/300], Step [17700/27733], Loss: 2.4226\n",
      "Epoch [53/300], Step [17800/27733], Loss: 3.0348\n",
      "Epoch [53/300], Step [17900/27733], Loss: 2.0535\n",
      "Epoch [53/300], Step [18000/27733], Loss: 2.2332\n",
      "Epoch [53/300], Step [18100/27733], Loss: 2.7188\n",
      "Epoch [53/300], Step [18200/27733], Loss: 1.7014\n",
      "Epoch [53/300], Step [18300/27733], Loss: 3.0941\n",
      "Epoch [53/300], Step [18400/27733], Loss: 3.1770\n",
      "Epoch [53/300], Step [18500/27733], Loss: 2.3053\n",
      "Epoch [53/300], Step [18600/27733], Loss: 2.7244\n",
      "Epoch [53/300], Step [18700/27733], Loss: 2.4719\n",
      "Epoch [53/300], Step [18800/27733], Loss: 2.3035\n",
      "Epoch [53/300], Step [18900/27733], Loss: 2.5318\n",
      "Epoch [53/300], Step [19000/27733], Loss: 2.3900\n",
      "Epoch [53/300], Step [19100/27733], Loss: 2.8916\n",
      "Epoch [53/300], Step [19200/27733], Loss: 2.3319\n",
      "Epoch [53/300], Step [19300/27733], Loss: 3.4500\n",
      "Epoch [53/300], Step [19400/27733], Loss: 3.3776\n",
      "Epoch [53/300], Step [19500/27733], Loss: 3.0395\n",
      "Epoch [53/300], Step [19600/27733], Loss: 2.8845\n",
      "Epoch [53/300], Step [19700/27733], Loss: 3.6264\n",
      "Epoch [53/300], Step [19800/27733], Loss: 2.1589\n",
      "Epoch [53/300], Step [19900/27733], Loss: 2.3323\n",
      "Epoch [53/300], Step [20000/27733], Loss: 3.5439\n",
      "Epoch [53/300], Step [20100/27733], Loss: 3.4735\n",
      "Epoch [53/300], Step [20200/27733], Loss: 2.8579\n",
      "Epoch [53/300], Step [20300/27733], Loss: 2.6803\n",
      "Epoch [53/300], Step [20400/27733], Loss: 2.3879\n",
      "Epoch [53/300], Step [20500/27733], Loss: 3.7313\n",
      "Epoch [53/300], Step [20600/27733], Loss: 1.9066\n",
      "Epoch [53/300], Step [20700/27733], Loss: 2.9233\n",
      "Epoch [53/300], Step [20800/27733], Loss: 3.1840\n",
      "Epoch [53/300], Step [20900/27733], Loss: 2.9118\n",
      "Epoch [53/300], Step [21000/27733], Loss: 3.6789\n",
      "Epoch [53/300], Step [21100/27733], Loss: 3.1313\n",
      "Epoch [53/300], Step [21200/27733], Loss: 3.1531\n",
      "Epoch [53/300], Step [21300/27733], Loss: 2.9423\n",
      "Epoch [53/300], Step [21400/27733], Loss: 3.0305\n",
      "Epoch [53/300], Step [21500/27733], Loss: 3.2510\n",
      "Epoch [53/300], Step [21600/27733], Loss: 2.6697\n",
      "Epoch [53/300], Step [21700/27733], Loss: 2.8718\n",
      "Epoch [53/300], Step [21800/27733], Loss: 3.5618\n",
      "Epoch [53/300], Step [21900/27733], Loss: 3.6829\n",
      "Epoch [53/300], Step [22000/27733], Loss: 3.2015\n",
      "Epoch [53/300], Step [22100/27733], Loss: 3.2647\n",
      "Epoch [53/300], Step [22200/27733], Loss: 2.4220\n",
      "Epoch [53/300], Step [22300/27733], Loss: 3.7016\n",
      "Epoch [53/300], Step [22400/27733], Loss: 3.6492\n",
      "Epoch [53/300], Step [22500/27733], Loss: 2.5833\n",
      "Epoch [53/300], Step [22600/27733], Loss: 3.7720\n",
      "Epoch [53/300], Step [22700/27733], Loss: 2.6070\n",
      "Epoch [53/300], Step [22800/27733], Loss: 2.7815\n",
      "Epoch [53/300], Step [22900/27733], Loss: 3.0271\n",
      "Epoch [53/300], Step [23000/27733], Loss: 3.4460\n",
      "Epoch [53/300], Step [23100/27733], Loss: 2.6570\n",
      "Epoch [53/300], Step [23200/27733], Loss: 2.6858\n",
      "Epoch [53/300], Step [23300/27733], Loss: 2.8502\n",
      "Epoch [53/300], Step [23400/27733], Loss: 3.1902\n",
      "Epoch [53/300], Step [23500/27733], Loss: 2.6984\n",
      "Epoch [53/300], Step [23600/27733], Loss: 2.6869\n",
      "Epoch [53/300], Step [23700/27733], Loss: 3.2025\n",
      "Epoch [53/300], Step [23800/27733], Loss: 3.0152\n",
      "Epoch [53/300], Step [23900/27733], Loss: 2.9781\n",
      "Epoch [53/300], Step [24000/27733], Loss: 3.3714\n",
      "Epoch [53/300], Step [24100/27733], Loss: 3.5525\n",
      "Epoch [53/300], Step [24200/27733], Loss: 2.9961\n",
      "Epoch [53/300], Step [24300/27733], Loss: 2.3396\n",
      "Epoch [53/300], Step [24400/27733], Loss: 3.6291\n",
      "Epoch [53/300], Step [24500/27733], Loss: 2.9870\n",
      "Epoch [53/300], Step [24600/27733], Loss: 3.0743\n",
      "Epoch [53/300], Step [24700/27733], Loss: 3.3231\n",
      "Epoch [53/300], Step [24800/27733], Loss: 2.4586\n",
      "Epoch [53/300], Step [24900/27733], Loss: 3.7481\n",
      "Epoch [53/300], Step [25000/27733], Loss: 3.8940\n",
      "Epoch [53/300], Step [25100/27733], Loss: 2.9367\n",
      "Epoch [53/300], Step [25200/27733], Loss: 2.4573\n",
      "Epoch [53/300], Step [25300/27733], Loss: 3.2133\n",
      "Epoch [53/300], Step [25400/27733], Loss: 2.8839\n",
      "Epoch [53/300], Step [25500/27733], Loss: 2.7346\n",
      "Epoch [53/300], Step [25600/27733], Loss: 2.4942\n",
      "Epoch [53/300], Step [25700/27733], Loss: 3.4922\n",
      "Epoch [53/300], Step [25800/27733], Loss: 2.7307\n",
      "Epoch [53/300], Step [25900/27733], Loss: 3.4059\n",
      "Epoch [53/300], Step [26000/27733], Loss: 2.2461\n",
      "Epoch [53/300], Step [26100/27733], Loss: 3.1788\n",
      "Epoch [53/300], Step [26200/27733], Loss: 3.5909\n",
      "Epoch [53/300], Step [26300/27733], Loss: 2.9036\n",
      "Epoch [53/300], Step [26400/27733], Loss: 3.6939\n",
      "Epoch [53/300], Step [26500/27733], Loss: 2.8304\n",
      "Epoch [53/300], Step [26600/27733], Loss: 2.2845\n",
      "Epoch [53/300], Step [26700/27733], Loss: 2.6096\n",
      "Epoch [53/300], Step [26800/27733], Loss: 2.6490\n",
      "Epoch [53/300], Step [26900/27733], Loss: 2.9968\n",
      "Epoch [53/300], Step [27000/27733], Loss: 3.2851\n",
      "Epoch [53/300], Step [27100/27733], Loss: 3.0471\n",
      "Epoch [53/300], Step [27200/27733], Loss: 2.6265\n",
      "Epoch [53/300], Step [27300/27733], Loss: 2.7397\n",
      "Epoch [53/300], Step [27400/27733], Loss: 2.5075\n",
      "Epoch [53/300], Step [27500/27733], Loss: 2.9659\n",
      "Epoch [53/300], Step [27600/27733], Loss: 2.3615\n",
      "Epoch [53/300], Step [27700/27733], Loss: 2.9361\n",
      "Epoch [54/300], Step [100/27733], Loss: 1.8261\n",
      "Epoch [54/300], Step [200/27733], Loss: 2.8095\n",
      "Epoch [54/300], Step [300/27733], Loss: 2.3416\n",
      "Epoch [54/300], Step [400/27733], Loss: 2.6481\n",
      "Epoch [54/300], Step [500/27733], Loss: 2.2700\n",
      "Epoch [54/300], Step [600/27733], Loss: 1.6400\n",
      "Epoch [54/300], Step [700/27733], Loss: 2.3401\n",
      "Epoch [54/300], Step [800/27733], Loss: 2.6672\n",
      "Epoch [54/300], Step [900/27733], Loss: 2.3892\n",
      "Epoch [54/300], Step [1000/27733], Loss: 3.0922\n",
      "Epoch [54/300], Step [1100/27733], Loss: 1.8624\n",
      "Epoch [54/300], Step [1200/27733], Loss: 2.2688\n",
      "Epoch [54/300], Step [1300/27733], Loss: 2.3980\n",
      "Epoch [54/300], Step [1400/27733], Loss: 1.8977\n",
      "Epoch [54/300], Step [1500/27733], Loss: 1.7011\n",
      "Epoch [54/300], Step [1600/27733], Loss: 2.4572\n",
      "Epoch [54/300], Step [1700/27733], Loss: 2.5852\n",
      "Epoch [54/300], Step [1800/27733], Loss: 2.9279\n",
      "Epoch [54/300], Step [1900/27733], Loss: 2.1657\n",
      "Epoch [54/300], Step [2000/27733], Loss: 2.6119\n",
      "Epoch [54/300], Step [2100/27733], Loss: 2.0577\n",
      "Epoch [54/300], Step [2200/27733], Loss: 2.2949\n",
      "Epoch [54/300], Step [2300/27733], Loss: 2.5902\n",
      "Epoch [54/300], Step [2400/27733], Loss: 3.1284\n",
      "Epoch [54/300], Step [2500/27733], Loss: 2.0570\n",
      "Epoch [54/300], Step [2600/27733], Loss: 2.4047\n",
      "Epoch [54/300], Step [2700/27733], Loss: 2.4292\n",
      "Epoch [54/300], Step [2800/27733], Loss: 2.5499\n",
      "Epoch [54/300], Step [2900/27733], Loss: 2.3556\n",
      "Epoch [54/300], Step [3000/27733], Loss: 2.3528\n",
      "Epoch [54/300], Step [3100/27733], Loss: 1.8797\n",
      "Epoch [54/300], Step [3200/27733], Loss: 3.2075\n",
      "Epoch [54/300], Step [3300/27733], Loss: 1.9835\n",
      "Epoch [54/300], Step [3400/27733], Loss: 2.3208\n",
      "Epoch [54/300], Step [3500/27733], Loss: 2.4192\n",
      "Epoch [54/300], Step [3600/27733], Loss: 2.8794\n",
      "Epoch [54/300], Step [3700/27733], Loss: 2.6339\n",
      "Epoch [54/300], Step [3800/27733], Loss: 2.4628\n",
      "Epoch [54/300], Step [3900/27733], Loss: 2.6482\n",
      "Epoch [54/300], Step [4000/27733], Loss: 3.0452\n",
      "Epoch [54/300], Step [4100/27733], Loss: 1.9614\n",
      "Epoch [54/300], Step [4200/27733], Loss: 2.3000\n",
      "Epoch [54/300], Step [4300/27733], Loss: 2.8056\n",
      "Epoch [54/300], Step [4400/27733], Loss: 2.1465\n",
      "Epoch [54/300], Step [4500/27733], Loss: 3.3358\n",
      "Epoch [54/300], Step [4600/27733], Loss: 3.3593\n",
      "Epoch [54/300], Step [4700/27733], Loss: 2.1671\n",
      "Epoch [54/300], Step [4800/27733], Loss: 2.4996\n",
      "Epoch [54/300], Step [4900/27733], Loss: 1.7245\n",
      "Epoch [54/300], Step [5000/27733], Loss: 2.3414\n",
      "Epoch [54/300], Step [5100/27733], Loss: 3.1373\n",
      "Epoch [54/300], Step [5200/27733], Loss: 2.8617\n",
      "Epoch [54/300], Step [5300/27733], Loss: 3.0426\n",
      "Epoch [54/300], Step [5400/27733], Loss: 2.7800\n",
      "Epoch [54/300], Step [5500/27733], Loss: 2.7092\n",
      "Epoch [54/300], Step [5600/27733], Loss: 2.4175\n",
      "Epoch [54/300], Step [5700/27733], Loss: 3.2910\n",
      "Epoch [54/300], Step [5800/27733], Loss: 1.9909\n",
      "Epoch [54/300], Step [5900/27733], Loss: 2.6379\n",
      "Epoch [54/300], Step [6000/27733], Loss: 2.6358\n",
      "Epoch [54/300], Step [6100/27733], Loss: 2.5081\n",
      "Epoch [54/300], Step [6200/27733], Loss: 2.8672\n",
      "Epoch [54/300], Step [6300/27733], Loss: 2.0882\n",
      "Epoch [54/300], Step [6400/27733], Loss: 1.9600\n",
      "Epoch [54/300], Step [6500/27733], Loss: 3.3945\n",
      "Epoch [54/300], Step [6600/27733], Loss: 2.4529\n",
      "Epoch [54/300], Step [6700/27733], Loss: 2.8736\n",
      "Epoch [54/300], Step [6800/27733], Loss: 2.8435\n",
      "Epoch [54/300], Step [6900/27733], Loss: 4.0021\n",
      "Epoch [54/300], Step [7000/27733], Loss: 2.3091\n",
      "Epoch [54/300], Step [7100/27733], Loss: 1.8903\n",
      "Epoch [54/300], Step [7200/27733], Loss: 3.4012\n",
      "Epoch [54/300], Step [7300/27733], Loss: 2.5783\n",
      "Epoch [54/300], Step [7400/27733], Loss: 3.1705\n",
      "Epoch [54/300], Step [7500/27733], Loss: 2.0704\n",
      "Epoch [54/300], Step [7600/27733], Loss: 2.2569\n",
      "Epoch [54/300], Step [7700/27733], Loss: 2.5269\n",
      "Epoch [54/300], Step [7800/27733], Loss: 2.9344\n",
      "Epoch [54/300], Step [7900/27733], Loss: 2.0978\n",
      "Epoch [54/300], Step [8000/27733], Loss: 2.5928\n",
      "Epoch [54/300], Step [8100/27733], Loss: 2.8603\n",
      "Epoch [54/300], Step [8200/27733], Loss: 2.7760\n",
      "Epoch [54/300], Step [8300/27733], Loss: 2.1552\n",
      "Epoch [54/300], Step [8400/27733], Loss: 3.3105\n",
      "Epoch [54/300], Step [8500/27733], Loss: 2.4618\n",
      "Epoch [54/300], Step [8600/27733], Loss: 2.6438\n",
      "Epoch [54/300], Step [8700/27733], Loss: 2.5888\n",
      "Epoch [54/300], Step [8800/27733], Loss: 2.0467\n",
      "Epoch [54/300], Step [8900/27733], Loss: 2.5422\n",
      "Epoch [54/300], Step [9000/27733], Loss: 2.8825\n",
      "Epoch [54/300], Step [9100/27733], Loss: 2.8893\n",
      "Epoch [54/300], Step [9200/27733], Loss: 3.0849\n",
      "Epoch [54/300], Step [9300/27733], Loss: 3.0804\n",
      "Epoch [54/300], Step [9400/27733], Loss: 3.0784\n",
      "Epoch [54/300], Step [9500/27733], Loss: 2.0398\n",
      "Epoch [54/300], Step [9600/27733], Loss: 2.4649\n",
      "Epoch [54/300], Step [9700/27733], Loss: 3.0462\n",
      "Epoch [54/300], Step [9800/27733], Loss: 3.0698\n",
      "Epoch [54/300], Step [9900/27733], Loss: 3.5405\n",
      "Epoch [54/300], Step [10000/27733], Loss: 2.9912\n",
      "Epoch [54/300], Step [10100/27733], Loss: 3.4260\n",
      "Epoch [54/300], Step [10200/27733], Loss: 2.2483\n",
      "Epoch [54/300], Step [10300/27733], Loss: 4.2275\n",
      "Epoch [54/300], Step [10400/27733], Loss: 2.5252\n",
      "Epoch [54/300], Step [10500/27733], Loss: 3.0466\n",
      "Epoch [54/300], Step [10600/27733], Loss: 2.0329\n",
      "Epoch [54/300], Step [10700/27733], Loss: 2.9525\n",
      "Epoch [54/300], Step [10800/27733], Loss: 2.8705\n",
      "Epoch [54/300], Step [10900/27733], Loss: 2.9318\n",
      "Epoch [54/300], Step [11000/27733], Loss: 2.4572\n",
      "Epoch [54/300], Step [11100/27733], Loss: 2.9351\n",
      "Epoch [54/300], Step [11200/27733], Loss: 3.0492\n",
      "Epoch [54/300], Step [11300/27733], Loss: 2.7069\n",
      "Epoch [54/300], Step [11400/27733], Loss: 2.6763\n",
      "Epoch [54/300], Step [11500/27733], Loss: 2.9574\n",
      "Epoch [54/300], Step [11600/27733], Loss: 2.6013\n",
      "Epoch [54/300], Step [11700/27733], Loss: 2.2324\n",
      "Epoch [54/300], Step [11800/27733], Loss: 3.8788\n",
      "Epoch [54/300], Step [11900/27733], Loss: 3.5860\n",
      "Epoch [54/300], Step [12000/27733], Loss: 2.2642\n",
      "Epoch [54/300], Step [12100/27733], Loss: 3.8260\n",
      "Epoch [54/300], Step [12200/27733], Loss: 1.9296\n",
      "Epoch [54/300], Step [12300/27733], Loss: 3.6909\n",
      "Epoch [54/300], Step [12400/27733], Loss: 3.0108\n",
      "Epoch [54/300], Step [12500/27733], Loss: 3.0327\n",
      "Epoch [54/300], Step [12600/27733], Loss: 2.2915\n",
      "Epoch [54/300], Step [12700/27733], Loss: 1.7729\n",
      "Epoch [54/300], Step [12800/27733], Loss: 2.2407\n",
      "Epoch [54/300], Step [12900/27733], Loss: 2.9069\n",
      "Epoch [54/300], Step [13000/27733], Loss: 3.3851\n",
      "Epoch [54/300], Step [13100/27733], Loss: 2.9918\n",
      "Epoch [54/300], Step [13200/27733], Loss: 2.6528\n",
      "Epoch [54/300], Step [13300/27733], Loss: 2.9696\n",
      "Epoch [54/300], Step [13400/27733], Loss: 3.1908\n",
      "Epoch [54/300], Step [13500/27733], Loss: 2.0823\n",
      "Epoch [54/300], Step [13600/27733], Loss: 2.6692\n",
      "Epoch [54/300], Step [13700/27733], Loss: 3.4016\n",
      "Epoch [54/300], Step [13800/27733], Loss: 2.3359\n",
      "Epoch [54/300], Step [13900/27733], Loss: 1.9629\n",
      "Epoch [54/300], Step [14000/27733], Loss: 2.2443\n",
      "Epoch [54/300], Step [14100/27733], Loss: 2.2544\n",
      "Epoch [54/300], Step [14200/27733], Loss: 2.2585\n",
      "Epoch [54/300], Step [14300/27733], Loss: 2.7668\n",
      "Epoch [54/300], Step [14400/27733], Loss: 2.8124\n",
      "Epoch [54/300], Step [14500/27733], Loss: 3.3646\n",
      "Epoch [54/300], Step [14600/27733], Loss: 2.8316\n",
      "Epoch [54/300], Step [14700/27733], Loss: 2.1005\n",
      "Epoch [54/300], Step [14800/27733], Loss: 2.6311\n",
      "Epoch [54/300], Step [14900/27733], Loss: 3.4804\n",
      "Epoch [54/300], Step [15000/27733], Loss: 2.5912\n",
      "Epoch [54/300], Step [15100/27733], Loss: 2.2769\n",
      "Epoch [54/300], Step [15200/27733], Loss: 2.1504\n",
      "Epoch [54/300], Step [15300/27733], Loss: 2.6832\n",
      "Epoch [54/300], Step [15400/27733], Loss: 2.6679\n",
      "Epoch [54/300], Step [15500/27733], Loss: 2.0345\n",
      "Epoch [54/300], Step [15600/27733], Loss: 3.5818\n",
      "Epoch [54/300], Step [15700/27733], Loss: 2.6340\n",
      "Epoch [54/300], Step [15800/27733], Loss: 2.2684\n",
      "Epoch [54/300], Step [15900/27733], Loss: 2.4430\n",
      "Epoch [54/300], Step [16000/27733], Loss: 2.3327\n",
      "Epoch [54/300], Step [16100/27733], Loss: 2.9363\n",
      "Epoch [54/300], Step [16200/27733], Loss: 2.8237\n",
      "Epoch [54/300], Step [16300/27733], Loss: 2.8871\n",
      "Epoch [54/300], Step [16400/27733], Loss: 2.5385\n",
      "Epoch [54/300], Step [16500/27733], Loss: 3.2242\n",
      "Epoch [54/300], Step [16600/27733], Loss: 3.0780\n",
      "Epoch [54/300], Step [16700/27733], Loss: 2.8973\n",
      "Epoch [54/300], Step [16800/27733], Loss: 3.3026\n",
      "Epoch [54/300], Step [16900/27733], Loss: 2.4515\n",
      "Epoch [54/300], Step [17000/27733], Loss: 2.9929\n",
      "Epoch [54/300], Step [17100/27733], Loss: 2.4492\n",
      "Epoch [54/300], Step [17200/27733], Loss: 1.9173\n",
      "Epoch [54/300], Step [17300/27733], Loss: 2.6727\n",
      "Epoch [54/300], Step [17400/27733], Loss: 3.2983\n",
      "Epoch [54/300], Step [17500/27733], Loss: 2.6924\n",
      "Epoch [54/300], Step [17600/27733], Loss: 3.1706\n",
      "Epoch [54/300], Step [17700/27733], Loss: 3.1676\n",
      "Epoch [54/300], Step [17800/27733], Loss: 2.9811\n",
      "Epoch [54/300], Step [17900/27733], Loss: 3.4712\n",
      "Epoch [54/300], Step [18000/27733], Loss: 2.6099\n",
      "Epoch [54/300], Step [18100/27733], Loss: 2.6266\n",
      "Epoch [54/300], Step [18200/27733], Loss: 3.1468\n",
      "Epoch [54/300], Step [18300/27733], Loss: 3.2078\n",
      "Epoch [54/300], Step [18400/27733], Loss: 4.0707\n",
      "Epoch [54/300], Step [18500/27733], Loss: 2.6089\n",
      "Epoch [54/300], Step [18600/27733], Loss: 2.3286\n",
      "Epoch [54/300], Step [18700/27733], Loss: 2.3964\n",
      "Epoch [54/300], Step [18800/27733], Loss: 2.3494\n",
      "Epoch [54/300], Step [18900/27733], Loss: 2.8453\n",
      "Epoch [54/300], Step [19000/27733], Loss: 2.7014\n",
      "Epoch [54/300], Step [19100/27733], Loss: 2.7299\n",
      "Epoch [54/300], Step [19200/27733], Loss: 2.2833\n",
      "Epoch [54/300], Step [19300/27733], Loss: 2.4056\n",
      "Epoch [54/300], Step [19400/27733], Loss: 2.1274\n",
      "Epoch [54/300], Step [19500/27733], Loss: 2.3877\n",
      "Epoch [54/300], Step [19600/27733], Loss: 2.8620\n",
      "Epoch [54/300], Step [19700/27733], Loss: 3.4750\n",
      "Epoch [54/300], Step [19800/27733], Loss: 3.3037\n",
      "Epoch [54/300], Step [19900/27733], Loss: 2.5318\n",
      "Epoch [54/300], Step [20000/27733], Loss: 2.8815\n",
      "Epoch [54/300], Step [20100/27733], Loss: 3.4267\n",
      "Epoch [54/300], Step [20200/27733], Loss: 2.5554\n",
      "Epoch [54/300], Step [20300/27733], Loss: 3.1494\n",
      "Epoch [54/300], Step [20400/27733], Loss: 3.1758\n",
      "Epoch [54/300], Step [20500/27733], Loss: 3.3283\n",
      "Epoch [54/300], Step [20600/27733], Loss: 3.2909\n",
      "Epoch [54/300], Step [20700/27733], Loss: 2.5098\n",
      "Epoch [54/300], Step [20800/27733], Loss: 3.6266\n",
      "Epoch [54/300], Step [20900/27733], Loss: 2.7035\n",
      "Epoch [54/300], Step [21000/27733], Loss: 3.6728\n",
      "Epoch [54/300], Step [21100/27733], Loss: 2.5736\n",
      "Epoch [54/300], Step [21200/27733], Loss: 3.6062\n",
      "Epoch [54/300], Step [21300/27733], Loss: 2.8174\n",
      "Epoch [54/300], Step [21400/27733], Loss: 3.1329\n",
      "Epoch [54/300], Step [21500/27733], Loss: 3.0414\n",
      "Epoch [54/300], Step [21600/27733], Loss: 2.7462\n",
      "Epoch [54/300], Step [21700/27733], Loss: 2.8179\n",
      "Epoch [54/300], Step [21800/27733], Loss: 2.6090\n",
      "Epoch [54/300], Step [21900/27733], Loss: 2.0680\n",
      "Epoch [54/300], Step [22000/27733], Loss: 3.2173\n",
      "Epoch [54/300], Step [22100/27733], Loss: 2.6697\n",
      "Epoch [54/300], Step [22200/27733], Loss: 2.6269\n",
      "Epoch [54/300], Step [22300/27733], Loss: 3.1518\n",
      "Epoch [54/300], Step [22400/27733], Loss: 3.4603\n",
      "Epoch [54/300], Step [22500/27733], Loss: 2.3389\n",
      "Epoch [54/300], Step [22600/27733], Loss: 3.1030\n",
      "Epoch [54/300], Step [22700/27733], Loss: 2.9805\n",
      "Epoch [54/300], Step [22800/27733], Loss: 2.9514\n",
      "Epoch [54/300], Step [22900/27733], Loss: 3.1214\n",
      "Epoch [54/300], Step [23000/27733], Loss: 2.6828\n",
      "Epoch [54/300], Step [23100/27733], Loss: 3.5046\n",
      "Epoch [54/300], Step [23200/27733], Loss: 2.1444\n",
      "Epoch [54/300], Step [23300/27733], Loss: 2.8770\n",
      "Epoch [54/300], Step [23400/27733], Loss: 2.7552\n",
      "Epoch [54/300], Step [23500/27733], Loss: 2.5486\n",
      "Epoch [54/300], Step [23600/27733], Loss: 3.2742\n",
      "Epoch [54/300], Step [23700/27733], Loss: 3.4800\n",
      "Epoch [54/300], Step [23800/27733], Loss: 2.7364\n",
      "Epoch [54/300], Step [23900/27733], Loss: 3.0562\n",
      "Epoch [54/300], Step [24000/27733], Loss: 2.9958\n",
      "Epoch [54/300], Step [24100/27733], Loss: 2.2526\n",
      "Epoch [54/300], Step [24200/27733], Loss: 3.4733\n",
      "Epoch [54/300], Step [24300/27733], Loss: 2.7260\n",
      "Epoch [54/300], Step [24400/27733], Loss: 2.1927\n",
      "Epoch [54/300], Step [24500/27733], Loss: 3.0065\n",
      "Epoch [54/300], Step [24600/27733], Loss: 3.3898\n",
      "Epoch [54/300], Step [24700/27733], Loss: 2.8061\n",
      "Epoch [54/300], Step [24800/27733], Loss: 3.1610\n",
      "Epoch [54/300], Step [24900/27733], Loss: 2.8655\n",
      "Epoch [54/300], Step [25000/27733], Loss: 2.6683\n",
      "Epoch [54/300], Step [25100/27733], Loss: 2.7427\n",
      "Epoch [54/300], Step [25200/27733], Loss: 3.2839\n",
      "Epoch [54/300], Step [25300/27733], Loss: 3.2699\n",
      "Epoch [54/300], Step [25400/27733], Loss: 2.8491\n",
      "Epoch [54/300], Step [25500/27733], Loss: 2.8788\n",
      "Epoch [54/300], Step [25600/27733], Loss: 2.9790\n",
      "Epoch [54/300], Step [25700/27733], Loss: 3.5528\n",
      "Epoch [54/300], Step [25800/27733], Loss: 2.6046\n",
      "Epoch [54/300], Step [25900/27733], Loss: 2.9282\n",
      "Epoch [54/300], Step [26000/27733], Loss: 3.2191\n",
      "Epoch [54/300], Step [26100/27733], Loss: 2.8548\n",
      "Epoch [54/300], Step [26200/27733], Loss: 2.8344\n",
      "Epoch [54/300], Step [26300/27733], Loss: 3.0869\n",
      "Epoch [54/300], Step [26400/27733], Loss: 2.8034\n",
      "Epoch [54/300], Step [26500/27733], Loss: 2.6816\n",
      "Epoch [54/300], Step [26600/27733], Loss: 2.2662\n",
      "Epoch [54/300], Step [26700/27733], Loss: 3.3973\n",
      "Epoch [54/300], Step [26800/27733], Loss: 2.7555\n",
      "Epoch [54/300], Step [26900/27733], Loss: 2.4219\n",
      "Epoch [54/300], Step [27000/27733], Loss: 2.5381\n",
      "Epoch [54/300], Step [27100/27733], Loss: 2.7904\n",
      "Epoch [54/300], Step [27200/27733], Loss: 2.9662\n",
      "Epoch [54/300], Step [27300/27733], Loss: 2.8553\n",
      "Epoch [54/300], Step [27400/27733], Loss: 2.6126\n",
      "Epoch [54/300], Step [27500/27733], Loss: 3.6754\n",
      "Epoch [54/300], Step [27600/27733], Loss: 3.2504\n",
      "Epoch [54/300], Step [27700/27733], Loss: 2.2521\n",
      "Epoch [55/300], Step [100/27733], Loss: 2.4818\n",
      "Epoch [55/300], Step [200/27733], Loss: 2.0500\n",
      "Epoch [55/300], Step [300/27733], Loss: 3.0266\n",
      "Epoch [55/300], Step [400/27733], Loss: 1.9417\n",
      "Epoch [55/300], Step [500/27733], Loss: 2.6828\n",
      "Epoch [55/300], Step [600/27733], Loss: 2.5450\n",
      "Epoch [55/300], Step [700/27733], Loss: 2.0907\n",
      "Epoch [55/300], Step [800/27733], Loss: 2.5740\n",
      "Epoch [55/300], Step [900/27733], Loss: 2.0602\n",
      "Epoch [55/300], Step [1000/27733], Loss: 2.7898\n",
      "Epoch [55/300], Step [1100/27733], Loss: 2.2127\n",
      "Epoch [55/300], Step [1200/27733], Loss: 2.4035\n",
      "Epoch [55/300], Step [1300/27733], Loss: 2.4388\n",
      "Epoch [55/300], Step [1400/27733], Loss: 2.1380\n",
      "Epoch [55/300], Step [1500/27733], Loss: 2.4778\n",
      "Epoch [55/300], Step [1600/27733], Loss: 2.4668\n",
      "Epoch [55/300], Step [1700/27733], Loss: 2.2544\n",
      "Epoch [55/300], Step [1800/27733], Loss: 2.5576\n",
      "Epoch [55/300], Step [1900/27733], Loss: 2.6795\n",
      "Epoch [55/300], Step [2000/27733], Loss: 1.8047\n",
      "Epoch [55/300], Step [2100/27733], Loss: 2.1822\n",
      "Epoch [55/300], Step [2200/27733], Loss: 3.2372\n",
      "Epoch [55/300], Step [2300/27733], Loss: 2.5967\n",
      "Epoch [55/300], Step [2400/27733], Loss: 2.3864\n",
      "Epoch [55/300], Step [2500/27733], Loss: 2.4249\n",
      "Epoch [55/300], Step [2600/27733], Loss: 2.4912\n",
      "Epoch [55/300], Step [2700/27733], Loss: 3.5904\n",
      "Epoch [55/300], Step [2800/27733], Loss: 3.4256\n",
      "Epoch [55/300], Step [2900/27733], Loss: 1.6531\n",
      "Epoch [55/300], Step [3000/27733], Loss: 2.8397\n",
      "Epoch [55/300], Step [3100/27733], Loss: 2.1133\n",
      "Epoch [55/300], Step [3200/27733], Loss: 2.2863\n",
      "Epoch [55/300], Step [3300/27733], Loss: 2.8393\n",
      "Epoch [55/300], Step [3400/27733], Loss: 2.6540\n",
      "Epoch [55/300], Step [3500/27733], Loss: 2.3383\n",
      "Epoch [55/300], Step [3600/27733], Loss: 2.4499\n",
      "Epoch [55/300], Step [3700/27733], Loss: 1.6091\n",
      "Epoch [55/300], Step [3800/27733], Loss: 2.2777\n",
      "Epoch [55/300], Step [3900/27733], Loss: 2.4423\n",
      "Epoch [55/300], Step [4000/27733], Loss: 2.2885\n",
      "Epoch [55/300], Step [4100/27733], Loss: 2.9657\n",
      "Epoch [55/300], Step [4200/27733], Loss: 1.8397\n",
      "Epoch [55/300], Step [4300/27733], Loss: 2.2876\n",
      "Epoch [55/300], Step [4400/27733], Loss: 2.3278\n",
      "Epoch [55/300], Step [4500/27733], Loss: 2.7719\n",
      "Epoch [55/300], Step [4600/27733], Loss: 2.2983\n",
      "Epoch [55/300], Step [4700/27733], Loss: 2.6372\n",
      "Epoch [55/300], Step [4800/27733], Loss: 2.3876\n",
      "Epoch [55/300], Step [4900/27733], Loss: 2.6104\n",
      "Epoch [55/300], Step [5000/27733], Loss: 2.9666\n",
      "Epoch [55/300], Step [5100/27733], Loss: 2.3653\n",
      "Epoch [55/300], Step [5200/27733], Loss: 2.4177\n",
      "Epoch [55/300], Step [5300/27733], Loss: 3.0449\n",
      "Epoch [55/300], Step [5400/27733], Loss: 2.1325\n",
      "Epoch [55/300], Step [5500/27733], Loss: 2.3452\n",
      "Epoch [55/300], Step [5600/27733], Loss: 2.3248\n",
      "Epoch [55/300], Step [5700/27733], Loss: 2.8860\n",
      "Epoch [55/300], Step [5800/27733], Loss: 2.4167\n",
      "Epoch [55/300], Step [5900/27733], Loss: 2.7513\n",
      "Epoch [55/300], Step [6000/27733], Loss: 2.3617\n",
      "Epoch [55/300], Step [6100/27733], Loss: 2.9120\n",
      "Epoch [55/300], Step [6200/27733], Loss: 1.6523\n",
      "Epoch [55/300], Step [6300/27733], Loss: 2.3631\n",
      "Epoch [55/300], Step [6400/27733], Loss: 3.2502\n",
      "Epoch [55/300], Step [6500/27733], Loss: 2.6045\n",
      "Epoch [55/300], Step [6600/27733], Loss: 3.2946\n",
      "Epoch [55/300], Step [6700/27733], Loss: 1.8672\n",
      "Epoch [55/300], Step [6800/27733], Loss: 2.2247\n",
      "Epoch [55/300], Step [6900/27733], Loss: 2.7721\n",
      "Epoch [55/300], Step [7000/27733], Loss: 2.9614\n",
      "Epoch [55/300], Step [7100/27733], Loss: 2.2475\n",
      "Epoch [55/300], Step [7200/27733], Loss: 3.1144\n",
      "Epoch [55/300], Step [7300/27733], Loss: 2.3494\n",
      "Epoch [55/300], Step [7400/27733], Loss: 2.5062\n",
      "Epoch [55/300], Step [7500/27733], Loss: 1.9300\n",
      "Epoch [55/300], Step [7600/27733], Loss: 2.2755\n",
      "Epoch [55/300], Step [7700/27733], Loss: 2.4657\n",
      "Epoch [55/300], Step [7800/27733], Loss: 1.9395\n",
      "Epoch [55/300], Step [7900/27733], Loss: 2.8342\n",
      "Epoch [55/300], Step [8000/27733], Loss: 2.5536\n",
      "Epoch [55/300], Step [8100/27733], Loss: 2.4202\n",
      "Epoch [55/300], Step [8200/27733], Loss: 2.4800\n",
      "Epoch [55/300], Step [8300/27733], Loss: 2.2170\n",
      "Epoch [55/300], Step [8400/27733], Loss: 2.5513\n",
      "Epoch [55/300], Step [8500/27733], Loss: 2.6259\n",
      "Epoch [55/300], Step [8600/27733], Loss: 3.2920\n",
      "Epoch [55/300], Step [8700/27733], Loss: 3.1260\n",
      "Epoch [55/300], Step [8800/27733], Loss: 2.9446\n",
      "Epoch [55/300], Step [8900/27733], Loss: 2.1925\n",
      "Epoch [55/300], Step [9000/27733], Loss: 3.1551\n",
      "Epoch [55/300], Step [9100/27733], Loss: 2.5943\n",
      "Epoch [55/300], Step [9200/27733], Loss: 2.2378\n",
      "Epoch [55/300], Step [9300/27733], Loss: 2.6802\n",
      "Epoch [55/300], Step [9400/27733], Loss: 2.2860\n",
      "Epoch [55/300], Step [9500/27733], Loss: 2.8460\n",
      "Epoch [55/300], Step [9600/27733], Loss: 3.2015\n",
      "Epoch [55/300], Step [9700/27733], Loss: 1.7688\n",
      "Epoch [55/300], Step [9800/27733], Loss: 2.7462\n",
      "Epoch [55/300], Step [9900/27733], Loss: 2.4582\n",
      "Epoch [55/300], Step [10000/27733], Loss: 2.9839\n",
      "Epoch [55/300], Step [10100/27733], Loss: 2.8936\n",
      "Epoch [55/300], Step [10200/27733], Loss: 2.4326\n",
      "Epoch [55/300], Step [10300/27733], Loss: 2.4645\n",
      "Epoch [55/300], Step [10400/27733], Loss: 2.5481\n",
      "Epoch [55/300], Step [10500/27733], Loss: 2.0680\n",
      "Epoch [55/300], Step [10600/27733], Loss: 3.6179\n",
      "Epoch [55/300], Step [10700/27733], Loss: 2.5886\n",
      "Epoch [55/300], Step [10800/27733], Loss: 2.1690\n",
      "Epoch [55/300], Step [10900/27733], Loss: 3.1888\n",
      "Epoch [55/300], Step [11000/27733], Loss: 3.4903\n",
      "Epoch [55/300], Step [11100/27733], Loss: 2.7966\n",
      "Epoch [55/300], Step [11200/27733], Loss: 2.4952\n",
      "Epoch [55/300], Step [11300/27733], Loss: 2.7686\n",
      "Epoch [55/300], Step [11400/27733], Loss: 3.0338\n",
      "Epoch [55/300], Step [11500/27733], Loss: 2.8760\n",
      "Epoch [55/300], Step [11600/27733], Loss: 3.9835\n",
      "Epoch [55/300], Step [11700/27733], Loss: 2.5291\n",
      "Epoch [55/300], Step [11800/27733], Loss: 3.2885\n",
      "Epoch [55/300], Step [11900/27733], Loss: 3.0775\n",
      "Epoch [55/300], Step [12000/27733], Loss: 3.6636\n",
      "Epoch [55/300], Step [12100/27733], Loss: 3.3643\n",
      "Epoch [55/300], Step [12200/27733], Loss: 2.5196\n",
      "Epoch [55/300], Step [12300/27733], Loss: 2.7033\n",
      "Epoch [55/300], Step [12400/27733], Loss: 3.2546\n",
      "Epoch [55/300], Step [12500/27733], Loss: 2.5173\n",
      "Epoch [55/300], Step [12600/27733], Loss: 2.9933\n",
      "Epoch [55/300], Step [12700/27733], Loss: 3.1797\n",
      "Epoch [55/300], Step [12800/27733], Loss: 2.5616\n",
      "Epoch [55/300], Step [12900/27733], Loss: 3.0365\n",
      "Epoch [55/300], Step [13000/27733], Loss: 2.2242\n",
      "Epoch [55/300], Step [13100/27733], Loss: 3.0040\n",
      "Epoch [55/300], Step [13200/27733], Loss: 2.1159\n",
      "Epoch [55/300], Step [13300/27733], Loss: 2.5436\n",
      "Epoch [55/300], Step [13400/27733], Loss: 2.8759\n",
      "Epoch [55/300], Step [13500/27733], Loss: 2.7410\n",
      "Epoch [55/300], Step [13600/27733], Loss: 3.6679\n",
      "Epoch [55/300], Step [13700/27733], Loss: 3.2285\n",
      "Epoch [55/300], Step [13800/27733], Loss: 2.8359\n",
      "Epoch [55/300], Step [13900/27733], Loss: 2.4284\n",
      "Epoch [55/300], Step [14000/27733], Loss: 1.8357\n",
      "Epoch [55/300], Step [14100/27733], Loss: 3.5804\n",
      "Epoch [55/300], Step [14200/27733], Loss: 3.1140\n",
      "Epoch [55/300], Step [14300/27733], Loss: 2.6639\n",
      "Epoch [55/300], Step [14400/27733], Loss: 2.5098\n",
      "Epoch [55/300], Step [14500/27733], Loss: 2.2257\n",
      "Epoch [55/300], Step [14600/27733], Loss: 2.7368\n",
      "Epoch [55/300], Step [14700/27733], Loss: 2.1609\n",
      "Epoch [55/300], Step [14800/27733], Loss: 2.4199\n",
      "Epoch [55/300], Step [14900/27733], Loss: 3.4237\n",
      "Epoch [55/300], Step [15000/27733], Loss: 3.1448\n",
      "Epoch [55/300], Step [15100/27733], Loss: 3.5475\n",
      "Epoch [55/300], Step [15200/27733], Loss: 2.7251\n",
      "Epoch [55/300], Step [15300/27733], Loss: 2.9779\n",
      "Epoch [55/300], Step [15400/27733], Loss: 2.7627\n",
      "Epoch [55/300], Step [15500/27733], Loss: 2.7525\n",
      "Epoch [55/300], Step [15600/27733], Loss: 2.1785\n",
      "Epoch [55/300], Step [15700/27733], Loss: 3.2634\n",
      "Epoch [55/300], Step [15800/27733], Loss: 3.4699\n",
      "Epoch [55/300], Step [15900/27733], Loss: 2.9416\n",
      "Epoch [55/300], Step [16000/27733], Loss: 3.3931\n",
      "Epoch [55/300], Step [16100/27733], Loss: 2.6300\n",
      "Epoch [55/300], Step [16200/27733], Loss: 3.0014\n",
      "Epoch [55/300], Step [16300/27733], Loss: 2.8496\n",
      "Epoch [55/300], Step [16400/27733], Loss: 2.8042\n",
      "Epoch [55/300], Step [16500/27733], Loss: 3.3762\n",
      "Epoch [55/300], Step [16600/27733], Loss: 2.7204\n",
      "Epoch [55/300], Step [16700/27733], Loss: 4.1833\n",
      "Epoch [55/300], Step [16800/27733], Loss: 2.5116\n",
      "Epoch [55/300], Step [16900/27733], Loss: 2.9308\n",
      "Epoch [55/300], Step [17000/27733], Loss: 3.0107\n",
      "Epoch [55/300], Step [17100/27733], Loss: 2.3090\n",
      "Epoch [55/300], Step [17200/27733], Loss: 3.2091\n",
      "Epoch [55/300], Step [17300/27733], Loss: 3.0434\n",
      "Epoch [55/300], Step [17400/27733], Loss: 3.4083\n",
      "Epoch [55/300], Step [17500/27733], Loss: 2.6320\n",
      "Epoch [55/300], Step [17600/27733], Loss: 2.4450\n",
      "Epoch [55/300], Step [17700/27733], Loss: 2.4613\n",
      "Epoch [55/300], Step [17800/27733], Loss: 3.0031\n",
      "Epoch [55/300], Step [17900/27733], Loss: 2.8954\n",
      "Epoch [55/300], Step [18000/27733], Loss: 3.3316\n",
      "Epoch [55/300], Step [18100/27733], Loss: 3.8617\n",
      "Epoch [55/300], Step [18200/27733], Loss: 2.1140\n",
      "Epoch [55/300], Step [18300/27733], Loss: 2.8532\n",
      "Epoch [55/300], Step [18400/27733], Loss: 2.3833\n",
      "Epoch [55/300], Step [18500/27733], Loss: 3.0393\n",
      "Epoch [55/300], Step [18600/27733], Loss: 2.6526\n",
      "Epoch [55/300], Step [18700/27733], Loss: 2.3124\n",
      "Epoch [55/300], Step [18800/27733], Loss: 3.1641\n",
      "Epoch [55/300], Step [18900/27733], Loss: 3.3065\n",
      "Epoch [55/300], Step [19000/27733], Loss: 3.1535\n",
      "Epoch [55/300], Step [19100/27733], Loss: 2.8201\n",
      "Epoch [55/300], Step [19200/27733], Loss: 2.3962\n",
      "Epoch [55/300], Step [19300/27733], Loss: 2.5270\n",
      "Epoch [55/300], Step [19400/27733], Loss: 2.2717\n",
      "Epoch [55/300], Step [19500/27733], Loss: 3.3528\n",
      "Epoch [55/300], Step [19600/27733], Loss: 2.7434\n",
      "Epoch [55/300], Step [19700/27733], Loss: 2.5719\n",
      "Epoch [55/300], Step [19800/27733], Loss: 3.8133\n",
      "Epoch [55/300], Step [19900/27733], Loss: 2.4801\n",
      "Epoch [55/300], Step [20000/27733], Loss: 2.9031\n",
      "Epoch [55/300], Step [20100/27733], Loss: 2.9433\n",
      "Epoch [55/300], Step [20200/27733], Loss: 2.6903\n",
      "Epoch [55/300], Step [20300/27733], Loss: 3.0675\n",
      "Epoch [55/300], Step [20400/27733], Loss: 2.9627\n",
      "Epoch [55/300], Step [20500/27733], Loss: 2.8195\n",
      "Epoch [55/300], Step [20600/27733], Loss: 2.5102\n",
      "Epoch [55/300], Step [20700/27733], Loss: 2.8395\n",
      "Epoch [55/300], Step [20800/27733], Loss: 2.6882\n",
      "Epoch [55/300], Step [20900/27733], Loss: 3.2712\n",
      "Epoch [55/300], Step [21000/27733], Loss: 2.3786\n",
      "Epoch [55/300], Step [21100/27733], Loss: 3.0968\n",
      "Epoch [55/300], Step [21200/27733], Loss: 2.8628\n",
      "Epoch [55/300], Step [21300/27733], Loss: 2.8489\n",
      "Epoch [55/300], Step [21400/27733], Loss: 2.3079\n",
      "Epoch [55/300], Step [21500/27733], Loss: 3.5513\n",
      "Epoch [55/300], Step [21600/27733], Loss: 2.9432\n",
      "Epoch [55/300], Step [21700/27733], Loss: 2.4326\n",
      "Epoch [55/300], Step [21800/27733], Loss: 3.1463\n",
      "Epoch [55/300], Step [21900/27733], Loss: 3.4084\n",
      "Epoch [55/300], Step [22000/27733], Loss: 3.4024\n",
      "Epoch [55/300], Step [22100/27733], Loss: 3.1431\n",
      "Epoch [55/300], Step [22200/27733], Loss: 3.4396\n",
      "Epoch [55/300], Step [22300/27733], Loss: 2.2350\n",
      "Epoch [55/300], Step [22400/27733], Loss: 3.1167\n",
      "Epoch [55/300], Step [22500/27733], Loss: 3.5449\n",
      "Epoch [55/300], Step [22600/27733], Loss: 2.5155\n",
      "Epoch [55/300], Step [22700/27733], Loss: 3.3940\n",
      "Epoch [55/300], Step [22800/27733], Loss: 2.2406\n",
      "Epoch [55/300], Step [22900/27733], Loss: 2.7617\n",
      "Epoch [55/300], Step [23000/27733], Loss: 2.8930\n",
      "Epoch [55/300], Step [23100/27733], Loss: 2.2461\n",
      "Epoch [55/300], Step [23200/27733], Loss: 2.8585\n",
      "Epoch [55/300], Step [23300/27733], Loss: 2.5652\n",
      "Epoch [55/300], Step [23400/27733], Loss: 4.3766\n",
      "Epoch [55/300], Step [23500/27733], Loss: 3.2949\n",
      "Epoch [55/300], Step [23600/27733], Loss: 3.1967\n",
      "Epoch [55/300], Step [23700/27733], Loss: 3.0264\n",
      "Epoch [55/300], Step [23800/27733], Loss: 2.8985\n",
      "Epoch [55/300], Step [23900/27733], Loss: 3.1220\n",
      "Epoch [55/300], Step [24000/27733], Loss: 3.5384\n",
      "Epoch [55/300], Step [24100/27733], Loss: 2.8973\n",
      "Epoch [55/300], Step [24200/27733], Loss: 2.2334\n",
      "Epoch [55/300], Step [24300/27733], Loss: 2.7785\n",
      "Epoch [55/300], Step [24400/27733], Loss: 2.1613\n",
      "Epoch [55/300], Step [24500/27733], Loss: 2.8210\n",
      "Epoch [55/300], Step [24600/27733], Loss: 2.6587\n",
      "Epoch [55/300], Step [24700/27733], Loss: 2.8250\n",
      "Epoch [55/300], Step [24800/27733], Loss: 2.4430\n",
      "Epoch [55/300], Step [24900/27733], Loss: 2.4018\n",
      "Epoch [55/300], Step [25000/27733], Loss: 1.9359\n",
      "Epoch [55/300], Step [25100/27733], Loss: 3.2175\n",
      "Epoch [55/300], Step [25200/27733], Loss: 3.2191\n",
      "Epoch [55/300], Step [25300/27733], Loss: 3.9055\n",
      "Epoch [55/300], Step [25400/27733], Loss: 3.0271\n",
      "Epoch [55/300], Step [25500/27733], Loss: 2.2993\n",
      "Epoch [55/300], Step [25600/27733], Loss: 2.8577\n",
      "Epoch [55/300], Step [25700/27733], Loss: 3.9483\n",
      "Epoch [55/300], Step [25800/27733], Loss: 2.6074\n",
      "Epoch [55/300], Step [25900/27733], Loss: 3.0458\n",
      "Epoch [55/300], Step [26000/27733], Loss: 3.2711\n",
      "Epoch [55/300], Step [26100/27733], Loss: 2.1819\n",
      "Epoch [55/300], Step [26200/27733], Loss: 3.0492\n",
      "Epoch [55/300], Step [26300/27733], Loss: 3.3541\n",
      "Epoch [55/300], Step [26400/27733], Loss: 2.9771\n",
      "Epoch [55/300], Step [26500/27733], Loss: 2.5018\n",
      "Epoch [55/300], Step [26600/27733], Loss: 2.5285\n",
      "Epoch [55/300], Step [26700/27733], Loss: 3.0319\n",
      "Epoch [55/300], Step [26800/27733], Loss: 2.8975\n",
      "Epoch [55/300], Step [26900/27733], Loss: 2.3050\n",
      "Epoch [55/300], Step [27000/27733], Loss: 2.5562\n",
      "Epoch [55/300], Step [27100/27733], Loss: 3.1440\n",
      "Epoch [55/300], Step [27200/27733], Loss: 2.6194\n",
      "Epoch [55/300], Step [27300/27733], Loss: 3.5135\n",
      "Epoch [55/300], Step [27400/27733], Loss: 2.6460\n",
      "Epoch [55/300], Step [27500/27733], Loss: 2.9793\n",
      "Epoch [55/300], Step [27600/27733], Loss: 3.3129\n",
      "Epoch [55/300], Step [27700/27733], Loss: 3.2955\n",
      "Epoch [56/300], Step [100/27733], Loss: 2.1102\n",
      "Epoch [56/300], Step [200/27733], Loss: 1.9985\n",
      "Epoch [56/300], Step [300/27733], Loss: 2.3551\n",
      "Epoch [56/300], Step [400/27733], Loss: 2.2787\n",
      "Epoch [56/300], Step [500/27733], Loss: 2.1220\n",
      "Epoch [56/300], Step [600/27733], Loss: 1.9033\n",
      "Epoch [56/300], Step [700/27733], Loss: 2.1624\n",
      "Epoch [56/300], Step [800/27733], Loss: 2.3019\n",
      "Epoch [56/300], Step [900/27733], Loss: 2.6140\n",
      "Epoch [56/300], Step [1000/27733], Loss: 1.7162\n",
      "Epoch [56/300], Step [1100/27733], Loss: 2.4167\n",
      "Epoch [56/300], Step [1200/27733], Loss: 2.5343\n",
      "Epoch [56/300], Step [1300/27733], Loss: 3.3428\n",
      "Epoch [56/300], Step [1400/27733], Loss: 2.4867\n",
      "Epoch [56/300], Step [1500/27733], Loss: 2.5487\n",
      "Epoch [56/300], Step [1600/27733], Loss: 2.8413\n",
      "Epoch [56/300], Step [1700/27733], Loss: 2.4313\n",
      "Epoch [56/300], Step [1800/27733], Loss: 2.5545\n",
      "Epoch [56/300], Step [1900/27733], Loss: 2.7964\n",
      "Epoch [56/300], Step [2000/27733], Loss: 2.6848\n",
      "Epoch [56/300], Step [2100/27733], Loss: 2.0188\n",
      "Epoch [56/300], Step [2200/27733], Loss: 2.8693\n",
      "Epoch [56/300], Step [2300/27733], Loss: 1.9232\n",
      "Epoch [56/300], Step [2400/27733], Loss: 2.0852\n",
      "Epoch [56/300], Step [2500/27733], Loss: 2.3057\n",
      "Epoch [56/300], Step [2600/27733], Loss: 2.1038\n",
      "Epoch [56/300], Step [2700/27733], Loss: 2.4985\n",
      "Epoch [56/300], Step [2800/27733], Loss: 2.6483\n",
      "Epoch [56/300], Step [2900/27733], Loss: 2.0782\n",
      "Epoch [56/300], Step [3000/27733], Loss: 2.8448\n",
      "Epoch [56/300], Step [3100/27733], Loss: 1.9641\n",
      "Epoch [56/300], Step [3200/27733], Loss: 2.7607\n",
      "Epoch [56/300], Step [3300/27733], Loss: 2.6762\n",
      "Epoch [56/300], Step [3400/27733], Loss: 2.1486\n",
      "Epoch [56/300], Step [3500/27733], Loss: 2.3958\n",
      "Epoch [56/300], Step [3600/27733], Loss: 2.5332\n",
      "Epoch [56/300], Step [3700/27733], Loss: 1.9427\n",
      "Epoch [56/300], Step [3800/27733], Loss: 2.6866\n",
      "Epoch [56/300], Step [3900/27733], Loss: 1.9016\n",
      "Epoch [56/300], Step [4000/27733], Loss: 2.0051\n",
      "Epoch [56/300], Step [4100/27733], Loss: 2.5496\n",
      "Epoch [56/300], Step [4200/27733], Loss: 2.1996\n",
      "Epoch [56/300], Step [4300/27733], Loss: 2.0145\n",
      "Epoch [56/300], Step [4400/27733], Loss: 2.7833\n",
      "Epoch [56/300], Step [4500/27733], Loss: 2.0494\n",
      "Epoch [56/300], Step [4600/27733], Loss: 3.1280\n",
      "Epoch [56/300], Step [4700/27733], Loss: 3.0270\n",
      "Epoch [56/300], Step [4800/27733], Loss: 2.7040\n",
      "Epoch [56/300], Step [4900/27733], Loss: 2.4872\n",
      "Epoch [56/300], Step [5000/27733], Loss: 2.5914\n",
      "Epoch [56/300], Step [5100/27733], Loss: 3.5285\n",
      "Epoch [56/300], Step [5200/27733], Loss: 3.0445\n",
      "Epoch [56/300], Step [5300/27733], Loss: 2.7919\n",
      "Epoch [56/300], Step [5400/27733], Loss: 3.0658\n",
      "Epoch [56/300], Step [5500/27733], Loss: 2.7951\n",
      "Epoch [56/300], Step [5600/27733], Loss: 2.4487\n",
      "Epoch [56/300], Step [5700/27733], Loss: 2.5726\n",
      "Epoch [56/300], Step [5800/27733], Loss: 2.2198\n",
      "Epoch [56/300], Step [5900/27733], Loss: 2.4595\n",
      "Epoch [56/300], Step [6000/27733], Loss: 2.5948\n",
      "Epoch [56/300], Step [6100/27733], Loss: 2.6339\n",
      "Epoch [56/300], Step [6200/27733], Loss: 3.3362\n",
      "Epoch [56/300], Step [6300/27733], Loss: 3.2487\n",
      "Epoch [56/300], Step [6400/27733], Loss: 2.9472\n",
      "Epoch [56/300], Step [6500/27733], Loss: 2.9031\n",
      "Epoch [56/300], Step [6600/27733], Loss: 2.0630\n",
      "Epoch [56/300], Step [6700/27733], Loss: 2.3535\n",
      "Epoch [56/300], Step [6800/27733], Loss: 3.2036\n",
      "Epoch [56/300], Step [6900/27733], Loss: 2.8560\n",
      "Epoch [56/300], Step [7000/27733], Loss: 1.9134\n",
      "Epoch [56/300], Step [7100/27733], Loss: 2.7584\n",
      "Epoch [56/300], Step [7200/27733], Loss: 3.4313\n",
      "Epoch [56/300], Step [7300/27733], Loss: 3.0677\n",
      "Epoch [56/300], Step [7400/27733], Loss: 2.9973\n",
      "Epoch [56/300], Step [7500/27733], Loss: 2.1306\n",
      "Epoch [56/300], Step [7600/27733], Loss: 3.2925\n",
      "Epoch [56/300], Step [7700/27733], Loss: 2.4632\n",
      "Epoch [56/300], Step [7800/27733], Loss: 2.3415\n",
      "Epoch [56/300], Step [7900/27733], Loss: 2.5890\n",
      "Epoch [56/300], Step [8000/27733], Loss: 2.2084\n",
      "Epoch [56/300], Step [8100/27733], Loss: 2.9516\n",
      "Epoch [56/300], Step [8200/27733], Loss: 2.1171\n",
      "Epoch [56/300], Step [8300/27733], Loss: 2.5377\n",
      "Epoch [56/300], Step [8400/27733], Loss: 2.3493\n",
      "Epoch [56/300], Step [8500/27733], Loss: 2.8416\n",
      "Epoch [56/300], Step [8600/27733], Loss: 2.9855\n",
      "Epoch [56/300], Step [8700/27733], Loss: 2.6613\n",
      "Epoch [56/300], Step [8800/27733], Loss: 2.6809\n",
      "Epoch [56/300], Step [8900/27733], Loss: 2.6643\n",
      "Epoch [56/300], Step [9000/27733], Loss: 2.5397\n",
      "Epoch [56/300], Step [9100/27733], Loss: 2.5923\n",
      "Epoch [56/300], Step [9200/27733], Loss: 1.8802\n",
      "Epoch [56/300], Step [9300/27733], Loss: 2.5454\n",
      "Epoch [56/300], Step [9400/27733], Loss: 1.8311\n",
      "Epoch [56/300], Step [9500/27733], Loss: 3.1298\n",
      "Epoch [56/300], Step [9600/27733], Loss: 1.9866\n",
      "Epoch [56/300], Step [9700/27733], Loss: 2.5259\n",
      "Epoch [56/300], Step [9800/27733], Loss: 2.7336\n",
      "Epoch [56/300], Step [9900/27733], Loss: 2.6891\n",
      "Epoch [56/300], Step [10000/27733], Loss: 2.2948\n",
      "Epoch [56/300], Step [10100/27733], Loss: 2.7924\n",
      "Epoch [56/300], Step [10200/27733], Loss: 2.9125\n",
      "Epoch [56/300], Step [10300/27733], Loss: 2.8806\n",
      "Epoch [56/300], Step [10400/27733], Loss: 2.1219\n",
      "Epoch [56/300], Step [10500/27733], Loss: 2.5535\n",
      "Epoch [56/300], Step [10600/27733], Loss: 2.2302\n",
      "Epoch [56/300], Step [10700/27733], Loss: 2.5582\n",
      "Epoch [56/300], Step [10800/27733], Loss: 2.4586\n",
      "Epoch [56/300], Step [10900/27733], Loss: 3.1895\n",
      "Epoch [56/300], Step [11000/27733], Loss: 2.3974\n",
      "Epoch [56/300], Step [11100/27733], Loss: 2.5572\n",
      "Epoch [56/300], Step [11200/27733], Loss: 2.8964\n",
      "Epoch [56/300], Step [11300/27733], Loss: 3.2443\n",
      "Epoch [56/300], Step [11400/27733], Loss: 2.2703\n",
      "Epoch [56/300], Step [11500/27733], Loss: 1.9981\n",
      "Epoch [56/300], Step [11600/27733], Loss: 2.4336\n",
      "Epoch [56/300], Step [11700/27733], Loss: 2.0475\n",
      "Epoch [56/300], Step [11800/27733], Loss: 4.0651\n",
      "Epoch [56/300], Step [11900/27733], Loss: 2.7630\n",
      "Epoch [56/300], Step [12000/27733], Loss: 1.9725\n",
      "Epoch [56/300], Step [12100/27733], Loss: 2.5120\n",
      "Epoch [56/300], Step [12200/27733], Loss: 2.5124\n",
      "Epoch [56/300], Step [12300/27733], Loss: 1.8220\n",
      "Epoch [56/300], Step [12400/27733], Loss: 1.9572\n",
      "Epoch [56/300], Step [12500/27733], Loss: 3.4355\n",
      "Epoch [56/300], Step [12600/27733], Loss: 2.2006\n",
      "Epoch [56/300], Step [12700/27733], Loss: 2.6648\n",
      "Epoch [56/300], Step [12800/27733], Loss: 2.3960\n",
      "Epoch [56/300], Step [12900/27733], Loss: 2.7852\n",
      "Epoch [56/300], Step [13000/27733], Loss: 2.8173\n",
      "Epoch [56/300], Step [13100/27733], Loss: 3.4357\n",
      "Epoch [56/300], Step [13200/27733], Loss: 3.2374\n",
      "Epoch [56/300], Step [13300/27733], Loss: 2.2896\n",
      "Epoch [56/300], Step [13400/27733], Loss: 2.2125\n",
      "Epoch [56/300], Step [13500/27733], Loss: 3.3054\n",
      "Epoch [56/300], Step [13600/27733], Loss: 2.7149\n",
      "Epoch [56/300], Step [13700/27733], Loss: 2.7413\n",
      "Epoch [56/300], Step [13800/27733], Loss: 2.6521\n",
      "Epoch [56/300], Step [13900/27733], Loss: 2.3918\n",
      "Epoch [56/300], Step [14000/27733], Loss: 3.1981\n",
      "Epoch [56/300], Step [14100/27733], Loss: 2.8152\n",
      "Epoch [56/300], Step [14200/27733], Loss: 2.9253\n",
      "Epoch [56/300], Step [14300/27733], Loss: 2.1286\n",
      "Epoch [56/300], Step [14400/27733], Loss: 2.0585\n",
      "Epoch [56/300], Step [14500/27733], Loss: 2.6033\n",
      "Epoch [56/300], Step [14600/27733], Loss: 3.2205\n",
      "Epoch [56/300], Step [14700/27733], Loss: 2.3833\n",
      "Epoch [56/300], Step [14800/27733], Loss: 2.3363\n",
      "Epoch [56/300], Step [14900/27733], Loss: 3.0302\n",
      "Epoch [56/300], Step [15000/27733], Loss: 2.5353\n",
      "Epoch [56/300], Step [15100/27733], Loss: 3.5596\n",
      "Epoch [56/300], Step [15200/27733], Loss: 2.6779\n",
      "Epoch [56/300], Step [15300/27733], Loss: 2.2567\n",
      "Epoch [56/300], Step [15400/27733], Loss: 2.8474\n",
      "Epoch [56/300], Step [15500/27733], Loss: 2.5983\n",
      "Epoch [56/300], Step [15600/27733], Loss: 2.3690\n",
      "Epoch [56/300], Step [15700/27733], Loss: 2.5233\n",
      "Epoch [56/300], Step [15800/27733], Loss: 3.7788\n",
      "Epoch [56/300], Step [15900/27733], Loss: 2.7092\n",
      "Epoch [56/300], Step [16000/27733], Loss: 2.2305\n",
      "Epoch [56/300], Step [16100/27733], Loss: 1.9391\n",
      "Epoch [56/300], Step [16200/27733], Loss: 2.4156\n",
      "Epoch [56/300], Step [16300/27733], Loss: 2.2586\n",
      "Epoch [56/300], Step [16400/27733], Loss: 2.6690\n",
      "Epoch [56/300], Step [16500/27733], Loss: 3.2632\n",
      "Epoch [56/300], Step [16600/27733], Loss: 2.9622\n",
      "Epoch [56/300], Step [16700/27733], Loss: 2.7933\n",
      "Epoch [56/300], Step [16800/27733], Loss: 2.1706\n",
      "Epoch [56/300], Step [16900/27733], Loss: 2.6633\n",
      "Epoch [56/300], Step [17000/27733], Loss: 2.0739\n",
      "Epoch [56/300], Step [17100/27733], Loss: 3.1267\n",
      "Epoch [56/300], Step [17200/27733], Loss: 2.9196\n",
      "Epoch [56/300], Step [17300/27733], Loss: 2.4792\n",
      "Epoch [56/300], Step [17400/27733], Loss: 2.4519\n",
      "Epoch [56/300], Step [17500/27733], Loss: 2.6414\n",
      "Epoch [56/300], Step [17600/27733], Loss: 3.0943\n",
      "Epoch [56/300], Step [17700/27733], Loss: 3.0161\n",
      "Epoch [56/300], Step [17800/27733], Loss: 2.9409\n",
      "Epoch [56/300], Step [17900/27733], Loss: 2.5086\n",
      "Epoch [56/300], Step [18000/27733], Loss: 2.9939\n",
      "Epoch [56/300], Step [18100/27733], Loss: 2.9404\n",
      "Epoch [56/300], Step [18200/27733], Loss: 2.7231\n",
      "Epoch [56/300], Step [18300/27733], Loss: 2.6136\n",
      "Epoch [56/300], Step [18400/27733], Loss: 2.5268\n",
      "Epoch [56/300], Step [18500/27733], Loss: 3.3815\n",
      "Epoch [56/300], Step [18600/27733], Loss: 2.6489\n",
      "Epoch [56/300], Step [18700/27733], Loss: 2.6486\n",
      "Epoch [56/300], Step [18800/27733], Loss: 3.0480\n",
      "Epoch [56/300], Step [18900/27733], Loss: 3.1688\n",
      "Epoch [56/300], Step [19000/27733], Loss: 2.9499\n",
      "Epoch [56/300], Step [19100/27733], Loss: 2.7696\n",
      "Epoch [56/300], Step [19200/27733], Loss: 3.2460\n",
      "Epoch [56/300], Step [19300/27733], Loss: 3.0092\n",
      "Epoch [56/300], Step [19400/27733], Loss: 3.1028\n",
      "Epoch [56/300], Step [19500/27733], Loss: 2.6533\n",
      "Epoch [56/300], Step [19600/27733], Loss: 3.1425\n",
      "Epoch [56/300], Step [19700/27733], Loss: 2.3130\n",
      "Epoch [56/300], Step [19800/27733], Loss: 2.8062\n",
      "Epoch [56/300], Step [19900/27733], Loss: 2.7036\n",
      "Epoch [56/300], Step [20000/27733], Loss: 2.4588\n",
      "Epoch [56/300], Step [20100/27733], Loss: 3.7366\n",
      "Epoch [56/300], Step [20200/27733], Loss: 3.2352\n",
      "Epoch [56/300], Step [20300/27733], Loss: 3.4317\n",
      "Epoch [56/300], Step [20400/27733], Loss: 2.6820\n",
      "Epoch [56/300], Step [20500/27733], Loss: 3.4805\n",
      "Epoch [56/300], Step [20600/27733], Loss: 2.6239\n",
      "Epoch [56/300], Step [20700/27733], Loss: 2.8194\n",
      "Epoch [56/300], Step [20800/27733], Loss: 2.4534\n",
      "Epoch [56/300], Step [20900/27733], Loss: 3.4161\n",
      "Epoch [56/300], Step [21000/27733], Loss: 2.3211\n",
      "Epoch [56/300], Step [21100/27733], Loss: 3.5983\n",
      "Epoch [56/300], Step [21200/27733], Loss: 2.6158\n",
      "Epoch [56/300], Step [21300/27733], Loss: 3.0195\n",
      "Epoch [56/300], Step [21400/27733], Loss: 3.5902\n",
      "Epoch [56/300], Step [21500/27733], Loss: 2.8466\n",
      "Epoch [56/300], Step [21600/27733], Loss: 2.3922\n",
      "Epoch [56/300], Step [21700/27733], Loss: 2.8610\n",
      "Epoch [56/300], Step [21800/27733], Loss: 2.7036\n",
      "Epoch [56/300], Step [21900/27733], Loss: 2.3923\n",
      "Epoch [56/300], Step [22000/27733], Loss: 2.9451\n",
      "Epoch [56/300], Step [22100/27733], Loss: 3.1696\n",
      "Epoch [56/300], Step [22200/27733], Loss: 3.4235\n",
      "Epoch [56/300], Step [22300/27733], Loss: 3.6287\n",
      "Epoch [56/300], Step [22400/27733], Loss: 1.9338\n",
      "Epoch [56/300], Step [22500/27733], Loss: 2.7643\n",
      "Epoch [56/300], Step [22600/27733], Loss: 1.9941\n",
      "Epoch [56/300], Step [22700/27733], Loss: 2.6816\n",
      "Epoch [56/300], Step [22800/27733], Loss: 2.2328\n",
      "Epoch [56/300], Step [22900/27733], Loss: 2.8894\n",
      "Epoch [56/300], Step [23000/27733], Loss: 3.2490\n",
      "Epoch [56/300], Step [23100/27733], Loss: 3.7352\n",
      "Epoch [56/300], Step [23200/27733], Loss: 3.0787\n",
      "Epoch [56/300], Step [23300/27733], Loss: 3.2806\n",
      "Epoch [56/300], Step [23400/27733], Loss: 3.0950\n",
      "Epoch [56/300], Step [23500/27733], Loss: 2.5882\n",
      "Epoch [56/300], Step [23600/27733], Loss: 3.7955\n",
      "Epoch [56/300], Step [23700/27733], Loss: 3.5669\n",
      "Epoch [56/300], Step [23800/27733], Loss: 2.7298\n",
      "Epoch [56/300], Step [23900/27733], Loss: 1.9804\n",
      "Epoch [56/300], Step [24000/27733], Loss: 3.1199\n",
      "Epoch [56/300], Step [24100/27733], Loss: 2.9648\n",
      "Epoch [56/300], Step [24200/27733], Loss: 3.2710\n",
      "Epoch [56/300], Step [24300/27733], Loss: 3.1315\n",
      "Epoch [56/300], Step [24400/27733], Loss: 2.6375\n",
      "Epoch [56/300], Step [24500/27733], Loss: 3.6870\n",
      "Epoch [56/300], Step [24600/27733], Loss: 3.0738\n",
      "Epoch [56/300], Step [24700/27733], Loss: 2.2010\n",
      "Epoch [56/300], Step [24800/27733], Loss: 3.8739\n",
      "Epoch [56/300], Step [24900/27733], Loss: 3.1692\n",
      "Epoch [56/300], Step [25000/27733], Loss: 2.8731\n",
      "Epoch [56/300], Step [25100/27733], Loss: 2.4493\n",
      "Epoch [56/300], Step [25200/27733], Loss: 2.7697\n",
      "Epoch [56/300], Step [25300/27733], Loss: 2.6554\n",
      "Epoch [56/300], Step [25400/27733], Loss: 3.4494\n",
      "Epoch [56/300], Step [25500/27733], Loss: 2.4941\n",
      "Epoch [56/300], Step [25600/27733], Loss: 2.8646\n",
      "Epoch [56/300], Step [25700/27733], Loss: 2.3553\n",
      "Epoch [56/300], Step [25800/27733], Loss: 2.3924\n",
      "Epoch [56/300], Step [25900/27733], Loss: 2.9698\n",
      "Epoch [56/300], Step [26000/27733], Loss: 2.4803\n",
      "Epoch [56/300], Step [26100/27733], Loss: 2.7639\n",
      "Epoch [56/300], Step [26200/27733], Loss: 2.8132\n",
      "Epoch [56/300], Step [26300/27733], Loss: 3.1308\n",
      "Epoch [56/300], Step [26400/27733], Loss: 3.4110\n",
      "Epoch [56/300], Step [26500/27733], Loss: 3.6558\n",
      "Epoch [56/300], Step [26600/27733], Loss: 3.4134\n",
      "Epoch [56/300], Step [26700/27733], Loss: 2.5112\n",
      "Epoch [56/300], Step [26800/27733], Loss: 2.9079\n",
      "Epoch [56/300], Step [26900/27733], Loss: 3.1375\n",
      "Epoch [56/300], Step [27000/27733], Loss: 3.0170\n",
      "Epoch [56/300], Step [27100/27733], Loss: 2.9265\n",
      "Epoch [56/300], Step [27200/27733], Loss: 3.3303\n",
      "Epoch [56/300], Step [27300/27733], Loss: 2.1746\n",
      "Epoch [56/300], Step [27400/27733], Loss: 2.4731\n",
      "Epoch [56/300], Step [27500/27733], Loss: 2.8351\n",
      "Epoch [56/300], Step [27600/27733], Loss: 2.4614\n",
      "Epoch [56/300], Step [27700/27733], Loss: 2.9119\n",
      "Epoch [57/300], Step [100/27733], Loss: 2.5436\n",
      "Epoch [57/300], Step [200/27733], Loss: 2.0726\n",
      "Epoch [57/300], Step [300/27733], Loss: 2.3738\n",
      "Epoch [57/300], Step [400/27733], Loss: 2.0563\n",
      "Epoch [57/300], Step [500/27733], Loss: 2.6291\n",
      "Epoch [57/300], Step [600/27733], Loss: 2.8465\n",
      "Epoch [57/300], Step [700/27733], Loss: 2.0805\n",
      "Epoch [57/300], Step [800/27733], Loss: 2.5109\n",
      "Epoch [57/300], Step [900/27733], Loss: 2.8688\n",
      "Epoch [57/300], Step [1000/27733], Loss: 2.7130\n",
      "Epoch [57/300], Step [1100/27733], Loss: 2.3295\n",
      "Epoch [57/300], Step [1200/27733], Loss: 3.1474\n",
      "Epoch [57/300], Step [1300/27733], Loss: 2.9708\n",
      "Epoch [57/300], Step [1400/27733], Loss: 2.3588\n",
      "Epoch [57/300], Step [1500/27733], Loss: 2.7345\n",
      "Epoch [57/300], Step [1600/27733], Loss: 1.7633\n",
      "Epoch [57/300], Step [1700/27733], Loss: 1.9042\n",
      "Epoch [57/300], Step [1800/27733], Loss: 2.4588\n",
      "Epoch [57/300], Step [1900/27733], Loss: 2.6099\n",
      "Epoch [57/300], Step [2000/27733], Loss: 2.3594\n",
      "Epoch [57/300], Step [2100/27733], Loss: 2.0430\n",
      "Epoch [57/300], Step [2200/27733], Loss: 2.4817\n",
      "Epoch [57/300], Step [2300/27733], Loss: 2.0272\n",
      "Epoch [57/300], Step [2400/27733], Loss: 2.7193\n",
      "Epoch [57/300], Step [2500/27733], Loss: 1.9889\n",
      "Epoch [57/300], Step [2600/27733], Loss: 2.8804\n",
      "Epoch [57/300], Step [2700/27733], Loss: 2.4777\n",
      "Epoch [57/300], Step [2800/27733], Loss: 2.8513\n",
      "Epoch [57/300], Step [2900/27733], Loss: 2.4796\n",
      "Epoch [57/300], Step [3000/27733], Loss: 2.7671\n",
      "Epoch [57/300], Step [3100/27733], Loss: 3.1146\n",
      "Epoch [57/300], Step [3200/27733], Loss: 2.4946\n",
      "Epoch [57/300], Step [3300/27733], Loss: 2.3240\n",
      "Epoch [57/300], Step [3400/27733], Loss: 2.1504\n",
      "Epoch [57/300], Step [3500/27733], Loss: 2.8527\n",
      "Epoch [57/300], Step [3600/27733], Loss: 2.4102\n",
      "Epoch [57/300], Step [3700/27733], Loss: 3.4306\n",
      "Epoch [57/300], Step [3800/27733], Loss: 2.4039\n",
      "Epoch [57/300], Step [3900/27733], Loss: 1.8729\n",
      "Epoch [57/300], Step [4000/27733], Loss: 2.3486\n",
      "Epoch [57/300], Step [4100/27733], Loss: 2.2864\n",
      "Epoch [57/300], Step [4200/27733], Loss: 2.1213\n",
      "Epoch [57/300], Step [4300/27733], Loss: 2.7165\n",
      "Epoch [57/300], Step [4400/27733], Loss: 1.7722\n",
      "Epoch [57/300], Step [4500/27733], Loss: 2.2477\n",
      "Epoch [57/300], Step [4600/27733], Loss: 2.7252\n",
      "Epoch [57/300], Step [4700/27733], Loss: 2.0564\n",
      "Epoch [57/300], Step [4800/27733], Loss: 2.2517\n",
      "Epoch [57/300], Step [4900/27733], Loss: 2.8704\n",
      "Epoch [57/300], Step [5000/27733], Loss: 2.6877\n",
      "Epoch [57/300], Step [5100/27733], Loss: 2.0269\n",
      "Epoch [57/300], Step [5200/27733], Loss: 2.7767\n",
      "Epoch [57/300], Step [5300/27733], Loss: 2.6428\n",
      "Epoch [57/300], Step [5400/27733], Loss: 2.1688\n",
      "Epoch [57/300], Step [5500/27733], Loss: 2.4836\n",
      "Epoch [57/300], Step [5600/27733], Loss: 2.6414\n",
      "Epoch [57/300], Step [5700/27733], Loss: 2.7066\n",
      "Epoch [57/300], Step [5800/27733], Loss: 2.7434\n",
      "Epoch [57/300], Step [5900/27733], Loss: 2.4838\n",
      "Epoch [57/300], Step [6000/27733], Loss: 2.7607\n",
      "Epoch [57/300], Step [6100/27733], Loss: 2.5252\n",
      "Epoch [57/300], Step [6200/27733], Loss: 2.5372\n",
      "Epoch [57/300], Step [6300/27733], Loss: 2.8317\n",
      "Epoch [57/300], Step [6400/27733], Loss: 2.5800\n",
      "Epoch [57/300], Step [6500/27733], Loss: 2.2690\n",
      "Epoch [57/300], Step [6600/27733], Loss: 3.2018\n",
      "Epoch [57/300], Step [6700/27733], Loss: 2.3763\n",
      "Epoch [57/300], Step [6800/27733], Loss: 2.3166\n",
      "Epoch [57/300], Step [6900/27733], Loss: 2.5191\n",
      "Epoch [57/300], Step [7000/27733], Loss: 2.5633\n",
      "Epoch [57/300], Step [7100/27733], Loss: 2.4321\n",
      "Epoch [57/300], Step [7200/27733], Loss: 3.0568\n",
      "Epoch [57/300], Step [7300/27733], Loss: 2.5784\n",
      "Epoch [57/300], Step [7400/27733], Loss: 2.4379\n",
      "Epoch [57/300], Step [7500/27733], Loss: 2.3646\n",
      "Epoch [57/300], Step [7600/27733], Loss: 2.7199\n",
      "Epoch [57/300], Step [7700/27733], Loss: 2.1142\n",
      "Epoch [57/300], Step [7800/27733], Loss: 2.4675\n",
      "Epoch [57/300], Step [7900/27733], Loss: 2.1418\n",
      "Epoch [57/300], Step [8000/27733], Loss: 2.6204\n",
      "Epoch [57/300], Step [8100/27733], Loss: 1.9393\n",
      "Epoch [57/300], Step [8200/27733], Loss: 2.1893\n",
      "Epoch [57/300], Step [8300/27733], Loss: 2.6828\n",
      "Epoch [57/300], Step [8400/27733], Loss: 2.9713\n",
      "Epoch [57/300], Step [8500/27733], Loss: 2.1975\n",
      "Epoch [57/300], Step [8600/27733], Loss: 1.5573\n",
      "Epoch [57/300], Step [8700/27733], Loss: 2.0509\n",
      "Epoch [57/300], Step [8800/27733], Loss: 2.9718\n",
      "Epoch [57/300], Step [8900/27733], Loss: 2.4902\n",
      "Epoch [57/300], Step [9000/27733], Loss: 2.6877\n",
      "Epoch [57/300], Step [9100/27733], Loss: 2.1310\n",
      "Epoch [57/300], Step [9200/27733], Loss: 2.3224\n",
      "Epoch [57/300], Step [9300/27733], Loss: 2.0206\n",
      "Epoch [57/300], Step [9400/27733], Loss: 3.0804\n",
      "Epoch [57/300], Step [9500/27733], Loss: 3.1713\n",
      "Epoch [57/300], Step [9600/27733], Loss: 2.0731\n",
      "Epoch [57/300], Step [9700/27733], Loss: 2.9297\n",
      "Epoch [57/300], Step [9800/27733], Loss: 2.1136\n",
      "Epoch [57/300], Step [9900/27733], Loss: 2.1193\n",
      "Epoch [57/300], Step [10000/27733], Loss: 2.5331\n",
      "Epoch [57/300], Step [10100/27733], Loss: 2.9719\n",
      "Epoch [57/300], Step [10200/27733], Loss: 2.7606\n",
      "Epoch [57/300], Step [10300/27733], Loss: 2.7283\n",
      "Epoch [57/300], Step [10400/27733], Loss: 1.8940\n",
      "Epoch [57/300], Step [10500/27733], Loss: 2.2294\n",
      "Epoch [57/300], Step [10600/27733], Loss: 3.1428\n",
      "Epoch [57/300], Step [10700/27733], Loss: 3.2322\n",
      "Epoch [57/300], Step [10800/27733], Loss: 2.5598\n",
      "Epoch [57/300], Step [10900/27733], Loss: 2.9668\n",
      "Epoch [57/300], Step [11000/27733], Loss: 2.3863\n",
      "Epoch [57/300], Step [11100/27733], Loss: 1.7085\n",
      "Epoch [57/300], Step [11200/27733], Loss: 3.1042\n",
      "Epoch [57/300], Step [11300/27733], Loss: 2.4393\n",
      "Epoch [57/300], Step [11400/27733], Loss: 3.2842\n",
      "Epoch [57/300], Step [11500/27733], Loss: 2.3896\n",
      "Epoch [57/300], Step [11600/27733], Loss: 2.8620\n",
      "Epoch [57/300], Step [11700/27733], Loss: 2.5339\n",
      "Epoch [57/300], Step [11800/27733], Loss: 2.9863\n",
      "Epoch [57/300], Step [11900/27733], Loss: 2.6717\n",
      "Epoch [57/300], Step [12000/27733], Loss: 3.0277\n",
      "Epoch [57/300], Step [12100/27733], Loss: 2.0369\n",
      "Epoch [57/300], Step [12200/27733], Loss: 2.4573\n",
      "Epoch [57/300], Step [12300/27733], Loss: 3.4799\n",
      "Epoch [57/300], Step [12400/27733], Loss: 2.1146\n",
      "Epoch [57/300], Step [12500/27733], Loss: 2.7277\n",
      "Epoch [57/300], Step [12600/27733], Loss: 3.8449\n",
      "Epoch [57/300], Step [12700/27733], Loss: 2.6146\n",
      "Epoch [57/300], Step [12800/27733], Loss: 2.7364\n",
      "Epoch [57/300], Step [12900/27733], Loss: 3.1314\n",
      "Epoch [57/300], Step [13000/27733], Loss: 2.5657\n",
      "Epoch [57/300], Step [13100/27733], Loss: 3.1477\n",
      "Epoch [57/300], Step [13200/27733], Loss: 3.2622\n",
      "Epoch [57/300], Step [13300/27733], Loss: 2.9035\n",
      "Epoch [57/300], Step [13400/27733], Loss: 2.2034\n",
      "Epoch [57/300], Step [13500/27733], Loss: 2.3907\n",
      "Epoch [57/300], Step [13600/27733], Loss: 2.7800\n",
      "Epoch [57/300], Step [13700/27733], Loss: 2.7215\n",
      "Epoch [57/300], Step [13800/27733], Loss: 2.8989\n",
      "Epoch [57/300], Step [13900/27733], Loss: 3.0094\n",
      "Epoch [57/300], Step [14000/27733], Loss: 3.0903\n",
      "Epoch [57/300], Step [14100/27733], Loss: 2.5778\n",
      "Epoch [57/300], Step [14200/27733], Loss: 2.6719\n",
      "Epoch [57/300], Step [14300/27733], Loss: 2.4751\n",
      "Epoch [57/300], Step [14400/27733], Loss: 2.1823\n",
      "Epoch [57/300], Step [14500/27733], Loss: 3.2195\n",
      "Epoch [57/300], Step [14600/27733], Loss: 2.5499\n",
      "Epoch [57/300], Step [14700/27733], Loss: 2.9571\n",
      "Epoch [57/300], Step [14800/27733], Loss: 2.6042\n",
      "Epoch [57/300], Step [14900/27733], Loss: 2.9840\n",
      "Epoch [57/300], Step [15000/27733], Loss: 2.6710\n",
      "Epoch [57/300], Step [15100/27733], Loss: 2.9536\n",
      "Epoch [57/300], Step [15200/27733], Loss: 3.0615\n",
      "Epoch [57/300], Step [15300/27733], Loss: 2.9042\n",
      "Epoch [57/300], Step [15400/27733], Loss: 2.4795\n",
      "Epoch [57/300], Step [15500/27733], Loss: 1.9716\n",
      "Epoch [57/300], Step [15600/27733], Loss: 2.6518\n",
      "Epoch [57/300], Step [15700/27733], Loss: 3.3830\n",
      "Epoch [57/300], Step [15800/27733], Loss: 3.7781\n",
      "Epoch [57/300], Step [15900/27733], Loss: 3.1256\n",
      "Epoch [57/300], Step [16000/27733], Loss: 3.5815\n",
      "Epoch [57/300], Step [16100/27733], Loss: 2.5095\n",
      "Epoch [57/300], Step [16200/27733], Loss: 2.4709\n",
      "Epoch [57/300], Step [16300/27733], Loss: 2.7095\n",
      "Epoch [57/300], Step [16400/27733], Loss: 2.6056\n",
      "Epoch [57/300], Step [16500/27733], Loss: 2.5035\n",
      "Epoch [57/300], Step [16600/27733], Loss: 2.7985\n",
      "Epoch [57/300], Step [16700/27733], Loss: 2.4750\n",
      "Epoch [57/300], Step [16800/27733], Loss: 2.7977\n",
      "Epoch [57/300], Step [16900/27733], Loss: 2.7904\n",
      "Epoch [57/300], Step [17000/27733], Loss: 3.7174\n",
      "Epoch [57/300], Step [17100/27733], Loss: 1.4918\n",
      "Epoch [57/300], Step [17200/27733], Loss: 2.8033\n",
      "Epoch [57/300], Step [17300/27733], Loss: 2.8213\n",
      "Epoch [57/300], Step [17400/27733], Loss: 3.2813\n",
      "Epoch [57/300], Step [17500/27733], Loss: 2.7488\n",
      "Epoch [57/300], Step [17600/27733], Loss: 2.7194\n",
      "Epoch [57/300], Step [17700/27733], Loss: 2.2280\n",
      "Epoch [57/300], Step [17800/27733], Loss: 3.4026\n",
      "Epoch [57/300], Step [17900/27733], Loss: 2.4825\n",
      "Epoch [57/300], Step [18000/27733], Loss: 2.7111\n",
      "Epoch [57/300], Step [18100/27733], Loss: 2.3388\n",
      "Epoch [57/300], Step [18200/27733], Loss: 2.4556\n",
      "Epoch [57/300], Step [18300/27733], Loss: 3.6150\n",
      "Epoch [57/300], Step [18400/27733], Loss: 2.3244\n",
      "Epoch [57/300], Step [18500/27733], Loss: 3.0129\n",
      "Epoch [57/300], Step [18600/27733], Loss: 3.0167\n",
      "Epoch [57/300], Step [18700/27733], Loss: 2.6594\n",
      "Epoch [57/300], Step [18800/27733], Loss: 2.3993\n",
      "Epoch [57/300], Step [18900/27733], Loss: 2.2157\n",
      "Epoch [57/300], Step [19000/27733], Loss: 2.6050\n",
      "Epoch [57/300], Step [19100/27733], Loss: 2.6048\n",
      "Epoch [57/300], Step [19200/27733], Loss: 3.1871\n",
      "Epoch [57/300], Step [19300/27733], Loss: 2.1224\n",
      "Epoch [57/300], Step [19400/27733], Loss: 3.7639\n",
      "Epoch [57/300], Step [19500/27733], Loss: 2.4651\n",
      "Epoch [57/300], Step [19600/27733], Loss: 3.0458\n",
      "Epoch [57/300], Step [19700/27733], Loss: 2.9729\n",
      "Epoch [57/300], Step [19800/27733], Loss: 2.7195\n",
      "Epoch [57/300], Step [19900/27733], Loss: 2.8220\n",
      "Epoch [57/300], Step [20000/27733], Loss: 2.4802\n",
      "Epoch [57/300], Step [20100/27733], Loss: 2.6695\n",
      "Epoch [57/300], Step [20200/27733], Loss: 2.3613\n",
      "Epoch [57/300], Step [20300/27733], Loss: 3.3334\n",
      "Epoch [57/300], Step [20400/27733], Loss: 2.3152\n",
      "Epoch [57/300], Step [20500/27733], Loss: 3.1596\n",
      "Epoch [57/300], Step [20600/27733], Loss: 2.9341\n",
      "Epoch [57/300], Step [20700/27733], Loss: 2.2583\n",
      "Epoch [57/300], Step [20800/27733], Loss: 2.4961\n",
      "Epoch [57/300], Step [20900/27733], Loss: 2.4272\n",
      "Epoch [57/300], Step [21000/27733], Loss: 2.8728\n",
      "Epoch [57/300], Step [21100/27733], Loss: 3.2008\n",
      "Epoch [57/300], Step [21200/27733], Loss: 2.6688\n",
      "Epoch [57/300], Step [21300/27733], Loss: 2.7415\n",
      "Epoch [57/300], Step [21400/27733], Loss: 2.4294\n",
      "Epoch [57/300], Step [21500/27733], Loss: 2.9078\n",
      "Epoch [57/300], Step [21600/27733], Loss: 3.1854\n",
      "Epoch [57/300], Step [21700/27733], Loss: 3.7248\n",
      "Epoch [57/300], Step [21800/27733], Loss: 2.1395\n",
      "Epoch [57/300], Step [21900/27733], Loss: 2.7326\n",
      "Epoch [57/300], Step [22000/27733], Loss: 3.2453\n",
      "Epoch [57/300], Step [22100/27733], Loss: 3.8371\n",
      "Epoch [57/300], Step [22200/27733], Loss: 3.3211\n",
      "Epoch [57/300], Step [22300/27733], Loss: 2.5208\n",
      "Epoch [57/300], Step [22400/27733], Loss: 2.8574\n",
      "Epoch [57/300], Step [22500/27733], Loss: 2.5051\n",
      "Epoch [57/300], Step [22600/27733], Loss: 3.0452\n",
      "Epoch [57/300], Step [22700/27733], Loss: 3.5053\n",
      "Epoch [57/300], Step [22800/27733], Loss: 3.5028\n",
      "Epoch [57/300], Step [22900/27733], Loss: 2.3804\n",
      "Epoch [57/300], Step [23000/27733], Loss: 2.9707\n",
      "Epoch [57/300], Step [23100/27733], Loss: 3.0974\n",
      "Epoch [57/300], Step [23200/27733], Loss: 2.7008\n",
      "Epoch [57/300], Step [23300/27733], Loss: 2.6334\n",
      "Epoch [57/300], Step [23400/27733], Loss: 2.8257\n",
      "Epoch [57/300], Step [23500/27733], Loss: 2.8329\n",
      "Epoch [57/300], Step [23600/27733], Loss: 2.6035\n",
      "Epoch [57/300], Step [23700/27733], Loss: 3.5674\n",
      "Epoch [57/300], Step [23800/27733], Loss: 3.6489\n",
      "Epoch [57/300], Step [23900/27733], Loss: 2.4174\n",
      "Epoch [57/300], Step [24000/27733], Loss: 2.8009\n",
      "Epoch [57/300], Step [24100/27733], Loss: 2.4519\n",
      "Epoch [57/300], Step [24200/27733], Loss: 2.7345\n",
      "Epoch [57/300], Step [24300/27733], Loss: 3.3511\n",
      "Epoch [57/300], Step [24400/27733], Loss: 3.8082\n",
      "Epoch [57/300], Step [24500/27733], Loss: 2.5987\n",
      "Epoch [57/300], Step [24600/27733], Loss: 2.2768\n",
      "Epoch [57/300], Step [24700/27733], Loss: 2.9901\n",
      "Epoch [57/300], Step [24800/27733], Loss: 2.3755\n",
      "Epoch [57/300], Step [24900/27733], Loss: 2.3228\n",
      "Epoch [57/300], Step [25000/27733], Loss: 2.7675\n",
      "Epoch [57/300], Step [25100/27733], Loss: 1.7480\n",
      "Epoch [57/300], Step [25200/27733], Loss: 2.9322\n",
      "Epoch [57/300], Step [25300/27733], Loss: 2.4924\n",
      "Epoch [57/300], Step [25400/27733], Loss: 3.3942\n",
      "Epoch [57/300], Step [25500/27733], Loss: 2.8604\n",
      "Epoch [57/300], Step [25600/27733], Loss: 2.4818\n",
      "Epoch [57/300], Step [25700/27733], Loss: 2.9896\n",
      "Epoch [57/300], Step [25800/27733], Loss: 3.3107\n",
      "Epoch [57/300], Step [25900/27733], Loss: 2.7313\n",
      "Epoch [57/300], Step [26000/27733], Loss: 2.9628\n",
      "Epoch [57/300], Step [26100/27733], Loss: 3.0155\n",
      "Epoch [57/300], Step [26200/27733], Loss: 2.8189\n",
      "Epoch [57/300], Step [26300/27733], Loss: 2.7539\n",
      "Epoch [57/300], Step [26400/27733], Loss: 3.0045\n",
      "Epoch [57/300], Step [26500/27733], Loss: 2.5788\n",
      "Epoch [57/300], Step [26600/27733], Loss: 3.0823\n",
      "Epoch [57/300], Step [26700/27733], Loss: 2.9763\n",
      "Epoch [57/300], Step [26800/27733], Loss: 2.6642\n",
      "Epoch [57/300], Step [26900/27733], Loss: 3.0419\n",
      "Epoch [57/300], Step [27000/27733], Loss: 3.7340\n",
      "Epoch [57/300], Step [27100/27733], Loss: 2.6496\n",
      "Epoch [57/300], Step [27200/27733], Loss: 3.5606\n",
      "Epoch [57/300], Step [27300/27733], Loss: 3.4386\n",
      "Epoch [57/300], Step [27400/27733], Loss: 4.2898\n",
      "Epoch [57/300], Step [27500/27733], Loss: 3.6405\n",
      "Epoch [57/300], Step [27600/27733], Loss: 2.7073\n",
      "Epoch [57/300], Step [27700/27733], Loss: 2.3661\n",
      "Epoch [58/300], Step [100/27733], Loss: 2.3544\n",
      "Epoch [58/300], Step [200/27733], Loss: 1.9238\n",
      "Epoch [58/300], Step [300/27733], Loss: 2.8400\n",
      "Epoch [58/300], Step [400/27733], Loss: 2.9287\n",
      "Epoch [58/300], Step [500/27733], Loss: 2.3782\n",
      "Epoch [58/300], Step [600/27733], Loss: 2.6473\n",
      "Epoch [58/300], Step [700/27733], Loss: 3.0252\n",
      "Epoch [58/300], Step [800/27733], Loss: 3.1225\n",
      "Epoch [58/300], Step [900/27733], Loss: 3.0491\n",
      "Epoch [58/300], Step [1000/27733], Loss: 1.8535\n",
      "Epoch [58/300], Step [1100/27733], Loss: 2.3453\n",
      "Epoch [58/300], Step [1200/27733], Loss: 3.0190\n",
      "Epoch [58/300], Step [1300/27733], Loss: 2.3658\n",
      "Epoch [58/300], Step [1400/27733], Loss: 2.3257\n",
      "Epoch [58/300], Step [1500/27733], Loss: 3.1391\n",
      "Epoch [58/300], Step [1600/27733], Loss: 2.4971\n",
      "Epoch [58/300], Step [1700/27733], Loss: 2.4741\n",
      "Epoch [58/300], Step [1800/27733], Loss: 2.9387\n",
      "Epoch [58/300], Step [1900/27733], Loss: 2.1008\n",
      "Epoch [58/300], Step [2000/27733], Loss: 2.6802\n",
      "Epoch [58/300], Step [2100/27733], Loss: 2.7914\n",
      "Epoch [58/300], Step [2200/27733], Loss: 2.7268\n",
      "Epoch [58/300], Step [2300/27733], Loss: 1.9472\n",
      "Epoch [58/300], Step [2400/27733], Loss: 2.4728\n",
      "Epoch [58/300], Step [2500/27733], Loss: 3.0523\n",
      "Epoch [58/300], Step [2600/27733], Loss: 1.9429\n",
      "Epoch [58/300], Step [2700/27733], Loss: 2.1465\n",
      "Epoch [58/300], Step [2800/27733], Loss: 2.8237\n",
      "Epoch [58/300], Step [2900/27733], Loss: 2.4215\n",
      "Epoch [58/300], Step [3000/27733], Loss: 2.0397\n",
      "Epoch [58/300], Step [3100/27733], Loss: 2.2400\n",
      "Epoch [58/300], Step [3200/27733], Loss: 2.1705\n",
      "Epoch [58/300], Step [3300/27733], Loss: 1.9173\n",
      "Epoch [58/300], Step [3400/27733], Loss: 2.7034\n",
      "Epoch [58/300], Step [3500/27733], Loss: 2.8243\n",
      "Epoch [58/300], Step [3600/27733], Loss: 2.3074\n",
      "Epoch [58/300], Step [3700/27733], Loss: 2.8137\n",
      "Epoch [58/300], Step [3800/27733], Loss: 1.9592\n",
      "Epoch [58/300], Step [3900/27733], Loss: 2.0626\n",
      "Epoch [58/300], Step [4000/27733], Loss: 2.4098\n",
      "Epoch [58/300], Step [4100/27733], Loss: 3.1075\n",
      "Epoch [58/300], Step [4200/27733], Loss: 2.8956\n",
      "Epoch [58/300], Step [4300/27733], Loss: 3.2058\n",
      "Epoch [58/300], Step [4400/27733], Loss: 2.0344\n",
      "Epoch [58/300], Step [4500/27733], Loss: 2.9787\n",
      "Epoch [58/300], Step [4600/27733], Loss: 2.4772\n",
      "Epoch [58/300], Step [4700/27733], Loss: 2.1887\n",
      "Epoch [58/300], Step [4800/27733], Loss: 2.5952\n",
      "Epoch [58/300], Step [4900/27733], Loss: 2.1891\n",
      "Epoch [58/300], Step [5000/27733], Loss: 2.5726\n",
      "Epoch [58/300], Step [5100/27733], Loss: 2.6850\n",
      "Epoch [58/300], Step [5200/27733], Loss: 2.3706\n",
      "Epoch [58/300], Step [5300/27733], Loss: 2.1133\n",
      "Epoch [58/300], Step [5400/27733], Loss: 2.5976\n",
      "Epoch [58/300], Step [5500/27733], Loss: 2.7917\n",
      "Epoch [58/300], Step [5600/27733], Loss: 1.5015\n",
      "Epoch [58/300], Step [5700/27733], Loss: 3.0975\n",
      "Epoch [58/300], Step [5800/27733], Loss: 3.5500\n",
      "Epoch [58/300], Step [5900/27733], Loss: 2.9951\n",
      "Epoch [58/300], Step [6000/27733], Loss: 2.8967\n",
      "Epoch [58/300], Step [6100/27733], Loss: 2.9941\n",
      "Epoch [58/300], Step [6200/27733], Loss: 1.7390\n",
      "Epoch [58/300], Step [6300/27733], Loss: 1.9149\n",
      "Epoch [58/300], Step [6400/27733], Loss: 3.0408\n",
      "Epoch [58/300], Step [6500/27733], Loss: 2.6805\n",
      "Epoch [58/300], Step [6600/27733], Loss: 2.4873\n",
      "Epoch [58/300], Step [6700/27733], Loss: 3.0819\n",
      "Epoch [58/300], Step [6800/27733], Loss: 2.7174\n",
      "Epoch [58/300], Step [6900/27733], Loss: 2.4596\n",
      "Epoch [58/300], Step [7000/27733], Loss: 2.3611\n",
      "Epoch [58/300], Step [7100/27733], Loss: 2.8328\n",
      "Epoch [58/300], Step [7200/27733], Loss: 2.5196\n",
      "Epoch [58/300], Step [7300/27733], Loss: 2.1620\n",
      "Epoch [58/300], Step [7400/27733], Loss: 2.5987\n",
      "Epoch [58/300], Step [7500/27733], Loss: 2.6987\n",
      "Epoch [58/300], Step [7600/27733], Loss: 3.0430\n",
      "Epoch [58/300], Step [7700/27733], Loss: 2.4635\n",
      "Epoch [58/300], Step [7800/27733], Loss: 2.6669\n",
      "Epoch [58/300], Step [7900/27733], Loss: 2.9361\n",
      "Epoch [58/300], Step [8000/27733], Loss: 2.2841\n",
      "Epoch [58/300], Step [8100/27733], Loss: 2.4181\n",
      "Epoch [58/300], Step [8200/27733], Loss: 2.1365\n",
      "Epoch [58/300], Step [8300/27733], Loss: 3.2031\n",
      "Epoch [58/300], Step [8400/27733], Loss: 2.4475\n",
      "Epoch [58/300], Step [8500/27733], Loss: 2.6406\n",
      "Epoch [58/300], Step [8600/27733], Loss: 2.2937\n",
      "Epoch [58/300], Step [8700/27733], Loss: 2.5036\n",
      "Epoch [58/300], Step [8800/27733], Loss: 2.8625\n",
      "Epoch [58/300], Step [8900/27733], Loss: 2.1232\n",
      "Epoch [58/300], Step [9000/27733], Loss: 2.4784\n",
      "Epoch [58/300], Step [9100/27733], Loss: 2.5232\n",
      "Epoch [58/300], Step [9200/27733], Loss: 3.2588\n",
      "Epoch [58/300], Step [9300/27733], Loss: 2.7047\n",
      "Epoch [58/300], Step [9400/27733], Loss: 2.7827\n",
      "Epoch [58/300], Step [9500/27733], Loss: 2.5139\n",
      "Epoch [58/300], Step [9600/27733], Loss: 2.3647\n",
      "Epoch [58/300], Step [9700/27733], Loss: 2.9505\n",
      "Epoch [58/300], Step [9800/27733], Loss: 2.4855\n",
      "Epoch [58/300], Step [9900/27733], Loss: 2.7463\n",
      "Epoch [58/300], Step [10000/27733], Loss: 3.2470\n",
      "Epoch [58/300], Step [10100/27733], Loss: 2.6449\n",
      "Epoch [58/300], Step [10200/27733], Loss: 3.0342\n",
      "Epoch [58/300], Step [10300/27733], Loss: 2.8336\n",
      "Epoch [58/300], Step [10400/27733], Loss: 2.6182\n",
      "Epoch [58/300], Step [10500/27733], Loss: 2.1488\n",
      "Epoch [58/300], Step [10600/27733], Loss: 2.3961\n",
      "Epoch [58/300], Step [10700/27733], Loss: 2.4592\n",
      "Epoch [58/300], Step [10800/27733], Loss: 2.7270\n",
      "Epoch [58/300], Step [10900/27733], Loss: 1.9887\n",
      "Epoch [58/300], Step [11000/27733], Loss: 3.1188\n",
      "Epoch [58/300], Step [11100/27733], Loss: 2.9911\n",
      "Epoch [58/300], Step [11200/27733], Loss: 2.4074\n",
      "Epoch [58/300], Step [11300/27733], Loss: 2.5447\n",
      "Epoch [58/300], Step [11400/27733], Loss: 2.7264\n",
      "Epoch [58/300], Step [11500/27733], Loss: 2.2428\n",
      "Epoch [58/300], Step [11600/27733], Loss: 2.9162\n",
      "Epoch [58/300], Step [11700/27733], Loss: 2.1497\n",
      "Epoch [58/300], Step [11800/27733], Loss: 2.7839\n",
      "Epoch [58/300], Step [11900/27733], Loss: 2.9711\n",
      "Epoch [58/300], Step [12000/27733], Loss: 2.4563\n",
      "Epoch [58/300], Step [12100/27733], Loss: 2.3524\n",
      "Epoch [58/300], Step [12200/27733], Loss: 2.5435\n",
      "Epoch [58/300], Step [12300/27733], Loss: 2.5924\n",
      "Epoch [58/300], Step [12400/27733], Loss: 3.7787\n",
      "Epoch [58/300], Step [12500/27733], Loss: 2.9255\n",
      "Epoch [58/300], Step [12600/27733], Loss: 2.8657\n",
      "Epoch [58/300], Step [12700/27733], Loss: 2.5689\n",
      "Epoch [58/300], Step [12800/27733], Loss: 2.6622\n",
      "Epoch [58/300], Step [12900/27733], Loss: 3.2165\n",
      "Epoch [58/300], Step [13000/27733], Loss: 1.9361\n",
      "Epoch [58/300], Step [13100/27733], Loss: 3.0464\n",
      "Epoch [58/300], Step [13200/27733], Loss: 2.9501\n",
      "Epoch [58/300], Step [13300/27733], Loss: 2.5364\n",
      "Epoch [58/300], Step [13400/27733], Loss: 2.5166\n",
      "Epoch [58/300], Step [13500/27733], Loss: 3.1885\n",
      "Epoch [58/300], Step [13600/27733], Loss: 2.6028\n",
      "Epoch [58/300], Step [13700/27733], Loss: 2.5725\n",
      "Epoch [58/300], Step [13800/27733], Loss: 2.8974\n",
      "Epoch [58/300], Step [13900/27733], Loss: 3.6268\n",
      "Epoch [58/300], Step [14000/27733], Loss: 2.5751\n",
      "Epoch [58/300], Step [14100/27733], Loss: 1.9340\n",
      "Epoch [58/300], Step [14200/27733], Loss: 1.9783\n",
      "Epoch [58/300], Step [14300/27733], Loss: 2.3649\n",
      "Epoch [58/300], Step [14400/27733], Loss: 2.7178\n",
      "Epoch [58/300], Step [14500/27733], Loss: 2.2735\n",
      "Epoch [58/300], Step [14600/27733], Loss: 2.4506\n",
      "Epoch [58/300], Step [14700/27733], Loss: 2.3991\n",
      "Epoch [58/300], Step [14800/27733], Loss: 2.7836\n",
      "Epoch [58/300], Step [14900/27733], Loss: 3.1028\n",
      "Epoch [58/300], Step [15000/27733], Loss: 3.0582\n",
      "Epoch [58/300], Step [15100/27733], Loss: 2.1964\n",
      "Epoch [58/300], Step [15200/27733], Loss: 2.0689\n",
      "Epoch [58/300], Step [15300/27733], Loss: 2.5290\n",
      "Epoch [58/300], Step [15400/27733], Loss: 2.7868\n",
      "Epoch [58/300], Step [15500/27733], Loss: 2.4774\n",
      "Epoch [58/300], Step [15600/27733], Loss: 2.4652\n",
      "Epoch [58/300], Step [15700/27733], Loss: 2.5158\n",
      "Epoch [58/300], Step [15800/27733], Loss: 2.2856\n",
      "Epoch [58/300], Step [15900/27733], Loss: 2.4039\n",
      "Epoch [58/300], Step [16000/27733], Loss: 2.3172\n",
      "Epoch [58/300], Step [16100/27733], Loss: 2.0722\n",
      "Epoch [58/300], Step [16200/27733], Loss: 2.7161\n",
      "Epoch [58/300], Step [16300/27733], Loss: 1.5991\n",
      "Epoch [58/300], Step [16400/27733], Loss: 3.0641\n",
      "Epoch [58/300], Step [16500/27733], Loss: 3.1194\n",
      "Epoch [58/300], Step [16600/27733], Loss: 2.1881\n",
      "Epoch [58/300], Step [16700/27733], Loss: 2.9539\n",
      "Epoch [58/300], Step [16800/27733], Loss: 2.8769\n",
      "Epoch [58/300], Step [16900/27733], Loss: 2.5667\n",
      "Epoch [58/300], Step [17000/27733], Loss: 1.9578\n",
      "Epoch [58/300], Step [17100/27733], Loss: 3.3510\n",
      "Epoch [58/300], Step [17200/27733], Loss: 2.5543\n",
      "Epoch [58/300], Step [17300/27733], Loss: 2.6248\n",
      "Epoch [58/300], Step [17400/27733], Loss: 2.6163\n",
      "Epoch [58/300], Step [17500/27733], Loss: 3.2254\n",
      "Epoch [58/300], Step [17600/27733], Loss: 3.5643\n",
      "Epoch [58/300], Step [17700/27733], Loss: 2.8831\n",
      "Epoch [58/300], Step [17800/27733], Loss: 3.1683\n",
      "Epoch [58/300], Step [17900/27733], Loss: 2.9443\n",
      "Epoch [58/300], Step [18000/27733], Loss: 3.0254\n",
      "Epoch [58/300], Step [18100/27733], Loss: 2.5041\n",
      "Epoch [58/300], Step [18200/27733], Loss: 2.3594\n",
      "Epoch [58/300], Step [18300/27733], Loss: 2.7741\n",
      "Epoch [58/300], Step [18400/27733], Loss: 3.4008\n",
      "Epoch [58/300], Step [18500/27733], Loss: 3.2012\n",
      "Epoch [58/300], Step [18600/27733], Loss: 4.2389\n",
      "Epoch [58/300], Step [18700/27733], Loss: 2.6189\n",
      "Epoch [58/300], Step [18800/27733], Loss: 2.6337\n",
      "Epoch [58/300], Step [18900/27733], Loss: 2.5304\n",
      "Epoch [58/300], Step [19000/27733], Loss: 2.6379\n",
      "Epoch [58/300], Step [19100/27733], Loss: 2.7317\n",
      "Epoch [58/300], Step [19200/27733], Loss: 2.8098\n",
      "Epoch [58/300], Step [19300/27733], Loss: 2.6681\n",
      "Epoch [58/300], Step [19400/27733], Loss: 3.2570\n",
      "Epoch [58/300], Step [19500/27733], Loss: 3.6152\n",
      "Epoch [58/300], Step [19600/27733], Loss: 3.0207\n",
      "Epoch [58/300], Step [19700/27733], Loss: 2.9479\n",
      "Epoch [58/300], Step [19800/27733], Loss: 2.9160\n",
      "Epoch [58/300], Step [19900/27733], Loss: 3.8882\n",
      "Epoch [58/300], Step [20000/27733], Loss: 2.3243\n",
      "Epoch [58/300], Step [20100/27733], Loss: 3.2449\n",
      "Epoch [58/300], Step [20200/27733], Loss: 3.3865\n",
      "Epoch [58/300], Step [20300/27733], Loss: 2.1337\n",
      "Epoch [58/300], Step [20400/27733], Loss: 1.8923\n",
      "Epoch [58/300], Step [20500/27733], Loss: 3.1476\n",
      "Epoch [58/300], Step [20600/27733], Loss: 3.3542\n",
      "Epoch [58/300], Step [20700/27733], Loss: 2.8789\n",
      "Epoch [58/300], Step [20800/27733], Loss: 2.5134\n",
      "Epoch [58/300], Step [20900/27733], Loss: 2.9658\n",
      "Epoch [58/300], Step [21000/27733], Loss: 2.7728\n",
      "Epoch [58/300], Step [21100/27733], Loss: 3.0220\n",
      "Epoch [58/300], Step [21200/27733], Loss: 2.4989\n",
      "Epoch [58/300], Step [21300/27733], Loss: 2.6083\n",
      "Epoch [58/300], Step [21400/27733], Loss: 2.4511\n",
      "Epoch [58/300], Step [21500/27733], Loss: 2.6542\n",
      "Epoch [58/300], Step [21600/27733], Loss: 2.3338\n",
      "Epoch [58/300], Step [21700/27733], Loss: 2.2799\n",
      "Epoch [58/300], Step [21800/27733], Loss: 2.6932\n",
      "Epoch [58/300], Step [21900/27733], Loss: 3.2183\n",
      "Epoch [58/300], Step [22000/27733], Loss: 2.7915\n",
      "Epoch [58/300], Step [22100/27733], Loss: 3.0085\n",
      "Epoch [58/300], Step [22200/27733], Loss: 2.8697\n",
      "Epoch [58/300], Step [22300/27733], Loss: 2.5528\n",
      "Epoch [58/300], Step [22400/27733], Loss: 3.1891\n",
      "Epoch [58/300], Step [22500/27733], Loss: 3.9048\n",
      "Epoch [58/300], Step [22600/27733], Loss: 3.6099\n",
      "Epoch [58/300], Step [22700/27733], Loss: 2.8111\n",
      "Epoch [58/300], Step [22800/27733], Loss: 2.4379\n",
      "Epoch [58/300], Step [22900/27733], Loss: 2.8625\n",
      "Epoch [58/300], Step [23000/27733], Loss: 3.0687\n",
      "Epoch [58/300], Step [23100/27733], Loss: 2.4171\n",
      "Epoch [58/300], Step [23200/27733], Loss: 3.9923\n",
      "Epoch [58/300], Step [23300/27733], Loss: 3.3140\n",
      "Epoch [58/300], Step [23400/27733], Loss: 2.5478\n",
      "Epoch [58/300], Step [23500/27733], Loss: 2.5436\n",
      "Epoch [58/300], Step [23600/27733], Loss: 3.1542\n",
      "Epoch [58/300], Step [23700/27733], Loss: 3.1747\n",
      "Epoch [58/300], Step [23800/27733], Loss: 3.1402\n",
      "Epoch [58/300], Step [23900/27733], Loss: 3.0828\n",
      "Epoch [58/300], Step [24000/27733], Loss: 2.7718\n",
      "Epoch [58/300], Step [24100/27733], Loss: 2.6067\n",
      "Epoch [58/300], Step [24200/27733], Loss: 1.9653\n",
      "Epoch [58/300], Step [24300/27733], Loss: 3.2733\n",
      "Epoch [58/300], Step [24400/27733], Loss: 3.5250\n",
      "Epoch [58/300], Step [24500/27733], Loss: 2.9908\n",
      "Epoch [58/300], Step [24600/27733], Loss: 3.3631\n",
      "Epoch [58/300], Step [24700/27733], Loss: 2.7530\n",
      "Epoch [58/300], Step [24800/27733], Loss: 2.8264\n",
      "Epoch [58/300], Step [24900/27733], Loss: 2.7030\n",
      "Epoch [58/300], Step [25000/27733], Loss: 2.7118\n",
      "Epoch [58/300], Step [25100/27733], Loss: 2.7269\n",
      "Epoch [58/300], Step [25200/27733], Loss: 2.7603\n",
      "Epoch [58/300], Step [25300/27733], Loss: 3.9558\n",
      "Epoch [58/300], Step [25400/27733], Loss: 2.7490\n",
      "Epoch [58/300], Step [25500/27733], Loss: 2.1450\n",
      "Epoch [58/300], Step [25600/27733], Loss: 2.9049\n",
      "Epoch [58/300], Step [25700/27733], Loss: 2.7128\n",
      "Epoch [58/300], Step [25800/27733], Loss: 2.5274\n",
      "Epoch [58/300], Step [25900/27733], Loss: 3.7406\n",
      "Epoch [58/300], Step [26000/27733], Loss: 3.0299\n",
      "Epoch [58/300], Step [26100/27733], Loss: 1.8962\n",
      "Epoch [58/300], Step [26200/27733], Loss: 2.1520\n",
      "Epoch [58/300], Step [26300/27733], Loss: 2.6693\n",
      "Epoch [58/300], Step [26400/27733], Loss: 3.1432\n",
      "Epoch [58/300], Step [26500/27733], Loss: 2.2711\n",
      "Epoch [58/300], Step [26600/27733], Loss: 3.1395\n",
      "Epoch [58/300], Step [26700/27733], Loss: 3.1229\n",
      "Epoch [58/300], Step [26800/27733], Loss: 3.7787\n",
      "Epoch [58/300], Step [26900/27733], Loss: 2.3458\n",
      "Epoch [58/300], Step [27000/27733], Loss: 2.4611\n",
      "Epoch [58/300], Step [27100/27733], Loss: 2.7288\n",
      "Epoch [58/300], Step [27200/27733], Loss: 2.8552\n",
      "Epoch [58/300], Step [27300/27733], Loss: 3.0927\n",
      "Epoch [58/300], Step [27400/27733], Loss: 2.4734\n",
      "Epoch [58/300], Step [27500/27733], Loss: 2.4209\n",
      "Epoch [58/300], Step [27600/27733], Loss: 3.1542\n",
      "Epoch [58/300], Step [27700/27733], Loss: 3.0310\n",
      "Epoch [59/300], Step [100/27733], Loss: 2.3473\n",
      "Epoch [59/300], Step [200/27733], Loss: 1.8346\n",
      "Epoch [59/300], Step [300/27733], Loss: 3.5871\n",
      "Epoch [59/300], Step [400/27733], Loss: 2.1433\n",
      "Epoch [59/300], Step [500/27733], Loss: 1.8846\n",
      "Epoch [59/300], Step [600/27733], Loss: 2.5369\n",
      "Epoch [59/300], Step [700/27733], Loss: 2.0438\n",
      "Epoch [59/300], Step [800/27733], Loss: 2.5353\n",
      "Epoch [59/300], Step [900/27733], Loss: 2.1840\n",
      "Epoch [59/300], Step [1000/27733], Loss: 2.4168\n",
      "Epoch [59/300], Step [1100/27733], Loss: 2.4768\n",
      "Epoch [59/300], Step [1200/27733], Loss: 1.6833\n",
      "Epoch [59/300], Step [1300/27733], Loss: 2.2936\n",
      "Epoch [59/300], Step [1400/27733], Loss: 2.0063\n",
      "Epoch [59/300], Step [1500/27733], Loss: 2.2861\n",
      "Epoch [59/300], Step [1600/27733], Loss: 1.8921\n",
      "Epoch [59/300], Step [1700/27733], Loss: 2.3360\n",
      "Epoch [59/300], Step [1800/27733], Loss: 2.9545\n",
      "Epoch [59/300], Step [1900/27733], Loss: 1.5210\n",
      "Epoch [59/300], Step [2000/27733], Loss: 2.6786\n",
      "Epoch [59/300], Step [2100/27733], Loss: 3.5029\n",
      "Epoch [59/300], Step [2200/27733], Loss: 2.5558\n",
      "Epoch [59/300], Step [2300/27733], Loss: 2.9268\n",
      "Epoch [59/300], Step [2400/27733], Loss: 2.1226\n",
      "Epoch [59/300], Step [2500/27733], Loss: 2.3155\n",
      "Epoch [59/300], Step [2600/27733], Loss: 2.6465\n",
      "Epoch [59/300], Step [2700/27733], Loss: 2.3925\n",
      "Epoch [59/300], Step [2800/27733], Loss: 2.2441\n",
      "Epoch [59/300], Step [2900/27733], Loss: 2.8974\n",
      "Epoch [59/300], Step [3000/27733], Loss: 1.5560\n",
      "Epoch [59/300], Step [3100/27733], Loss: 2.6226\n",
      "Epoch [59/300], Step [3200/27733], Loss: 2.0886\n",
      "Epoch [59/300], Step [3300/27733], Loss: 2.7708\n",
      "Epoch [59/300], Step [3400/27733], Loss: 3.1680\n",
      "Epoch [59/300], Step [3500/27733], Loss: 2.6509\n",
      "Epoch [59/300], Step [3600/27733], Loss: 2.1264\n",
      "Epoch [59/300], Step [3700/27733], Loss: 2.5550\n",
      "Epoch [59/300], Step [3800/27733], Loss: 2.2415\n",
      "Epoch [59/300], Step [3900/27733], Loss: 2.9549\n",
      "Epoch [59/300], Step [4000/27733], Loss: 1.7642\n",
      "Epoch [59/300], Step [4100/27733], Loss: 2.7030\n",
      "Epoch [59/300], Step [4200/27733], Loss: 3.0357\n",
      "Epoch [59/300], Step [4300/27733], Loss: 2.1680\n",
      "Epoch [59/300], Step [4400/27733], Loss: 1.8769\n",
      "Epoch [59/300], Step [4500/27733], Loss: 2.3749\n",
      "Epoch [59/300], Step [4600/27733], Loss: 2.7080\n",
      "Epoch [59/300], Step [4700/27733], Loss: 2.9787\n",
      "Epoch [59/300], Step [4800/27733], Loss: 2.4347\n",
      "Epoch [59/300], Step [4900/27733], Loss: 2.8429\n",
      "Epoch [59/300], Step [5000/27733], Loss: 2.8399\n",
      "Epoch [59/300], Step [5100/27733], Loss: 2.3706\n",
      "Epoch [59/300], Step [5200/27733], Loss: 2.0790\n",
      "Epoch [59/300], Step [5300/27733], Loss: 2.2047\n",
      "Epoch [59/300], Step [5400/27733], Loss: 2.8249\n",
      "Epoch [59/300], Step [5500/27733], Loss: 2.1930\n",
      "Epoch [59/300], Step [5600/27733], Loss: 1.9048\n",
      "Epoch [59/300], Step [5700/27733], Loss: 3.0541\n",
      "Epoch [59/300], Step [5800/27733], Loss: 2.0003\n",
      "Epoch [59/300], Step [5900/27733], Loss: 2.2852\n",
      "Epoch [59/300], Step [6000/27733], Loss: 3.1711\n",
      "Epoch [59/300], Step [6100/27733], Loss: 1.7813\n",
      "Epoch [59/300], Step [6200/27733], Loss: 2.5057\n",
      "Epoch [59/300], Step [6300/27733], Loss: 3.2269\n",
      "Epoch [59/300], Step [6400/27733], Loss: 2.8842\n",
      "Epoch [59/300], Step [6500/27733], Loss: 1.9699\n",
      "Epoch [59/300], Step [6600/27733], Loss: 2.4042\n",
      "Epoch [59/300], Step [6700/27733], Loss: 3.0428\n",
      "Epoch [59/300], Step [6800/27733], Loss: 2.7044\n",
      "Epoch [59/300], Step [6900/27733], Loss: 2.2743\n",
      "Epoch [59/300], Step [7000/27733], Loss: 3.0940\n",
      "Epoch [59/300], Step [7100/27733], Loss: 1.7008\n",
      "Epoch [59/300], Step [7200/27733], Loss: 3.3999\n",
      "Epoch [59/300], Step [7300/27733], Loss: 3.0908\n",
      "Epoch [59/300], Step [7400/27733], Loss: 2.3971\n",
      "Epoch [59/300], Step [7500/27733], Loss: 3.0593\n",
      "Epoch [59/300], Step [7600/27733], Loss: 3.1499\n",
      "Epoch [59/300], Step [7700/27733], Loss: 1.9827\n",
      "Epoch [59/300], Step [7800/27733], Loss: 3.4340\n",
      "Epoch [59/300], Step [7900/27733], Loss: 2.3722\n",
      "Epoch [59/300], Step [8000/27733], Loss: 3.8290\n",
      "Epoch [59/300], Step [8100/27733], Loss: 2.3205\n",
      "Epoch [59/300], Step [8200/27733], Loss: 2.2867\n",
      "Epoch [59/300], Step [8300/27733], Loss: 2.6847\n",
      "Epoch [59/300], Step [8400/27733], Loss: 3.6005\n",
      "Epoch [59/300], Step [8500/27733], Loss: 2.0138\n",
      "Epoch [59/300], Step [8600/27733], Loss: 2.7135\n",
      "Epoch [59/300], Step [8700/27733], Loss: 2.8774\n",
      "Epoch [59/300], Step [8800/27733], Loss: 2.4189\n",
      "Epoch [59/300], Step [8900/27733], Loss: 3.1429\n",
      "Epoch [59/300], Step [9000/27733], Loss: 2.8997\n",
      "Epoch [59/300], Step [9100/27733], Loss: 2.7801\n",
      "Epoch [59/300], Step [9200/27733], Loss: 2.1173\n",
      "Epoch [59/300], Step [9300/27733], Loss: 3.2201\n",
      "Epoch [59/300], Step [9400/27733], Loss: 2.0194\n",
      "Epoch [59/300], Step [9500/27733], Loss: 2.3565\n",
      "Epoch [59/300], Step [9600/27733], Loss: 1.8978\n",
      "Epoch [59/300], Step [9700/27733], Loss: 2.3835\n",
      "Epoch [59/300], Step [9800/27733], Loss: 2.0948\n",
      "Epoch [59/300], Step [9900/27733], Loss: 2.1586\n",
      "Epoch [59/300], Step [10000/27733], Loss: 2.7048\n",
      "Epoch [59/300], Step [10100/27733], Loss: 2.2314\n",
      "Epoch [59/300], Step [10200/27733], Loss: 3.1073\n",
      "Epoch [59/300], Step [10300/27733], Loss: 2.6743\n",
      "Epoch [59/300], Step [10400/27733], Loss: 2.4042\n",
      "Epoch [59/300], Step [10500/27733], Loss: 2.0409\n",
      "Epoch [59/300], Step [10600/27733], Loss: 3.3234\n",
      "Epoch [59/300], Step [10700/27733], Loss: 2.4431\n",
      "Epoch [59/300], Step [10800/27733], Loss: 4.2095\n",
      "Epoch [59/300], Step [10900/27733], Loss: 2.6236\n",
      "Epoch [59/300], Step [11000/27733], Loss: 2.3233\n",
      "Epoch [59/300], Step [11100/27733], Loss: 3.0575\n",
      "Epoch [59/300], Step [11200/27733], Loss: 2.8692\n",
      "Epoch [59/300], Step [11300/27733], Loss: 1.6645\n",
      "Epoch [59/300], Step [11400/27733], Loss: 3.2207\n",
      "Epoch [59/300], Step [11500/27733], Loss: 2.6614\n",
      "Epoch [59/300], Step [11600/27733], Loss: 3.0262\n",
      "Epoch [59/300], Step [11700/27733], Loss: 2.5491\n",
      "Epoch [59/300], Step [11800/27733], Loss: 2.5956\n",
      "Epoch [59/300], Step [11900/27733], Loss: 2.1207\n",
      "Epoch [59/300], Step [12000/27733], Loss: 2.7000\n",
      "Epoch [59/300], Step [12100/27733], Loss: 2.2448\n",
      "Epoch [59/300], Step [12200/27733], Loss: 2.4774\n",
      "Epoch [59/300], Step [12300/27733], Loss: 3.1006\n",
      "Epoch [59/300], Step [12400/27733], Loss: 2.3728\n",
      "Epoch [59/300], Step [12500/27733], Loss: 2.0306\n",
      "Epoch [59/300], Step [12600/27733], Loss: 2.0292\n",
      "Epoch [59/300], Step [12700/27733], Loss: 2.4628\n",
      "Epoch [59/300], Step [12800/27733], Loss: 2.4727\n",
      "Epoch [59/300], Step [12900/27733], Loss: 2.6981\n",
      "Epoch [59/300], Step [13000/27733], Loss: 2.6442\n",
      "Epoch [59/300], Step [13100/27733], Loss: 2.7933\n",
      "Epoch [59/300], Step [13200/27733], Loss: 2.4952\n",
      "Epoch [59/300], Step [13300/27733], Loss: 3.7492\n",
      "Epoch [59/300], Step [13400/27733], Loss: 2.8739\n",
      "Epoch [59/300], Step [13500/27733], Loss: 2.6029\n",
      "Epoch [59/300], Step [13600/27733], Loss: 3.1323\n",
      "Epoch [59/300], Step [13700/27733], Loss: 2.4569\n",
      "Epoch [59/300], Step [13800/27733], Loss: 2.9317\n",
      "Epoch [59/300], Step [13900/27733], Loss: 2.0742\n",
      "Epoch [59/300], Step [14000/27733], Loss: 2.9457\n",
      "Epoch [59/300], Step [14100/27733], Loss: 1.6994\n",
      "Epoch [59/300], Step [14200/27733], Loss: 2.4389\n",
      "Epoch [59/300], Step [14300/27733], Loss: 3.0994\n",
      "Epoch [59/300], Step [14400/27733], Loss: 2.6241\n",
      "Epoch [59/300], Step [14500/27733], Loss: 3.9656\n",
      "Epoch [59/300], Step [14600/27733], Loss: 2.3913\n",
      "Epoch [59/300], Step [14700/27733], Loss: 2.7985\n",
      "Epoch [59/300], Step [14800/27733], Loss: 2.8517\n",
      "Epoch [59/300], Step [14900/27733], Loss: 2.9064\n",
      "Epoch [59/300], Step [15000/27733], Loss: 2.5399\n",
      "Epoch [59/300], Step [15100/27733], Loss: 2.8268\n",
      "Epoch [59/300], Step [15200/27733], Loss: 3.0795\n",
      "Epoch [59/300], Step [15300/27733], Loss: 2.7346\n",
      "Epoch [59/300], Step [15400/27733], Loss: 2.1580\n",
      "Epoch [59/300], Step [15500/27733], Loss: 2.8215\n",
      "Epoch [59/300], Step [15600/27733], Loss: 2.3157\n",
      "Epoch [59/300], Step [15700/27733], Loss: 3.6422\n",
      "Epoch [59/300], Step [15800/27733], Loss: 2.2864\n",
      "Epoch [59/300], Step [15900/27733], Loss: 2.4001\n",
      "Epoch [59/300], Step [16000/27733], Loss: 3.0396\n",
      "Epoch [59/300], Step [16100/27733], Loss: 3.5993\n",
      "Epoch [59/300], Step [16200/27733], Loss: 2.8559\n",
      "Epoch [59/300], Step [16300/27733], Loss: 2.1196\n",
      "Epoch [59/300], Step [16400/27733], Loss: 3.1369\n",
      "Epoch [59/300], Step [16500/27733], Loss: 2.7443\n",
      "Epoch [59/300], Step [16600/27733], Loss: 3.3892\n",
      "Epoch [59/300], Step [16700/27733], Loss: 2.3639\n",
      "Epoch [59/300], Step [16800/27733], Loss: 2.7410\n",
      "Epoch [59/300], Step [16900/27733], Loss: 2.9047\n",
      "Epoch [59/300], Step [17000/27733], Loss: 2.2031\n",
      "Epoch [59/300], Step [17100/27733], Loss: 3.4977\n",
      "Epoch [59/300], Step [17200/27733], Loss: 3.1251\n",
      "Epoch [59/300], Step [17300/27733], Loss: 2.7500\n",
      "Epoch [59/300], Step [17400/27733], Loss: 3.0365\n",
      "Epoch [59/300], Step [17500/27733], Loss: 2.9845\n",
      "Epoch [59/300], Step [17600/27733], Loss: 2.8154\n",
      "Epoch [59/300], Step [17700/27733], Loss: 2.6727\n",
      "Epoch [59/300], Step [17800/27733], Loss: 3.0567\n",
      "Epoch [59/300], Step [17900/27733], Loss: 2.4497\n",
      "Epoch [59/300], Step [18000/27733], Loss: 2.9173\n",
      "Epoch [59/300], Step [18100/27733], Loss: 3.0132\n",
      "Epoch [59/300], Step [18200/27733], Loss: 3.1465\n",
      "Epoch [59/300], Step [18300/27733], Loss: 2.8488\n",
      "Epoch [59/300], Step [18400/27733], Loss: 2.9962\n",
      "Epoch [59/300], Step [18500/27733], Loss: 3.0651\n",
      "Epoch [59/300], Step [18600/27733], Loss: 2.1654\n",
      "Epoch [59/300], Step [18700/27733], Loss: 3.2685\n",
      "Epoch [59/300], Step [18800/27733], Loss: 2.8650\n",
      "Epoch [59/300], Step [18900/27733], Loss: 2.5951\n",
      "Epoch [59/300], Step [19000/27733], Loss: 2.7235\n",
      "Epoch [59/300], Step [19100/27733], Loss: 3.3769\n",
      "Epoch [59/300], Step [19200/27733], Loss: 2.7564\n",
      "Epoch [59/300], Step [19300/27733], Loss: 3.0378\n",
      "Epoch [59/300], Step [19400/27733], Loss: 2.4999\n",
      "Epoch [59/300], Step [19500/27733], Loss: 2.5632\n",
      "Epoch [59/300], Step [19600/27733], Loss: 1.9575\n",
      "Epoch [59/300], Step [19700/27733], Loss: 3.3065\n",
      "Epoch [59/300], Step [19800/27733], Loss: 3.3838\n",
      "Epoch [59/300], Step [19900/27733], Loss: 2.5976\n",
      "Epoch [59/300], Step [20000/27733], Loss: 1.8373\n",
      "Epoch [59/300], Step [20100/27733], Loss: 2.3983\n",
      "Epoch [59/300], Step [20200/27733], Loss: 2.6198\n",
      "Epoch [59/300], Step [20300/27733], Loss: 3.2879\n",
      "Epoch [59/300], Step [20400/27733], Loss: 2.7036\n",
      "Epoch [59/300], Step [20500/27733], Loss: 2.7103\n",
      "Epoch [59/300], Step [20600/27733], Loss: 2.3617\n",
      "Epoch [59/300], Step [20700/27733], Loss: 3.4327\n",
      "Epoch [59/300], Step [20800/27733], Loss: 2.4640\n",
      "Epoch [59/300], Step [20900/27733], Loss: 2.8371\n",
      "Epoch [59/300], Step [21000/27733], Loss: 2.1040\n",
      "Epoch [59/300], Step [21100/27733], Loss: 3.2435\n",
      "Epoch [59/300], Step [21200/27733], Loss: 3.1779\n",
      "Epoch [59/300], Step [21300/27733], Loss: 3.1270\n",
      "Epoch [59/300], Step [21400/27733], Loss: 3.3102\n",
      "Epoch [59/300], Step [21500/27733], Loss: 3.0734\n",
      "Epoch [59/300], Step [21600/27733], Loss: 2.8026\n",
      "Epoch [59/300], Step [21700/27733], Loss: 3.4968\n",
      "Epoch [59/300], Step [21800/27733], Loss: 3.5203\n",
      "Epoch [59/300], Step [21900/27733], Loss: 2.2834\n",
      "Epoch [59/300], Step [22000/27733], Loss: 3.0095\n",
      "Epoch [59/300], Step [22100/27733], Loss: 2.5235\n",
      "Epoch [59/300], Step [22200/27733], Loss: 3.0784\n",
      "Epoch [59/300], Step [22300/27733], Loss: 2.9313\n",
      "Epoch [59/300], Step [22400/27733], Loss: 3.2450\n",
      "Epoch [59/300], Step [22500/27733], Loss: 3.1351\n",
      "Epoch [59/300], Step [22600/27733], Loss: 3.5614\n",
      "Epoch [59/300], Step [22700/27733], Loss: 3.0629\n",
      "Epoch [59/300], Step [22800/27733], Loss: 2.9133\n",
      "Epoch [59/300], Step [22900/27733], Loss: 2.2474\n",
      "Epoch [59/300], Step [23000/27733], Loss: 2.8906\n",
      "Epoch [59/300], Step [23100/27733], Loss: 3.4074\n",
      "Epoch [59/300], Step [23200/27733], Loss: 3.0373\n",
      "Epoch [59/300], Step [23300/27733], Loss: 3.9781\n",
      "Epoch [59/300], Step [23400/27733], Loss: 2.2824\n",
      "Epoch [59/300], Step [23500/27733], Loss: 3.4395\n",
      "Epoch [59/300], Step [23600/27733], Loss: 3.1355\n",
      "Epoch [59/300], Step [23700/27733], Loss: 3.3035\n",
      "Epoch [59/300], Step [23800/27733], Loss: 2.8328\n",
      "Epoch [59/300], Step [23900/27733], Loss: 2.8259\n",
      "Epoch [59/300], Step [24000/27733], Loss: 3.3373\n",
      "Epoch [59/300], Step [24100/27733], Loss: 2.5320\n",
      "Epoch [59/300], Step [24200/27733], Loss: 2.3934\n",
      "Epoch [59/300], Step [24300/27733], Loss: 2.2957\n",
      "Epoch [59/300], Step [24400/27733], Loss: 3.4179\n",
      "Epoch [59/300], Step [24500/27733], Loss: 3.1464\n",
      "Epoch [59/300], Step [24600/27733], Loss: 2.9607\n",
      "Epoch [59/300], Step [24700/27733], Loss: 2.3085\n",
      "Epoch [59/300], Step [24800/27733], Loss: 2.6553\n",
      "Epoch [59/300], Step [24900/27733], Loss: 3.4821\n",
      "Epoch [59/300], Step [25000/27733], Loss: 2.5035\n",
      "Epoch [59/300], Step [25100/27733], Loss: 3.4245\n",
      "Epoch [59/300], Step [25200/27733], Loss: 2.5308\n",
      "Epoch [59/300], Step [25300/27733], Loss: 3.0494\n",
      "Epoch [59/300], Step [25400/27733], Loss: 2.8597\n",
      "Epoch [59/300], Step [25500/27733], Loss: 2.5697\n",
      "Epoch [59/300], Step [25600/27733], Loss: 2.2239\n",
      "Epoch [59/300], Step [25700/27733], Loss: 3.1845\n",
      "Epoch [59/300], Step [25800/27733], Loss: 3.1718\n",
      "Epoch [59/300], Step [25900/27733], Loss: 2.8257\n",
      "Epoch [59/300], Step [26000/27733], Loss: 1.9841\n",
      "Epoch [59/300], Step [26100/27733], Loss: 3.0402\n",
      "Epoch [59/300], Step [26200/27733], Loss: 2.5386\n",
      "Epoch [59/300], Step [26300/27733], Loss: 3.1055\n",
      "Epoch [59/300], Step [26400/27733], Loss: 3.3903\n",
      "Epoch [59/300], Step [26500/27733], Loss: 3.9950\n",
      "Epoch [59/300], Step [26600/27733], Loss: 2.8702\n",
      "Epoch [59/300], Step [26700/27733], Loss: 2.8079\n",
      "Epoch [59/300], Step [26800/27733], Loss: 3.0865\n",
      "Epoch [59/300], Step [26900/27733], Loss: 2.9614\n",
      "Epoch [59/300], Step [27000/27733], Loss: 3.0220\n",
      "Epoch [59/300], Step [27100/27733], Loss: 3.1392\n",
      "Epoch [59/300], Step [27200/27733], Loss: 3.3654\n",
      "Epoch [59/300], Step [27300/27733], Loss: 3.1039\n",
      "Epoch [59/300], Step [27400/27733], Loss: 3.1606\n",
      "Epoch [59/300], Step [27500/27733], Loss: 2.5890\n",
      "Epoch [59/300], Step [27600/27733], Loss: 3.1775\n",
      "Epoch [59/300], Step [27700/27733], Loss: 3.2700\n",
      "Epoch [60/300], Step [100/27733], Loss: 2.2021\n",
      "Epoch [60/300], Step [200/27733], Loss: 1.8442\n",
      "Epoch [60/300], Step [300/27733], Loss: 1.5319\n",
      "Epoch [60/300], Step [400/27733], Loss: 2.4897\n",
      "Epoch [60/300], Step [500/27733], Loss: 1.8895\n",
      "Epoch [60/300], Step [600/27733], Loss: 2.0527\n",
      "Epoch [60/300], Step [700/27733], Loss: 2.4697\n",
      "Epoch [60/300], Step [800/27733], Loss: 2.5371\n",
      "Epoch [60/300], Step [900/27733], Loss: 1.8198\n",
      "Epoch [60/300], Step [1000/27733], Loss: 2.6821\n",
      "Epoch [60/300], Step [1100/27733], Loss: 2.1712\n",
      "Epoch [60/300], Step [1200/27733], Loss: 3.5174\n",
      "Epoch [60/300], Step [1300/27733], Loss: 2.5680\n",
      "Epoch [60/300], Step [1400/27733], Loss: 2.6479\n",
      "Epoch [60/300], Step [1500/27733], Loss: 2.3374\n",
      "Epoch [60/300], Step [1600/27733], Loss: 2.8919\n",
      "Epoch [60/300], Step [1700/27733], Loss: 1.9913\n",
      "Epoch [60/300], Step [1800/27733], Loss: 2.2218\n",
      "Epoch [60/300], Step [1900/27733], Loss: 2.0209\n",
      "Epoch [60/300], Step [2000/27733], Loss: 2.4935\n",
      "Epoch [60/300], Step [2100/27733], Loss: 3.2918\n",
      "Epoch [60/300], Step [2200/27733], Loss: 3.4297\n",
      "Epoch [60/300], Step [2300/27733], Loss: 2.4297\n",
      "Epoch [60/300], Step [2400/27733], Loss: 2.0702\n",
      "Epoch [60/300], Step [2500/27733], Loss: 1.8005\n",
      "Epoch [60/300], Step [2600/27733], Loss: 2.5810\n",
      "Epoch [60/300], Step [2700/27733], Loss: 2.7055\n",
      "Epoch [60/300], Step [2800/27733], Loss: 2.3219\n",
      "Epoch [60/300], Step [2900/27733], Loss: 2.1594\n",
      "Epoch [60/300], Step [3000/27733], Loss: 2.6118\n",
      "Epoch [60/300], Step [3100/27733], Loss: 1.6378\n",
      "Epoch [60/300], Step [3200/27733], Loss: 2.2117\n",
      "Epoch [60/300], Step [3300/27733], Loss: 1.8696\n",
      "Epoch [60/300], Step [3400/27733], Loss: 2.5783\n",
      "Epoch [60/300], Step [3500/27733], Loss: 2.6427\n",
      "Epoch [60/300], Step [3600/27733], Loss: 2.2100\n",
      "Epoch [60/300], Step [3700/27733], Loss: 3.3613\n",
      "Epoch [60/300], Step [3800/27733], Loss: 2.6132\n",
      "Epoch [60/300], Step [3900/27733], Loss: 2.9493\n",
      "Epoch [60/300], Step [4000/27733], Loss: 2.5566\n",
      "Epoch [60/300], Step [4100/27733], Loss: 1.7282\n",
      "Epoch [60/300], Step [4200/27733], Loss: 2.0504\n",
      "Epoch [60/300], Step [4300/27733], Loss: 2.6226\n",
      "Epoch [60/300], Step [4400/27733], Loss: 1.9860\n",
      "Epoch [60/300], Step [4500/27733], Loss: 2.5493\n",
      "Epoch [60/300], Step [4600/27733], Loss: 2.2915\n",
      "Epoch [60/300], Step [4700/27733], Loss: 2.3506\n",
      "Epoch [60/300], Step [4800/27733], Loss: 3.1926\n",
      "Epoch [60/300], Step [4900/27733], Loss: 2.4185\n",
      "Epoch [60/300], Step [5000/27733], Loss: 2.2104\n",
      "Epoch [60/300], Step [5100/27733], Loss: 3.2245\n",
      "Epoch [60/300], Step [5200/27733], Loss: 2.0824\n",
      "Epoch [60/300], Step [5300/27733], Loss: 2.7648\n",
      "Epoch [60/300], Step [5400/27733], Loss: 2.2026\n",
      "Epoch [60/300], Step [5500/27733], Loss: 2.5285\n",
      "Epoch [60/300], Step [5600/27733], Loss: 2.9668\n",
      "Epoch [60/300], Step [5700/27733], Loss: 2.8550\n",
      "Epoch [60/300], Step [5800/27733], Loss: 3.4726\n",
      "Epoch [60/300], Step [5900/27733], Loss: 2.9829\n",
      "Epoch [60/300], Step [6000/27733], Loss: 1.8013\n",
      "Epoch [60/300], Step [6100/27733], Loss: 1.9183\n",
      "Epoch [60/300], Step [6200/27733], Loss: 2.5312\n",
      "Epoch [60/300], Step [6300/27733], Loss: 2.3051\n",
      "Epoch [60/300], Step [6400/27733], Loss: 3.0162\n",
      "Epoch [60/300], Step [6500/27733], Loss: 2.6916\n",
      "Epoch [60/300], Step [6600/27733], Loss: 2.3352\n",
      "Epoch [60/300], Step [6700/27733], Loss: 2.3761\n",
      "Epoch [60/300], Step [6800/27733], Loss: 2.7207\n",
      "Epoch [60/300], Step [6900/27733], Loss: 3.0146\n",
      "Epoch [60/300], Step [7000/27733], Loss: 2.3101\n",
      "Epoch [60/300], Step [7100/27733], Loss: 3.2985\n",
      "Epoch [60/300], Step [7200/27733], Loss: 1.7940\n",
      "Epoch [60/300], Step [7300/27733], Loss: 2.3786\n",
      "Epoch [60/300], Step [7400/27733], Loss: 3.0382\n",
      "Epoch [60/300], Step [7500/27733], Loss: 2.1451\n",
      "Epoch [60/300], Step [7600/27733], Loss: 2.6402\n",
      "Epoch [60/300], Step [7700/27733], Loss: 2.8822\n",
      "Epoch [60/300], Step [7800/27733], Loss: 2.4577\n",
      "Epoch [60/300], Step [7900/27733], Loss: 2.3249\n",
      "Epoch [60/300], Step [8000/27733], Loss: 3.4989\n",
      "Epoch [60/300], Step [8100/27733], Loss: 2.7925\n",
      "Epoch [60/300], Step [8200/27733], Loss: 2.7356\n",
      "Epoch [60/300], Step [8300/27733], Loss: 2.5816\n",
      "Epoch [60/300], Step [8400/27733], Loss: 3.7473\n",
      "Epoch [60/300], Step [8500/27733], Loss: 2.1269\n",
      "Epoch [60/300], Step [8600/27733], Loss: 2.7357\n",
      "Epoch [60/300], Step [8700/27733], Loss: 2.2049\n",
      "Epoch [60/300], Step [8800/27733], Loss: 3.1809\n",
      "Epoch [60/300], Step [8900/27733], Loss: 2.0565\n",
      "Epoch [60/300], Step [9000/27733], Loss: 2.6837\n",
      "Epoch [60/300], Step [9100/27733], Loss: 3.0231\n",
      "Epoch [60/300], Step [9200/27733], Loss: 2.9259\n",
      "Epoch [60/300], Step [9300/27733], Loss: 3.4200\n",
      "Epoch [60/300], Step [9400/27733], Loss: 3.2548\n",
      "Epoch [60/300], Step [9500/27733], Loss: 2.4730\n",
      "Epoch [60/300], Step [9600/27733], Loss: 2.6827\n",
      "Epoch [60/300], Step [9700/27733], Loss: 3.3922\n",
      "Epoch [60/300], Step [9800/27733], Loss: 2.6058\n",
      "Epoch [60/300], Step [9900/27733], Loss: 2.8700\n",
      "Epoch [60/300], Step [10000/27733], Loss: 2.3030\n",
      "Epoch [60/300], Step [10100/27733], Loss: 2.2734\n",
      "Epoch [60/300], Step [10200/27733], Loss: 2.9992\n",
      "Epoch [60/300], Step [10300/27733], Loss: 2.8445\n",
      "Epoch [60/300], Step [10400/27733], Loss: 2.9992\n",
      "Epoch [60/300], Step [10500/27733], Loss: 2.4483\n",
      "Epoch [60/300], Step [10600/27733], Loss: 2.8496\n",
      "Epoch [60/300], Step [10700/27733], Loss: 1.8256\n",
      "Epoch [60/300], Step [10800/27733], Loss: 2.3501\n",
      "Epoch [60/300], Step [10900/27733], Loss: 2.5512\n",
      "Epoch [60/300], Step [11000/27733], Loss: 2.6102\n",
      "Epoch [60/300], Step [11100/27733], Loss: 2.1589\n",
      "Epoch [60/300], Step [11200/27733], Loss: 3.4266\n",
      "Epoch [60/300], Step [11300/27733], Loss: 3.2542\n",
      "Epoch [60/300], Step [11400/27733], Loss: 3.0294\n",
      "Epoch [60/300], Step [11500/27733], Loss: 3.1591\n",
      "Epoch [60/300], Step [11600/27733], Loss: 3.5786\n",
      "Epoch [60/300], Step [11700/27733], Loss: 2.6351\n",
      "Epoch [60/300], Step [11800/27733], Loss: 2.0135\n",
      "Epoch [60/300], Step [11900/27733], Loss: 3.7379\n",
      "Epoch [60/300], Step [12000/27733], Loss: 2.5384\n",
      "Epoch [60/300], Step [12100/27733], Loss: 2.8380\n",
      "Epoch [60/300], Step [12200/27733], Loss: 2.4164\n",
      "Epoch [60/300], Step [12300/27733], Loss: 3.2842\n",
      "Epoch [60/300], Step [12400/27733], Loss: 2.7597\n",
      "Epoch [60/300], Step [12500/27733], Loss: 2.2123\n",
      "Epoch [60/300], Step [12600/27733], Loss: 2.9846\n",
      "Epoch [60/300], Step [12700/27733], Loss: 3.2701\n",
      "Epoch [60/300], Step [12800/27733], Loss: 2.9977\n",
      "Epoch [60/300], Step [12900/27733], Loss: 2.8217\n",
      "Epoch [60/300], Step [13000/27733], Loss: 2.7662\n",
      "Epoch [60/300], Step [13100/27733], Loss: 2.6981\n",
      "Epoch [60/300], Step [13200/27733], Loss: 2.4951\n",
      "Epoch [60/300], Step [13300/27733], Loss: 2.3859\n",
      "Epoch [60/300], Step [13400/27733], Loss: 3.3214\n",
      "Epoch [60/300], Step [13500/27733], Loss: 2.9960\n",
      "Epoch [60/300], Step [13600/27733], Loss: 3.0983\n",
      "Epoch [60/300], Step [13700/27733], Loss: 2.2783\n",
      "Epoch [60/300], Step [13800/27733], Loss: 2.5845\n",
      "Epoch [60/300], Step [13900/27733], Loss: 2.7027\n",
      "Epoch [60/300], Step [14000/27733], Loss: 2.2745\n",
      "Epoch [60/300], Step [14100/27733], Loss: 2.5047\n",
      "Epoch [60/300], Step [14200/27733], Loss: 2.8392\n",
      "Epoch [60/300], Step [14300/27733], Loss: 2.5946\n",
      "Epoch [60/300], Step [14400/27733], Loss: 2.3550\n",
      "Epoch [60/300], Step [14500/27733], Loss: 3.2088\n",
      "Epoch [60/300], Step [14600/27733], Loss: 2.5426\n",
      "Epoch [60/300], Step [14700/27733], Loss: 2.4577\n",
      "Epoch [60/300], Step [14800/27733], Loss: 2.1740\n",
      "Epoch [60/300], Step [14900/27733], Loss: 2.5874\n",
      "Epoch [60/300], Step [15000/27733], Loss: 3.2565\n",
      "Epoch [60/300], Step [15100/27733], Loss: 3.0930\n",
      "Epoch [60/300], Step [15200/27733], Loss: 2.3697\n",
      "Epoch [60/300], Step [15300/27733], Loss: 3.1253\n",
      "Epoch [60/300], Step [15400/27733], Loss: 2.8229\n",
      "Epoch [60/300], Step [15500/27733], Loss: 2.8288\n",
      "Epoch [60/300], Step [15600/27733], Loss: 3.3195\n",
      "Epoch [60/300], Step [15700/27733], Loss: 2.4476\n",
      "Epoch [60/300], Step [15800/27733], Loss: 2.2883\n",
      "Epoch [60/300], Step [15900/27733], Loss: 2.7738\n",
      "Epoch [60/300], Step [16000/27733], Loss: 2.3107\n",
      "Epoch [60/300], Step [16100/27733], Loss: 2.4492\n",
      "Epoch [60/300], Step [16200/27733], Loss: 3.0786\n",
      "Epoch [60/300], Step [16300/27733], Loss: 1.5306\n",
      "Epoch [60/300], Step [16400/27733], Loss: 1.8710\n",
      "Epoch [60/300], Step [16500/27733], Loss: 2.3625\n",
      "Epoch [60/300], Step [16600/27733], Loss: 3.4047\n",
      "Epoch [60/300], Step [16700/27733], Loss: 3.1502\n",
      "Epoch [60/300], Step [16800/27733], Loss: 2.4175\n",
      "Epoch [60/300], Step [16900/27733], Loss: 3.0712\n",
      "Epoch [60/300], Step [17000/27733], Loss: 2.7338\n",
      "Epoch [60/300], Step [17100/27733], Loss: 2.3975\n",
      "Epoch [60/300], Step [17200/27733], Loss: 2.6390\n",
      "Epoch [60/300], Step [17300/27733], Loss: 2.6030\n",
      "Epoch [60/300], Step [17400/27733], Loss: 3.0338\n",
      "Epoch [60/300], Step [17500/27733], Loss: 3.0134\n",
      "Epoch [60/300], Step [17600/27733], Loss: 2.5746\n",
      "Epoch [60/300], Step [17700/27733], Loss: 2.7264\n",
      "Epoch [60/300], Step [17800/27733], Loss: 2.7363\n",
      "Epoch [60/300], Step [17900/27733], Loss: 2.6398\n",
      "Epoch [60/300], Step [18000/27733], Loss: 2.6787\n",
      "Epoch [60/300], Step [18100/27733], Loss: 2.3414\n",
      "Epoch [60/300], Step [18200/27733], Loss: 1.9050\n",
      "Epoch [60/300], Step [18300/27733], Loss: 2.4524\n",
      "Epoch [60/300], Step [18400/27733], Loss: 3.2786\n",
      "Epoch [60/300], Step [18500/27733], Loss: 1.9457\n",
      "Epoch [60/300], Step [18600/27733], Loss: 3.1999\n",
      "Epoch [60/300], Step [18700/27733], Loss: 3.6993\n",
      "Epoch [60/300], Step [18800/27733], Loss: 2.1547\n",
      "Epoch [60/300], Step [18900/27733], Loss: 3.7476\n",
      "Epoch [60/300], Step [19000/27733], Loss: 3.4842\n",
      "Epoch [60/300], Step [19100/27733], Loss: 3.3266\n",
      "Epoch [60/300], Step [19200/27733], Loss: 3.1529\n",
      "Epoch [60/300], Step [19300/27733], Loss: 2.7941\n",
      "Epoch [60/300], Step [19400/27733], Loss: 1.8364\n",
      "Epoch [60/300], Step [19500/27733], Loss: 3.2768\n",
      "Epoch [60/300], Step [19600/27733], Loss: 4.0244\n",
      "Epoch [60/300], Step [19700/27733], Loss: 2.5621\n",
      "Epoch [60/300], Step [19800/27733], Loss: 2.3515\n",
      "Epoch [60/300], Step [19900/27733], Loss: 2.8299\n",
      "Epoch [60/300], Step [20000/27733], Loss: 2.2921\n",
      "Epoch [60/300], Step [20100/27733], Loss: 2.3406\n",
      "Epoch [60/300], Step [20200/27733], Loss: 2.6823\n",
      "Epoch [60/300], Step [20300/27733], Loss: 2.4269\n",
      "Epoch [60/300], Step [20400/27733], Loss: 3.4490\n",
      "Epoch [60/300], Step [20500/27733], Loss: 4.2008\n",
      "Epoch [60/300], Step [20600/27733], Loss: 2.5826\n",
      "Epoch [60/300], Step [20700/27733], Loss: 2.7642\n",
      "Epoch [60/300], Step [20800/27733], Loss: 2.8990\n",
      "Epoch [60/300], Step [20900/27733], Loss: 3.4336\n",
      "Epoch [60/300], Step [21000/27733], Loss: 3.7539\n",
      "Epoch [60/300], Step [21100/27733], Loss: 3.4135\n",
      "Epoch [60/300], Step [21200/27733], Loss: 2.4169\n",
      "Epoch [60/300], Step [21300/27733], Loss: 2.8155\n",
      "Epoch [60/300], Step [21400/27733], Loss: 2.6056\n",
      "Epoch [60/300], Step [21500/27733], Loss: 2.5988\n",
      "Epoch [60/300], Step [21600/27733], Loss: 2.2908\n",
      "Epoch [60/300], Step [21700/27733], Loss: 3.1609\n",
      "Epoch [60/300], Step [21800/27733], Loss: 2.6219\n",
      "Epoch [60/300], Step [21900/27733], Loss: 3.9447\n",
      "Epoch [60/300], Step [22000/27733], Loss: 3.0437\n",
      "Epoch [60/300], Step [22100/27733], Loss: 2.9122\n",
      "Epoch [60/300], Step [22200/27733], Loss: 3.2287\n",
      "Epoch [60/300], Step [22300/27733], Loss: 2.9737\n",
      "Epoch [60/300], Step [22400/27733], Loss: 3.1847\n",
      "Epoch [60/300], Step [22500/27733], Loss: 3.0180\n",
      "Epoch [60/300], Step [22600/27733], Loss: 2.5126\n",
      "Epoch [60/300], Step [22700/27733], Loss: 1.8139\n",
      "Epoch [60/300], Step [22800/27733], Loss: 2.7956\n",
      "Epoch [60/300], Step [22900/27733], Loss: 2.7835\n",
      "Epoch [60/300], Step [23000/27733], Loss: 3.3141\n",
      "Epoch [60/300], Step [23100/27733], Loss: 2.6067\n",
      "Epoch [60/300], Step [23200/27733], Loss: 2.3567\n",
      "Epoch [60/300], Step [23300/27733], Loss: 2.4835\n",
      "Epoch [60/300], Step [23400/27733], Loss: 2.2300\n",
      "Epoch [60/300], Step [23500/27733], Loss: 3.1171\n",
      "Epoch [60/300], Step [23600/27733], Loss: 2.6752\n",
      "Epoch [60/300], Step [23700/27733], Loss: 2.8910\n",
      "Epoch [60/300], Step [23800/27733], Loss: 3.1188\n",
      "Epoch [60/300], Step [23900/27733], Loss: 3.4155\n",
      "Epoch [60/300], Step [24000/27733], Loss: 2.8340\n",
      "Epoch [60/300], Step [24100/27733], Loss: 4.1361\n",
      "Epoch [60/300], Step [24200/27733], Loss: 3.1384\n",
      "Epoch [60/300], Step [24300/27733], Loss: 3.1683\n",
      "Epoch [60/300], Step [24400/27733], Loss: 2.8889\n",
      "Epoch [60/300], Step [24500/27733], Loss: 3.2238\n",
      "Epoch [60/300], Step [24600/27733], Loss: 3.3291\n",
      "Epoch [60/300], Step [24700/27733], Loss: 2.8898\n",
      "Epoch [60/300], Step [24800/27733], Loss: 3.4873\n",
      "Epoch [60/300], Step [24900/27733], Loss: 2.3487\n",
      "Epoch [60/300], Step [25000/27733], Loss: 2.3853\n",
      "Epoch [60/300], Step [25100/27733], Loss: 3.5036\n",
      "Epoch [60/300], Step [25200/27733], Loss: 3.4444\n",
      "Epoch [60/300], Step [25300/27733], Loss: 2.4943\n",
      "Epoch [60/300], Step [25400/27733], Loss: 2.0742\n",
      "Epoch [60/300], Step [25500/27733], Loss: 3.3136\n",
      "Epoch [60/300], Step [25600/27733], Loss: 3.5972\n",
      "Epoch [60/300], Step [25700/27733], Loss: 2.9373\n",
      "Epoch [60/300], Step [25800/27733], Loss: 2.5843\n",
      "Epoch [60/300], Step [25900/27733], Loss: 2.1376\n",
      "Epoch [60/300], Step [26000/27733], Loss: 2.6796\n",
      "Epoch [60/300], Step [26100/27733], Loss: 3.5887\n",
      "Epoch [60/300], Step [26200/27733], Loss: 3.9156\n",
      "Epoch [60/300], Step [26300/27733], Loss: 3.1716\n",
      "Epoch [60/300], Step [26400/27733], Loss: 2.7017\n",
      "Epoch [60/300], Step [26500/27733], Loss: 3.2933\n",
      "Epoch [60/300], Step [26600/27733], Loss: 2.8786\n",
      "Epoch [60/300], Step [26700/27733], Loss: 3.8861\n",
      "Epoch [60/300], Step [26800/27733], Loss: 2.7477\n",
      "Epoch [60/300], Step [26900/27733], Loss: 3.2641\n",
      "Epoch [60/300], Step [27000/27733], Loss: 2.0370\n",
      "Epoch [60/300], Step [27100/27733], Loss: 2.7187\n",
      "Epoch [60/300], Step [27200/27733], Loss: 2.6308\n",
      "Epoch [60/300], Step [27300/27733], Loss: 2.4704\n",
      "Epoch [60/300], Step [27400/27733], Loss: 2.7358\n",
      "Epoch [60/300], Step [27500/27733], Loss: 2.8423\n",
      "Epoch [60/300], Step [27600/27733], Loss: 3.0861\n",
      "Epoch [60/300], Step [27700/27733], Loss: 3.3009\n",
      "Epoch [61/300], Step [100/27733], Loss: 2.1382\n",
      "Epoch [61/300], Step [200/27733], Loss: 2.6077\n",
      "Epoch [61/300], Step [300/27733], Loss: 2.7171\n",
      "Epoch [61/300], Step [400/27733], Loss: 2.4782\n",
      "Epoch [61/300], Step [500/27733], Loss: 2.1349\n",
      "Epoch [61/300], Step [600/27733], Loss: 2.4778\n",
      "Epoch [61/300], Step [700/27733], Loss: 2.8218\n",
      "Epoch [61/300], Step [800/27733], Loss: 2.1411\n",
      "Epoch [61/300], Step [900/27733], Loss: 2.1458\n",
      "Epoch [61/300], Step [1000/27733], Loss: 3.0729\n",
      "Epoch [61/300], Step [1100/27733], Loss: 1.6739\n",
      "Epoch [61/300], Step [1200/27733], Loss: 2.2401\n",
      "Epoch [61/300], Step [1300/27733], Loss: 2.0492\n",
      "Epoch [61/300], Step [1400/27733], Loss: 2.6618\n",
      "Epoch [61/300], Step [1500/27733], Loss: 2.9575\n",
      "Epoch [61/300], Step [1600/27733], Loss: 2.5752\n",
      "Epoch [61/300], Step [1700/27733], Loss: 2.4129\n",
      "Epoch [61/300], Step [1800/27733], Loss: 2.0361\n",
      "Epoch [61/300], Step [1900/27733], Loss: 2.3147\n",
      "Epoch [61/300], Step [2000/27733], Loss: 2.1662\n",
      "Epoch [61/300], Step [2100/27733], Loss: 2.4523\n",
      "Epoch [61/300], Step [2200/27733], Loss: 2.1865\n",
      "Epoch [61/300], Step [2300/27733], Loss: 2.2358\n",
      "Epoch [61/300], Step [2400/27733], Loss: 2.5012\n",
      "Epoch [61/300], Step [2500/27733], Loss: 1.6652\n",
      "Epoch [61/300], Step [2600/27733], Loss: 1.8938\n",
      "Epoch [61/300], Step [2700/27733], Loss: 2.5079\n",
      "Epoch [61/300], Step [2800/27733], Loss: 2.3996\n",
      "Epoch [61/300], Step [2900/27733], Loss: 2.3409\n",
      "Epoch [61/300], Step [3000/27733], Loss: 1.5407\n",
      "Epoch [61/300], Step [3100/27733], Loss: 2.1096\n",
      "Epoch [61/300], Step [3200/27733], Loss: 2.7109\n",
      "Epoch [61/300], Step [3300/27733], Loss: 2.1911\n",
      "Epoch [61/300], Step [3400/27733], Loss: 2.3857\n",
      "Epoch [61/300], Step [3500/27733], Loss: 2.1586\n",
      "Epoch [61/300], Step [3600/27733], Loss: 2.5439\n",
      "Epoch [61/300], Step [3700/27733], Loss: 2.3946\n",
      "Epoch [61/300], Step [3800/27733], Loss: 2.1445\n",
      "Epoch [61/300], Step [3900/27733], Loss: 2.3651\n",
      "Epoch [61/300], Step [4000/27733], Loss: 2.4557\n",
      "Epoch [61/300], Step [4100/27733], Loss: 2.6708\n",
      "Epoch [61/300], Step [4200/27733], Loss: 2.0154\n",
      "Epoch [61/300], Step [4300/27733], Loss: 3.4184\n",
      "Epoch [61/300], Step [4400/27733], Loss: 2.3284\n",
      "Epoch [61/300], Step [4500/27733], Loss: 1.6723\n",
      "Epoch [61/300], Step [4600/27733], Loss: 2.1343\n",
      "Epoch [61/300], Step [4700/27733], Loss: 2.9039\n",
      "Epoch [61/300], Step [4800/27733], Loss: 2.5399\n",
      "Epoch [61/300], Step [4900/27733], Loss: 2.5627\n",
      "Epoch [61/300], Step [5000/27733], Loss: 2.4765\n",
      "Epoch [61/300], Step [5100/27733], Loss: 2.5424\n",
      "Epoch [61/300], Step [5200/27733], Loss: 2.3069\n",
      "Epoch [61/300], Step [5300/27733], Loss: 2.8893\n",
      "Epoch [61/300], Step [5400/27733], Loss: 2.2109\n",
      "Epoch [61/300], Step [5500/27733], Loss: 1.8543\n",
      "Epoch [61/300], Step [5600/27733], Loss: 2.6339\n",
      "Epoch [61/300], Step [5700/27733], Loss: 2.2659\n",
      "Epoch [61/300], Step [5800/27733], Loss: 2.4258\n",
      "Epoch [61/300], Step [5900/27733], Loss: 2.7761\n",
      "Epoch [61/300], Step [6000/27733], Loss: 2.0725\n",
      "Epoch [61/300], Step [6100/27733], Loss: 2.5277\n",
      "Epoch [61/300], Step [6200/27733], Loss: 2.9448\n",
      "Epoch [61/300], Step [6300/27733], Loss: 2.9944\n",
      "Epoch [61/300], Step [6400/27733], Loss: 2.0163\n",
      "Epoch [61/300], Step [6500/27733], Loss: 1.9655\n",
      "Epoch [61/300], Step [6600/27733], Loss: 2.9955\n",
      "Epoch [61/300], Step [6700/27733], Loss: 2.8649\n",
      "Epoch [61/300], Step [6800/27733], Loss: 2.0927\n",
      "Epoch [61/300], Step [6900/27733], Loss: 3.3551\n",
      "Epoch [61/300], Step [7000/27733], Loss: 1.7881\n",
      "Epoch [61/300], Step [7100/27733], Loss: 2.7619\n",
      "Epoch [61/300], Step [7200/27733], Loss: 2.1804\n",
      "Epoch [61/300], Step [7300/27733], Loss: 1.7393\n",
      "Epoch [61/300], Step [7400/27733], Loss: 2.0574\n",
      "Epoch [61/300], Step [7500/27733], Loss: 2.6720\n",
      "Epoch [61/300], Step [7600/27733], Loss: 2.9094\n",
      "Epoch [61/300], Step [7700/27733], Loss: 2.4833\n",
      "Epoch [61/300], Step [7800/27733], Loss: 2.0428\n",
      "Epoch [61/300], Step [7900/27733], Loss: 2.6018\n",
      "Epoch [61/300], Step [8000/27733], Loss: 3.1555\n",
      "Epoch [61/300], Step [8100/27733], Loss: 2.9493\n",
      "Epoch [61/300], Step [8200/27733], Loss: 2.6849\n",
      "Epoch [61/300], Step [8300/27733], Loss: 3.4401\n",
      "Epoch [61/300], Step [8400/27733], Loss: 1.9503\n",
      "Epoch [61/300], Step [8500/27733], Loss: 2.5623\n",
      "Epoch [61/300], Step [8600/27733], Loss: 3.1990\n",
      "Epoch [61/300], Step [8700/27733], Loss: 3.9198\n",
      "Epoch [61/300], Step [8800/27733], Loss: 2.5612\n",
      "Epoch [61/300], Step [8900/27733], Loss: 2.1946\n",
      "Epoch [61/300], Step [9000/27733], Loss: 2.5309\n",
      "Epoch [61/300], Step [9100/27733], Loss: 2.7324\n",
      "Epoch [61/300], Step [9200/27733], Loss: 2.4764\n",
      "Epoch [61/300], Step [9300/27733], Loss: 3.2964\n",
      "Epoch [61/300], Step [9400/27733], Loss: 3.2222\n",
      "Epoch [61/300], Step [9500/27733], Loss: 2.8761\n",
      "Epoch [61/300], Step [9600/27733], Loss: 3.2149\n",
      "Epoch [61/300], Step [9700/27733], Loss: 2.9867\n",
      "Epoch [61/300], Step [9800/27733], Loss: 2.4887\n",
      "Epoch [61/300], Step [9900/27733], Loss: 3.0780\n",
      "Epoch [61/300], Step [10000/27733], Loss: 2.6826\n",
      "Epoch [61/300], Step [10100/27733], Loss: 2.6343\n",
      "Epoch [61/300], Step [10200/27733], Loss: 2.7426\n",
      "Epoch [61/300], Step [10300/27733], Loss: 2.2101\n",
      "Epoch [61/300], Step [10400/27733], Loss: 2.5413\n",
      "Epoch [61/300], Step [10500/27733], Loss: 3.1003\n",
      "Epoch [61/300], Step [10600/27733], Loss: 2.0857\n",
      "Epoch [61/300], Step [10700/27733], Loss: 2.5990\n",
      "Epoch [61/300], Step [10800/27733], Loss: 3.1072\n",
      "Epoch [61/300], Step [10900/27733], Loss: 3.2097\n",
      "Epoch [61/300], Step [11000/27733], Loss: 2.1616\n",
      "Epoch [61/300], Step [11100/27733], Loss: 2.6073\n",
      "Epoch [61/300], Step [11200/27733], Loss: 3.0183\n",
      "Epoch [61/300], Step [11300/27733], Loss: 2.8731\n",
      "Epoch [61/300], Step [11400/27733], Loss: 3.0427\n",
      "Epoch [61/300], Step [11500/27733], Loss: 2.4346\n",
      "Epoch [61/300], Step [11600/27733], Loss: 2.1066\n",
      "Epoch [61/300], Step [11700/27733], Loss: 2.8101\n",
      "Epoch [61/300], Step [11800/27733], Loss: 2.4825\n",
      "Epoch [61/300], Step [11900/27733], Loss: 2.9266\n",
      "Epoch [61/300], Step [12000/27733], Loss: 3.7482\n",
      "Epoch [61/300], Step [12100/27733], Loss: 2.4957\n",
      "Epoch [61/300], Step [12200/27733], Loss: 2.4126\n",
      "Epoch [61/300], Step [12300/27733], Loss: 3.8409\n",
      "Epoch [61/300], Step [12400/27733], Loss: 2.9281\n",
      "Epoch [61/300], Step [12500/27733], Loss: 2.6002\n",
      "Epoch [61/300], Step [12600/27733], Loss: 3.2193\n",
      "Epoch [61/300], Step [12700/27733], Loss: 2.4587\n",
      "Epoch [61/300], Step [12800/27733], Loss: 2.2098\n",
      "Epoch [61/300], Step [12900/27733], Loss: 2.1260\n",
      "Epoch [61/300], Step [13000/27733], Loss: 2.6712\n",
      "Epoch [61/300], Step [13100/27733], Loss: 2.7906\n",
      "Epoch [61/300], Step [13200/27733], Loss: 3.0193\n",
      "Epoch [61/300], Step [13300/27733], Loss: 2.5764\n",
      "Epoch [61/300], Step [13400/27733], Loss: 3.1335\n",
      "Epoch [61/300], Step [13500/27733], Loss: 3.1894\n",
      "Epoch [61/300], Step [13600/27733], Loss: 2.4490\n",
      "Epoch [61/300], Step [13700/27733], Loss: 2.5390\n",
      "Epoch [61/300], Step [13800/27733], Loss: 3.1204\n",
      "Epoch [61/300], Step [13900/27733], Loss: 2.3789\n",
      "Epoch [61/300], Step [14000/27733], Loss: 2.6413\n",
      "Epoch [61/300], Step [14100/27733], Loss: 2.5721\n",
      "Epoch [61/300], Step [14200/27733], Loss: 2.0078\n",
      "Epoch [61/300], Step [14300/27733], Loss: 2.7880\n",
      "Epoch [61/300], Step [14400/27733], Loss: 2.8240\n",
      "Epoch [61/300], Step [14500/27733], Loss: 2.7899\n",
      "Epoch [61/300], Step [14600/27733], Loss: 2.2341\n",
      "Epoch [61/300], Step [14700/27733], Loss: 3.1323\n",
      "Epoch [61/300], Step [14800/27733], Loss: 3.3703\n",
      "Epoch [61/300], Step [14900/27733], Loss: 2.8163\n",
      "Epoch [61/300], Step [15000/27733], Loss: 3.0869\n",
      "Epoch [61/300], Step [15100/27733], Loss: 2.6478\n",
      "Epoch [61/300], Step [15200/27733], Loss: 3.1471\n",
      "Epoch [61/300], Step [15300/27733], Loss: 3.1690\n",
      "Epoch [61/300], Step [15400/27733], Loss: 2.5394\n",
      "Epoch [61/300], Step [15500/27733], Loss: 3.2574\n",
      "Epoch [61/300], Step [15600/27733], Loss: 3.1780\n",
      "Epoch [61/300], Step [15700/27733], Loss: 2.8193\n",
      "Epoch [61/300], Step [15800/27733], Loss: 2.6436\n",
      "Epoch [61/300], Step [15900/27733], Loss: 2.5110\n",
      "Epoch [61/300], Step [16000/27733], Loss: 2.9828\n",
      "Epoch [61/300], Step [16100/27733], Loss: 3.5900\n",
      "Epoch [61/300], Step [16200/27733], Loss: 1.6202\n",
      "Epoch [61/300], Step [16300/27733], Loss: 2.8007\n",
      "Epoch [61/300], Step [16400/27733], Loss: 2.7272\n",
      "Epoch [61/300], Step [16500/27733], Loss: 2.6875\n",
      "Epoch [61/300], Step [16600/27733], Loss: 2.9202\n",
      "Epoch [61/300], Step [16700/27733], Loss: 2.7033\n",
      "Epoch [61/300], Step [16800/27733], Loss: 2.5584\n",
      "Epoch [61/300], Step [16900/27733], Loss: 2.8635\n",
      "Epoch [61/300], Step [17000/27733], Loss: 3.1178\n",
      "Epoch [61/300], Step [17100/27733], Loss: 2.2993\n",
      "Epoch [61/300], Step [17200/27733], Loss: 3.2322\n",
      "Epoch [61/300], Step [17300/27733], Loss: 3.7307\n",
      "Epoch [61/300], Step [17400/27733], Loss: 3.0573\n",
      "Epoch [61/300], Step [17500/27733], Loss: 2.4931\n",
      "Epoch [61/300], Step [17600/27733], Loss: 2.5141\n",
      "Epoch [61/300], Step [17700/27733], Loss: 2.4112\n",
      "Epoch [61/300], Step [17800/27733], Loss: 2.3933\n",
      "Epoch [61/300], Step [17900/27733], Loss: 3.3871\n",
      "Epoch [61/300], Step [18000/27733], Loss: 2.9611\n",
      "Epoch [61/300], Step [18100/27733], Loss: 3.4026\n",
      "Epoch [61/300], Step [18200/27733], Loss: 2.5972\n",
      "Epoch [61/300], Step [18300/27733], Loss: 2.3447\n",
      "Epoch [61/300], Step [18400/27733], Loss: 2.4695\n",
      "Epoch [61/300], Step [18500/27733], Loss: 2.8157\n",
      "Epoch [61/300], Step [18600/27733], Loss: 2.7221\n",
      "Epoch [61/300], Step [18700/27733], Loss: 2.4374\n",
      "Epoch [61/300], Step [18800/27733], Loss: 1.9210\n",
      "Epoch [61/300], Step [18900/27733], Loss: 3.1758\n",
      "Epoch [61/300], Step [19000/27733], Loss: 2.2909\n",
      "Epoch [61/300], Step [19100/27733], Loss: 3.4220\n",
      "Epoch [61/300], Step [19200/27733], Loss: 2.7872\n",
      "Epoch [61/300], Step [19300/27733], Loss: 2.7622\n",
      "Epoch [61/300], Step [19400/27733], Loss: 2.8577\n",
      "Epoch [61/300], Step [19500/27733], Loss: 2.8477\n",
      "Epoch [61/300], Step [19600/27733], Loss: 2.5879\n",
      "Epoch [61/300], Step [19700/27733], Loss: 2.1749\n",
      "Epoch [61/300], Step [19800/27733], Loss: 2.8156\n",
      "Epoch [61/300], Step [19900/27733], Loss: 2.7449\n",
      "Epoch [61/300], Step [20000/27733], Loss: 3.1942\n",
      "Epoch [61/300], Step [20100/27733], Loss: 2.1744\n",
      "Epoch [61/300], Step [20200/27733], Loss: 2.1245\n",
      "Epoch [61/300], Step [20300/27733], Loss: 3.0135\n",
      "Epoch [61/300], Step [20400/27733], Loss: 2.7468\n",
      "Epoch [61/300], Step [20500/27733], Loss: 2.9050\n",
      "Epoch [61/300], Step [20600/27733], Loss: 2.2277\n",
      "Epoch [61/300], Step [20700/27733], Loss: 2.9914\n",
      "Epoch [61/300], Step [20800/27733], Loss: 2.4421\n",
      "Epoch [61/300], Step [20900/27733], Loss: 2.8282\n",
      "Epoch [61/300], Step [21000/27733], Loss: 2.5586\n",
      "Epoch [61/300], Step [21100/27733], Loss: 2.4533\n",
      "Epoch [61/300], Step [21200/27733], Loss: 2.0772\n",
      "Epoch [61/300], Step [21300/27733], Loss: 2.9273\n",
      "Epoch [61/300], Step [21400/27733], Loss: 3.0773\n",
      "Epoch [61/300], Step [21500/27733], Loss: 3.4400\n",
      "Epoch [61/300], Step [21600/27733], Loss: 3.7440\n",
      "Epoch [61/300], Step [21700/27733], Loss: 3.2240\n",
      "Epoch [61/300], Step [21800/27733], Loss: 3.8462\n",
      "Epoch [61/300], Step [21900/27733], Loss: 2.4037\n",
      "Epoch [61/300], Step [22000/27733], Loss: 2.7956\n",
      "Epoch [61/300], Step [22100/27733], Loss: 2.9344\n",
      "Epoch [61/300], Step [22200/27733], Loss: 2.6168\n",
      "Epoch [61/300], Step [22300/27733], Loss: 2.0889\n",
      "Epoch [61/300], Step [22400/27733], Loss: 2.1768\n",
      "Epoch [61/300], Step [22500/27733], Loss: 2.5514\n",
      "Epoch [61/300], Step [22600/27733], Loss: 1.7148\n",
      "Epoch [61/300], Step [22700/27733], Loss: 2.9241\n",
      "Epoch [61/300], Step [22800/27733], Loss: 3.1530\n",
      "Epoch [61/300], Step [22900/27733], Loss: 2.7258\n",
      "Epoch [61/300], Step [23000/27733], Loss: 2.2011\n",
      "Epoch [61/300], Step [23100/27733], Loss: 2.9278\n",
      "Epoch [61/300], Step [23200/27733], Loss: 2.9074\n",
      "Epoch [61/300], Step [23300/27733], Loss: 2.5807\n",
      "Epoch [61/300], Step [23400/27733], Loss: 2.2611\n",
      "Epoch [61/300], Step [23500/27733], Loss: 4.1792\n",
      "Epoch [61/300], Step [23600/27733], Loss: 2.1418\n",
      "Epoch [61/300], Step [23700/27733], Loss: 2.4006\n",
      "Epoch [61/300], Step [23800/27733], Loss: 3.2182\n",
      "Epoch [61/300], Step [23900/27733], Loss: 2.8299\n",
      "Epoch [61/300], Step [24000/27733], Loss: 2.7911\n",
      "Epoch [61/300], Step [24100/27733], Loss: 2.6053\n",
      "Epoch [61/300], Step [24200/27733], Loss: 2.4786\n",
      "Epoch [61/300], Step [24300/27733], Loss: 3.1006\n",
      "Epoch [61/300], Step [24400/27733], Loss: 1.9244\n",
      "Epoch [61/300], Step [24500/27733], Loss: 2.3475\n",
      "Epoch [61/300], Step [24600/27733], Loss: 2.4265\n",
      "Epoch [61/300], Step [24700/27733], Loss: 1.8574\n",
      "Epoch [61/300], Step [24800/27733], Loss: 2.7894\n",
      "Epoch [61/300], Step [24900/27733], Loss: 2.5660\n",
      "Epoch [61/300], Step [25000/27733], Loss: 3.1513\n",
      "Epoch [61/300], Step [25100/27733], Loss: 2.8952\n",
      "Epoch [61/300], Step [25200/27733], Loss: 2.5082\n",
      "Epoch [61/300], Step [25300/27733], Loss: 2.5153\n",
      "Epoch [61/300], Step [25400/27733], Loss: 2.8620\n",
      "Epoch [61/300], Step [25500/27733], Loss: 3.4459\n",
      "Epoch [61/300], Step [25600/27733], Loss: 2.4543\n",
      "Epoch [61/300], Step [25700/27733], Loss: 2.2376\n",
      "Epoch [61/300], Step [25800/27733], Loss: 3.0194\n",
      "Epoch [61/300], Step [25900/27733], Loss: 2.4682\n",
      "Epoch [61/300], Step [26000/27733], Loss: 3.0964\n",
      "Epoch [61/300], Step [26100/27733], Loss: 2.7873\n",
      "Epoch [61/300], Step [26200/27733], Loss: 2.5450\n",
      "Epoch [61/300], Step [26300/27733], Loss: 2.7496\n",
      "Epoch [61/300], Step [26400/27733], Loss: 1.9114\n",
      "Epoch [61/300], Step [26500/27733], Loss: 3.8953\n",
      "Epoch [61/300], Step [26600/27733], Loss: 2.8529\n",
      "Epoch [61/300], Step [26700/27733], Loss: 2.5272\n",
      "Epoch [61/300], Step [26800/27733], Loss: 3.4621\n",
      "Epoch [61/300], Step [26900/27733], Loss: 3.6675\n",
      "Epoch [61/300], Step [27000/27733], Loss: 2.4138\n",
      "Epoch [61/300], Step [27100/27733], Loss: 3.0917\n",
      "Epoch [61/300], Step [27200/27733], Loss: 3.7704\n",
      "Epoch [61/300], Step [27300/27733], Loss: 3.5343\n",
      "Epoch [61/300], Step [27400/27733], Loss: 3.6291\n",
      "Epoch [61/300], Step [27500/27733], Loss: 3.0811\n",
      "Epoch [61/300], Step [27600/27733], Loss: 2.8285\n",
      "Epoch [61/300], Step [27700/27733], Loss: 3.4963\n",
      "Epoch [62/300], Step [100/27733], Loss: 2.1006\n",
      "Epoch [62/300], Step [200/27733], Loss: 2.5534\n",
      "Epoch [62/300], Step [300/27733], Loss: 3.0798\n",
      "Epoch [62/300], Step [400/27733], Loss: 2.5957\n",
      "Epoch [62/300], Step [500/27733], Loss: 2.4250\n",
      "Epoch [62/300], Step [600/27733], Loss: 2.3356\n",
      "Epoch [62/300], Step [700/27733], Loss: 2.6141\n",
      "Epoch [62/300], Step [800/27733], Loss: 2.3633\n",
      "Epoch [62/300], Step [900/27733], Loss: 1.8038\n",
      "Epoch [62/300], Step [1000/27733], Loss: 2.8418\n",
      "Epoch [62/300], Step [1100/27733], Loss: 2.6609\n",
      "Epoch [62/300], Step [1200/27733], Loss: 2.3651\n",
      "Epoch [62/300], Step [1300/27733], Loss: 2.3892\n",
      "Epoch [62/300], Step [1400/27733], Loss: 2.0522\n",
      "Epoch [62/300], Step [1500/27733], Loss: 2.4446\n",
      "Epoch [62/300], Step [1600/27733], Loss: 2.5747\n",
      "Epoch [62/300], Step [1700/27733], Loss: 2.0703\n",
      "Epoch [62/300], Step [1800/27733], Loss: 2.6840\n",
      "Epoch [62/300], Step [1900/27733], Loss: 2.2743\n",
      "Epoch [62/300], Step [2000/27733], Loss: 2.2606\n",
      "Epoch [62/300], Step [2100/27733], Loss: 2.9098\n",
      "Epoch [62/300], Step [2200/27733], Loss: 2.3676\n",
      "Epoch [62/300], Step [2300/27733], Loss: 1.6642\n",
      "Epoch [62/300], Step [2400/27733], Loss: 1.7637\n",
      "Epoch [62/300], Step [2500/27733], Loss: 2.7329\n",
      "Epoch [62/300], Step [2600/27733], Loss: 2.9689\n",
      "Epoch [62/300], Step [2700/27733], Loss: 2.4628\n",
      "Epoch [62/300], Step [2800/27733], Loss: 3.0259\n",
      "Epoch [62/300], Step [2900/27733], Loss: 2.0345\n",
      "Epoch [62/300], Step [3000/27733], Loss: 1.9495\n",
      "Epoch [62/300], Step [3100/27733], Loss: 2.2313\n",
      "Epoch [62/300], Step [3200/27733], Loss: 2.8770\n",
      "Epoch [62/300], Step [3300/27733], Loss: 2.7149\n",
      "Epoch [62/300], Step [3400/27733], Loss: 2.7619\n",
      "Epoch [62/300], Step [3500/27733], Loss: 1.9989\n",
      "Epoch [62/300], Step [3600/27733], Loss: 2.4180\n",
      "Epoch [62/300], Step [3700/27733], Loss: 2.4120\n",
      "Epoch [62/300], Step [3800/27733], Loss: 2.1936\n",
      "Epoch [62/300], Step [3900/27733], Loss: 2.2450\n",
      "Epoch [62/300], Step [4000/27733], Loss: 3.3591\n",
      "Epoch [62/300], Step [4100/27733], Loss: 2.8563\n",
      "Epoch [62/300], Step [4200/27733], Loss: 2.4632\n",
      "Epoch [62/300], Step [4300/27733], Loss: 2.1499\n",
      "Epoch [62/300], Step [4400/27733], Loss: 1.3918\n",
      "Epoch [62/300], Step [4500/27733], Loss: 2.1428\n",
      "Epoch [62/300], Step [4600/27733], Loss: 2.1988\n",
      "Epoch [62/300], Step [4700/27733], Loss: 2.5945\n",
      "Epoch [62/300], Step [4800/27733], Loss: 2.6407\n",
      "Epoch [62/300], Step [4900/27733], Loss: 2.3296\n",
      "Epoch [62/300], Step [5000/27733], Loss: 2.6410\n",
      "Epoch [62/300], Step [5100/27733], Loss: 1.7605\n",
      "Epoch [62/300], Step [5200/27733], Loss: 2.8486\n",
      "Epoch [62/300], Step [5300/27733], Loss: 2.9927\n",
      "Epoch [62/300], Step [5400/27733], Loss: 3.0680\n",
      "Epoch [62/300], Step [5500/27733], Loss: 2.8943\n",
      "Epoch [62/300], Step [5600/27733], Loss: 2.1885\n",
      "Epoch [62/300], Step [5700/27733], Loss: 2.0834\n",
      "Epoch [62/300], Step [5800/27733], Loss: 2.5401\n",
      "Epoch [62/300], Step [5900/27733], Loss: 2.4562\n",
      "Epoch [62/300], Step [6000/27733], Loss: 3.1071\n",
      "Epoch [62/300], Step [6100/27733], Loss: 2.6329\n",
      "Epoch [62/300], Step [6200/27733], Loss: 2.7189\n",
      "Epoch [62/300], Step [6300/27733], Loss: 2.7138\n",
      "Epoch [62/300], Step [6400/27733], Loss: 2.1409\n",
      "Epoch [62/300], Step [6500/27733], Loss: 2.0526\n",
      "Epoch [62/300], Step [6600/27733], Loss: 2.4140\n",
      "Epoch [62/300], Step [6700/27733], Loss: 2.5398\n",
      "Epoch [62/300], Step [6800/27733], Loss: 2.0193\n",
      "Epoch [62/300], Step [6900/27733], Loss: 2.6024\n",
      "Epoch [62/300], Step [7000/27733], Loss: 2.9662\n",
      "Epoch [62/300], Step [7100/27733], Loss: 2.5480\n",
      "Epoch [62/300], Step [7200/27733], Loss: 2.0464\n",
      "Epoch [62/300], Step [7300/27733], Loss: 3.1385\n",
      "Epoch [62/300], Step [7400/27733], Loss: 3.0688\n",
      "Epoch [62/300], Step [7500/27733], Loss: 2.8781\n",
      "Epoch [62/300], Step [7600/27733], Loss: 2.3596\n",
      "Epoch [62/300], Step [7700/27733], Loss: 2.3737\n",
      "Epoch [62/300], Step [7800/27733], Loss: 3.1157\n",
      "Epoch [62/300], Step [7900/27733], Loss: 2.5371\n",
      "Epoch [62/300], Step [8000/27733], Loss: 2.9276\n",
      "Epoch [62/300], Step [8100/27733], Loss: 2.2955\n",
      "Epoch [62/300], Step [8200/27733], Loss: 3.4084\n",
      "Epoch [62/300], Step [8300/27733], Loss: 3.2156\n",
      "Epoch [62/300], Step [8400/27733], Loss: 2.1820\n",
      "Epoch [62/300], Step [8500/27733], Loss: 1.8163\n",
      "Epoch [62/300], Step [8600/27733], Loss: 2.9431\n",
      "Epoch [62/300], Step [8700/27733], Loss: 2.7565\n",
      "Epoch [62/300], Step [8800/27733], Loss: 1.6210\n",
      "Epoch [62/300], Step [8900/27733], Loss: 2.1947\n",
      "Epoch [62/300], Step [9000/27733], Loss: 2.9321\n",
      "Epoch [62/300], Step [9100/27733], Loss: 2.7133\n",
      "Epoch [62/300], Step [9200/27733], Loss: 2.1818\n",
      "Epoch [62/300], Step [9300/27733], Loss: 2.2542\n",
      "Epoch [62/300], Step [9400/27733], Loss: 3.2181\n",
      "Epoch [62/300], Step [9500/27733], Loss: 1.3190\n",
      "Epoch [62/300], Step [9600/27733], Loss: 2.2464\n",
      "Epoch [62/300], Step [9700/27733], Loss: 2.7548\n",
      "Epoch [62/300], Step [9800/27733], Loss: 2.9791\n",
      "Epoch [62/300], Step [9900/27733], Loss: 2.7817\n",
      "Epoch [62/300], Step [10000/27733], Loss: 2.8941\n",
      "Epoch [62/300], Step [10100/27733], Loss: 3.0594\n",
      "Epoch [62/300], Step [10200/27733], Loss: 2.9615\n",
      "Epoch [62/300], Step [10300/27733], Loss: 2.2910\n",
      "Epoch [62/300], Step [10400/27733], Loss: 2.8309\n",
      "Epoch [62/300], Step [10500/27733], Loss: 2.6974\n",
      "Epoch [62/300], Step [10600/27733], Loss: 2.8578\n",
      "Epoch [62/300], Step [10700/27733], Loss: 2.9654\n",
      "Epoch [62/300], Step [10800/27733], Loss: 2.2950\n",
      "Epoch [62/300], Step [10900/27733], Loss: 2.3783\n",
      "Epoch [62/300], Step [11000/27733], Loss: 2.1280\n",
      "Epoch [62/300], Step [11100/27733], Loss: 2.8983\n",
      "Epoch [62/300], Step [11200/27733], Loss: 2.9981\n",
      "Epoch [62/300], Step [11300/27733], Loss: 2.0230\n",
      "Epoch [62/300], Step [11400/27733], Loss: 2.7063\n",
      "Epoch [62/300], Step [11500/27733], Loss: 2.2730\n",
      "Epoch [62/300], Step [11600/27733], Loss: 2.7110\n",
      "Epoch [62/300], Step [11700/27733], Loss: 2.5653\n",
      "Epoch [62/300], Step [11800/27733], Loss: 2.6956\n",
      "Epoch [62/300], Step [11900/27733], Loss: 2.6212\n",
      "Epoch [62/300], Step [12000/27733], Loss: 2.6985\n",
      "Epoch [62/300], Step [12100/27733], Loss: 2.8913\n",
      "Epoch [62/300], Step [12200/27733], Loss: 2.4389\n",
      "Epoch [62/300], Step [12300/27733], Loss: 2.3850\n",
      "Epoch [62/300], Step [12400/27733], Loss: 2.6716\n",
      "Epoch [62/300], Step [12500/27733], Loss: 2.2915\n",
      "Epoch [62/300], Step [12600/27733], Loss: 2.9158\n",
      "Epoch [62/300], Step [12700/27733], Loss: 2.1397\n",
      "Epoch [62/300], Step [12800/27733], Loss: 2.7529\n",
      "Epoch [62/300], Step [12900/27733], Loss: 2.5399\n",
      "Epoch [62/300], Step [13000/27733], Loss: 2.1937\n",
      "Epoch [62/300], Step [13100/27733], Loss: 2.4060\n",
      "Epoch [62/300], Step [13200/27733], Loss: 2.7217\n",
      "Epoch [62/300], Step [13300/27733], Loss: 2.6383\n",
      "Epoch [62/300], Step [13400/27733], Loss: 2.6685\n",
      "Epoch [62/300], Step [13500/27733], Loss: 3.7079\n",
      "Epoch [62/300], Step [13600/27733], Loss: 2.8768\n",
      "Epoch [62/300], Step [13700/27733], Loss: 2.6591\n",
      "Epoch [62/300], Step [13800/27733], Loss: 2.6649\n",
      "Epoch [62/300], Step [13900/27733], Loss: 2.7409\n",
      "Epoch [62/300], Step [14000/27733], Loss: 2.4444\n",
      "Epoch [62/300], Step [14100/27733], Loss: 2.8463\n",
      "Epoch [62/300], Step [14200/27733], Loss: 2.9188\n",
      "Epoch [62/300], Step [14300/27733], Loss: 3.4761\n",
      "Epoch [62/300], Step [14400/27733], Loss: 2.7593\n",
      "Epoch [62/300], Step [14500/27733], Loss: 2.6538\n",
      "Epoch [62/300], Step [14600/27733], Loss: 2.6206\n",
      "Epoch [62/300], Step [14700/27733], Loss: 3.2770\n",
      "Epoch [62/300], Step [14800/27733], Loss: 3.6325\n",
      "Epoch [62/300], Step [14900/27733], Loss: 2.4953\n",
      "Epoch [62/300], Step [15000/27733], Loss: 2.4273\n",
      "Epoch [62/300], Step [15100/27733], Loss: 2.6675\n",
      "Epoch [62/300], Step [15200/27733], Loss: 2.8711\n",
      "Epoch [62/300], Step [15300/27733], Loss: 3.2277\n",
      "Epoch [62/300], Step [15400/27733], Loss: 2.7698\n",
      "Epoch [62/300], Step [15500/27733], Loss: 3.6852\n",
      "Epoch [62/300], Step [15600/27733], Loss: 2.1328\n",
      "Epoch [62/300], Step [15700/27733], Loss: 2.5408\n",
      "Epoch [62/300], Step [15800/27733], Loss: 3.4119\n",
      "Epoch [62/300], Step [15900/27733], Loss: 2.2847\n",
      "Epoch [62/300], Step [16000/27733], Loss: 3.0002\n",
      "Epoch [62/300], Step [16100/27733], Loss: 2.4486\n",
      "Epoch [62/300], Step [16200/27733], Loss: 2.8107\n",
      "Epoch [62/300], Step [16300/27733], Loss: 2.7491\n",
      "Epoch [62/300], Step [16400/27733], Loss: 3.4049\n",
      "Epoch [62/300], Step [16500/27733], Loss: 2.4827\n",
      "Epoch [62/300], Step [16600/27733], Loss: 3.1142\n",
      "Epoch [62/300], Step [16700/27733], Loss: 3.1184\n",
      "Epoch [62/300], Step [16800/27733], Loss: 2.3381\n",
      "Epoch [62/300], Step [16900/27733], Loss: 3.1451\n",
      "Epoch [62/300], Step [17000/27733], Loss: 2.3735\n",
      "Epoch [62/300], Step [17100/27733], Loss: 3.5899\n",
      "Epoch [62/300], Step [17200/27733], Loss: 3.1909\n",
      "Epoch [62/300], Step [17300/27733], Loss: 2.1475\n",
      "Epoch [62/300], Step [17400/27733], Loss: 2.2766\n",
      "Epoch [62/300], Step [17500/27733], Loss: 2.5543\n",
      "Epoch [62/300], Step [17600/27733], Loss: 2.9838\n",
      "Epoch [62/300], Step [17700/27733], Loss: 3.3427\n",
      "Epoch [62/300], Step [17800/27733], Loss: 3.4674\n",
      "Epoch [62/300], Step [17900/27733], Loss: 1.7889\n",
      "Epoch [62/300], Step [18000/27733], Loss: 2.5146\n",
      "Epoch [62/300], Step [18100/27733], Loss: 2.8444\n",
      "Epoch [62/300], Step [18200/27733], Loss: 2.8326\n",
      "Epoch [62/300], Step [18300/27733], Loss: 2.9397\n",
      "Epoch [62/300], Step [18400/27733], Loss: 3.8455\n",
      "Epoch [62/300], Step [18500/27733], Loss: 2.8525\n",
      "Epoch [62/300], Step [18600/27733], Loss: 2.7408\n",
      "Epoch [62/300], Step [18700/27733], Loss: 2.5756\n",
      "Epoch [62/300], Step [18800/27733], Loss: 2.5902\n",
      "Epoch [62/300], Step [18900/27733], Loss: 2.9753\n",
      "Epoch [62/300], Step [19000/27733], Loss: 3.1101\n",
      "Epoch [62/300], Step [19100/27733], Loss: 2.6345\n",
      "Epoch [62/300], Step [19200/27733], Loss: 2.5292\n",
      "Epoch [62/300], Step [19300/27733], Loss: 2.3589\n",
      "Epoch [62/300], Step [19400/27733], Loss: 2.4986\n",
      "Epoch [62/300], Step [19500/27733], Loss: 2.6206\n",
      "Epoch [62/300], Step [19600/27733], Loss: 2.4503\n",
      "Epoch [62/300], Step [19700/27733], Loss: 2.8211\n",
      "Epoch [62/300], Step [19800/27733], Loss: 2.1166\n",
      "Epoch [62/300], Step [19900/27733], Loss: 3.5493\n",
      "Epoch [62/300], Step [20000/27733], Loss: 3.1487\n",
      "Epoch [62/300], Step [20100/27733], Loss: 3.7016\n",
      "Epoch [62/300], Step [20200/27733], Loss: 3.0883\n",
      "Epoch [62/300], Step [20300/27733], Loss: 2.4461\n",
      "Epoch [62/300], Step [20400/27733], Loss: 2.6593\n",
      "Epoch [62/300], Step [20500/27733], Loss: 2.9306\n",
      "Epoch [62/300], Step [20600/27733], Loss: 2.9252\n",
      "Epoch [62/300], Step [20700/27733], Loss: 2.3532\n",
      "Epoch [62/300], Step [20800/27733], Loss: 2.8540\n",
      "Epoch [62/300], Step [20900/27733], Loss: 2.4396\n",
      "Epoch [62/300], Step [21000/27733], Loss: 3.1181\n",
      "Epoch [62/300], Step [21100/27733], Loss: 2.9640\n",
      "Epoch [62/300], Step [21200/27733], Loss: 3.4138\n",
      "Epoch [62/300], Step [21300/27733], Loss: 2.7911\n",
      "Epoch [62/300], Step [21400/27733], Loss: 3.1019\n",
      "Epoch [62/300], Step [21500/27733], Loss: 2.9377\n",
      "Epoch [62/300], Step [21600/27733], Loss: 2.2734\n",
      "Epoch [62/300], Step [21700/27733], Loss: 3.0717\n",
      "Epoch [62/300], Step [21800/27733], Loss: 2.9432\n",
      "Epoch [62/300], Step [21900/27733], Loss: 2.5277\n",
      "Epoch [62/300], Step [22000/27733], Loss: 3.0674\n",
      "Epoch [62/300], Step [22100/27733], Loss: 3.8972\n",
      "Epoch [62/300], Step [22200/27733], Loss: 2.9153\n",
      "Epoch [62/300], Step [22300/27733], Loss: 2.8230\n",
      "Epoch [62/300], Step [22400/27733], Loss: 2.7614\n",
      "Epoch [62/300], Step [22500/27733], Loss: 2.5746\n",
      "Epoch [62/300], Step [22600/27733], Loss: 4.3607\n",
      "Epoch [62/300], Step [22700/27733], Loss: 3.8594\n",
      "Epoch [62/300], Step [22800/27733], Loss: 2.7535\n",
      "Epoch [62/300], Step [22900/27733], Loss: 2.6735\n",
      "Epoch [62/300], Step [23000/27733], Loss: 3.1068\n",
      "Epoch [62/300], Step [23100/27733], Loss: 3.6260\n",
      "Epoch [62/300], Step [23200/27733], Loss: 4.0864\n",
      "Epoch [62/300], Step [23300/27733], Loss: 2.7606\n",
      "Epoch [62/300], Step [23400/27733], Loss: 3.3847\n",
      "Epoch [62/300], Step [23500/27733], Loss: 3.1095\n",
      "Epoch [62/300], Step [23600/27733], Loss: 3.2180\n",
      "Epoch [62/300], Step [23700/27733], Loss: 3.1288\n",
      "Epoch [62/300], Step [23800/27733], Loss: 2.4884\n",
      "Epoch [62/300], Step [23900/27733], Loss: 3.0256\n",
      "Epoch [62/300], Step [24000/27733], Loss: 1.6631\n",
      "Epoch [62/300], Step [24100/27733], Loss: 3.8453\n",
      "Epoch [62/300], Step [24200/27733], Loss: 2.4036\n",
      "Epoch [62/300], Step [24300/27733], Loss: 3.3466\n",
      "Epoch [62/300], Step [24400/27733], Loss: 2.9749\n",
      "Epoch [62/300], Step [24500/27733], Loss: 2.9797\n",
      "Epoch [62/300], Step [24600/27733], Loss: 2.5463\n",
      "Epoch [62/300], Step [24700/27733], Loss: 2.9145\n",
      "Epoch [62/300], Step [24800/27733], Loss: 3.6656\n",
      "Epoch [62/300], Step [24900/27733], Loss: 3.0663\n",
      "Epoch [62/300], Step [25000/27733], Loss: 2.9276\n",
      "Epoch [62/300], Step [25100/27733], Loss: 2.0076\n",
      "Epoch [62/300], Step [25200/27733], Loss: 2.7883\n",
      "Epoch [62/300], Step [25300/27733], Loss: 2.9365\n",
      "Epoch [62/300], Step [25400/27733], Loss: 2.8998\n",
      "Epoch [62/300], Step [25500/27733], Loss: 3.1199\n",
      "Epoch [62/300], Step [25600/27733], Loss: 2.7122\n",
      "Epoch [62/300], Step [25700/27733], Loss: 2.8378\n",
      "Epoch [62/300], Step [25800/27733], Loss: 3.2926\n",
      "Epoch [62/300], Step [25900/27733], Loss: 2.8207\n",
      "Epoch [62/300], Step [26000/27733], Loss: 2.4556\n",
      "Epoch [62/300], Step [26100/27733], Loss: 2.5275\n",
      "Epoch [62/300], Step [26200/27733], Loss: 3.5255\n",
      "Epoch [62/300], Step [26300/27733], Loss: 3.4099\n",
      "Epoch [62/300], Step [26400/27733], Loss: 2.8074\n",
      "Epoch [62/300], Step [26500/27733], Loss: 3.0751\n",
      "Epoch [62/300], Step [26600/27733], Loss: 2.4375\n",
      "Epoch [62/300], Step [26700/27733], Loss: 2.7839\n",
      "Epoch [62/300], Step [26800/27733], Loss: 2.7428\n",
      "Epoch [62/300], Step [26900/27733], Loss: 2.5471\n",
      "Epoch [62/300], Step [27000/27733], Loss: 2.8998\n",
      "Epoch [62/300], Step [27100/27733], Loss: 2.9368\n",
      "Epoch [62/300], Step [27200/27733], Loss: 2.5612\n",
      "Epoch [62/300], Step [27300/27733], Loss: 2.6812\n",
      "Epoch [62/300], Step [27400/27733], Loss: 2.2363\n",
      "Epoch [62/300], Step [27500/27733], Loss: 2.5015\n",
      "Epoch [62/300], Step [27600/27733], Loss: 2.5345\n",
      "Epoch [62/300], Step [27700/27733], Loss: 3.0270\n",
      "Epoch [63/300], Step [100/27733], Loss: 2.4352\n",
      "Epoch [63/300], Step [200/27733], Loss: 2.2131\n",
      "Epoch [63/300], Step [300/27733], Loss: 2.1042\n",
      "Epoch [63/300], Step [400/27733], Loss: 2.4374\n",
      "Epoch [63/300], Step [500/27733], Loss: 2.4581\n",
      "Epoch [63/300], Step [600/27733], Loss: 1.9478\n",
      "Epoch [63/300], Step [700/27733], Loss: 2.0586\n",
      "Epoch [63/300], Step [800/27733], Loss: 1.8091\n",
      "Epoch [63/300], Step [900/27733], Loss: 3.0335\n",
      "Epoch [63/300], Step [1000/27733], Loss: 2.2339\n",
      "Epoch [63/300], Step [1100/27733], Loss: 2.6350\n",
      "Epoch [63/300], Step [1200/27733], Loss: 2.1146\n",
      "Epoch [63/300], Step [1300/27733], Loss: 2.1617\n",
      "Epoch [63/300], Step [1400/27733], Loss: 1.9142\n",
      "Epoch [63/300], Step [1500/27733], Loss: 2.3308\n",
      "Epoch [63/300], Step [1600/27733], Loss: 2.6970\n",
      "Epoch [63/300], Step [1700/27733], Loss: 2.1862\n",
      "Epoch [63/300], Step [1800/27733], Loss: 2.4331\n",
      "Epoch [63/300], Step [1900/27733], Loss: 3.2192\n",
      "Epoch [63/300], Step [2000/27733], Loss: 2.4002\n",
      "Epoch [63/300], Step [2100/27733], Loss: 2.7444\n",
      "Epoch [63/300], Step [2200/27733], Loss: 1.5687\n",
      "Epoch [63/300], Step [2300/27733], Loss: 2.7383\n",
      "Epoch [63/300], Step [2400/27733], Loss: 2.3230\n",
      "Epoch [63/300], Step [2500/27733], Loss: 3.1250\n",
      "Epoch [63/300], Step [2600/27733], Loss: 2.3126\n",
      "Epoch [63/300], Step [2700/27733], Loss: 2.4470\n",
      "Epoch [63/300], Step [2800/27733], Loss: 2.6331\n",
      "Epoch [63/300], Step [2900/27733], Loss: 1.3803\n",
      "Epoch [63/300], Step [3000/27733], Loss: 2.2205\n",
      "Epoch [63/300], Step [3100/27733], Loss: 2.8626\n",
      "Epoch [63/300], Step [3200/27733], Loss: 1.7982\n",
      "Epoch [63/300], Step [3300/27733], Loss: 2.4286\n",
      "Epoch [63/300], Step [3400/27733], Loss: 2.9456\n",
      "Epoch [63/300], Step [3500/27733], Loss: 2.0119\n",
      "Epoch [63/300], Step [3600/27733], Loss: 3.0717\n",
      "Epoch [63/300], Step [3700/27733], Loss: 2.6539\n",
      "Epoch [63/300], Step [3800/27733], Loss: 2.6651\n",
      "Epoch [63/300], Step [3900/27733], Loss: 2.8102\n",
      "Epoch [63/300], Step [4000/27733], Loss: 2.5598\n",
      "Epoch [63/300], Step [4100/27733], Loss: 2.4711\n",
      "Epoch [63/300], Step [4200/27733], Loss: 2.3308\n",
      "Epoch [63/300], Step [4300/27733], Loss: 3.3672\n",
      "Epoch [63/300], Step [4400/27733], Loss: 2.1053\n",
      "Epoch [63/300], Step [4500/27733], Loss: 2.2008\n",
      "Epoch [63/300], Step [4600/27733], Loss: 2.6225\n",
      "Epoch [63/300], Step [4700/27733], Loss: 2.9090\n",
      "Epoch [63/300], Step [4800/27733], Loss: 2.1313\n",
      "Epoch [63/300], Step [4900/27733], Loss: 2.4547\n",
      "Epoch [63/300], Step [5000/27733], Loss: 2.4057\n",
      "Epoch [63/300], Step [5100/27733], Loss: 2.7106\n",
      "Epoch [63/300], Step [5200/27733], Loss: 2.1399\n",
      "Epoch [63/300], Step [5300/27733], Loss: 2.0049\n",
      "Epoch [63/300], Step [5400/27733], Loss: 2.4692\n",
      "Epoch [63/300], Step [5500/27733], Loss: 2.4489\n",
      "Epoch [63/300], Step [5600/27733], Loss: 2.6023\n",
      "Epoch [63/300], Step [5700/27733], Loss: 2.6950\n",
      "Epoch [63/300], Step [5800/27733], Loss: 2.6281\n",
      "Epoch [63/300], Step [5900/27733], Loss: 2.5335\n",
      "Epoch [63/300], Step [6000/27733], Loss: 2.1835\n",
      "Epoch [63/300], Step [6100/27733], Loss: 2.7800\n",
      "Epoch [63/300], Step [6200/27733], Loss: 2.9875\n",
      "Epoch [63/300], Step [6300/27733], Loss: 2.5891\n",
      "Epoch [63/300], Step [6400/27733], Loss: 2.5577\n",
      "Epoch [63/300], Step [6500/27733], Loss: 2.7728\n",
      "Epoch [63/300], Step [6600/27733], Loss: 2.4663\n",
      "Epoch [63/300], Step [6700/27733], Loss: 2.5420\n",
      "Epoch [63/300], Step [6800/27733], Loss: 3.0204\n",
      "Epoch [63/300], Step [6900/27733], Loss: 2.1882\n",
      "Epoch [63/300], Step [7000/27733], Loss: 3.2039\n",
      "Epoch [63/300], Step [7100/27733], Loss: 3.0850\n",
      "Epoch [63/300], Step [7200/27733], Loss: 2.9027\n",
      "Epoch [63/300], Step [7300/27733], Loss: 2.8698\n",
      "Epoch [63/300], Step [7400/27733], Loss: 2.5924\n",
      "Epoch [63/300], Step [7500/27733], Loss: 2.1473\n",
      "Epoch [63/300], Step [7600/27733], Loss: 2.9063\n",
      "Epoch [63/300], Step [7700/27733], Loss: 2.9845\n",
      "Epoch [63/300], Step [7800/27733], Loss: 2.3411\n",
      "Epoch [63/300], Step [7900/27733], Loss: 2.8933\n",
      "Epoch [63/300], Step [8000/27733], Loss: 1.9977\n",
      "Epoch [63/300], Step [8100/27733], Loss: 2.3935\n",
      "Epoch [63/300], Step [8200/27733], Loss: 2.1522\n",
      "Epoch [63/300], Step [8300/27733], Loss: 3.0567\n",
      "Epoch [63/300], Step [8400/27733], Loss: 2.3295\n",
      "Epoch [63/300], Step [8500/27733], Loss: 3.4151\n",
      "Epoch [63/300], Step [8600/27733], Loss: 1.9403\n",
      "Epoch [63/300], Step [8700/27733], Loss: 2.4640\n",
      "Epoch [63/300], Step [8800/27733], Loss: 2.4432\n",
      "Epoch [63/300], Step [8900/27733], Loss: 4.0786\n",
      "Epoch [63/300], Step [9000/27733], Loss: 2.0152\n",
      "Epoch [63/300], Step [9100/27733], Loss: 2.6088\n",
      "Epoch [63/300], Step [9200/27733], Loss: 2.3318\n",
      "Epoch [63/300], Step [9300/27733], Loss: 2.4971\n",
      "Epoch [63/300], Step [9400/27733], Loss: 2.5755\n",
      "Epoch [63/300], Step [9500/27733], Loss: 2.8487\n",
      "Epoch [63/300], Step [9600/27733], Loss: 2.0194\n",
      "Epoch [63/300], Step [9700/27733], Loss: 3.1583\n",
      "Epoch [63/300], Step [9800/27733], Loss: 2.3215\n",
      "Epoch [63/300], Step [9900/27733], Loss: 1.3699\n",
      "Epoch [63/300], Step [10000/27733], Loss: 2.3275\n",
      "Epoch [63/300], Step [10100/27733], Loss: 2.4628\n",
      "Epoch [63/300], Step [10200/27733], Loss: 3.1273\n",
      "Epoch [63/300], Step [10300/27733], Loss: 2.8735\n",
      "Epoch [63/300], Step [10400/27733], Loss: 2.9688\n",
      "Epoch [63/300], Step [10500/27733], Loss: 3.2140\n",
      "Epoch [63/300], Step [10600/27733], Loss: 2.3545\n",
      "Epoch [63/300], Step [10700/27733], Loss: 4.0019\n",
      "Epoch [63/300], Step [10800/27733], Loss: 1.9703\n",
      "Epoch [63/300], Step [10900/27733], Loss: 3.0978\n",
      "Epoch [63/300], Step [11000/27733], Loss: 3.0453\n",
      "Epoch [63/300], Step [11100/27733], Loss: 2.8640\n",
      "Epoch [63/300], Step [11200/27733], Loss: 2.6465\n",
      "Epoch [63/300], Step [11300/27733], Loss: 2.1574\n",
      "Epoch [63/300], Step [11400/27733], Loss: 2.8610\n",
      "Epoch [63/300], Step [11500/27733], Loss: 2.6913\n",
      "Epoch [63/300], Step [11600/27733], Loss: 3.5225\n",
      "Epoch [63/300], Step [11700/27733], Loss: 2.3658\n",
      "Epoch [63/300], Step [11800/27733], Loss: 2.2531\n",
      "Epoch [63/300], Step [11900/27733], Loss: 2.4808\n",
      "Epoch [63/300], Step [12000/27733], Loss: 2.1609\n",
      "Epoch [63/300], Step [12100/27733], Loss: 2.6161\n",
      "Epoch [63/300], Step [12200/27733], Loss: 3.6228\n",
      "Epoch [63/300], Step [12300/27733], Loss: 2.7521\n",
      "Epoch [63/300], Step [12400/27733], Loss: 3.3820\n",
      "Epoch [63/300], Step [12500/27733], Loss: 2.5414\n",
      "Epoch [63/300], Step [12600/27733], Loss: 2.7783\n",
      "Epoch [63/300], Step [12700/27733], Loss: 2.4959\n",
      "Epoch [63/300], Step [12800/27733], Loss: 2.6124\n",
      "Epoch [63/300], Step [12900/27733], Loss: 2.7598\n",
      "Epoch [63/300], Step [13000/27733], Loss: 1.9315\n",
      "Epoch [63/300], Step [13100/27733], Loss: 2.1830\n",
      "Epoch [63/300], Step [13200/27733], Loss: 3.2968\n",
      "Epoch [63/300], Step [13300/27733], Loss: 3.1465\n",
      "Epoch [63/300], Step [13400/27733], Loss: 1.8750\n",
      "Epoch [63/300], Step [13500/27733], Loss: 3.0151\n",
      "Epoch [63/300], Step [13600/27733], Loss: 2.4754\n",
      "Epoch [63/300], Step [13700/27733], Loss: 3.2252\n",
      "Epoch [63/300], Step [13800/27733], Loss: 2.3337\n",
      "Epoch [63/300], Step [13900/27733], Loss: 2.7248\n",
      "Epoch [63/300], Step [14000/27733], Loss: 3.0069\n",
      "Epoch [63/300], Step [14100/27733], Loss: 3.1395\n",
      "Epoch [63/300], Step [14200/27733], Loss: 2.1419\n",
      "Epoch [63/300], Step [14300/27733], Loss: 3.0168\n",
      "Epoch [63/300], Step [14400/27733], Loss: 2.5652\n",
      "Epoch [63/300], Step [14500/27733], Loss: 2.7532\n",
      "Epoch [63/300], Step [14600/27733], Loss: 2.7997\n",
      "Epoch [63/300], Step [14700/27733], Loss: 2.2618\n",
      "Epoch [63/300], Step [14800/27733], Loss: 2.2058\n",
      "Epoch [63/300], Step [14900/27733], Loss: 3.2814\n",
      "Epoch [63/300], Step [15000/27733], Loss: 2.5194\n",
      "Epoch [63/300], Step [15100/27733], Loss: 2.5252\n",
      "Epoch [63/300], Step [15200/27733], Loss: 3.0882\n",
      "Epoch [63/300], Step [15300/27733], Loss: 2.0717\n",
      "Epoch [63/300], Step [15400/27733], Loss: 3.2023\n",
      "Epoch [63/300], Step [15500/27733], Loss: 3.0186\n",
      "Epoch [63/300], Step [15600/27733], Loss: 3.5789\n",
      "Epoch [63/300], Step [15700/27733], Loss: 3.9708\n",
      "Epoch [63/300], Step [15800/27733], Loss: 3.0069\n",
      "Epoch [63/300], Step [15900/27733], Loss: 2.6498\n",
      "Epoch [63/300], Step [16000/27733], Loss: 3.0454\n",
      "Epoch [63/300], Step [16100/27733], Loss: 3.0665\n",
      "Epoch [63/300], Step [16200/27733], Loss: 2.8304\n",
      "Epoch [63/300], Step [16300/27733], Loss: 2.5500\n",
      "Epoch [63/300], Step [16400/27733], Loss: 2.5491\n",
      "Epoch [63/300], Step [16500/27733], Loss: 2.8714\n",
      "Epoch [63/300], Step [16600/27733], Loss: 2.9061\n",
      "Epoch [63/300], Step [16700/27733], Loss: 3.2146\n",
      "Epoch [63/300], Step [16800/27733], Loss: 3.3947\n",
      "Epoch [63/300], Step [16900/27733], Loss: 2.1659\n",
      "Epoch [63/300], Step [17000/27733], Loss: 2.7304\n",
      "Epoch [63/300], Step [17100/27733], Loss: 1.8271\n",
      "Epoch [63/300], Step [17200/27733], Loss: 2.2968\n",
      "Epoch [63/300], Step [17300/27733], Loss: 3.0138\n",
      "Epoch [63/300], Step [17400/27733], Loss: 2.9391\n",
      "Epoch [63/300], Step [17500/27733], Loss: 2.3511\n",
      "Epoch [63/300], Step [17600/27733], Loss: 2.4721\n",
      "Epoch [63/300], Step [17700/27733], Loss: 2.8168\n",
      "Epoch [63/300], Step [17800/27733], Loss: 2.7115\n",
      "Epoch [63/300], Step [17900/27733], Loss: 3.7627\n",
      "Epoch [63/300], Step [18000/27733], Loss: 3.5552\n",
      "Epoch [63/300], Step [18100/27733], Loss: 2.2821\n",
      "Epoch [63/300], Step [18200/27733], Loss: 2.6116\n",
      "Epoch [63/300], Step [18300/27733], Loss: 3.5918\n",
      "Epoch [63/300], Step [18400/27733], Loss: 2.3527\n",
      "Epoch [63/300], Step [18500/27733], Loss: 2.6426\n",
      "Epoch [63/300], Step [18600/27733], Loss: 2.3333\n",
      "Epoch [63/300], Step [18700/27733], Loss: 3.0653\n",
      "Epoch [63/300], Step [18800/27733], Loss: 3.0410\n",
      "Epoch [63/300], Step [18900/27733], Loss: 3.4713\n",
      "Epoch [63/300], Step [19000/27733], Loss: 2.3663\n",
      "Epoch [63/300], Step [19100/27733], Loss: 3.6257\n",
      "Epoch [63/300], Step [19200/27733], Loss: 2.4863\n",
      "Epoch [63/300], Step [19300/27733], Loss: 3.0349\n",
      "Epoch [63/300], Step [19400/27733], Loss: 2.8320\n",
      "Epoch [63/300], Step [19500/27733], Loss: 2.4792\n",
      "Epoch [63/300], Step [19600/27733], Loss: 3.1727\n",
      "Epoch [63/300], Step [19700/27733], Loss: 3.0136\n",
      "Epoch [63/300], Step [19800/27733], Loss: 3.4006\n",
      "Epoch [63/300], Step [19900/27733], Loss: 2.8756\n",
      "Epoch [63/300], Step [20000/27733], Loss: 2.8687\n",
      "Epoch [63/300], Step [20100/27733], Loss: 3.1128\n",
      "Epoch [63/300], Step [20200/27733], Loss: 2.7084\n",
      "Epoch [63/300], Step [20300/27733], Loss: 2.8170\n",
      "Epoch [63/300], Step [20400/27733], Loss: 2.8669\n",
      "Epoch [63/300], Step [20500/27733], Loss: 2.9930\n",
      "Epoch [63/300], Step [20600/27733], Loss: 2.8556\n",
      "Epoch [63/300], Step [20700/27733], Loss: 2.7052\n",
      "Epoch [63/300], Step [20800/27733], Loss: 2.6805\n",
      "Epoch [63/300], Step [20900/27733], Loss: 3.0293\n",
      "Epoch [63/300], Step [21000/27733], Loss: 3.0336\n",
      "Epoch [63/300], Step [21100/27733], Loss: 3.1019\n",
      "Epoch [63/300], Step [21200/27733], Loss: 2.2430\n",
      "Epoch [63/300], Step [21300/27733], Loss: 3.1081\n",
      "Epoch [63/300], Step [21400/27733], Loss: 3.7765\n",
      "Epoch [63/300], Step [21500/27733], Loss: 2.1391\n",
      "Epoch [63/300], Step [21600/27733], Loss: 2.3543\n",
      "Epoch [63/300], Step [21700/27733], Loss: 3.0401\n",
      "Epoch [63/300], Step [21800/27733], Loss: 3.0351\n",
      "Epoch [63/300], Step [21900/27733], Loss: 2.8879\n",
      "Epoch [63/300], Step [22000/27733], Loss: 2.8100\n",
      "Epoch [63/300], Step [22100/27733], Loss: 2.7344\n",
      "Epoch [63/300], Step [22200/27733], Loss: 3.2273\n",
      "Epoch [63/300], Step [22300/27733], Loss: 2.1832\n",
      "Epoch [63/300], Step [22400/27733], Loss: 2.9623\n",
      "Epoch [63/300], Step [22500/27733], Loss: 2.2918\n",
      "Epoch [63/300], Step [22600/27733], Loss: 3.1167\n",
      "Epoch [63/300], Step [22700/27733], Loss: 2.2947\n",
      "Epoch [63/300], Step [22800/27733], Loss: 3.1513\n",
      "Epoch [63/300], Step [22900/27733], Loss: 2.6417\n",
      "Epoch [63/300], Step [23000/27733], Loss: 2.8436\n",
      "Epoch [63/300], Step [23100/27733], Loss: 2.8649\n",
      "Epoch [63/300], Step [23200/27733], Loss: 3.3120\n",
      "Epoch [63/300], Step [23300/27733], Loss: 2.7025\n",
      "Epoch [63/300], Step [23400/27733], Loss: 1.7987\n",
      "Epoch [63/300], Step [23500/27733], Loss: 3.1760\n",
      "Epoch [63/300], Step [23600/27733], Loss: 2.7593\n",
      "Epoch [63/300], Step [23700/27733], Loss: 3.0763\n",
      "Epoch [63/300], Step [23800/27733], Loss: 3.4037\n",
      "Epoch [63/300], Step [23900/27733], Loss: 2.2607\n",
      "Epoch [63/300], Step [24000/27733], Loss: 3.4946\n",
      "Epoch [63/300], Step [24100/27733], Loss: 2.3441\n",
      "Epoch [63/300], Step [24200/27733], Loss: 2.3781\n",
      "Epoch [63/300], Step [24300/27733], Loss: 2.4755\n",
      "Epoch [63/300], Step [24400/27733], Loss: 2.3380\n",
      "Epoch [63/300], Step [24500/27733], Loss: 3.3440\n",
      "Epoch [63/300], Step [24600/27733], Loss: 2.8191\n",
      "Epoch [63/300], Step [24700/27733], Loss: 2.7650\n",
      "Epoch [63/300], Step [24800/27733], Loss: 3.5107\n",
      "Epoch [63/300], Step [24900/27733], Loss: 3.3435\n",
      "Epoch [63/300], Step [25000/27733], Loss: 3.3568\n",
      "Epoch [63/300], Step [25100/27733], Loss: 2.9944\n",
      "Epoch [63/300], Step [25200/27733], Loss: 2.6675\n",
      "Epoch [63/300], Step [25300/27733], Loss: 3.5348\n",
      "Epoch [63/300], Step [25400/27733], Loss: 2.8507\n",
      "Epoch [63/300], Step [25500/27733], Loss: 2.5010\n",
      "Epoch [63/300], Step [25600/27733], Loss: 3.1049\n",
      "Epoch [63/300], Step [25700/27733], Loss: 3.3883\n",
      "Epoch [63/300], Step [25800/27733], Loss: 2.5627\n",
      "Epoch [63/300], Step [25900/27733], Loss: 3.2351\n",
      "Epoch [63/300], Step [26000/27733], Loss: 2.6801\n",
      "Epoch [63/300], Step [26100/27733], Loss: 2.9369\n",
      "Epoch [63/300], Step [26200/27733], Loss: 4.4072\n",
      "Epoch [63/300], Step [26300/27733], Loss: 2.9022\n",
      "Epoch [63/300], Step [26400/27733], Loss: 2.8536\n",
      "Epoch [63/300], Step [26500/27733], Loss: 3.3528\n",
      "Epoch [63/300], Step [26600/27733], Loss: 3.0383\n",
      "Epoch [63/300], Step [26700/27733], Loss: 3.6851\n",
      "Epoch [63/300], Step [26800/27733], Loss: 3.0920\n",
      "Epoch [63/300], Step [26900/27733], Loss: 3.2161\n",
      "Epoch [63/300], Step [27000/27733], Loss: 2.6230\n",
      "Epoch [63/300], Step [27100/27733], Loss: 3.1989\n",
      "Epoch [63/300], Step [27200/27733], Loss: 2.8731\n",
      "Epoch [63/300], Step [27300/27733], Loss: 3.5831\n",
      "Epoch [63/300], Step [27400/27733], Loss: 3.3664\n",
      "Epoch [63/300], Step [27500/27733], Loss: 2.7695\n",
      "Epoch [63/300], Step [27600/27733], Loss: 2.8719\n",
      "Epoch [63/300], Step [27700/27733], Loss: 3.1754\n",
      "Epoch [64/300], Step [100/27733], Loss: 2.1247\n",
      "Epoch [64/300], Step [200/27733], Loss: 2.5250\n",
      "Epoch [64/300], Step [300/27733], Loss: 2.3679\n",
      "Epoch [64/300], Step [400/27733], Loss: 2.4800\n",
      "Epoch [64/300], Step [500/27733], Loss: 1.9811\n",
      "Epoch [64/300], Step [600/27733], Loss: 2.2367\n",
      "Epoch [64/300], Step [700/27733], Loss: 2.1376\n",
      "Epoch [64/300], Step [800/27733], Loss: 2.6103\n",
      "Epoch [64/300], Step [900/27733], Loss: 2.2459\n",
      "Epoch [64/300], Step [1000/27733], Loss: 1.6319\n",
      "Epoch [64/300], Step [1100/27733], Loss: 2.4423\n",
      "Epoch [64/300], Step [1200/27733], Loss: 2.2517\n",
      "Epoch [64/300], Step [1300/27733], Loss: 2.0554\n",
      "Epoch [64/300], Step [1400/27733], Loss: 1.9464\n",
      "Epoch [64/300], Step [1500/27733], Loss: 2.4697\n",
      "Epoch [64/300], Step [1600/27733], Loss: 2.7388\n",
      "Epoch [64/300], Step [1700/27733], Loss: 2.2101\n",
      "Epoch [64/300], Step [1800/27733], Loss: 2.7317\n",
      "Epoch [64/300], Step [1900/27733], Loss: 2.1891\n",
      "Epoch [64/300], Step [2000/27733], Loss: 2.7982\n",
      "Epoch [64/300], Step [2100/27733], Loss: 2.0165\n",
      "Epoch [64/300], Step [2200/27733], Loss: 2.7541\n",
      "Epoch [64/300], Step [2300/27733], Loss: 2.0324\n",
      "Epoch [64/300], Step [2400/27733], Loss: 2.6604\n",
      "Epoch [64/300], Step [2500/27733], Loss: 2.7138\n",
      "Epoch [64/300], Step [2600/27733], Loss: 2.6632\n",
      "Epoch [64/300], Step [2700/27733], Loss: 1.7881\n",
      "Epoch [64/300], Step [2800/27733], Loss: 3.1085\n",
      "Epoch [64/300], Step [2900/27733], Loss: 1.8975\n",
      "Epoch [64/300], Step [3000/27733], Loss: 1.4689\n",
      "Epoch [64/300], Step [3100/27733], Loss: 2.0978\n",
      "Epoch [64/300], Step [3200/27733], Loss: 2.3400\n",
      "Epoch [64/300], Step [3300/27733], Loss: 2.2626\n",
      "Epoch [64/300], Step [3400/27733], Loss: 1.8224\n",
      "Epoch [64/300], Step [3500/27733], Loss: 1.6961\n",
      "Epoch [64/300], Step [3600/27733], Loss: 2.5340\n",
      "Epoch [64/300], Step [3700/27733], Loss: 2.5468\n",
      "Epoch [64/300], Step [3800/27733], Loss: 2.3702\n",
      "Epoch [64/300], Step [3900/27733], Loss: 2.0708\n",
      "Epoch [64/300], Step [4000/27733], Loss: 2.9157\n",
      "Epoch [64/300], Step [4100/27733], Loss: 2.0153\n",
      "Epoch [64/300], Step [4200/27733], Loss: 2.7162\n",
      "Epoch [64/300], Step [4300/27733], Loss: 2.7402\n",
      "Epoch [64/300], Step [4400/27733], Loss: 2.4903\n",
      "Epoch [64/300], Step [4500/27733], Loss: 3.0491\n",
      "Epoch [64/300], Step [4600/27733], Loss: 1.6736\n",
      "Epoch [64/300], Step [4700/27733], Loss: 3.6500\n",
      "Epoch [64/300], Step [4800/27733], Loss: 2.8970\n",
      "Epoch [64/300], Step [4900/27733], Loss: 2.1903\n",
      "Epoch [64/300], Step [5000/27733], Loss: 2.5614\n",
      "Epoch [64/300], Step [5100/27733], Loss: 2.2768\n",
      "Epoch [64/300], Step [5200/27733], Loss: 2.7275\n",
      "Epoch [64/300], Step [5300/27733], Loss: 2.7040\n",
      "Epoch [64/300], Step [5400/27733], Loss: 2.1538\n",
      "Epoch [64/300], Step [5500/27733], Loss: 2.5126\n",
      "Epoch [64/300], Step [5600/27733], Loss: 2.0643\n",
      "Epoch [64/300], Step [5700/27733], Loss: 2.3625\n",
      "Epoch [64/300], Step [5800/27733], Loss: 2.5759\n",
      "Epoch [64/300], Step [5900/27733], Loss: 3.2483\n",
      "Epoch [64/300], Step [6000/27733], Loss: 1.9717\n",
      "Epoch [64/300], Step [6100/27733], Loss: 2.4090\n",
      "Epoch [64/300], Step [6200/27733], Loss: 2.3917\n",
      "Epoch [64/300], Step [6300/27733], Loss: 2.0522\n",
      "Epoch [64/300], Step [6400/27733], Loss: 2.2023\n",
      "Epoch [64/300], Step [6500/27733], Loss: 2.8806\n",
      "Epoch [64/300], Step [6600/27733], Loss: 2.1931\n",
      "Epoch [64/300], Step [6700/27733], Loss: 2.7524\n",
      "Epoch [64/300], Step [6800/27733], Loss: 2.1378\n",
      "Epoch [64/300], Step [6900/27733], Loss: 3.1119\n",
      "Epoch [64/300], Step [7000/27733], Loss: 2.4503\n",
      "Epoch [64/300], Step [7100/27733], Loss: 2.6238\n",
      "Epoch [64/300], Step [7200/27733], Loss: 2.7232\n",
      "Epoch [64/300], Step [7300/27733], Loss: 2.4859\n",
      "Epoch [64/300], Step [7400/27733], Loss: 3.0456\n",
      "Epoch [64/300], Step [7500/27733], Loss: 2.5179\n",
      "Epoch [64/300], Step [7600/27733], Loss: 2.4162\n",
      "Epoch [64/300], Step [7700/27733], Loss: 2.3280\n",
      "Epoch [64/300], Step [7800/27733], Loss: 2.7345\n",
      "Epoch [64/300], Step [7900/27733], Loss: 3.1084\n",
      "Epoch [64/300], Step [8000/27733], Loss: 2.4017\n",
      "Epoch [64/300], Step [8100/27733], Loss: 2.6784\n",
      "Epoch [64/300], Step [8200/27733], Loss: 2.2845\n",
      "Epoch [64/300], Step [8300/27733], Loss: 2.6240\n",
      "Epoch [64/300], Step [8400/27733], Loss: 3.1726\n",
      "Epoch [64/300], Step [8500/27733], Loss: 3.4558\n",
      "Epoch [64/300], Step [8600/27733], Loss: 2.5880\n",
      "Epoch [64/300], Step [8700/27733], Loss: 1.7353\n",
      "Epoch [64/300], Step [8800/27733], Loss: 2.1837\n",
      "Epoch [64/300], Step [8900/27733], Loss: 3.2258\n",
      "Epoch [64/300], Step [9000/27733], Loss: 2.1283\n",
      "Epoch [64/300], Step [9100/27733], Loss: 2.8794\n",
      "Epoch [64/300], Step [9200/27733], Loss: 2.9373\n",
      "Epoch [64/300], Step [9300/27733], Loss: 2.7975\n",
      "Epoch [64/300], Step [9400/27733], Loss: 2.8855\n",
      "Epoch [64/300], Step [9500/27733], Loss: 2.1562\n",
      "Epoch [64/300], Step [9600/27733], Loss: 2.0978\n",
      "Epoch [64/300], Step [9700/27733], Loss: 3.1071\n",
      "Epoch [64/300], Step [9800/27733], Loss: 2.8902\n",
      "Epoch [64/300], Step [9900/27733], Loss: 3.2592\n",
      "Epoch [64/300], Step [10000/27733], Loss: 1.9727\n",
      "Epoch [64/300], Step [10100/27733], Loss: 2.1932\n",
      "Epoch [64/300], Step [10200/27733], Loss: 2.7931\n",
      "Epoch [64/300], Step [10300/27733], Loss: 2.6674\n",
      "Epoch [64/300], Step [10400/27733], Loss: 2.0926\n",
      "Epoch [64/300], Step [10500/27733], Loss: 3.5700\n",
      "Epoch [64/300], Step [10600/27733], Loss: 2.2979\n",
      "Epoch [64/300], Step [10700/27733], Loss: 2.5027\n",
      "Epoch [64/300], Step [10800/27733], Loss: 2.7131\n",
      "Epoch [64/300], Step [10900/27733], Loss: 2.5351\n",
      "Epoch [64/300], Step [11000/27733], Loss: 2.8580\n",
      "Epoch [64/300], Step [11100/27733], Loss: 2.6882\n",
      "Epoch [64/300], Step [11200/27733], Loss: 2.4698\n",
      "Epoch [64/300], Step [11300/27733], Loss: 2.6948\n",
      "Epoch [64/300], Step [11400/27733], Loss: 2.9772\n",
      "Epoch [64/300], Step [11500/27733], Loss: 3.0237\n",
      "Epoch [64/300], Step [11600/27733], Loss: 3.1703\n",
      "Epoch [64/300], Step [11700/27733], Loss: 2.3915\n",
      "Epoch [64/300], Step [11800/27733], Loss: 2.2588\n",
      "Epoch [64/300], Step [11900/27733], Loss: 2.5399\n",
      "Epoch [64/300], Step [12000/27733], Loss: 3.0637\n",
      "Epoch [64/300], Step [12100/27733], Loss: 3.1709\n",
      "Epoch [64/300], Step [12200/27733], Loss: 3.6327\n",
      "Epoch [64/300], Step [12300/27733], Loss: 2.6731\n",
      "Epoch [64/300], Step [12400/27733], Loss: 3.1287\n",
      "Epoch [64/300], Step [12500/27733], Loss: 2.1459\n",
      "Epoch [64/300], Step [12600/27733], Loss: 2.8978\n",
      "Epoch [64/300], Step [12700/27733], Loss: 2.6645\n",
      "Epoch [64/300], Step [12800/27733], Loss: 2.4370\n",
      "Epoch [64/300], Step [12900/27733], Loss: 2.8321\n",
      "Epoch [64/300], Step [13000/27733], Loss: 2.1085\n",
      "Epoch [64/300], Step [13100/27733], Loss: 2.9444\n",
      "Epoch [64/300], Step [13200/27733], Loss: 2.3483\n",
      "Epoch [64/300], Step [13300/27733], Loss: 2.9079\n",
      "Epoch [64/300], Step [13400/27733], Loss: 2.9824\n",
      "Epoch [64/300], Step [13500/27733], Loss: 2.3234\n",
      "Epoch [64/300], Step [13600/27733], Loss: 3.0172\n",
      "Epoch [64/300], Step [13700/27733], Loss: 2.6253\n",
      "Epoch [64/300], Step [13800/27733], Loss: 2.9051\n",
      "Epoch [64/300], Step [13900/27733], Loss: 2.8810\n",
      "Epoch [64/300], Step [14000/27733], Loss: 2.4914\n",
      "Epoch [64/300], Step [14100/27733], Loss: 2.8330\n",
      "Epoch [64/300], Step [14200/27733], Loss: 2.7779\n",
      "Epoch [64/300], Step [14300/27733], Loss: 2.9148\n",
      "Epoch [64/300], Step [14400/27733], Loss: 1.7772\n",
      "Epoch [64/300], Step [14500/27733], Loss: 2.8060\n",
      "Epoch [64/300], Step [14600/27733], Loss: 2.2305\n",
      "Epoch [64/300], Step [14700/27733], Loss: 2.3144\n",
      "Epoch [64/300], Step [14800/27733], Loss: 2.9288\n",
      "Epoch [64/300], Step [14900/27733], Loss: 2.9893\n",
      "Epoch [64/300], Step [15000/27733], Loss: 2.7973\n",
      "Epoch [64/300], Step [15100/27733], Loss: 2.2286\n",
      "Epoch [64/300], Step [15200/27733], Loss: 1.8446\n",
      "Epoch [64/300], Step [15300/27733], Loss: 3.5971\n",
      "Epoch [64/300], Step [15400/27733], Loss: 3.3785\n",
      "Epoch [64/300], Step [15500/27733], Loss: 2.3153\n",
      "Epoch [64/300], Step [15600/27733], Loss: 2.4046\n",
      "Epoch [64/300], Step [15700/27733], Loss: 2.8963\n",
      "Epoch [64/300], Step [15800/27733], Loss: 2.4490\n",
      "Epoch [64/300], Step [15900/27733], Loss: 2.1642\n",
      "Epoch [64/300], Step [16000/27733], Loss: 2.2091\n",
      "Epoch [64/300], Step [16100/27733], Loss: 2.9965\n",
      "Epoch [64/300], Step [16200/27733], Loss: 2.4765\n",
      "Epoch [64/300], Step [16300/27733], Loss: 1.9598\n",
      "Epoch [64/300], Step [16400/27733], Loss: 2.7996\n",
      "Epoch [64/300], Step [16500/27733], Loss: 2.8782\n",
      "Epoch [64/300], Step [16600/27733], Loss: 3.2638\n",
      "Epoch [64/300], Step [16700/27733], Loss: 2.3448\n",
      "Epoch [64/300], Step [16800/27733], Loss: 3.2012\n",
      "Epoch [64/300], Step [16900/27733], Loss: 2.1137\n",
      "Epoch [64/300], Step [17000/27733], Loss: 3.3682\n",
      "Epoch [64/300], Step [17100/27733], Loss: 3.4469\n",
      "Epoch [64/300], Step [17200/27733], Loss: 2.9771\n",
      "Epoch [64/300], Step [17300/27733], Loss: 3.7088\n",
      "Epoch [64/300], Step [17400/27733], Loss: 2.5720\n",
      "Epoch [64/300], Step [17500/27733], Loss: 2.4140\n",
      "Epoch [64/300], Step [17600/27733], Loss: 2.7895\n",
      "Epoch [64/300], Step [17700/27733], Loss: 2.8965\n",
      "Epoch [64/300], Step [17800/27733], Loss: 2.9805\n",
      "Epoch [64/300], Step [17900/27733], Loss: 2.8578\n",
      "Epoch [64/300], Step [18000/27733], Loss: 2.4547\n",
      "Epoch [64/300], Step [18100/27733], Loss: 1.9579\n",
      "Epoch [64/300], Step [18200/27733], Loss: 2.1108\n",
      "Epoch [64/300], Step [18300/27733], Loss: 2.6063\n",
      "Epoch [64/300], Step [18400/27733], Loss: 2.9648\n",
      "Epoch [64/300], Step [18500/27733], Loss: 2.3038\n",
      "Epoch [64/300], Step [18600/27733], Loss: 2.7903\n",
      "Epoch [64/300], Step [18700/27733], Loss: 2.2069\n",
      "Epoch [64/300], Step [18800/27733], Loss: 2.2858\n",
      "Epoch [64/300], Step [18900/27733], Loss: 2.6641\n",
      "Epoch [64/300], Step [19000/27733], Loss: 2.6940\n",
      "Epoch [64/300], Step [19100/27733], Loss: 2.9719\n",
      "Epoch [64/300], Step [19200/27733], Loss: 3.2650\n",
      "Epoch [64/300], Step [19300/27733], Loss: 3.0162\n",
      "Epoch [64/300], Step [19400/27733], Loss: 2.7829\n",
      "Epoch [64/300], Step [19500/27733], Loss: 2.2482\n",
      "Epoch [64/300], Step [19600/27733], Loss: 3.4850\n",
      "Epoch [64/300], Step [19700/27733], Loss: 2.3827\n",
      "Epoch [64/300], Step [19800/27733], Loss: 3.0079\n",
      "Epoch [64/300], Step [19900/27733], Loss: 2.3776\n",
      "Epoch [64/300], Step [20000/27733], Loss: 3.6543\n",
      "Epoch [64/300], Step [20100/27733], Loss: 3.8846\n",
      "Epoch [64/300], Step [20200/27733], Loss: 3.7772\n",
      "Epoch [64/300], Step [20300/27733], Loss: 2.6167\n",
      "Epoch [64/300], Step [20400/27733], Loss: 2.5430\n",
      "Epoch [64/300], Step [20500/27733], Loss: 2.7104\n",
      "Epoch [64/300], Step [20600/27733], Loss: 2.5510\n",
      "Epoch [64/300], Step [20700/27733], Loss: 2.7753\n",
      "Epoch [64/300], Step [20800/27733], Loss: 2.7081\n",
      "Epoch [64/300], Step [20900/27733], Loss: 2.8553\n",
      "Epoch [64/300], Step [21000/27733], Loss: 2.8711\n",
      "Epoch [64/300], Step [21100/27733], Loss: 3.8095\n",
      "Epoch [64/300], Step [21200/27733], Loss: 2.4407\n",
      "Epoch [64/300], Step [21300/27733], Loss: 2.4525\n",
      "Epoch [64/300], Step [21400/27733], Loss: 3.2051\n",
      "Epoch [64/300], Step [21500/27733], Loss: 3.0647\n",
      "Epoch [64/300], Step [21600/27733], Loss: 3.5025\n",
      "Epoch [64/300], Step [21700/27733], Loss: 3.3442\n",
      "Epoch [64/300], Step [21800/27733], Loss: 2.1037\n",
      "Epoch [64/300], Step [21900/27733], Loss: 2.7418\n",
      "Epoch [64/300], Step [22000/27733], Loss: 2.7220\n",
      "Epoch [64/300], Step [22100/27733], Loss: 2.1818\n",
      "Epoch [64/300], Step [22200/27733], Loss: 2.5052\n",
      "Epoch [64/300], Step [22300/27733], Loss: 2.4855\n",
      "Epoch [64/300], Step [22400/27733], Loss: 3.2188\n",
      "Epoch [64/300], Step [22500/27733], Loss: 3.1539\n",
      "Epoch [64/300], Step [22600/27733], Loss: 2.7706\n",
      "Epoch [64/300], Step [22700/27733], Loss: 2.4827\n",
      "Epoch [64/300], Step [22800/27733], Loss: 3.0956\n",
      "Epoch [64/300], Step [22900/27733], Loss: 3.1565\n",
      "Epoch [64/300], Step [23000/27733], Loss: 3.4753\n",
      "Epoch [64/300], Step [23100/27733], Loss: 2.6302\n",
      "Epoch [64/300], Step [23200/27733], Loss: 3.2927\n",
      "Epoch [64/300], Step [23300/27733], Loss: 3.6740\n",
      "Epoch [64/300], Step [23400/27733], Loss: 2.7301\n",
      "Epoch [64/300], Step [23500/27733], Loss: 2.8996\n",
      "Epoch [64/300], Step [23600/27733], Loss: 2.8003\n",
      "Epoch [64/300], Step [23700/27733], Loss: 3.2332\n",
      "Epoch [64/300], Step [23800/27733], Loss: 3.0839\n",
      "Epoch [64/300], Step [23900/27733], Loss: 3.9720\n",
      "Epoch [64/300], Step [24000/27733], Loss: 3.0391\n",
      "Epoch [64/300], Step [24100/27733], Loss: 3.2435\n",
      "Epoch [64/300], Step [24200/27733], Loss: 2.5647\n",
      "Epoch [64/300], Step [24300/27733], Loss: 2.7906\n",
      "Epoch [64/300], Step [24400/27733], Loss: 2.7570\n",
      "Epoch [64/300], Step [24500/27733], Loss: 3.0348\n",
      "Epoch [64/300], Step [24600/27733], Loss: 2.6334\n",
      "Epoch [64/300], Step [24700/27733], Loss: 2.9343\n",
      "Epoch [64/300], Step [24800/27733], Loss: 2.4504\n",
      "Epoch [64/300], Step [24900/27733], Loss: 3.1158\n",
      "Epoch [64/300], Step [25000/27733], Loss: 2.5050\n",
      "Epoch [64/300], Step [25100/27733], Loss: 2.9177\n",
      "Epoch [64/300], Step [25200/27733], Loss: 3.8482\n",
      "Epoch [64/300], Step [25300/27733], Loss: 3.0886\n",
      "Epoch [64/300], Step [25400/27733], Loss: 2.6374\n",
      "Epoch [64/300], Step [25500/27733], Loss: 2.6355\n",
      "Epoch [64/300], Step [25600/27733], Loss: 2.5703\n",
      "Epoch [64/300], Step [25700/27733], Loss: 3.1812\n",
      "Epoch [64/300], Step [25800/27733], Loss: 3.3616\n",
      "Epoch [64/300], Step [25900/27733], Loss: 2.7710\n",
      "Epoch [64/300], Step [26000/27733], Loss: 2.3646\n",
      "Epoch [64/300], Step [26100/27733], Loss: 2.7467\n",
      "Epoch [64/300], Step [26200/27733], Loss: 3.8261\n",
      "Epoch [64/300], Step [26300/27733], Loss: 3.1444\n",
      "Epoch [64/300], Step [26400/27733], Loss: 2.8537\n",
      "Epoch [64/300], Step [26500/27733], Loss: 2.7515\n",
      "Epoch [64/300], Step [26600/27733], Loss: 2.9736\n",
      "Epoch [64/300], Step [26700/27733], Loss: 2.4506\n",
      "Epoch [64/300], Step [26800/27733], Loss: 3.2977\n",
      "Epoch [64/300], Step [26900/27733], Loss: 2.5154\n",
      "Epoch [64/300], Step [27000/27733], Loss: 3.6189\n",
      "Epoch [64/300], Step [27100/27733], Loss: 3.0001\n",
      "Epoch [64/300], Step [27200/27733], Loss: 2.9517\n",
      "Epoch [64/300], Step [27300/27733], Loss: 3.1348\n",
      "Epoch [64/300], Step [27400/27733], Loss: 2.1647\n",
      "Epoch [64/300], Step [27500/27733], Loss: 3.0034\n",
      "Epoch [64/300], Step [27600/27733], Loss: 2.4935\n",
      "Epoch [64/300], Step [27700/27733], Loss: 3.3900\n",
      "Epoch [65/300], Step [100/27733], Loss: 3.0582\n",
      "Epoch [65/300], Step [200/27733], Loss: 1.8181\n",
      "Epoch [65/300], Step [300/27733], Loss: 3.1657\n",
      "Epoch [65/300], Step [400/27733], Loss: 2.9288\n",
      "Epoch [65/300], Step [500/27733], Loss: 2.3041\n",
      "Epoch [65/300], Step [600/27733], Loss: 2.1506\n",
      "Epoch [65/300], Step [700/27733], Loss: 1.6992\n",
      "Epoch [65/300], Step [800/27733], Loss: 2.0180\n",
      "Epoch [65/300], Step [900/27733], Loss: 2.3418\n",
      "Epoch [65/300], Step [1000/27733], Loss: 2.0512\n",
      "Epoch [65/300], Step [1100/27733], Loss: 2.5146\n",
      "Epoch [65/300], Step [1200/27733], Loss: 2.1657\n",
      "Epoch [65/300], Step [1300/27733], Loss: 2.7734\n",
      "Epoch [65/300], Step [1400/27733], Loss: 1.7299\n",
      "Epoch [65/300], Step [1500/27733], Loss: 2.1352\n",
      "Epoch [65/300], Step [1600/27733], Loss: 2.5977\n",
      "Epoch [65/300], Step [1700/27733], Loss: 2.1047\n",
      "Epoch [65/300], Step [1800/27733], Loss: 1.8760\n",
      "Epoch [65/300], Step [1900/27733], Loss: 2.4567\n",
      "Epoch [65/300], Step [2000/27733], Loss: 1.6619\n",
      "Epoch [65/300], Step [2100/27733], Loss: 2.1622\n",
      "Epoch [65/300], Step [2200/27733], Loss: 1.9457\n",
      "Epoch [65/300], Step [2300/27733], Loss: 2.5312\n",
      "Epoch [65/300], Step [2400/27733], Loss: 1.5539\n",
      "Epoch [65/300], Step [2500/27733], Loss: 2.4778\n",
      "Epoch [65/300], Step [2600/27733], Loss: 2.0334\n",
      "Epoch [65/300], Step [2700/27733], Loss: 3.1058\n",
      "Epoch [65/300], Step [2800/27733], Loss: 2.6668\n",
      "Epoch [65/300], Step [2900/27733], Loss: 2.0787\n",
      "Epoch [65/300], Step [3000/27733], Loss: 1.6237\n",
      "Epoch [65/300], Step [3100/27733], Loss: 2.3944\n",
      "Epoch [65/300], Step [3200/27733], Loss: 2.1644\n",
      "Epoch [65/300], Step [3300/27733], Loss: 1.8988\n",
      "Epoch [65/300], Step [3400/27733], Loss: 2.5199\n",
      "Epoch [65/300], Step [3500/27733], Loss: 2.1840\n",
      "Epoch [65/300], Step [3600/27733], Loss: 2.5229\n",
      "Epoch [65/300], Step [3700/27733], Loss: 2.1018\n",
      "Epoch [65/300], Step [3800/27733], Loss: 1.7449\n",
      "Epoch [65/300], Step [3900/27733], Loss: 2.6239\n",
      "Epoch [65/300], Step [4000/27733], Loss: 2.5656\n",
      "Epoch [65/300], Step [4100/27733], Loss: 2.1847\n",
      "Epoch [65/300], Step [4200/27733], Loss: 2.5045\n",
      "Epoch [65/300], Step [4300/27733], Loss: 2.6881\n",
      "Epoch [65/300], Step [4400/27733], Loss: 2.5178\n",
      "Epoch [65/300], Step [4500/27733], Loss: 2.2939\n",
      "Epoch [65/300], Step [4600/27733], Loss: 1.9977\n",
      "Epoch [65/300], Step [4700/27733], Loss: 2.3664\n",
      "Epoch [65/300], Step [4800/27733], Loss: 2.7430\n",
      "Epoch [65/300], Step [4900/27733], Loss: 2.2490\n",
      "Epoch [65/300], Step [5000/27733], Loss: 3.2086\n",
      "Epoch [65/300], Step [5100/27733], Loss: 2.6549\n",
      "Epoch [65/300], Step [5200/27733], Loss: 1.9712\n",
      "Epoch [65/300], Step [5300/27733], Loss: 2.2837\n",
      "Epoch [65/300], Step [5400/27733], Loss: 2.7982\n",
      "Epoch [65/300], Step [5500/27733], Loss: 2.2923\n",
      "Epoch [65/300], Step [5600/27733], Loss: 3.8176\n",
      "Epoch [65/300], Step [5700/27733], Loss: 1.9335\n",
      "Epoch [65/300], Step [5800/27733], Loss: 2.0427\n",
      "Epoch [65/300], Step [5900/27733], Loss: 2.4637\n",
      "Epoch [65/300], Step [6000/27733], Loss: 2.6742\n",
      "Epoch [65/300], Step [6100/27733], Loss: 2.5736\n",
      "Epoch [65/300], Step [6200/27733], Loss: 1.9341\n",
      "Epoch [65/300], Step [6300/27733], Loss: 1.7771\n",
      "Epoch [65/300], Step [6400/27733], Loss: 3.2640\n",
      "Epoch [65/300], Step [6500/27733], Loss: 2.8009\n",
      "Epoch [65/300], Step [6600/27733], Loss: 1.6822\n",
      "Epoch [65/300], Step [6700/27733], Loss: 3.1519\n",
      "Epoch [65/300], Step [6800/27733], Loss: 2.9036\n",
      "Epoch [65/300], Step [6900/27733], Loss: 2.6072\n",
      "Epoch [65/300], Step [7000/27733], Loss: 2.2087\n",
      "Epoch [65/300], Step [7100/27733], Loss: 2.5806\n",
      "Epoch [65/300], Step [7200/27733], Loss: 2.3626\n",
      "Epoch [65/300], Step [7300/27733], Loss: 2.5274\n",
      "Epoch [65/300], Step [7400/27733], Loss: 2.0303\n",
      "Epoch [65/300], Step [7500/27733], Loss: 2.2218\n",
      "Epoch [65/300], Step [7600/27733], Loss: 2.4018\n",
      "Epoch [65/300], Step [7700/27733], Loss: 1.3676\n",
      "Epoch [65/300], Step [7800/27733], Loss: 2.2271\n",
      "Epoch [65/300], Step [7900/27733], Loss: 2.4526\n",
      "Epoch [65/300], Step [8000/27733], Loss: 2.6993\n",
      "Epoch [65/300], Step [8100/27733], Loss: 2.1455\n",
      "Epoch [65/300], Step [8200/27733], Loss: 2.3631\n",
      "Epoch [65/300], Step [8300/27733], Loss: 2.3782\n",
      "Epoch [65/300], Step [8400/27733], Loss: 2.0702\n",
      "Epoch [65/300], Step [8500/27733], Loss: 2.5050\n",
      "Epoch [65/300], Step [8600/27733], Loss: 2.1983\n",
      "Epoch [65/300], Step [8700/27733], Loss: 2.4665\n",
      "Epoch [65/300], Step [8800/27733], Loss: 3.1943\n",
      "Epoch [65/300], Step [8900/27733], Loss: 2.5109\n",
      "Epoch [65/300], Step [9000/27733], Loss: 3.1525\n",
      "Epoch [65/300], Step [9100/27733], Loss: 2.6881\n",
      "Epoch [65/300], Step [9200/27733], Loss: 2.6034\n",
      "Epoch [65/300], Step [9300/27733], Loss: 2.5713\n",
      "Epoch [65/300], Step [9400/27733], Loss: 1.8910\n",
      "Epoch [65/300], Step [9500/27733], Loss: 2.5051\n",
      "Epoch [65/300], Step [9600/27733], Loss: 2.8805\n",
      "Epoch [65/300], Step [9700/27733], Loss: 2.5649\n",
      "Epoch [65/300], Step [9800/27733], Loss: 3.6498\n",
      "Epoch [65/300], Step [9900/27733], Loss: 2.8164\n",
      "Epoch [65/300], Step [10000/27733], Loss: 2.8194\n",
      "Epoch [65/300], Step [10100/27733], Loss: 2.1356\n",
      "Epoch [65/300], Step [10200/27733], Loss: 2.2131\n",
      "Epoch [65/300], Step [10300/27733], Loss: 2.2860\n",
      "Epoch [65/300], Step [10400/27733], Loss: 2.3873\n",
      "Epoch [65/300], Step [10500/27733], Loss: 2.5974\n",
      "Epoch [65/300], Step [10600/27733], Loss: 2.0597\n",
      "Epoch [65/300], Step [10700/27733], Loss: 2.7357\n",
      "Epoch [65/300], Step [10800/27733], Loss: 2.9682\n",
      "Epoch [65/300], Step [10900/27733], Loss: 2.6152\n",
      "Epoch [65/300], Step [11000/27733], Loss: 2.5075\n",
      "Epoch [65/300], Step [11100/27733], Loss: 3.1866\n",
      "Epoch [65/300], Step [11200/27733], Loss: 2.6006\n",
      "Epoch [65/300], Step [11300/27733], Loss: 2.2751\n",
      "Epoch [65/300], Step [11400/27733], Loss: 3.4922\n",
      "Epoch [65/300], Step [11500/27733], Loss: 3.1941\n",
      "Epoch [65/300], Step [11600/27733], Loss: 3.0231\n",
      "Epoch [65/300], Step [11700/27733], Loss: 2.7980\n",
      "Epoch [65/300], Step [11800/27733], Loss: 2.5996\n",
      "Epoch [65/300], Step [11900/27733], Loss: 3.0316\n",
      "Epoch [65/300], Step [12000/27733], Loss: 1.7320\n",
      "Epoch [65/300], Step [12100/27733], Loss: 2.0615\n",
      "Epoch [65/300], Step [12200/27733], Loss: 3.4281\n",
      "Epoch [65/300], Step [12300/27733], Loss: 2.4339\n",
      "Epoch [65/300], Step [12400/27733], Loss: 2.4493\n",
      "Epoch [65/300], Step [12500/27733], Loss: 3.4039\n",
      "Epoch [65/300], Step [12600/27733], Loss: 2.8849\n",
      "Epoch [65/300], Step [12700/27733], Loss: 2.7627\n",
      "Epoch [65/300], Step [12800/27733], Loss: 3.1957\n",
      "Epoch [65/300], Step [12900/27733], Loss: 3.0153\n",
      "Epoch [65/300], Step [13000/27733], Loss: 2.4852\n",
      "Epoch [65/300], Step [13100/27733], Loss: 2.6420\n",
      "Epoch [65/300], Step [13200/27733], Loss: 3.7877\n",
      "Epoch [65/300], Step [13300/27733], Loss: 2.4857\n",
      "Epoch [65/300], Step [13400/27733], Loss: 2.9144\n",
      "Epoch [65/300], Step [13500/27733], Loss: 3.4817\n",
      "Epoch [65/300], Step [13600/27733], Loss: 3.0912\n",
      "Epoch [65/300], Step [13700/27733], Loss: 2.7049\n",
      "Epoch [65/300], Step [13800/27733], Loss: 2.7755\n",
      "Epoch [65/300], Step [13900/27733], Loss: 2.0752\n",
      "Epoch [65/300], Step [14000/27733], Loss: 2.6627\n",
      "Epoch [65/300], Step [14100/27733], Loss: 2.1661\n",
      "Epoch [65/300], Step [14200/27733], Loss: 2.5043\n",
      "Epoch [65/300], Step [14300/27733], Loss: 2.6295\n",
      "Epoch [65/300], Step [14400/27733], Loss: 2.7587\n",
      "Epoch [65/300], Step [14500/27733], Loss: 2.6643\n",
      "Epoch [65/300], Step [14600/27733], Loss: 3.1496\n",
      "Epoch [65/300], Step [14700/27733], Loss: 2.8937\n",
      "Epoch [65/300], Step [14800/27733], Loss: 2.5359\n",
      "Epoch [65/300], Step [14900/27733], Loss: 2.4933\n",
      "Epoch [65/300], Step [15000/27733], Loss: 3.1924\n",
      "Epoch [65/300], Step [15100/27733], Loss: 1.9025\n",
      "Epoch [65/300], Step [15200/27733], Loss: 3.4764\n",
      "Epoch [65/300], Step [15300/27733], Loss: 3.0168\n",
      "Epoch [65/300], Step [15400/27733], Loss: 2.5444\n",
      "Epoch [65/300], Step [15500/27733], Loss: 3.0798\n",
      "Epoch [65/300], Step [15600/27733], Loss: 2.8916\n",
      "Epoch [65/300], Step [15700/27733], Loss: 3.1215\n",
      "Epoch [65/300], Step [15800/27733], Loss: 2.5036\n",
      "Epoch [65/300], Step [15900/27733], Loss: 2.5194\n",
      "Epoch [65/300], Step [16000/27733], Loss: 2.6190\n",
      "Epoch [65/300], Step [16100/27733], Loss: 1.8092\n",
      "Epoch [65/300], Step [16200/27733], Loss: 3.1650\n",
      "Epoch [65/300], Step [16300/27733], Loss: 2.1599\n",
      "Epoch [65/300], Step [16400/27733], Loss: 3.1963\n",
      "Epoch [65/300], Step [16500/27733], Loss: 2.0630\n",
      "Epoch [65/300], Step [16600/27733], Loss: 2.3476\n",
      "Epoch [65/300], Step [16700/27733], Loss: 3.3348\n",
      "Epoch [65/300], Step [16800/27733], Loss: 2.2266\n",
      "Epoch [65/300], Step [16900/27733], Loss: 3.1147\n",
      "Epoch [65/300], Step [17000/27733], Loss: 3.3607\n",
      "Epoch [65/300], Step [17100/27733], Loss: 3.2108\n",
      "Epoch [65/300], Step [17200/27733], Loss: 2.5634\n",
      "Epoch [65/300], Step [17300/27733], Loss: 2.1912\n",
      "Epoch [65/300], Step [17400/27733], Loss: 3.5027\n",
      "Epoch [65/300], Step [17500/27733], Loss: 3.2667\n",
      "Epoch [65/300], Step [17600/27733], Loss: 2.7094\n",
      "Epoch [65/300], Step [17700/27733], Loss: 2.1486\n",
      "Epoch [65/300], Step [17800/27733], Loss: 2.3318\n",
      "Epoch [65/300], Step [17900/27733], Loss: 3.1583\n",
      "Epoch [65/300], Step [18000/27733], Loss: 3.4167\n",
      "Epoch [65/300], Step [18100/27733], Loss: 1.9066\n",
      "Epoch [65/300], Step [18200/27733], Loss: 2.5222\n",
      "Epoch [65/300], Step [18300/27733], Loss: 3.1813\n",
      "Epoch [65/300], Step [18400/27733], Loss: 2.4971\n",
      "Epoch [65/300], Step [18500/27733], Loss: 3.0590\n",
      "Epoch [65/300], Step [18600/27733], Loss: 3.5334\n",
      "Epoch [65/300], Step [18700/27733], Loss: 2.8972\n",
      "Epoch [65/300], Step [18800/27733], Loss: 2.7964\n",
      "Epoch [65/300], Step [18900/27733], Loss: 2.6666\n",
      "Epoch [65/300], Step [19000/27733], Loss: 3.4681\n",
      "Epoch [65/300], Step [19100/27733], Loss: 3.1895\n",
      "Epoch [65/300], Step [19200/27733], Loss: 2.6589\n",
      "Epoch [65/300], Step [19300/27733], Loss: 2.5243\n",
      "Epoch [65/300], Step [19400/27733], Loss: 1.8962\n",
      "Epoch [65/300], Step [19500/27733], Loss: 3.0828\n",
      "Epoch [65/300], Step [19600/27733], Loss: 2.6140\n",
      "Epoch [65/300], Step [19700/27733], Loss: 3.0920\n",
      "Epoch [65/300], Step [19800/27733], Loss: 3.3036\n",
      "Epoch [65/300], Step [19900/27733], Loss: 3.7361\n",
      "Epoch [65/300], Step [20000/27733], Loss: 2.8111\n",
      "Epoch [65/300], Step [20100/27733], Loss: 3.0416\n",
      "Epoch [65/300], Step [20200/27733], Loss: 4.0080\n",
      "Epoch [65/300], Step [20300/27733], Loss: 2.7324\n",
      "Epoch [65/300], Step [20400/27733], Loss: 2.6096\n",
      "Epoch [65/300], Step [20500/27733], Loss: 3.5682\n",
      "Epoch [65/300], Step [20600/27733], Loss: 2.5361\n",
      "Epoch [65/300], Step [20700/27733], Loss: 2.4180\n",
      "Epoch [65/300], Step [20800/27733], Loss: 3.3935\n",
      "Epoch [65/300], Step [20900/27733], Loss: 3.5077\n",
      "Epoch [65/300], Step [21000/27733], Loss: 3.3163\n",
      "Epoch [65/300], Step [21100/27733], Loss: 2.1615\n",
      "Epoch [65/300], Step [21200/27733], Loss: 2.4982\n",
      "Epoch [65/300], Step [21300/27733], Loss: 2.5052\n",
      "Epoch [65/300], Step [21400/27733], Loss: 2.8502\n",
      "Epoch [65/300], Step [21500/27733], Loss: 2.4315\n",
      "Epoch [65/300], Step [21600/27733], Loss: 2.8361\n",
      "Epoch [65/300], Step [21700/27733], Loss: 2.8648\n",
      "Epoch [65/300], Step [21800/27733], Loss: 2.9381\n",
      "Epoch [65/300], Step [21900/27733], Loss: 2.4982\n",
      "Epoch [65/300], Step [22000/27733], Loss: 2.6460\n",
      "Epoch [65/300], Step [22100/27733], Loss: 2.9608\n",
      "Epoch [65/300], Step [22200/27733], Loss: 2.9408\n",
      "Epoch [65/300], Step [22300/27733], Loss: 2.5169\n",
      "Epoch [65/300], Step [22400/27733], Loss: 3.6409\n",
      "Epoch [65/300], Step [22500/27733], Loss: 3.2083\n",
      "Epoch [65/300], Step [22600/27733], Loss: 2.4617\n",
      "Epoch [65/300], Step [22700/27733], Loss: 3.7638\n",
      "Epoch [65/300], Step [22800/27733], Loss: 2.7361\n",
      "Epoch [65/300], Step [22900/27733], Loss: 2.3911\n",
      "Epoch [65/300], Step [23000/27733], Loss: 3.7821\n",
      "Epoch [65/300], Step [23100/27733], Loss: 2.2829\n",
      "Epoch [65/300], Step [23200/27733], Loss: 3.3567\n",
      "Epoch [65/300], Step [23300/27733], Loss: 2.5493\n",
      "Epoch [65/300], Step [23400/27733], Loss: 3.2854\n",
      "Epoch [65/300], Step [23500/27733], Loss: 3.0950\n",
      "Epoch [65/300], Step [23600/27733], Loss: 3.0372\n",
      "Epoch [65/300], Step [23700/27733], Loss: 3.6845\n",
      "Epoch [65/300], Step [23800/27733], Loss: 2.8743\n",
      "Epoch [65/300], Step [23900/27733], Loss: 3.3106\n",
      "Epoch [65/300], Step [24000/27733], Loss: 3.4869\n",
      "Epoch [65/300], Step [24100/27733], Loss: 2.6217\n",
      "Epoch [65/300], Step [24200/27733], Loss: 2.7016\n",
      "Epoch [65/300], Step [24300/27733], Loss: 2.6165\n",
      "Epoch [65/300], Step [24400/27733], Loss: 3.3959\n",
      "Epoch [65/300], Step [24500/27733], Loss: 3.2089\n",
      "Epoch [65/300], Step [24600/27733], Loss: 3.3730\n",
      "Epoch [65/300], Step [24700/27733], Loss: 2.8638\n",
      "Epoch [65/300], Step [24800/27733], Loss: 3.0962\n",
      "Epoch [65/300], Step [24900/27733], Loss: 3.1161\n",
      "Epoch [65/300], Step [25000/27733], Loss: 1.7927\n",
      "Epoch [65/300], Step [25100/27733], Loss: 3.2282\n",
      "Epoch [65/300], Step [25200/27733], Loss: 3.3557\n",
      "Epoch [65/300], Step [25300/27733], Loss: 2.6947\n",
      "Epoch [65/300], Step [25400/27733], Loss: 2.3537\n",
      "Epoch [65/300], Step [25500/27733], Loss: 3.2377\n",
      "Epoch [65/300], Step [25600/27733], Loss: 3.2660\n",
      "Epoch [65/300], Step [25700/27733], Loss: 3.4840\n",
      "Epoch [65/300], Step [25800/27733], Loss: 3.2416\n",
      "Epoch [65/300], Step [25900/27733], Loss: 2.4966\n",
      "Epoch [65/300], Step [26000/27733], Loss: 2.5971\n",
      "Epoch [65/300], Step [26100/27733], Loss: 2.7973\n",
      "Epoch [65/300], Step [26200/27733], Loss: 2.5268\n",
      "Epoch [65/300], Step [26300/27733], Loss: 3.0387\n",
      "Epoch [65/300], Step [26400/27733], Loss: 2.8130\n",
      "Epoch [65/300], Step [26500/27733], Loss: 3.3201\n",
      "Epoch [65/300], Step [26600/27733], Loss: 2.6952\n",
      "Epoch [65/300], Step [26700/27733], Loss: 2.8731\n",
      "Epoch [65/300], Step [26800/27733], Loss: 2.8186\n",
      "Epoch [65/300], Step [26900/27733], Loss: 3.2686\n",
      "Epoch [65/300], Step [27000/27733], Loss: 2.4723\n",
      "Epoch [65/300], Step [27100/27733], Loss: 3.2433\n",
      "Epoch [65/300], Step [27200/27733], Loss: 2.8855\n",
      "Epoch [65/300], Step [27300/27733], Loss: 2.8835\n",
      "Epoch [65/300], Step [27400/27733], Loss: 3.6195\n",
      "Epoch [65/300], Step [27500/27733], Loss: 3.2181\n",
      "Epoch [65/300], Step [27600/27733], Loss: 3.0345\n",
      "Epoch [65/300], Step [27700/27733], Loss: 2.6251\n",
      "Epoch [66/300], Step [100/27733], Loss: 1.8717\n",
      "Epoch [66/300], Step [200/27733], Loss: 2.2374\n",
      "Epoch [66/300], Step [300/27733], Loss: 2.7646\n",
      "Epoch [66/300], Step [400/27733], Loss: 2.0542\n",
      "Epoch [66/300], Step [500/27733], Loss: 3.0568\n",
      "Epoch [66/300], Step [600/27733], Loss: 1.0479\n",
      "Epoch [66/300], Step [700/27733], Loss: 1.8853\n",
      "Epoch [66/300], Step [800/27733], Loss: 2.4013\n",
      "Epoch [66/300], Step [900/27733], Loss: 3.0717\n",
      "Epoch [66/300], Step [1000/27733], Loss: 2.2656\n",
      "Epoch [66/300], Step [1100/27733], Loss: 1.8725\n",
      "Epoch [66/300], Step [1200/27733], Loss: 2.6410\n",
      "Epoch [66/300], Step [1300/27733], Loss: 2.0720\n",
      "Epoch [66/300], Step [1400/27733], Loss: 1.7075\n",
      "Epoch [66/300], Step [1500/27733], Loss: 2.4263\n",
      "Epoch [66/300], Step [1600/27733], Loss: 2.5079\n",
      "Epoch [66/300], Step [1700/27733], Loss: 2.5025\n",
      "Epoch [66/300], Step [1800/27733], Loss: 1.9674\n",
      "Epoch [66/300], Step [1900/27733], Loss: 2.7567\n",
      "Epoch [66/300], Step [2000/27733], Loss: 2.3680\n",
      "Epoch [66/300], Step [2100/27733], Loss: 2.2538\n",
      "Epoch [66/300], Step [2200/27733], Loss: 2.4942\n",
      "Epoch [66/300], Step [2300/27733], Loss: 3.1127\n",
      "Epoch [66/300], Step [2400/27733], Loss: 2.7556\n",
      "Epoch [66/300], Step [2500/27733], Loss: 2.0350\n",
      "Epoch [66/300], Step [2600/27733], Loss: 2.6716\n",
      "Epoch [66/300], Step [2700/27733], Loss: 2.1908\n",
      "Epoch [66/300], Step [2800/27733], Loss: 2.8615\n",
      "Epoch [66/300], Step [2900/27733], Loss: 2.4760\n",
      "Epoch [66/300], Step [3000/27733], Loss: 2.4687\n",
      "Epoch [66/300], Step [3100/27733], Loss: 2.2395\n",
      "Epoch [66/300], Step [3200/27733], Loss: 2.7352\n",
      "Epoch [66/300], Step [3300/27733], Loss: 2.1901\n",
      "Epoch [66/300], Step [3400/27733], Loss: 2.0413\n",
      "Epoch [66/300], Step [3500/27733], Loss: 1.9690\n",
      "Epoch [66/300], Step [3600/27733], Loss: 2.5433\n",
      "Epoch [66/300], Step [3700/27733], Loss: 2.6330\n",
      "Epoch [66/300], Step [3800/27733], Loss: 3.0456\n",
      "Epoch [66/300], Step [3900/27733], Loss: 2.5051\n",
      "Epoch [66/300], Step [4000/27733], Loss: 2.3831\n",
      "Epoch [66/300], Step [4100/27733], Loss: 2.7069\n",
      "Epoch [66/300], Step [4200/27733], Loss: 2.1449\n",
      "Epoch [66/300], Step [4300/27733], Loss: 2.4188\n",
      "Epoch [66/300], Step [4400/27733], Loss: 2.0822\n",
      "Epoch [66/300], Step [4500/27733], Loss: 3.2080\n",
      "Epoch [66/300], Step [4600/27733], Loss: 2.7862\n",
      "Epoch [66/300], Step [4700/27733], Loss: 2.0615\n",
      "Epoch [66/300], Step [4800/27733], Loss: 2.6066\n",
      "Epoch [66/300], Step [4900/27733], Loss: 2.3117\n",
      "Epoch [66/300], Step [5000/27733], Loss: 1.6111\n",
      "Epoch [66/300], Step [5100/27733], Loss: 2.5246\n",
      "Epoch [66/300], Step [5200/27733], Loss: 2.2328\n",
      "Epoch [66/300], Step [5300/27733], Loss: 2.0592\n",
      "Epoch [66/300], Step [5400/27733], Loss: 2.4996\n",
      "Epoch [66/300], Step [5500/27733], Loss: 2.8705\n",
      "Epoch [66/300], Step [5600/27733], Loss: 2.9804\n",
      "Epoch [66/300], Step [5700/27733], Loss: 3.4201\n",
      "Epoch [66/300], Step [5800/27733], Loss: 1.4944\n",
      "Epoch [66/300], Step [5900/27733], Loss: 1.9545\n",
      "Epoch [66/300], Step [6000/27733], Loss: 3.6397\n",
      "Epoch [66/300], Step [6100/27733], Loss: 2.2911\n",
      "Epoch [66/300], Step [6200/27733], Loss: 2.4648\n",
      "Epoch [66/300], Step [6300/27733], Loss: 2.3252\n",
      "Epoch [66/300], Step [6400/27733], Loss: 2.6376\n",
      "Epoch [66/300], Step [6500/27733], Loss: 2.3226\n",
      "Epoch [66/300], Step [6600/27733], Loss: 2.6775\n",
      "Epoch [66/300], Step [6700/27733], Loss: 2.2274\n",
      "Epoch [66/300], Step [6800/27733], Loss: 3.1792\n",
      "Epoch [66/300], Step [6900/27733], Loss: 3.1917\n",
      "Epoch [66/300], Step [7000/27733], Loss: 2.5848\n",
      "Epoch [66/300], Step [7100/27733], Loss: 2.8173\n",
      "Epoch [66/300], Step [7200/27733], Loss: 2.3088\n",
      "Epoch [66/300], Step [7300/27733], Loss: 2.5333\n",
      "Epoch [66/300], Step [7400/27733], Loss: 1.9226\n",
      "Epoch [66/300], Step [7500/27733], Loss: 2.2118\n",
      "Epoch [66/300], Step [7600/27733], Loss: 3.1763\n",
      "Epoch [66/300], Step [7700/27733], Loss: 2.5879\n",
      "Epoch [66/300], Step [7800/27733], Loss: 2.3523\n",
      "Epoch [66/300], Step [7900/27733], Loss: 2.9003\n",
      "Epoch [66/300], Step [8000/27733], Loss: 2.1928\n",
      "Epoch [66/300], Step [8100/27733], Loss: 2.7423\n",
      "Epoch [66/300], Step [8200/27733], Loss: 2.9519\n",
      "Epoch [66/300], Step [8300/27733], Loss: 3.7536\n",
      "Epoch [66/300], Step [8400/27733], Loss: 2.4022\n",
      "Epoch [66/300], Step [8500/27733], Loss: 3.1503\n",
      "Epoch [66/300], Step [8600/27733], Loss: 2.3278\n",
      "Epoch [66/300], Step [8700/27733], Loss: 1.8464\n",
      "Epoch [66/300], Step [8800/27733], Loss: 1.8756\n",
      "Epoch [66/300], Step [8900/27733], Loss: 3.1405\n",
      "Epoch [66/300], Step [9000/27733], Loss: 2.3576\n",
      "Epoch [66/300], Step [9100/27733], Loss: 3.0139\n",
      "Epoch [66/300], Step [9200/27733], Loss: 1.7665\n",
      "Epoch [66/300], Step [9300/27733], Loss: 2.9982\n",
      "Epoch [66/300], Step [9400/27733], Loss: 2.3638\n",
      "Epoch [66/300], Step [9500/27733], Loss: 3.5415\n",
      "Epoch [66/300], Step [9600/27733], Loss: 3.2434\n",
      "Epoch [66/300], Step [9700/27733], Loss: 2.1303\n",
      "Epoch [66/300], Step [9800/27733], Loss: 3.1739\n",
      "Epoch [66/300], Step [9900/27733], Loss: 2.5476\n",
      "Epoch [66/300], Step [10000/27733], Loss: 2.1879\n",
      "Epoch [66/300], Step [10100/27733], Loss: 2.8298\n",
      "Epoch [66/300], Step [10200/27733], Loss: 2.3868\n",
      "Epoch [66/300], Step [10300/27733], Loss: 2.5171\n",
      "Epoch [66/300], Step [10400/27733], Loss: 2.6370\n",
      "Epoch [66/300], Step [10500/27733], Loss: 2.6597\n",
      "Epoch [66/300], Step [10600/27733], Loss: 2.5098\n",
      "Epoch [66/300], Step [10700/27733], Loss: 3.3158\n",
      "Epoch [66/300], Step [10800/27733], Loss: 2.4148\n",
      "Epoch [66/300], Step [10900/27733], Loss: 2.8207\n",
      "Epoch [66/300], Step [11000/27733], Loss: 2.7113\n",
      "Epoch [66/300], Step [11100/27733], Loss: 2.5736\n",
      "Epoch [66/300], Step [11200/27733], Loss: 2.6987\n",
      "Epoch [66/300], Step [11300/27733], Loss: 3.2780\n",
      "Epoch [66/300], Step [11400/27733], Loss: 2.7403\n",
      "Epoch [66/300], Step [11500/27733], Loss: 2.8915\n",
      "Epoch [66/300], Step [11600/27733], Loss: 2.3934\n",
      "Epoch [66/300], Step [11700/27733], Loss: 2.3661\n",
      "Epoch [66/300], Step [11800/27733], Loss: 2.4138\n",
      "Epoch [66/300], Step [11900/27733], Loss: 2.5138\n",
      "Epoch [66/300], Step [12000/27733], Loss: 2.9701\n",
      "Epoch [66/300], Step [12100/27733], Loss: 2.5626\n",
      "Epoch [66/300], Step [12200/27733], Loss: 2.7470\n",
      "Epoch [66/300], Step [12300/27733], Loss: 2.4814\n",
      "Epoch [66/300], Step [12400/27733], Loss: 3.3833\n",
      "Epoch [66/300], Step [12500/27733], Loss: 2.6910\n",
      "Epoch [66/300], Step [12600/27733], Loss: 2.7753\n",
      "Epoch [66/300], Step [12700/27733], Loss: 2.5991\n",
      "Epoch [66/300], Step [12800/27733], Loss: 2.0294\n",
      "Epoch [66/300], Step [12900/27733], Loss: 2.7139\n",
      "Epoch [66/300], Step [13000/27733], Loss: 2.5151\n",
      "Epoch [66/300], Step [13100/27733], Loss: 2.1143\n",
      "Epoch [66/300], Step [13200/27733], Loss: 2.3947\n",
      "Epoch [66/300], Step [13300/27733], Loss: 3.1769\n",
      "Epoch [66/300], Step [13400/27733], Loss: 2.3743\n",
      "Epoch [66/300], Step [13500/27733], Loss: 2.2132\n",
      "Epoch [66/300], Step [13600/27733], Loss: 3.1181\n",
      "Epoch [66/300], Step [13700/27733], Loss: 3.3529\n",
      "Epoch [66/300], Step [13800/27733], Loss: 2.5122\n",
      "Epoch [66/300], Step [13900/27733], Loss: 2.8345\n",
      "Epoch [66/300], Step [14000/27733], Loss: 3.8975\n",
      "Epoch [66/300], Step [14100/27733], Loss: 3.3113\n",
      "Epoch [66/300], Step [14200/27733], Loss: 3.2727\n",
      "Epoch [66/300], Step [14300/27733], Loss: 2.8976\n",
      "Epoch [66/300], Step [14400/27733], Loss: 2.7715\n",
      "Epoch [66/300], Step [14500/27733], Loss: 2.5256\n",
      "Epoch [66/300], Step [14600/27733], Loss: 2.1261\n",
      "Epoch [66/300], Step [14700/27733], Loss: 2.6015\n",
      "Epoch [66/300], Step [14800/27733], Loss: 2.5155\n",
      "Epoch [66/300], Step [14900/27733], Loss: 3.3746\n",
      "Epoch [66/300], Step [15000/27733], Loss: 2.5730\n",
      "Epoch [66/300], Step [15100/27733], Loss: 2.3738\n",
      "Epoch [66/300], Step [15200/27733], Loss: 3.0815\n",
      "Epoch [66/300], Step [15300/27733], Loss: 2.7257\n",
      "Epoch [66/300], Step [15400/27733], Loss: 2.8632\n",
      "Epoch [66/300], Step [15500/27733], Loss: 3.0243\n",
      "Epoch [66/300], Step [15600/27733], Loss: 3.1024\n",
      "Epoch [66/300], Step [15700/27733], Loss: 2.7453\n",
      "Epoch [66/300], Step [15800/27733], Loss: 3.3861\n",
      "Epoch [66/300], Step [15900/27733], Loss: 2.2983\n",
      "Epoch [66/300], Step [16000/27733], Loss: 3.8668\n",
      "Epoch [66/300], Step [16100/27733], Loss: 2.3226\n",
      "Epoch [66/300], Step [16200/27733], Loss: 2.2017\n",
      "Epoch [66/300], Step [16300/27733], Loss: 2.8416\n",
      "Epoch [66/300], Step [16400/27733], Loss: 2.3486\n",
      "Epoch [66/300], Step [16500/27733], Loss: 2.9749\n",
      "Epoch [66/300], Step [16600/27733], Loss: 3.6349\n",
      "Epoch [66/300], Step [16700/27733], Loss: 2.4281\n",
      "Epoch [66/300], Step [16800/27733], Loss: 3.0896\n",
      "Epoch [66/300], Step [16900/27733], Loss: 2.2348\n",
      "Epoch [66/300], Step [17000/27733], Loss: 2.6918\n",
      "Epoch [66/300], Step [17100/27733], Loss: 2.5295\n",
      "Epoch [66/300], Step [17200/27733], Loss: 2.9680\n",
      "Epoch [66/300], Step [17300/27733], Loss: 3.4921\n",
      "Epoch [66/300], Step [17400/27733], Loss: 3.0326\n",
      "Epoch [66/300], Step [17500/27733], Loss: 2.6997\n",
      "Epoch [66/300], Step [17600/27733], Loss: 2.4530\n",
      "Epoch [66/300], Step [17700/27733], Loss: 3.0608\n",
      "Epoch [66/300], Step [17800/27733], Loss: 2.9953\n",
      "Epoch [66/300], Step [17900/27733], Loss: 3.5759\n",
      "Epoch [66/300], Step [18000/27733], Loss: 2.9287\n",
      "Epoch [66/300], Step [18100/27733], Loss: 2.8911\n",
      "Epoch [66/300], Step [18200/27733], Loss: 1.9960\n",
      "Epoch [66/300], Step [18300/27733], Loss: 2.9662\n",
      "Epoch [66/300], Step [18400/27733], Loss: 2.7695\n",
      "Epoch [66/300], Step [18500/27733], Loss: 2.2443\n",
      "Epoch [66/300], Step [18600/27733], Loss: 2.2837\n",
      "Epoch [66/300], Step [18700/27733], Loss: 2.4528\n",
      "Epoch [66/300], Step [18800/27733], Loss: 2.4221\n",
      "Epoch [66/300], Step [18900/27733], Loss: 2.1548\n",
      "Epoch [66/300], Step [19000/27733], Loss: 3.1183\n",
      "Epoch [66/300], Step [19100/27733], Loss: 3.1367\n",
      "Epoch [66/300], Step [19200/27733], Loss: 2.4994\n",
      "Epoch [66/300], Step [19300/27733], Loss: 3.7390\n",
      "Epoch [66/300], Step [19400/27733], Loss: 3.3659\n",
      "Epoch [66/300], Step [19500/27733], Loss: 2.3930\n",
      "Epoch [66/300], Step [19600/27733], Loss: 2.7012\n",
      "Epoch [66/300], Step [19700/27733], Loss: 2.8634\n",
      "Epoch [66/300], Step [19800/27733], Loss: 2.8189\n",
      "Epoch [66/300], Step [19900/27733], Loss: 2.6342\n",
      "Epoch [66/300], Step [20000/27733], Loss: 2.7099\n",
      "Epoch [66/300], Step [20100/27733], Loss: 3.3582\n",
      "Epoch [66/300], Step [20200/27733], Loss: 3.0822\n",
      "Epoch [66/300], Step [20300/27733], Loss: 2.3802\n",
      "Epoch [66/300], Step [20400/27733], Loss: 2.4320\n",
      "Epoch [66/300], Step [20500/27733], Loss: 2.9724\n",
      "Epoch [66/300], Step [20600/27733], Loss: 2.1688\n",
      "Epoch [66/300], Step [20700/27733], Loss: 1.7005\n",
      "Epoch [66/300], Step [20800/27733], Loss: 2.9994\n",
      "Epoch [66/300], Step [20900/27733], Loss: 2.5288\n",
      "Epoch [66/300], Step [21000/27733], Loss: 3.3230\n",
      "Epoch [66/300], Step [21100/27733], Loss: 2.6406\n",
      "Epoch [66/300], Step [21200/27733], Loss: 3.8728\n",
      "Epoch [66/300], Step [21300/27733], Loss: 2.7195\n",
      "Epoch [66/300], Step [21400/27733], Loss: 2.0292\n",
      "Epoch [66/300], Step [21500/27733], Loss: 2.9769\n",
      "Epoch [66/300], Step [21600/27733], Loss: 3.0350\n",
      "Epoch [66/300], Step [21700/27733], Loss: 3.3569\n",
      "Epoch [66/300], Step [21800/27733], Loss: 2.9804\n",
      "Epoch [66/300], Step [21900/27733], Loss: 3.0643\n",
      "Epoch [66/300], Step [22000/27733], Loss: 2.7549\n",
      "Epoch [66/300], Step [22100/27733], Loss: 3.0166\n",
      "Epoch [66/300], Step [22200/27733], Loss: 2.6286\n",
      "Epoch [66/300], Step [22300/27733], Loss: 3.0187\n",
      "Epoch [66/300], Step [22400/27733], Loss: 2.6198\n",
      "Epoch [66/300], Step [22500/27733], Loss: 3.6813\n",
      "Epoch [66/300], Step [22600/27733], Loss: 2.3055\n",
      "Epoch [66/300], Step [22700/27733], Loss: 3.2031\n",
      "Epoch [66/300], Step [22800/27733], Loss: 2.7391\n",
      "Epoch [66/300], Step [22900/27733], Loss: 3.3317\n",
      "Epoch [66/300], Step [23000/27733], Loss: 3.2113\n",
      "Epoch [66/300], Step [23100/27733], Loss: 2.4444\n",
      "Epoch [66/300], Step [23200/27733], Loss: 2.6675\n",
      "Epoch [66/300], Step [23300/27733], Loss: 2.0302\n",
      "Epoch [66/300], Step [23400/27733], Loss: 3.1596\n",
      "Epoch [66/300], Step [23500/27733], Loss: 3.1479\n",
      "Epoch [66/300], Step [23600/27733], Loss: 2.7491\n",
      "Epoch [66/300], Step [23700/27733], Loss: 2.5173\n",
      "Epoch [66/300], Step [23800/27733], Loss: 3.4247\n",
      "Epoch [66/300], Step [23900/27733], Loss: 2.6600\n",
      "Epoch [66/300], Step [24000/27733], Loss: 3.3090\n",
      "Epoch [66/300], Step [24100/27733], Loss: 3.4754\n",
      "Epoch [66/300], Step [24200/27733], Loss: 2.2199\n",
      "Epoch [66/300], Step [24300/27733], Loss: 2.7010\n",
      "Epoch [66/300], Step [24400/27733], Loss: 2.3537\n",
      "Epoch [66/300], Step [24500/27733], Loss: 3.6734\n",
      "Epoch [66/300], Step [24600/27733], Loss: 2.7440\n",
      "Epoch [66/300], Step [24700/27733], Loss: 3.0920\n",
      "Epoch [66/300], Step [24800/27733], Loss: 2.6424\n",
      "Epoch [66/300], Step [24900/27733], Loss: 2.8304\n",
      "Epoch [66/300], Step [25000/27733], Loss: 3.0975\n",
      "Epoch [66/300], Step [25100/27733], Loss: 2.8586\n",
      "Epoch [66/300], Step [25200/27733], Loss: 2.9766\n",
      "Epoch [66/300], Step [25300/27733], Loss: 2.6503\n",
      "Epoch [66/300], Step [25400/27733], Loss: 2.4909\n",
      "Epoch [66/300], Step [25500/27733], Loss: 2.5133\n",
      "Epoch [66/300], Step [25600/27733], Loss: 3.1638\n",
      "Epoch [66/300], Step [25700/27733], Loss: 2.4614\n",
      "Epoch [66/300], Step [25800/27733], Loss: 2.8995\n",
      "Epoch [66/300], Step [25900/27733], Loss: 2.4995\n",
      "Epoch [66/300], Step [26000/27733], Loss: 2.7110\n",
      "Epoch [66/300], Step [26100/27733], Loss: 2.7180\n",
      "Epoch [66/300], Step [26200/27733], Loss: 3.0248\n",
      "Epoch [66/300], Step [26300/27733], Loss: 2.4249\n",
      "Epoch [66/300], Step [26400/27733], Loss: 2.4136\n",
      "Epoch [66/300], Step [26500/27733], Loss: 2.5463\n",
      "Epoch [66/300], Step [26600/27733], Loss: 3.7022\n",
      "Epoch [66/300], Step [26700/27733], Loss: 2.5959\n",
      "Epoch [66/300], Step [26800/27733], Loss: 2.9710\n",
      "Epoch [66/300], Step [26900/27733], Loss: 3.0664\n",
      "Epoch [66/300], Step [27000/27733], Loss: 2.8198\n",
      "Epoch [66/300], Step [27100/27733], Loss: 3.6055\n",
      "Epoch [66/300], Step [27200/27733], Loss: 3.1584\n",
      "Epoch [66/300], Step [27300/27733], Loss: 3.3089\n",
      "Epoch [66/300], Step [27400/27733], Loss: 2.8769\n",
      "Epoch [66/300], Step [27500/27733], Loss: 2.9868\n",
      "Epoch [66/300], Step [27600/27733], Loss: 2.6614\n",
      "Epoch [66/300], Step [27700/27733], Loss: 2.9325\n",
      "Epoch [67/300], Step [100/27733], Loss: 2.2361\n",
      "Epoch [67/300], Step [200/27733], Loss: 2.3649\n",
      "Epoch [67/300], Step [300/27733], Loss: 2.4270\n",
      "Epoch [67/300], Step [400/27733], Loss: 2.9839\n",
      "Epoch [67/300], Step [500/27733], Loss: 2.6057\n",
      "Epoch [67/300], Step [600/27733], Loss: 1.4956\n",
      "Epoch [67/300], Step [700/27733], Loss: 1.7079\n",
      "Epoch [67/300], Step [800/27733], Loss: 2.8905\n",
      "Epoch [67/300], Step [900/27733], Loss: 2.0758\n",
      "Epoch [67/300], Step [1000/27733], Loss: 2.3987\n",
      "Epoch [67/300], Step [1100/27733], Loss: 2.6863\n",
      "Epoch [67/300], Step [1200/27733], Loss: 2.6016\n",
      "Epoch [67/300], Step [1300/27733], Loss: 3.6294\n",
      "Epoch [67/300], Step [1400/27733], Loss: 2.1892\n",
      "Epoch [67/300], Step [1500/27733], Loss: 2.0397\n",
      "Epoch [67/300], Step [1600/27733], Loss: 2.3321\n",
      "Epoch [67/300], Step [1700/27733], Loss: 2.7760\n",
      "Epoch [67/300], Step [1800/27733], Loss: 2.4616\n",
      "Epoch [67/300], Step [1900/27733], Loss: 2.0714\n",
      "Epoch [67/300], Step [2000/27733], Loss: 2.5987\n",
      "Epoch [67/300], Step [2100/27733], Loss: 1.6027\n",
      "Epoch [67/300], Step [2200/27733], Loss: 2.4324\n",
      "Epoch [67/300], Step [2300/27733], Loss: 2.2941\n",
      "Epoch [67/300], Step [2400/27733], Loss: 2.5140\n",
      "Epoch [67/300], Step [2500/27733], Loss: 3.5775\n",
      "Epoch [67/300], Step [2600/27733], Loss: 3.0404\n",
      "Epoch [67/300], Step [2700/27733], Loss: 2.0904\n",
      "Epoch [67/300], Step [2800/27733], Loss: 2.9724\n",
      "Epoch [67/300], Step [2900/27733], Loss: 2.7402\n",
      "Epoch [67/300], Step [3000/27733], Loss: 3.1396\n",
      "Epoch [67/300], Step [3100/27733], Loss: 1.8610\n",
      "Epoch [67/300], Step [3200/27733], Loss: 1.9762\n",
      "Epoch [67/300], Step [3300/27733], Loss: 2.7525\n",
      "Epoch [67/300], Step [3400/27733], Loss: 2.1583\n",
      "Epoch [67/300], Step [3500/27733], Loss: 2.2588\n",
      "Epoch [67/300], Step [3600/27733], Loss: 2.6779\n",
      "Epoch [67/300], Step [3700/27733], Loss: 3.7080\n",
      "Epoch [67/300], Step [3800/27733], Loss: 2.4834\n",
      "Epoch [67/300], Step [3900/27733], Loss: 2.3913\n",
      "Epoch [67/300], Step [4000/27733], Loss: 2.4802\n",
      "Epoch [67/300], Step [4100/27733], Loss: 2.5868\n",
      "Epoch [67/300], Step [4200/27733], Loss: 2.5192\n",
      "Epoch [67/300], Step [4300/27733], Loss: 1.8140\n",
      "Epoch [67/300], Step [4400/27733], Loss: 2.1464\n",
      "Epoch [67/300], Step [4500/27733], Loss: 3.2049\n",
      "Epoch [67/300], Step [4600/27733], Loss: 3.4132\n",
      "Epoch [67/300], Step [4700/27733], Loss: 3.1171\n",
      "Epoch [67/300], Step [4800/27733], Loss: 2.8817\n",
      "Epoch [67/300], Step [4900/27733], Loss: 2.2817\n",
      "Epoch [67/300], Step [5000/27733], Loss: 2.1772\n",
      "Epoch [67/300], Step [5100/27733], Loss: 2.6372\n",
      "Epoch [67/300], Step [5200/27733], Loss: 2.4295\n",
      "Epoch [67/300], Step [5300/27733], Loss: 2.3757\n",
      "Epoch [67/300], Step [5400/27733], Loss: 3.2185\n",
      "Epoch [67/300], Step [5500/27733], Loss: 2.8021\n",
      "Epoch [67/300], Step [5600/27733], Loss: 2.5600\n",
      "Epoch [67/300], Step [5700/27733], Loss: 2.9099\n",
      "Epoch [67/300], Step [5800/27733], Loss: 2.6838\n",
      "Epoch [67/300], Step [5900/27733], Loss: 2.3301\n",
      "Epoch [67/300], Step [6000/27733], Loss: 2.6605\n",
      "Epoch [67/300], Step [6100/27733], Loss: 2.2856\n",
      "Epoch [67/300], Step [6200/27733], Loss: 2.4818\n",
      "Epoch [67/300], Step [6300/27733], Loss: 1.9099\n",
      "Epoch [67/300], Step [6400/27733], Loss: 2.2480\n",
      "Epoch [67/300], Step [6500/27733], Loss: 2.6095\n",
      "Epoch [67/300], Step [6600/27733], Loss: 2.3744\n",
      "Epoch [67/300], Step [6700/27733], Loss: 2.4728\n",
      "Epoch [67/300], Step [6800/27733], Loss: 2.2901\n",
      "Epoch [67/300], Step [6900/27733], Loss: 2.5741\n",
      "Epoch [67/300], Step [7000/27733], Loss: 2.2964\n",
      "Epoch [67/300], Step [7100/27733], Loss: 2.4299\n",
      "Epoch [67/300], Step [7200/27733], Loss: 2.5965\n",
      "Epoch [67/300], Step [7300/27733], Loss: 3.3204\n",
      "Epoch [67/300], Step [7400/27733], Loss: 2.7246\n",
      "Epoch [67/300], Step [7500/27733], Loss: 2.8452\n",
      "Epoch [67/300], Step [7600/27733], Loss: 2.8385\n",
      "Epoch [67/300], Step [7700/27733], Loss: 2.4002\n",
      "Epoch [67/300], Step [7800/27733], Loss: 1.9404\n",
      "Epoch [67/300], Step [7900/27733], Loss: 2.1942\n",
      "Epoch [67/300], Step [8000/27733], Loss: 2.3881\n",
      "Epoch [67/300], Step [8100/27733], Loss: 2.4478\n",
      "Epoch [67/300], Step [8200/27733], Loss: 3.1803\n",
      "Epoch [67/300], Step [8300/27733], Loss: 2.7767\n",
      "Epoch [67/300], Step [8400/27733], Loss: 1.9074\n",
      "Epoch [67/300], Step [8500/27733], Loss: 3.0933\n",
      "Epoch [67/300], Step [8600/27733], Loss: 2.7226\n",
      "Epoch [67/300], Step [8700/27733], Loss: 3.0842\n",
      "Epoch [67/300], Step [8800/27733], Loss: 2.0067\n",
      "Epoch [67/300], Step [8900/27733], Loss: 3.1380\n",
      "Epoch [67/300], Step [9000/27733], Loss: 3.0607\n",
      "Epoch [67/300], Step [9100/27733], Loss: 3.0367\n",
      "Epoch [67/300], Step [9200/27733], Loss: 2.6746\n",
      "Epoch [67/300], Step [9300/27733], Loss: 2.4465\n",
      "Epoch [67/300], Step [9400/27733], Loss: 2.5270\n",
      "Epoch [67/300], Step [9500/27733], Loss: 2.0568\n",
      "Epoch [67/300], Step [9600/27733], Loss: 1.7045\n",
      "Epoch [67/300], Step [9700/27733], Loss: 4.0471\n",
      "Epoch [67/300], Step [9800/27733], Loss: 2.8317\n",
      "Epoch [67/300], Step [9900/27733], Loss: 3.0218\n",
      "Epoch [67/300], Step [10000/27733], Loss: 2.9835\n",
      "Epoch [67/300], Step [10100/27733], Loss: 2.7118\n",
      "Epoch [67/300], Step [10200/27733], Loss: 3.3879\n",
      "Epoch [67/300], Step [10300/27733], Loss: 2.7815\n",
      "Epoch [67/300], Step [10400/27733], Loss: 3.1701\n",
      "Epoch [67/300], Step [10500/27733], Loss: 3.1949\n",
      "Epoch [67/300], Step [10600/27733], Loss: 2.7080\n",
      "Epoch [67/300], Step [10700/27733], Loss: 3.0253\n",
      "Epoch [67/300], Step [10800/27733], Loss: 3.0897\n",
      "Epoch [67/300], Step [10900/27733], Loss: 2.6010\n",
      "Epoch [67/300], Step [11000/27733], Loss: 2.0216\n",
      "Epoch [67/300], Step [11100/27733], Loss: 2.1037\n",
      "Epoch [67/300], Step [11200/27733], Loss: 2.7029\n",
      "Epoch [67/300], Step [11300/27733], Loss: 3.2529\n",
      "Epoch [67/300], Step [11400/27733], Loss: 2.8966\n",
      "Epoch [67/300], Step [11500/27733], Loss: 2.9001\n",
      "Epoch [67/300], Step [11600/27733], Loss: 2.4486\n",
      "Epoch [67/300], Step [11700/27733], Loss: 2.8095\n",
      "Epoch [67/300], Step [11800/27733], Loss: 2.7078\n",
      "Epoch [67/300], Step [11900/27733], Loss: 2.6169\n",
      "Epoch [67/300], Step [12000/27733], Loss: 2.6871\n",
      "Epoch [67/300], Step [12100/27733], Loss: 3.5340\n",
      "Epoch [67/300], Step [12200/27733], Loss: 2.4005\n",
      "Epoch [67/300], Step [12300/27733], Loss: 2.2413\n",
      "Epoch [67/300], Step [12400/27733], Loss: 3.3090\n",
      "Epoch [67/300], Step [12500/27733], Loss: 2.9347\n",
      "Epoch [67/300], Step [12600/27733], Loss: 2.7046\n",
      "Epoch [67/300], Step [12700/27733], Loss: 3.0729\n",
      "Epoch [67/300], Step [12800/27733], Loss: 2.6999\n",
      "Epoch [67/300], Step [12900/27733], Loss: 2.8613\n",
      "Epoch [67/300], Step [13000/27733], Loss: 2.6120\n",
      "Epoch [67/300], Step [13100/27733], Loss: 2.2524\n",
      "Epoch [67/300], Step [13200/27733], Loss: 2.4767\n",
      "Epoch [67/300], Step [13300/27733], Loss: 2.5212\n",
      "Epoch [67/300], Step [13400/27733], Loss: 3.5483\n",
      "Epoch [67/300], Step [13500/27733], Loss: 2.8059\n",
      "Epoch [67/300], Step [13600/27733], Loss: 3.4973\n",
      "Epoch [67/300], Step [13700/27733], Loss: 2.8246\n",
      "Epoch [67/300], Step [13800/27733], Loss: 2.5941\n",
      "Epoch [67/300], Step [13900/27733], Loss: 3.2643\n",
      "Epoch [67/300], Step [14000/27733], Loss: 2.3358\n",
      "Epoch [67/300], Step [14100/27733], Loss: 2.7922\n",
      "Epoch [67/300], Step [14200/27733], Loss: 3.0265\n",
      "Epoch [67/300], Step [14300/27733], Loss: 1.8768\n",
      "Epoch [67/300], Step [14400/27733], Loss: 2.0498\n",
      "Epoch [67/300], Step [14500/27733], Loss: 2.6661\n",
      "Epoch [67/300], Step [14600/27733], Loss: 2.6726\n",
      "Epoch [67/300], Step [14700/27733], Loss: 2.2036\n",
      "Epoch [67/300], Step [14800/27733], Loss: 2.7202\n",
      "Epoch [67/300], Step [14900/27733], Loss: 2.9710\n",
      "Epoch [67/300], Step [15000/27733], Loss: 3.3499\n",
      "Epoch [67/300], Step [15100/27733], Loss: 2.2283\n",
      "Epoch [67/300], Step [15200/27733], Loss: 2.9104\n",
      "Epoch [67/300], Step [15300/27733], Loss: 3.5906\n",
      "Epoch [67/300], Step [15400/27733], Loss: 3.0734\n",
      "Epoch [67/300], Step [15500/27733], Loss: 2.6870\n",
      "Epoch [67/300], Step [15600/27733], Loss: 3.5930\n",
      "Epoch [67/300], Step [15700/27733], Loss: 1.6351\n",
      "Epoch [67/300], Step [15800/27733], Loss: 2.5881\n",
      "Epoch [67/300], Step [15900/27733], Loss: 2.2393\n",
      "Epoch [67/300], Step [16000/27733], Loss: 2.1863\n",
      "Epoch [67/300], Step [16100/27733], Loss: 3.4813\n",
      "Epoch [67/300], Step [16200/27733], Loss: 3.0403\n",
      "Epoch [67/300], Step [16300/27733], Loss: 2.3312\n",
      "Epoch [67/300], Step [16400/27733], Loss: 2.9165\n",
      "Epoch [67/300], Step [16500/27733], Loss: 2.6531\n",
      "Epoch [67/300], Step [16600/27733], Loss: 3.0998\n",
      "Epoch [67/300], Step [16700/27733], Loss: 2.5862\n",
      "Epoch [67/300], Step [16800/27733], Loss: 2.8577\n",
      "Epoch [67/300], Step [16900/27733], Loss: 2.4110\n",
      "Epoch [67/300], Step [17000/27733], Loss: 2.9623\n",
      "Epoch [67/300], Step [17100/27733], Loss: 3.1146\n",
      "Epoch [67/300], Step [17200/27733], Loss: 2.6370\n",
      "Epoch [67/300], Step [17300/27733], Loss: 2.7376\n",
      "Epoch [67/300], Step [17400/27733], Loss: 2.8313\n",
      "Epoch [67/300], Step [17500/27733], Loss: 2.5331\n",
      "Epoch [67/300], Step [17600/27733], Loss: 3.4687\n",
      "Epoch [67/300], Step [17700/27733], Loss: 2.1821\n",
      "Epoch [67/300], Step [17800/27733], Loss: 3.2157\n",
      "Epoch [67/300], Step [17900/27733], Loss: 2.5041\n",
      "Epoch [67/300], Step [18000/27733], Loss: 2.9955\n",
      "Epoch [67/300], Step [18100/27733], Loss: 2.6450\n",
      "Epoch [67/300], Step [18200/27733], Loss: 2.6520\n",
      "Epoch [67/300], Step [18300/27733], Loss: 2.1357\n",
      "Epoch [67/300], Step [18400/27733], Loss: 3.2132\n",
      "Epoch [67/300], Step [18500/27733], Loss: 3.4976\n",
      "Epoch [67/300], Step [18600/27733], Loss: 2.9022\n",
      "Epoch [67/300], Step [18700/27733], Loss: 2.2560\n",
      "Epoch [67/300], Step [18800/27733], Loss: 3.1242\n",
      "Epoch [67/300], Step [18900/27733], Loss: 3.8718\n",
      "Epoch [67/300], Step [19000/27733], Loss: 2.3339\n",
      "Epoch [67/300], Step [19100/27733], Loss: 3.5199\n",
      "Epoch [67/300], Step [19200/27733], Loss: 3.2077\n",
      "Epoch [67/300], Step [19300/27733], Loss: 3.0852\n",
      "Epoch [67/300], Step [19400/27733], Loss: 3.7827\n",
      "Epoch [67/300], Step [19500/27733], Loss: 3.0657\n",
      "Epoch [67/300], Step [19600/27733], Loss: 2.0996\n",
      "Epoch [67/300], Step [19700/27733], Loss: 3.2446\n",
      "Epoch [67/300], Step [19800/27733], Loss: 2.3681\n",
      "Epoch [67/300], Step [19900/27733], Loss: 2.5045\n",
      "Epoch [67/300], Step [20000/27733], Loss: 2.7964\n",
      "Epoch [67/300], Step [20100/27733], Loss: 2.9531\n",
      "Epoch [67/300], Step [20200/27733], Loss: 3.1022\n",
      "Epoch [67/300], Step [20300/27733], Loss: 2.8440\n",
      "Epoch [67/300], Step [20400/27733], Loss: 2.8115\n",
      "Epoch [67/300], Step [20500/27733], Loss: 3.3615\n",
      "Epoch [67/300], Step [20600/27733], Loss: 2.0645\n",
      "Epoch [67/300], Step [20700/27733], Loss: 1.7822\n",
      "Epoch [67/300], Step [20800/27733], Loss: 3.6479\n",
      "Epoch [67/300], Step [20900/27733], Loss: 2.4809\n",
      "Epoch [67/300], Step [21000/27733], Loss: 2.6317\n",
      "Epoch [67/300], Step [21100/27733], Loss: 2.8206\n",
      "Epoch [67/300], Step [21200/27733], Loss: 3.1430\n",
      "Epoch [67/300], Step [21300/27733], Loss: 2.9519\n",
      "Epoch [67/300], Step [21400/27733], Loss: 3.1019\n",
      "Epoch [67/300], Step [21500/27733], Loss: 2.3103\n",
      "Epoch [67/300], Step [21600/27733], Loss: 2.7109\n",
      "Epoch [67/300], Step [21700/27733], Loss: 2.5765\n",
      "Epoch [67/300], Step [21800/27733], Loss: 3.4013\n",
      "Epoch [67/300], Step [21900/27733], Loss: 3.4180\n",
      "Epoch [67/300], Step [22000/27733], Loss: 2.6264\n",
      "Epoch [67/300], Step [22100/27733], Loss: 2.9375\n",
      "Epoch [67/300], Step [22200/27733], Loss: 2.9184\n",
      "Epoch [67/300], Step [22300/27733], Loss: 3.1438\n",
      "Epoch [67/300], Step [22400/27733], Loss: 3.0767\n",
      "Epoch [67/300], Step [22500/27733], Loss: 2.6931\n",
      "Epoch [67/300], Step [22600/27733], Loss: 3.3635\n",
      "Epoch [67/300], Step [22700/27733], Loss: 2.9280\n",
      "Epoch [67/300], Step [22800/27733], Loss: 2.3876\n",
      "Epoch [67/300], Step [22900/27733], Loss: 3.2881\n",
      "Epoch [67/300], Step [23000/27733], Loss: 3.1799\n",
      "Epoch [67/300], Step [23100/27733], Loss: 2.9025\n",
      "Epoch [67/300], Step [23200/27733], Loss: 2.1460\n",
      "Epoch [67/300], Step [23300/27733], Loss: 2.8557\n",
      "Epoch [67/300], Step [23400/27733], Loss: 3.0074\n",
      "Epoch [67/300], Step [23500/27733], Loss: 3.3652\n",
      "Epoch [67/300], Step [23600/27733], Loss: 2.2230\n",
      "Epoch [67/300], Step [23700/27733], Loss: 2.7035\n",
      "Epoch [67/300], Step [23800/27733], Loss: 3.1173\n",
      "Epoch [67/300], Step [23900/27733], Loss: 2.9647\n",
      "Epoch [67/300], Step [24000/27733], Loss: 2.7678\n",
      "Epoch [67/300], Step [24100/27733], Loss: 2.7611\n",
      "Epoch [67/300], Step [24200/27733], Loss: 2.8941\n",
      "Epoch [67/300], Step [24300/27733], Loss: 3.4889\n",
      "Epoch [67/300], Step [24400/27733], Loss: 2.7586\n",
      "Epoch [67/300], Step [24500/27733], Loss: 3.2659\n",
      "Epoch [67/300], Step [24600/27733], Loss: 3.9395\n",
      "Epoch [67/300], Step [24700/27733], Loss: 3.0733\n",
      "Epoch [67/300], Step [24800/27733], Loss: 3.5704\n",
      "Epoch [67/300], Step [24900/27733], Loss: 2.7279\n",
      "Epoch [67/300], Step [25000/27733], Loss: 2.5177\n",
      "Epoch [67/300], Step [25100/27733], Loss: 2.4977\n",
      "Epoch [67/300], Step [25200/27733], Loss: 2.4602\n",
      "Epoch [67/300], Step [25300/27733], Loss: 2.0795\n",
      "Epoch [67/300], Step [25400/27733], Loss: 3.0723\n",
      "Epoch [67/300], Step [25500/27733], Loss: 2.5494\n",
      "Epoch [67/300], Step [25600/27733], Loss: 2.3315\n",
      "Epoch [67/300], Step [25700/27733], Loss: 2.8093\n",
      "Epoch [67/300], Step [25800/27733], Loss: 3.1308\n",
      "Epoch [67/300], Step [25900/27733], Loss: 3.0546\n",
      "Epoch [67/300], Step [26000/27733], Loss: 2.9123\n",
      "Epoch [67/300], Step [26100/27733], Loss: 2.1837\n",
      "Epoch [67/300], Step [26200/27733], Loss: 2.7070\n",
      "Epoch [67/300], Step [26300/27733], Loss: 2.8327\n",
      "Epoch [67/300], Step [26400/27733], Loss: 2.3977\n",
      "Epoch [67/300], Step [26500/27733], Loss: 3.7432\n",
      "Epoch [67/300], Step [26600/27733], Loss: 3.2849\n",
      "Epoch [67/300], Step [26700/27733], Loss: 3.5526\n",
      "Epoch [67/300], Step [26800/27733], Loss: 2.9086\n",
      "Epoch [67/300], Step [26900/27733], Loss: 3.3384\n",
      "Epoch [67/300], Step [27000/27733], Loss: 3.2115\n",
      "Epoch [67/300], Step [27100/27733], Loss: 3.2598\n",
      "Epoch [67/300], Step [27200/27733], Loss: 3.0708\n",
      "Epoch [67/300], Step [27300/27733], Loss: 3.4235\n",
      "Epoch [67/300], Step [27400/27733], Loss: 3.3668\n",
      "Epoch [67/300], Step [27500/27733], Loss: 3.1244\n",
      "Epoch [67/300], Step [27600/27733], Loss: 2.4898\n",
      "Epoch [67/300], Step [27700/27733], Loss: 2.9843\n",
      "Epoch [68/300], Step [100/27733], Loss: 2.5554\n",
      "Epoch [68/300], Step [200/27733], Loss: 2.4091\n",
      "Epoch [68/300], Step [300/27733], Loss: 2.2083\n",
      "Epoch [68/300], Step [400/27733], Loss: 2.5661\n",
      "Epoch [68/300], Step [500/27733], Loss: 2.9428\n",
      "Epoch [68/300], Step [600/27733], Loss: 1.9985\n",
      "Epoch [68/300], Step [700/27733], Loss: 2.7575\n",
      "Epoch [68/300], Step [800/27733], Loss: 2.1076\n",
      "Epoch [68/300], Step [900/27733], Loss: 1.5738\n",
      "Epoch [68/300], Step [1000/27733], Loss: 2.5597\n",
      "Epoch [68/300], Step [1100/27733], Loss: 2.5050\n",
      "Epoch [68/300], Step [1200/27733], Loss: 2.3312\n",
      "Epoch [68/300], Step [1300/27733], Loss: 2.0109\n",
      "Epoch [68/300], Step [1400/27733], Loss: 2.0278\n",
      "Epoch [68/300], Step [1500/27733], Loss: 2.0236\n",
      "Epoch [68/300], Step [1600/27733], Loss: 2.5792\n",
      "Epoch [68/300], Step [1700/27733], Loss: 2.2308\n",
      "Epoch [68/300], Step [1800/27733], Loss: 2.0488\n",
      "Epoch [68/300], Step [1900/27733], Loss: 2.5505\n",
      "Epoch [68/300], Step [2000/27733], Loss: 2.0947\n",
      "Epoch [68/300], Step [2100/27733], Loss: 2.6636\n",
      "Epoch [68/300], Step [2200/27733], Loss: 2.4072\n",
      "Epoch [68/300], Step [2300/27733], Loss: 2.0297\n",
      "Epoch [68/300], Step [2400/27733], Loss: 2.3559\n",
      "Epoch [68/300], Step [2500/27733], Loss: 2.5030\n",
      "Epoch [68/300], Step [2600/27733], Loss: 1.9232\n",
      "Epoch [68/300], Step [2700/27733], Loss: 2.0819\n",
      "Epoch [68/300], Step [2800/27733], Loss: 1.9440\n",
      "Epoch [68/300], Step [2900/27733], Loss: 2.1570\n",
      "Epoch [68/300], Step [3000/27733], Loss: 2.3935\n",
      "Epoch [68/300], Step [3100/27733], Loss: 2.5519\n",
      "Epoch [68/300], Step [3200/27733], Loss: 1.9342\n",
      "Epoch [68/300], Step [3300/27733], Loss: 2.5899\n",
      "Epoch [68/300], Step [3400/27733], Loss: 2.3853\n",
      "Epoch [68/300], Step [3500/27733], Loss: 2.8860\n",
      "Epoch [68/300], Step [3600/27733], Loss: 2.4145\n",
      "Epoch [68/300], Step [3700/27733], Loss: 2.8014\n",
      "Epoch [68/300], Step [3800/27733], Loss: 2.1994\n",
      "Epoch [68/300], Step [3900/27733], Loss: 2.7129\n",
      "Epoch [68/300], Step [4000/27733], Loss: 2.2256\n",
      "Epoch [68/300], Step [4100/27733], Loss: 2.6897\n",
      "Epoch [68/300], Step [4200/27733], Loss: 1.8746\n",
      "Epoch [68/300], Step [4300/27733], Loss: 1.8601\n",
      "Epoch [68/300], Step [4400/27733], Loss: 3.3059\n",
      "Epoch [68/300], Step [4500/27733], Loss: 3.0194\n",
      "Epoch [68/300], Step [4600/27733], Loss: 2.8257\n",
      "Epoch [68/300], Step [4700/27733], Loss: 2.2076\n",
      "Epoch [68/300], Step [4800/27733], Loss: 2.4823\n",
      "Epoch [68/300], Step [4900/27733], Loss: 2.6704\n",
      "Epoch [68/300], Step [5000/27733], Loss: 2.6162\n",
      "Epoch [68/300], Step [5100/27733], Loss: 1.8689\n",
      "Epoch [68/300], Step [5200/27733], Loss: 2.9534\n",
      "Epoch [68/300], Step [5300/27733], Loss: 2.4919\n",
      "Epoch [68/300], Step [5400/27733], Loss: 2.4260\n",
      "Epoch [68/300], Step [5500/27733], Loss: 2.2998\n",
      "Epoch [68/300], Step [5600/27733], Loss: 3.0153\n",
      "Epoch [68/300], Step [5700/27733], Loss: 2.3603\n",
      "Epoch [68/300], Step [5800/27733], Loss: 2.8726\n",
      "Epoch [68/300], Step [5900/27733], Loss: 2.0220\n",
      "Epoch [68/300], Step [6000/27733], Loss: 3.2794\n",
      "Epoch [68/300], Step [6100/27733], Loss: 2.2520\n",
      "Epoch [68/300], Step [6200/27733], Loss: 2.1660\n",
      "Epoch [68/300], Step [6300/27733], Loss: 3.0034\n",
      "Epoch [68/300], Step [6400/27733], Loss: 2.5022\n",
      "Epoch [68/300], Step [6500/27733], Loss: 2.4768\n",
      "Epoch [68/300], Step [6600/27733], Loss: 2.3606\n",
      "Epoch [68/300], Step [6700/27733], Loss: 3.0793\n",
      "Epoch [68/300], Step [6800/27733], Loss: 2.4661\n",
      "Epoch [68/300], Step [6900/27733], Loss: 2.6267\n",
      "Epoch [68/300], Step [7000/27733], Loss: 3.2184\n",
      "Epoch [68/300], Step [7100/27733], Loss: 1.8023\n",
      "Epoch [68/300], Step [7200/27733], Loss: 2.8877\n",
      "Epoch [68/300], Step [7300/27733], Loss: 2.3061\n",
      "Epoch [68/300], Step [7400/27733], Loss: 2.4581\n",
      "Epoch [68/300], Step [7500/27733], Loss: 2.5698\n",
      "Epoch [68/300], Step [7600/27733], Loss: 2.2007\n",
      "Epoch [68/300], Step [7700/27733], Loss: 2.6969\n",
      "Epoch [68/300], Step [7800/27733], Loss: 2.4338\n",
      "Epoch [68/300], Step [7900/27733], Loss: 2.5048\n",
      "Epoch [68/300], Step [8000/27733], Loss: 3.2896\n",
      "Epoch [68/300], Step [8100/27733], Loss: 2.5499\n",
      "Epoch [68/300], Step [8200/27733], Loss: 2.4537\n",
      "Epoch [68/300], Step [8300/27733], Loss: 1.7855\n",
      "Epoch [68/300], Step [8400/27733], Loss: 2.3789\n",
      "Epoch [68/300], Step [8500/27733], Loss: 2.6636\n",
      "Epoch [68/300], Step [8600/27733], Loss: 2.3681\n",
      "Epoch [68/300], Step [8700/27733], Loss: 2.9517\n",
      "Epoch [68/300], Step [8800/27733], Loss: 1.7214\n",
      "Epoch [68/300], Step [8900/27733], Loss: 2.3687\n",
      "Epoch [68/300], Step [9000/27733], Loss: 2.4558\n",
      "Epoch [68/300], Step [9100/27733], Loss: 2.0810\n",
      "Epoch [68/300], Step [9200/27733], Loss: 2.4539\n",
      "Epoch [68/300], Step [9300/27733], Loss: 2.4196\n",
      "Epoch [68/300], Step [9400/27733], Loss: 3.0153\n",
      "Epoch [68/300], Step [9500/27733], Loss: 2.2721\n",
      "Epoch [68/300], Step [9600/27733], Loss: 3.1703\n",
      "Epoch [68/300], Step [9700/27733], Loss: 2.5197\n",
      "Epoch [68/300], Step [9800/27733], Loss: 2.9164\n",
      "Epoch [68/300], Step [9900/27733], Loss: 2.0021\n",
      "Epoch [68/300], Step [10000/27733], Loss: 3.0565\n",
      "Epoch [68/300], Step [10100/27733], Loss: 2.7590\n",
      "Epoch [68/300], Step [10200/27733], Loss: 3.1260\n",
      "Epoch [68/300], Step [10300/27733], Loss: 2.5239\n",
      "Epoch [68/300], Step [10400/27733], Loss: 2.4858\n",
      "Epoch [68/300], Step [10500/27733], Loss: 3.0621\n",
      "Epoch [68/300], Step [10600/27733], Loss: 2.7723\n",
      "Epoch [68/300], Step [10700/27733], Loss: 3.9566\n",
      "Epoch [68/300], Step [10800/27733], Loss: 2.0547\n",
      "Epoch [68/300], Step [10900/27733], Loss: 1.5553\n",
      "Epoch [68/300], Step [11000/27733], Loss: 2.8848\n",
      "Epoch [68/300], Step [11100/27733], Loss: 2.2713\n",
      "Epoch [68/300], Step [11200/27733], Loss: 2.2112\n",
      "Epoch [68/300], Step [11300/27733], Loss: 2.5547\n",
      "Epoch [68/300], Step [11400/27733], Loss: 2.5950\n",
      "Epoch [68/300], Step [11500/27733], Loss: 2.5152\n",
      "Epoch [68/300], Step [11600/27733], Loss: 2.5366\n",
      "Epoch [68/300], Step [11700/27733], Loss: 2.9555\n",
      "Epoch [68/300], Step [11800/27733], Loss: 2.3984\n",
      "Epoch [68/300], Step [11900/27733], Loss: 3.2947\n",
      "Epoch [68/300], Step [12000/27733], Loss: 2.7559\n",
      "Epoch [68/300], Step [12100/27733], Loss: 2.7779\n",
      "Epoch [68/300], Step [12200/27733], Loss: 2.1414\n",
      "Epoch [68/300], Step [12300/27733], Loss: 1.9214\n",
      "Epoch [68/300], Step [12400/27733], Loss: 2.4098\n",
      "Epoch [68/300], Step [12500/27733], Loss: 3.4596\n",
      "Epoch [68/300], Step [12600/27733], Loss: 2.4749\n",
      "Epoch [68/300], Step [12700/27733], Loss: 2.4117\n",
      "Epoch [68/300], Step [12800/27733], Loss: 2.5870\n",
      "Epoch [68/300], Step [12900/27733], Loss: 2.9002\n",
      "Epoch [68/300], Step [13000/27733], Loss: 2.2528\n",
      "Epoch [68/300], Step [13100/27733], Loss: 3.3198\n",
      "Epoch [68/300], Step [13200/27733], Loss: 3.1284\n",
      "Epoch [68/300], Step [13300/27733], Loss: 3.0689\n",
      "Epoch [68/300], Step [13400/27733], Loss: 1.9246\n",
      "Epoch [68/300], Step [13500/27733], Loss: 2.0485\n",
      "Epoch [68/300], Step [13600/27733], Loss: 3.1754\n",
      "Epoch [68/300], Step [13700/27733], Loss: 2.3513\n",
      "Epoch [68/300], Step [13800/27733], Loss: 2.5464\n",
      "Epoch [68/300], Step [13900/27733], Loss: 3.2143\n",
      "Epoch [68/300], Step [14000/27733], Loss: 2.6408\n",
      "Epoch [68/300], Step [14100/27733], Loss: 2.5189\n",
      "Epoch [68/300], Step [14200/27733], Loss: 2.5045\n",
      "Epoch [68/300], Step [14300/27733], Loss: 2.6823\n",
      "Epoch [68/300], Step [14400/27733], Loss: 2.6450\n",
      "Epoch [68/300], Step [14500/27733], Loss: 2.3142\n",
      "Epoch [68/300], Step [14600/27733], Loss: 2.5377\n",
      "Epoch [68/300], Step [14700/27733], Loss: 2.6496\n",
      "Epoch [68/300], Step [14800/27733], Loss: 3.0109\n",
      "Epoch [68/300], Step [14900/27733], Loss: 2.4344\n",
      "Epoch [68/300], Step [15000/27733], Loss: 2.6208\n",
      "Epoch [68/300], Step [15100/27733], Loss: 2.8741\n",
      "Epoch [68/300], Step [15200/27733], Loss: 3.5143\n",
      "Epoch [68/300], Step [15300/27733], Loss: 3.1411\n",
      "Epoch [68/300], Step [15400/27733], Loss: 3.0584\n",
      "Epoch [68/300], Step [15500/27733], Loss: 2.6727\n",
      "Epoch [68/300], Step [15600/27733], Loss: 3.3601\n",
      "Epoch [68/300], Step [15700/27733], Loss: 2.5411\n",
      "Epoch [68/300], Step [15800/27733], Loss: 3.0292\n",
      "Epoch [68/300], Step [15900/27733], Loss: 2.9555\n",
      "Epoch [68/300], Step [16000/27733], Loss: 2.3547\n",
      "Epoch [68/300], Step [16100/27733], Loss: 2.3718\n",
      "Epoch [68/300], Step [16200/27733], Loss: 2.5237\n",
      "Epoch [68/300], Step [16300/27733], Loss: 2.9423\n",
      "Epoch [68/300], Step [16400/27733], Loss: 3.1924\n",
      "Epoch [68/300], Step [16500/27733], Loss: 2.7701\n",
      "Epoch [68/300], Step [16600/27733], Loss: 2.4098\n",
      "Epoch [68/300], Step [16700/27733], Loss: 3.0162\n",
      "Epoch [68/300], Step [16800/27733], Loss: 2.4001\n",
      "Epoch [68/300], Step [16900/27733], Loss: 2.7738\n",
      "Epoch [68/300], Step [17000/27733], Loss: 3.0200\n",
      "Epoch [68/300], Step [17100/27733], Loss: 2.5857\n",
      "Epoch [68/300], Step [17200/27733], Loss: 2.7640\n",
      "Epoch [68/300], Step [17300/27733], Loss: 2.4969\n",
      "Epoch [68/300], Step [17400/27733], Loss: 3.1744\n",
      "Epoch [68/300], Step [17500/27733], Loss: 2.3404\n",
      "Epoch [68/300], Step [17600/27733], Loss: 2.7675\n",
      "Epoch [68/300], Step [17700/27733], Loss: 2.7719\n",
      "Epoch [68/300], Step [17800/27733], Loss: 3.3031\n",
      "Epoch [68/300], Step [17900/27733], Loss: 2.4274\n",
      "Epoch [68/300], Step [18000/27733], Loss: 2.7542\n",
      "Epoch [68/300], Step [18100/27733], Loss: 3.0653\n",
      "Epoch [68/300], Step [18200/27733], Loss: 3.0168\n",
      "Epoch [68/300], Step [18300/27733], Loss: 2.9046\n",
      "Epoch [68/300], Step [18400/27733], Loss: 3.0829\n",
      "Epoch [68/300], Step [18500/27733], Loss: 3.1683\n",
      "Epoch [68/300], Step [18600/27733], Loss: 2.6748\n",
      "Epoch [68/300], Step [18700/27733], Loss: 2.5327\n",
      "Epoch [68/300], Step [18800/27733], Loss: 3.1784\n",
      "Epoch [68/300], Step [18900/27733], Loss: 2.9003\n",
      "Epoch [68/300], Step [19000/27733], Loss: 2.1448\n",
      "Epoch [68/300], Step [19100/27733], Loss: 1.9173\n",
      "Epoch [68/300], Step [19200/27733], Loss: 3.7560\n",
      "Epoch [68/300], Step [19300/27733], Loss: 3.0228\n",
      "Epoch [68/300], Step [19400/27733], Loss: 3.0121\n",
      "Epoch [68/300], Step [19500/27733], Loss: 2.8738\n",
      "Epoch [68/300], Step [19600/27733], Loss: 2.4945\n",
      "Epoch [68/300], Step [19700/27733], Loss: 3.3424\n",
      "Epoch [68/300], Step [19800/27733], Loss: 2.3722\n",
      "Epoch [68/300], Step [19900/27733], Loss: 2.8278\n",
      "Epoch [68/300], Step [20000/27733], Loss: 2.3917\n",
      "Epoch [68/300], Step [20100/27733], Loss: 2.6867\n",
      "Epoch [68/300], Step [20200/27733], Loss: 3.2593\n",
      "Epoch [68/300], Step [20300/27733], Loss: 3.3161\n",
      "Epoch [68/300], Step [20400/27733], Loss: 3.3062\n",
      "Epoch [68/300], Step [20500/27733], Loss: 2.8798\n",
      "Epoch [68/300], Step [20600/27733], Loss: 2.8859\n",
      "Epoch [68/300], Step [20700/27733], Loss: 2.6958\n",
      "Epoch [68/300], Step [20800/27733], Loss: 3.4203\n",
      "Epoch [68/300], Step [20900/27733], Loss: 2.9854\n",
      "Epoch [68/300], Step [21000/27733], Loss: 2.7933\n",
      "Epoch [68/300], Step [21100/27733], Loss: 2.7366\n",
      "Epoch [68/300], Step [21200/27733], Loss: 2.4734\n",
      "Epoch [68/300], Step [21300/27733], Loss: 3.1923\n",
      "Epoch [68/300], Step [21400/27733], Loss: 3.3357\n",
      "Epoch [68/300], Step [21500/27733], Loss: 3.2528\n",
      "Epoch [68/300], Step [21600/27733], Loss: 2.4550\n",
      "Epoch [68/300], Step [21700/27733], Loss: 2.8944\n",
      "Epoch [68/300], Step [21800/27733], Loss: 3.5786\n",
      "Epoch [68/300], Step [21900/27733], Loss: 3.0601\n",
      "Epoch [68/300], Step [22000/27733], Loss: 2.5820\n",
      "Epoch [68/300], Step [22100/27733], Loss: 4.1202\n",
      "Epoch [68/300], Step [22200/27733], Loss: 2.5455\n",
      "Epoch [68/300], Step [22300/27733], Loss: 2.9672\n",
      "Epoch [68/300], Step [22400/27733], Loss: 3.1294\n",
      "Epoch [68/300], Step [22500/27733], Loss: 2.3834\n",
      "Epoch [68/300], Step [22600/27733], Loss: 2.6089\n",
      "Epoch [68/300], Step [22700/27733], Loss: 2.7249\n",
      "Epoch [68/300], Step [22800/27733], Loss: 3.3552\n",
      "Epoch [68/300], Step [22900/27733], Loss: 2.7459\n",
      "Epoch [68/300], Step [23000/27733], Loss: 2.8883\n",
      "Epoch [68/300], Step [23100/27733], Loss: 3.4982\n",
      "Epoch [68/300], Step [23200/27733], Loss: 2.3444\n",
      "Epoch [68/300], Step [23300/27733], Loss: 2.9187\n",
      "Epoch [68/300], Step [23400/27733], Loss: 2.0343\n",
      "Epoch [68/300], Step [23500/27733], Loss: 2.5600\n",
      "Epoch [68/300], Step [23600/27733], Loss: 2.7265\n",
      "Epoch [68/300], Step [23700/27733], Loss: 3.0284\n",
      "Epoch [68/300], Step [23800/27733], Loss: 3.0279\n",
      "Epoch [68/300], Step [23900/27733], Loss: 3.4276\n",
      "Epoch [68/300], Step [24000/27733], Loss: 3.2366\n",
      "Epoch [68/300], Step [24100/27733], Loss: 3.1499\n",
      "Epoch [68/300], Step [24200/27733], Loss: 1.6749\n",
      "Epoch [68/300], Step [24300/27733], Loss: 2.7662\n",
      "Epoch [68/300], Step [24400/27733], Loss: 3.5947\n",
      "Epoch [68/300], Step [24500/27733], Loss: 2.6955\n",
      "Epoch [68/300], Step [24600/27733], Loss: 2.5323\n",
      "Epoch [68/300], Step [24700/27733], Loss: 2.8139\n",
      "Epoch [68/300], Step [24800/27733], Loss: 2.5427\n",
      "Epoch [68/300], Step [24900/27733], Loss: 2.9633\n",
      "Epoch [68/300], Step [25000/27733], Loss: 2.4212\n",
      "Epoch [68/300], Step [25100/27733], Loss: 3.3879\n",
      "Epoch [68/300], Step [25200/27733], Loss: 2.5754\n",
      "Epoch [68/300], Step [25300/27733], Loss: 2.4334\n",
      "Epoch [68/300], Step [25400/27733], Loss: 2.9275\n",
      "Epoch [68/300], Step [25500/27733], Loss: 3.0218\n",
      "Epoch [68/300], Step [25600/27733], Loss: 3.0527\n",
      "Epoch [68/300], Step [25700/27733], Loss: 2.2693\n",
      "Epoch [68/300], Step [25800/27733], Loss: 2.9320\n",
      "Epoch [68/300], Step [25900/27733], Loss: 2.6970\n",
      "Epoch [68/300], Step [26000/27733], Loss: 2.2946\n",
      "Epoch [68/300], Step [26100/27733], Loss: 2.9941\n",
      "Epoch [68/300], Step [26200/27733], Loss: 2.8747\n",
      "Epoch [68/300], Step [26300/27733], Loss: 2.8059\n",
      "Epoch [68/300], Step [26400/27733], Loss: 2.4949\n",
      "Epoch [68/300], Step [26500/27733], Loss: 2.7884\n",
      "Epoch [68/300], Step [26600/27733], Loss: 3.1692\n",
      "Epoch [68/300], Step [26700/27733], Loss: 3.2186\n",
      "Epoch [68/300], Step [26800/27733], Loss: 2.5456\n",
      "Epoch [68/300], Step [26900/27733], Loss: 2.4710\n",
      "Epoch [68/300], Step [27000/27733], Loss: 3.7285\n",
      "Epoch [68/300], Step [27100/27733], Loss: 2.6205\n",
      "Epoch [68/300], Step [27200/27733], Loss: 3.2430\n",
      "Epoch [68/300], Step [27300/27733], Loss: 2.3133\n",
      "Epoch [68/300], Step [27400/27733], Loss: 2.1616\n",
      "Epoch [68/300], Step [27500/27733], Loss: 2.9980\n",
      "Epoch [68/300], Step [27600/27733], Loss: 2.3326\n",
      "Epoch [68/300], Step [27700/27733], Loss: 2.5470\n",
      "Epoch [69/300], Step [100/27733], Loss: 2.0341\n",
      "Epoch [69/300], Step [200/27733], Loss: 1.5807\n",
      "Epoch [69/300], Step [300/27733], Loss: 2.3112\n",
      "Epoch [69/300], Step [400/27733], Loss: 3.2277\n",
      "Epoch [69/300], Step [500/27733], Loss: 2.0583\n",
      "Epoch [69/300], Step [600/27733], Loss: 2.9096\n",
      "Epoch [69/300], Step [700/27733], Loss: 2.5804\n",
      "Epoch [69/300], Step [800/27733], Loss: 2.5454\n",
      "Epoch [69/300], Step [900/27733], Loss: 2.1893\n",
      "Epoch [69/300], Step [1000/27733], Loss: 2.6431\n",
      "Epoch [69/300], Step [1100/27733], Loss: 3.0208\n",
      "Epoch [69/300], Step [1200/27733], Loss: 2.7305\n",
      "Epoch [69/300], Step [1300/27733], Loss: 1.9969\n",
      "Epoch [69/300], Step [1400/27733], Loss: 2.9508\n",
      "Epoch [69/300], Step [1500/27733], Loss: 2.9858\n",
      "Epoch [69/300], Step [1600/27733], Loss: 2.9913\n",
      "Epoch [69/300], Step [1700/27733], Loss: 2.1940\n",
      "Epoch [69/300], Step [1800/27733], Loss: 2.9241\n",
      "Epoch [69/300], Step [1900/27733], Loss: 2.2800\n",
      "Epoch [69/300], Step [2000/27733], Loss: 1.6958\n",
      "Epoch [69/300], Step [2100/27733], Loss: 2.7464\n",
      "Epoch [69/300], Step [2200/27733], Loss: 2.4980\n",
      "Epoch [69/300], Step [2300/27733], Loss: 1.9590\n",
      "Epoch [69/300], Step [2400/27733], Loss: 3.0131\n",
      "Epoch [69/300], Step [2500/27733], Loss: 2.3250\n",
      "Epoch [69/300], Step [2600/27733], Loss: 2.2446\n",
      "Epoch [69/300], Step [2700/27733], Loss: 2.5755\n",
      "Epoch [69/300], Step [2800/27733], Loss: 2.3802\n",
      "Epoch [69/300], Step [2900/27733], Loss: 2.0822\n",
      "Epoch [69/300], Step [3000/27733], Loss: 1.9788\n",
      "Epoch [69/300], Step [3100/27733], Loss: 2.5382\n",
      "Epoch [69/300], Step [3200/27733], Loss: 2.4775\n",
      "Epoch [69/300], Step [3300/27733], Loss: 2.0693\n",
      "Epoch [69/300], Step [3400/27733], Loss: 2.1950\n",
      "Epoch [69/300], Step [3500/27733], Loss: 2.1366\n",
      "Epoch [69/300], Step [3600/27733], Loss: 1.6968\n",
      "Epoch [69/300], Step [3700/27733], Loss: 2.9922\n",
      "Epoch [69/300], Step [3800/27733], Loss: 3.0435\n",
      "Epoch [69/300], Step [3900/27733], Loss: 1.8848\n",
      "Epoch [69/300], Step [4000/27733], Loss: 1.9403\n",
      "Epoch [69/300], Step [4100/27733], Loss: 2.0830\n",
      "Epoch [69/300], Step [4200/27733], Loss: 2.8143\n",
      "Epoch [69/300], Step [4300/27733], Loss: 2.7561\n",
      "Epoch [69/300], Step [4400/27733], Loss: 3.2845\n",
      "Epoch [69/300], Step [4500/27733], Loss: 2.5390\n",
      "Epoch [69/300], Step [4600/27733], Loss: 3.5165\n",
      "Epoch [69/300], Step [4700/27733], Loss: 3.0115\n",
      "Epoch [69/300], Step [4800/27733], Loss: 3.0519\n",
      "Epoch [69/300], Step [4900/27733], Loss: 2.1994\n",
      "Epoch [69/300], Step [5000/27733], Loss: 3.1303\n",
      "Epoch [69/300], Step [5100/27733], Loss: 2.5510\n",
      "Epoch [69/300], Step [5200/27733], Loss: 2.2883\n",
      "Epoch [69/300], Step [5300/27733], Loss: 2.1744\n",
      "Epoch [69/300], Step [5400/27733], Loss: 2.9234\n",
      "Epoch [69/300], Step [5500/27733], Loss: 2.2765\n",
      "Epoch [69/300], Step [5600/27733], Loss: 2.7115\n",
      "Epoch [69/300], Step [5700/27733], Loss: 2.8116\n",
      "Epoch [69/300], Step [5800/27733], Loss: 2.2141\n",
      "Epoch [69/300], Step [5900/27733], Loss: 2.4821\n",
      "Epoch [69/300], Step [6000/27733], Loss: 2.3641\n",
      "Epoch [69/300], Step [6100/27733], Loss: 2.6172\n",
      "Epoch [69/300], Step [6200/27733], Loss: 3.2115\n",
      "Epoch [69/300], Step [6300/27733], Loss: 2.5081\n",
      "Epoch [69/300], Step [6400/27733], Loss: 3.1241\n",
      "Epoch [69/300], Step [6500/27733], Loss: 2.4254\n",
      "Epoch [69/300], Step [6600/27733], Loss: 2.2954\n",
      "Epoch [69/300], Step [6700/27733], Loss: 1.7202\n",
      "Epoch [69/300], Step [6800/27733], Loss: 2.3287\n",
      "Epoch [69/300], Step [6900/27733], Loss: 2.2531\n",
      "Epoch [69/300], Step [7000/27733], Loss: 3.1128\n",
      "Epoch [69/300], Step [7100/27733], Loss: 2.7273\n",
      "Epoch [69/300], Step [7200/27733], Loss: 2.6290\n",
      "Epoch [69/300], Step [7300/27733], Loss: 2.6409\n",
      "Epoch [69/300], Step [7400/27733], Loss: 2.4752\n",
      "Epoch [69/300], Step [7500/27733], Loss: 3.6754\n",
      "Epoch [69/300], Step [7600/27733], Loss: 2.7828\n",
      "Epoch [69/300], Step [7700/27733], Loss: 2.6088\n",
      "Epoch [69/300], Step [7800/27733], Loss: 2.9364\n",
      "Epoch [69/300], Step [7900/27733], Loss: 2.2343\n",
      "Epoch [69/300], Step [8000/27733], Loss: 3.3312\n",
      "Epoch [69/300], Step [8100/27733], Loss: 3.1761\n",
      "Epoch [69/300], Step [8200/27733], Loss: 3.9476\n",
      "Epoch [69/300], Step [8300/27733], Loss: 3.3680\n",
      "Epoch [69/300], Step [8400/27733], Loss: 2.6628\n",
      "Epoch [69/300], Step [8500/27733], Loss: 3.0127\n",
      "Epoch [69/300], Step [8600/27733], Loss: 2.6384\n",
      "Epoch [69/300], Step [8700/27733], Loss: 3.6978\n",
      "Epoch [69/300], Step [8800/27733], Loss: 2.4024\n",
      "Epoch [69/300], Step [8900/27733], Loss: 2.6685\n",
      "Epoch [69/300], Step [9000/27733], Loss: 2.8688\n",
      "Epoch [69/300], Step [9100/27733], Loss: 2.8438\n",
      "Epoch [69/300], Step [9200/27733], Loss: 2.0466\n",
      "Epoch [69/300], Step [9300/27733], Loss: 2.5245\n",
      "Epoch [69/300], Step [9400/27733], Loss: 2.0774\n",
      "Epoch [69/300], Step [9500/27733], Loss: 2.5612\n",
      "Epoch [69/300], Step [9600/27733], Loss: 2.3588\n",
      "Epoch [69/300], Step [9700/27733], Loss: 2.0976\n",
      "Epoch [69/300], Step [9800/27733], Loss: 2.6766\n",
      "Epoch [69/300], Step [9900/27733], Loss: 3.0993\n",
      "Epoch [69/300], Step [10000/27733], Loss: 2.3496\n",
      "Epoch [69/300], Step [10100/27733], Loss: 3.1126\n",
      "Epoch [69/300], Step [10200/27733], Loss: 2.2887\n",
      "Epoch [69/300], Step [10300/27733], Loss: 2.5945\n",
      "Epoch [69/300], Step [10400/27733], Loss: 2.3465\n",
      "Epoch [69/300], Step [10500/27733], Loss: 2.4123\n",
      "Epoch [69/300], Step [10600/27733], Loss: 3.2401\n",
      "Epoch [69/300], Step [10700/27733], Loss: 2.8021\n",
      "Epoch [69/300], Step [10800/27733], Loss: 3.1139\n",
      "Epoch [69/300], Step [10900/27733], Loss: 2.3106\n",
      "Epoch [69/300], Step [11000/27733], Loss: 2.4704\n",
      "Epoch [69/300], Step [11100/27733], Loss: 3.1152\n",
      "Epoch [69/300], Step [11200/27733], Loss: 2.9828\n",
      "Epoch [69/300], Step [11300/27733], Loss: 2.9564\n",
      "Epoch [69/300], Step [11400/27733], Loss: 2.1503\n",
      "Epoch [69/300], Step [11500/27733], Loss: 2.9442\n",
      "Epoch [69/300], Step [11600/27733], Loss: 2.6167\n",
      "Epoch [69/300], Step [11700/27733], Loss: 2.4395\n",
      "Epoch [69/300], Step [11800/27733], Loss: 2.7434\n",
      "Epoch [69/300], Step [11900/27733], Loss: 2.5142\n",
      "Epoch [69/300], Step [12000/27733], Loss: 3.2649\n",
      "Epoch [69/300], Step [12100/27733], Loss: 2.0833\n",
      "Epoch [69/300], Step [12200/27733], Loss: 2.7923\n",
      "Epoch [69/300], Step [12300/27733], Loss: 2.3192\n",
      "Epoch [69/300], Step [12400/27733], Loss: 2.6334\n",
      "Epoch [69/300], Step [12500/27733], Loss: 2.0992\n",
      "Epoch [69/300], Step [12600/27733], Loss: 3.4737\n",
      "Epoch [69/300], Step [12700/27733], Loss: 2.2180\n",
      "Epoch [69/300], Step [12800/27733], Loss: 3.4735\n",
      "Epoch [69/300], Step [12900/27733], Loss: 2.4933\n",
      "Epoch [69/300], Step [13000/27733], Loss: 3.0392\n",
      "Epoch [69/300], Step [13100/27733], Loss: 2.3245\n",
      "Epoch [69/300], Step [13200/27733], Loss: 2.9366\n",
      "Epoch [69/300], Step [13300/27733], Loss: 2.1222\n",
      "Epoch [69/300], Step [13400/27733], Loss: 2.9354\n",
      "Epoch [69/300], Step [13500/27733], Loss: 3.0454\n",
      "Epoch [69/300], Step [13600/27733], Loss: 3.0297\n",
      "Epoch [69/300], Step [13700/27733], Loss: 3.0199\n",
      "Epoch [69/300], Step [13800/27733], Loss: 2.3606\n",
      "Epoch [69/300], Step [13900/27733], Loss: 3.8337\n",
      "Epoch [69/300], Step [14000/27733], Loss: 2.7409\n",
      "Epoch [69/300], Step [14100/27733], Loss: 2.3265\n",
      "Epoch [69/300], Step [14200/27733], Loss: 2.7792\n",
      "Epoch [69/300], Step [14300/27733], Loss: 3.1979\n",
      "Epoch [69/300], Step [14400/27733], Loss: 2.8598\n",
      "Epoch [69/300], Step [14500/27733], Loss: 2.8628\n",
      "Epoch [69/300], Step [14600/27733], Loss: 2.6861\n",
      "Epoch [69/300], Step [14700/27733], Loss: 2.1004\n",
      "Epoch [69/300], Step [14800/27733], Loss: 3.0248\n",
      "Epoch [69/300], Step [14900/27733], Loss: 3.0063\n",
      "Epoch [69/300], Step [15000/27733], Loss: 2.7858\n",
      "Epoch [69/300], Step [15100/27733], Loss: 3.0813\n",
      "Epoch [69/300], Step [15200/27733], Loss: 3.0870\n",
      "Epoch [69/300], Step [15300/27733], Loss: 2.9252\n",
      "Epoch [69/300], Step [15400/27733], Loss: 2.8644\n",
      "Epoch [69/300], Step [15500/27733], Loss: 2.1794\n",
      "Epoch [69/300], Step [15600/27733], Loss: 2.6671\n",
      "Epoch [69/300], Step [15700/27733], Loss: 2.7906\n",
      "Epoch [69/300], Step [15800/27733], Loss: 3.0827\n",
      "Epoch [69/300], Step [15900/27733], Loss: 2.8291\n",
      "Epoch [69/300], Step [16000/27733], Loss: 2.9923\n",
      "Epoch [69/300], Step [16100/27733], Loss: 2.6726\n",
      "Epoch [69/300], Step [16200/27733], Loss: 3.3622\n",
      "Epoch [69/300], Step [16300/27733], Loss: 2.9431\n",
      "Epoch [69/300], Step [16400/27733], Loss: 2.6137\n",
      "Epoch [69/300], Step [16500/27733], Loss: 2.3559\n",
      "Epoch [69/300], Step [16600/27733], Loss: 2.5382\n",
      "Epoch [69/300], Step [16700/27733], Loss: 2.3772\n",
      "Epoch [69/300], Step [16800/27733], Loss: 2.8772\n",
      "Epoch [69/300], Step [16900/27733], Loss: 2.6250\n",
      "Epoch [69/300], Step [17000/27733], Loss: 3.2480\n",
      "Epoch [69/300], Step [17100/27733], Loss: 2.7975\n",
      "Epoch [69/300], Step [17200/27733], Loss: 2.6030\n",
      "Epoch [69/300], Step [17300/27733], Loss: 2.6232\n",
      "Epoch [69/300], Step [17400/27733], Loss: 2.8397\n",
      "Epoch [69/300], Step [17500/27733], Loss: 3.7106\n",
      "Epoch [69/300], Step [17600/27733], Loss: 2.6988\n",
      "Epoch [69/300], Step [17700/27733], Loss: 2.0363\n",
      "Epoch [69/300], Step [17800/27733], Loss: 3.7118\n",
      "Epoch [69/300], Step [17900/27733], Loss: 2.9427\n",
      "Epoch [69/300], Step [18000/27733], Loss: 2.2449\n",
      "Epoch [69/300], Step [18100/27733], Loss: 2.1339\n",
      "Epoch [69/300], Step [18200/27733], Loss: 2.5757\n",
      "Epoch [69/300], Step [18300/27733], Loss: 2.7824\n",
      "Epoch [69/300], Step [18400/27733], Loss: 2.3157\n",
      "Epoch [69/300], Step [18500/27733], Loss: 2.8036\n",
      "Epoch [69/300], Step [18600/27733], Loss: 3.9460\n",
      "Epoch [69/300], Step [18700/27733], Loss: 2.4118\n",
      "Epoch [69/300], Step [18800/27733], Loss: 2.2940\n",
      "Epoch [69/300], Step [18900/27733], Loss: 3.0717\n",
      "Epoch [69/300], Step [19000/27733], Loss: 2.2781\n",
      "Epoch [69/300], Step [19100/27733], Loss: 2.8480\n",
      "Epoch [69/300], Step [19200/27733], Loss: 2.5919\n",
      "Epoch [69/300], Step [19300/27733], Loss: 2.9524\n",
      "Epoch [69/300], Step [19400/27733], Loss: 3.2774\n",
      "Epoch [69/300], Step [19500/27733], Loss: 3.6374\n",
      "Epoch [69/300], Step [19600/27733], Loss: 2.6349\n",
      "Epoch [69/300], Step [19700/27733], Loss: 2.9478\n",
      "Epoch [69/300], Step [19800/27733], Loss: 3.0473\n",
      "Epoch [69/300], Step [19900/27733], Loss: 2.9855\n",
      "Epoch [69/300], Step [20000/27733], Loss: 2.2901\n",
      "Epoch [69/300], Step [20100/27733], Loss: 2.4499\n",
      "Epoch [69/300], Step [20200/27733], Loss: 1.9275\n",
      "Epoch [69/300], Step [20300/27733], Loss: 2.2317\n",
      "Epoch [69/300], Step [20400/27733], Loss: 2.9925\n",
      "Epoch [69/300], Step [20500/27733], Loss: 3.0639\n",
      "Epoch [69/300], Step [20600/27733], Loss: 1.9660\n",
      "Epoch [69/300], Step [20700/27733], Loss: 3.2649\n",
      "Epoch [69/300], Step [20800/27733], Loss: 3.2516\n",
      "Epoch [69/300], Step [20900/27733], Loss: 4.0256\n",
      "Epoch [69/300], Step [21000/27733], Loss: 2.5811\n",
      "Epoch [69/300], Step [21100/27733], Loss: 3.2048\n",
      "Epoch [69/300], Step [21200/27733], Loss: 2.8921\n",
      "Epoch [69/300], Step [21300/27733], Loss: 3.2231\n",
      "Epoch [69/300], Step [21400/27733], Loss: 2.9877\n",
      "Epoch [69/300], Step [21500/27733], Loss: 2.3898\n",
      "Epoch [69/300], Step [21600/27733], Loss: 3.0241\n",
      "Epoch [69/300], Step [21700/27733], Loss: 2.4837\n",
      "Epoch [69/300], Step [21800/27733], Loss: 3.1149\n",
      "Epoch [69/300], Step [21900/27733], Loss: 2.5561\n",
      "Epoch [69/300], Step [22000/27733], Loss: 3.4827\n",
      "Epoch [69/300], Step [22100/27733], Loss: 2.7604\n",
      "Epoch [69/300], Step [22200/27733], Loss: 2.4735\n",
      "Epoch [69/300], Step [22300/27733], Loss: 2.2570\n",
      "Epoch [69/300], Step [22400/27733], Loss: 2.8561\n",
      "Epoch [69/300], Step [22500/27733], Loss: 2.6625\n",
      "Epoch [69/300], Step [22600/27733], Loss: 2.4156\n",
      "Epoch [69/300], Step [22700/27733], Loss: 4.0293\n",
      "Epoch [69/300], Step [22800/27733], Loss: 2.8348\n",
      "Epoch [69/300], Step [22900/27733], Loss: 2.5572\n",
      "Epoch [69/300], Step [23000/27733], Loss: 4.3187\n",
      "Epoch [69/300], Step [23100/27733], Loss: 2.1286\n",
      "Epoch [69/300], Step [23200/27733], Loss: 2.6297\n",
      "Epoch [69/300], Step [23300/27733], Loss: 3.1460\n",
      "Epoch [69/300], Step [23400/27733], Loss: 2.6796\n",
      "Epoch [69/300], Step [23500/27733], Loss: 3.3334\n",
      "Epoch [69/300], Step [23600/27733], Loss: 3.2284\n",
      "Epoch [69/300], Step [23700/27733], Loss: 3.4794\n",
      "Epoch [69/300], Step [23800/27733], Loss: 2.3645\n",
      "Epoch [69/300], Step [23900/27733], Loss: 3.3742\n",
      "Epoch [69/300], Step [24000/27733], Loss: 2.7990\n",
      "Epoch [69/300], Step [24100/27733], Loss: 2.3824\n",
      "Epoch [69/300], Step [24200/27733], Loss: 3.0030\n",
      "Epoch [69/300], Step [24300/27733], Loss: 2.2891\n",
      "Epoch [69/300], Step [24400/27733], Loss: 3.2879\n",
      "Epoch [69/300], Step [24500/27733], Loss: 2.4969\n",
      "Epoch [69/300], Step [24600/27733], Loss: 3.0073\n",
      "Epoch [69/300], Step [24700/27733], Loss: 2.2923\n",
      "Epoch [69/300], Step [24800/27733], Loss: 2.5303\n",
      "Epoch [69/300], Step [24900/27733], Loss: 3.2772\n",
      "Epoch [69/300], Step [25000/27733], Loss: 2.5561\n",
      "Epoch [69/300], Step [25100/27733], Loss: 2.6500\n",
      "Epoch [69/300], Step [25200/27733], Loss: 3.3964\n",
      "Epoch [69/300], Step [25300/27733], Loss: 3.5128\n",
      "Epoch [69/300], Step [25400/27733], Loss: 2.8781\n",
      "Epoch [69/300], Step [25500/27733], Loss: 2.9913\n",
      "Epoch [69/300], Step [25600/27733], Loss: 2.4780\n",
      "Epoch [69/300], Step [25700/27733], Loss: 2.8281\n",
      "Epoch [69/300], Step [25800/27733], Loss: 2.5785\n",
      "Epoch [69/300], Step [25900/27733], Loss: 2.3195\n",
      "Epoch [69/300], Step [26000/27733], Loss: 2.9205\n",
      "Epoch [69/300], Step [26100/27733], Loss: 3.0412\n",
      "Epoch [69/300], Step [26200/27733], Loss: 2.8806\n",
      "Epoch [69/300], Step [26300/27733], Loss: 2.2568\n",
      "Epoch [69/300], Step [26400/27733], Loss: 2.6189\n",
      "Epoch [69/300], Step [26500/27733], Loss: 3.3958\n",
      "Epoch [69/300], Step [26600/27733], Loss: 2.5670\n",
      "Epoch [69/300], Step [26700/27733], Loss: 2.7042\n",
      "Epoch [69/300], Step [26800/27733], Loss: 2.6581\n",
      "Epoch [69/300], Step [26900/27733], Loss: 2.1463\n",
      "Epoch [69/300], Step [27000/27733], Loss: 3.4506\n",
      "Epoch [69/300], Step [27100/27733], Loss: 2.5854\n",
      "Epoch [69/300], Step [27200/27733], Loss: 3.2837\n",
      "Epoch [69/300], Step [27300/27733], Loss: 3.2063\n",
      "Epoch [69/300], Step [27400/27733], Loss: 2.1185\n",
      "Epoch [69/300], Step [27500/27733], Loss: 3.1117\n",
      "Epoch [69/300], Step [27600/27733], Loss: 3.2365\n",
      "Epoch [69/300], Step [27700/27733], Loss: 3.7893\n",
      "Epoch [70/300], Step [100/27733], Loss: 2.0314\n",
      "Epoch [70/300], Step [200/27733], Loss: 2.1107\n",
      "Epoch [70/300], Step [300/27733], Loss: 3.1048\n",
      "Epoch [70/300], Step [400/27733], Loss: 2.0545\n",
      "Epoch [70/300], Step [500/27733], Loss: 2.7977\n",
      "Epoch [70/300], Step [600/27733], Loss: 2.4312\n",
      "Epoch [70/300], Step [700/27733], Loss: 2.5951\n",
      "Epoch [70/300], Step [800/27733], Loss: 2.7577\n",
      "Epoch [70/300], Step [900/27733], Loss: 2.2707\n",
      "Epoch [70/300], Step [1000/27733], Loss: 2.3105\n",
      "Epoch [70/300], Step [1100/27733], Loss: 2.3078\n",
      "Epoch [70/300], Step [1200/27733], Loss: 2.3833\n",
      "Epoch [70/300], Step [1300/27733], Loss: 2.7419\n",
      "Epoch [70/300], Step [1400/27733], Loss: 1.8221\n",
      "Epoch [70/300], Step [1500/27733], Loss: 2.6505\n",
      "Epoch [70/300], Step [1600/27733], Loss: 2.3144\n",
      "Epoch [70/300], Step [1700/27733], Loss: 3.5367\n",
      "Epoch [70/300], Step [1800/27733], Loss: 2.5831\n",
      "Epoch [70/300], Step [1900/27733], Loss: 2.8060\n",
      "Epoch [70/300], Step [2000/27733], Loss: 2.4754\n",
      "Epoch [70/300], Step [2100/27733], Loss: 2.2795\n",
      "Epoch [70/300], Step [2200/27733], Loss: 2.2684\n",
      "Epoch [70/300], Step [2300/27733], Loss: 2.2345\n",
      "Epoch [70/300], Step [2400/27733], Loss: 1.9401\n",
      "Epoch [70/300], Step [2500/27733], Loss: 3.1370\n",
      "Epoch [70/300], Step [2600/27733], Loss: 2.6525\n",
      "Epoch [70/300], Step [2700/27733], Loss: 2.3620\n",
      "Epoch [70/300], Step [2800/27733], Loss: 2.0929\n",
      "Epoch [70/300], Step [2900/27733], Loss: 2.3444\n",
      "Epoch [70/300], Step [3000/27733], Loss: 1.9009\n",
      "Epoch [70/300], Step [3100/27733], Loss: 2.0669\n",
      "Epoch [70/300], Step [3200/27733], Loss: 2.0623\n",
      "Epoch [70/300], Step [3300/27733], Loss: 2.9118\n",
      "Epoch [70/300], Step [3400/27733], Loss: 1.7377\n",
      "Epoch [70/300], Step [3500/27733], Loss: 2.4913\n",
      "Epoch [70/300], Step [3600/27733], Loss: 2.4019\n",
      "Epoch [70/300], Step [3700/27733], Loss: 3.1050\n",
      "Epoch [70/300], Step [3800/27733], Loss: 3.2653\n",
      "Epoch [70/300], Step [3900/27733], Loss: 2.5494\n",
      "Epoch [70/300], Step [4000/27733], Loss: 3.3312\n",
      "Epoch [70/300], Step [4100/27733], Loss: 1.9663\n",
      "Epoch [70/300], Step [4200/27733], Loss: 2.7308\n",
      "Epoch [70/300], Step [4300/27733], Loss: 3.1966\n",
      "Epoch [70/300], Step [4400/27733], Loss: 2.8053\n",
      "Epoch [70/300], Step [4500/27733], Loss: 2.5676\n",
      "Epoch [70/300], Step [4600/27733], Loss: 2.1338\n",
      "Epoch [70/300], Step [4700/27733], Loss: 2.1999\n",
      "Epoch [70/300], Step [4800/27733], Loss: 3.0933\n",
      "Epoch [70/300], Step [4900/27733], Loss: 2.4281\n",
      "Epoch [70/300], Step [5000/27733], Loss: 2.2784\n",
      "Epoch [70/300], Step [5100/27733], Loss: 3.1065\n",
      "Epoch [70/300], Step [5200/27733], Loss: 2.3915\n",
      "Epoch [70/300], Step [5300/27733], Loss: 2.7842\n",
      "Epoch [70/300], Step [5400/27733], Loss: 2.8874\n",
      "Epoch [70/300], Step [5500/27733], Loss: 2.5566\n",
      "Epoch [70/300], Step [5600/27733], Loss: 2.9486\n",
      "Epoch [70/300], Step [5700/27733], Loss: 2.5608\n",
      "Epoch [70/300], Step [5800/27733], Loss: 2.2718\n",
      "Epoch [70/300], Step [5900/27733], Loss: 3.2992\n",
      "Epoch [70/300], Step [6000/27733], Loss: 2.2691\n",
      "Epoch [70/300], Step [6100/27733], Loss: 2.8641\n",
      "Epoch [70/300], Step [6200/27733], Loss: 2.6784\n",
      "Epoch [70/300], Step [6300/27733], Loss: 2.4454\n",
      "Epoch [70/300], Step [6400/27733], Loss: 1.9215\n",
      "Epoch [70/300], Step [6500/27733], Loss: 2.5254\n",
      "Epoch [70/300], Step [6600/27733], Loss: 2.0349\n",
      "Epoch [70/300], Step [6700/27733], Loss: 2.9363\n",
      "Epoch [70/300], Step [6800/27733], Loss: 3.4032\n",
      "Epoch [70/300], Step [6900/27733], Loss: 2.5460\n",
      "Epoch [70/300], Step [7000/27733], Loss: 2.6341\n",
      "Epoch [70/300], Step [7100/27733], Loss: 2.4613\n",
      "Epoch [70/300], Step [7200/27733], Loss: 2.4078\n",
      "Epoch [70/300], Step [7300/27733], Loss: 2.9048\n",
      "Epoch [70/300], Step [7400/27733], Loss: 2.2252\n",
      "Epoch [70/300], Step [7500/27733], Loss: 2.1272\n",
      "Epoch [70/300], Step [7600/27733], Loss: 1.7282\n",
      "Epoch [70/300], Step [7700/27733], Loss: 3.0855\n",
      "Epoch [70/300], Step [7800/27733], Loss: 1.8627\n",
      "Epoch [70/300], Step [7900/27733], Loss: 2.4505\n",
      "Epoch [70/300], Step [8000/27733], Loss: 2.6951\n",
      "Epoch [70/300], Step [8100/27733], Loss: 2.6565\n",
      "Epoch [70/300], Step [8200/27733], Loss: 2.7566\n",
      "Epoch [70/300], Step [8300/27733], Loss: 2.1151\n",
      "Epoch [70/300], Step [8400/27733], Loss: 2.3017\n",
      "Epoch [70/300], Step [8500/27733], Loss: 3.5684\n",
      "Epoch [70/300], Step [8600/27733], Loss: 1.9324\n",
      "Epoch [70/300], Step [8700/27733], Loss: 2.5560\n",
      "Epoch [70/300], Step [8800/27733], Loss: 2.0885\n",
      "Epoch [70/300], Step [8900/27733], Loss: 2.5076\n",
      "Epoch [70/300], Step [9000/27733], Loss: 2.4568\n",
      "Epoch [70/300], Step [9100/27733], Loss: 2.9014\n",
      "Epoch [70/300], Step [9200/27733], Loss: 2.2536\n",
      "Epoch [70/300], Step [9300/27733], Loss: 2.8807\n",
      "Epoch [70/300], Step [9400/27733], Loss: 2.7255\n",
      "Epoch [70/300], Step [9500/27733], Loss: 3.6084\n",
      "Epoch [70/300], Step [9600/27733], Loss: 2.9588\n",
      "Epoch [70/300], Step [9700/27733], Loss: 2.7744\n",
      "Epoch [70/300], Step [9800/27733], Loss: 2.4172\n",
      "Epoch [70/300], Step [9900/27733], Loss: 2.4540\n",
      "Epoch [70/300], Step [10000/27733], Loss: 2.6698\n",
      "Epoch [70/300], Step [10100/27733], Loss: 3.0047\n",
      "Epoch [70/300], Step [10200/27733], Loss: 2.1280\n",
      "Epoch [70/300], Step [10300/27733], Loss: 1.8978\n",
      "Epoch [70/300], Step [10400/27733], Loss: 2.8363\n",
      "Epoch [70/300], Step [10500/27733], Loss: 2.7807\n",
      "Epoch [70/300], Step [10600/27733], Loss: 2.7042\n",
      "Epoch [70/300], Step [10700/27733], Loss: 2.7698\n",
      "Epoch [70/300], Step [10800/27733], Loss: 2.8288\n",
      "Epoch [70/300], Step [10900/27733], Loss: 2.3595\n",
      "Epoch [70/300], Step [11000/27733], Loss: 2.3563\n",
      "Epoch [70/300], Step [11100/27733], Loss: 3.3441\n",
      "Epoch [70/300], Step [11200/27733], Loss: 2.8911\n",
      "Epoch [70/300], Step [11300/27733], Loss: 2.9561\n",
      "Epoch [70/300], Step [11400/27733], Loss: 2.9991\n",
      "Epoch [70/300], Step [11500/27733], Loss: 2.8530\n",
      "Epoch [70/300], Step [11600/27733], Loss: 2.3840\n",
      "Epoch [70/300], Step [11700/27733], Loss: 2.9857\n",
      "Epoch [70/300], Step [11800/27733], Loss: 2.3952\n",
      "Epoch [70/300], Step [11900/27733], Loss: 2.2762\n",
      "Epoch [70/300], Step [12000/27733], Loss: 2.8428\n",
      "Epoch [70/300], Step [12100/27733], Loss: 2.1577\n",
      "Epoch [70/300], Step [12200/27733], Loss: 2.8989\n",
      "Epoch [70/300], Step [12300/27733], Loss: 2.3342\n",
      "Epoch [70/300], Step [12400/27733], Loss: 2.7521\n",
      "Epoch [70/300], Step [12500/27733], Loss: 3.2580\n",
      "Epoch [70/300], Step [12600/27733], Loss: 2.8410\n",
      "Epoch [70/300], Step [12700/27733], Loss: 2.6267\n",
      "Epoch [70/300], Step [12800/27733], Loss: 2.2537\n",
      "Epoch [70/300], Step [12900/27733], Loss: 2.2490\n",
      "Epoch [70/300], Step [13000/27733], Loss: 2.7924\n",
      "Epoch [70/300], Step [13100/27733], Loss: 3.0445\n",
      "Epoch [70/300], Step [13200/27733], Loss: 2.8127\n",
      "Epoch [70/300], Step [13300/27733], Loss: 2.9589\n",
      "Epoch [70/300], Step [13400/27733], Loss: 3.4357\n",
      "Epoch [70/300], Step [13500/27733], Loss: 2.7575\n",
      "Epoch [70/300], Step [13600/27733], Loss: 2.5425\n",
      "Epoch [70/300], Step [13700/27733], Loss: 3.3428\n",
      "Epoch [70/300], Step [13800/27733], Loss: 1.9966\n",
      "Epoch [70/300], Step [13900/27733], Loss: 2.6904\n",
      "Epoch [70/300], Step [14000/27733], Loss: 3.1473\n",
      "Epoch [70/300], Step [14100/27733], Loss: 2.5007\n",
      "Epoch [70/300], Step [14200/27733], Loss: 3.2263\n",
      "Epoch [70/300], Step [14300/27733], Loss: 2.3848\n",
      "Epoch [70/300], Step [14400/27733], Loss: 3.0818\n",
      "Epoch [70/300], Step [14500/27733], Loss: 3.4823\n",
      "Epoch [70/300], Step [14600/27733], Loss: 2.0562\n",
      "Epoch [70/300], Step [14700/27733], Loss: 3.6876\n",
      "Epoch [70/300], Step [14800/27733], Loss: 3.2111\n",
      "Epoch [70/300], Step [14900/27733], Loss: 3.3039\n",
      "Epoch [70/300], Step [15000/27733], Loss: 2.8321\n",
      "Epoch [70/300], Step [15100/27733], Loss: 1.7411\n",
      "Epoch [70/300], Step [15200/27733], Loss: 3.2371\n",
      "Epoch [70/300], Step [15300/27733], Loss: 2.8195\n",
      "Epoch [70/300], Step [15400/27733], Loss: 3.7082\n",
      "Epoch [70/300], Step [15500/27733], Loss: 3.0397\n",
      "Epoch [70/300], Step [15600/27733], Loss: 2.2473\n",
      "Epoch [70/300], Step [15700/27733], Loss: 2.8854\n",
      "Epoch [70/300], Step [15800/27733], Loss: 3.3746\n",
      "Epoch [70/300], Step [15900/27733], Loss: 2.6437\n",
      "Epoch [70/300], Step [16000/27733], Loss: 2.0923\n",
      "Epoch [70/300], Step [16100/27733], Loss: 2.0121\n",
      "Epoch [70/300], Step [16200/27733], Loss: 2.8376\n",
      "Epoch [70/300], Step [16300/27733], Loss: 3.1652\n",
      "Epoch [70/300], Step [16400/27733], Loss: 2.3463\n",
      "Epoch [70/300], Step [16500/27733], Loss: 2.8708\n",
      "Epoch [70/300], Step [16600/27733], Loss: 2.5635\n",
      "Epoch [70/300], Step [16700/27733], Loss: 2.3789\n",
      "Epoch [70/300], Step [16800/27733], Loss: 3.7489\n",
      "Epoch [70/300], Step [16900/27733], Loss: 2.7417\n",
      "Epoch [70/300], Step [17000/27733], Loss: 2.5167\n",
      "Epoch [70/300], Step [17100/27733], Loss: 2.8529\n",
      "Epoch [70/300], Step [17200/27733], Loss: 2.3309\n",
      "Epoch [70/300], Step [17300/27733], Loss: 2.5771\n",
      "Epoch [70/300], Step [17400/27733], Loss: 3.5998\n",
      "Epoch [70/300], Step [17500/27733], Loss: 2.8751\n",
      "Epoch [70/300], Step [17600/27733], Loss: 2.8952\n",
      "Epoch [70/300], Step [17700/27733], Loss: 2.7253\n",
      "Epoch [70/300], Step [17800/27733], Loss: 2.5675\n",
      "Epoch [70/300], Step [17900/27733], Loss: 2.9145\n",
      "Epoch [70/300], Step [18000/27733], Loss: 2.2732\n",
      "Epoch [70/300], Step [18100/27733], Loss: 2.8035\n",
      "Epoch [70/300], Step [18200/27733], Loss: 2.5635\n",
      "Epoch [70/300], Step [18300/27733], Loss: 2.2816\n",
      "Epoch [70/300], Step [18400/27733], Loss: 2.7478\n",
      "Epoch [70/300], Step [18500/27733], Loss: 3.2138\n",
      "Epoch [70/300], Step [18600/27733], Loss: 3.0902\n",
      "Epoch [70/300], Step [18700/27733], Loss: 2.4604\n",
      "Epoch [70/300], Step [18800/27733], Loss: 2.4495\n",
      "Epoch [70/300], Step [18900/27733], Loss: 2.1201\n",
      "Epoch [70/300], Step [19000/27733], Loss: 2.3827\n",
      "Epoch [70/300], Step [19100/27733], Loss: 3.6312\n",
      "Epoch [70/300], Step [19200/27733], Loss: 2.7758\n",
      "Epoch [70/300], Step [19300/27733], Loss: 2.7702\n",
      "Epoch [70/300], Step [19400/27733], Loss: 3.2142\n",
      "Epoch [70/300], Step [19500/27733], Loss: 3.0588\n",
      "Epoch [70/300], Step [19600/27733], Loss: 3.0849\n",
      "Epoch [70/300], Step [19700/27733], Loss: 1.8816\n",
      "Epoch [70/300], Step [19800/27733], Loss: 2.7847\n",
      "Epoch [70/300], Step [19900/27733], Loss: 2.3920\n",
      "Epoch [70/300], Step [20000/27733], Loss: 1.7282\n",
      "Epoch [70/300], Step [20100/27733], Loss: 2.7185\n",
      "Epoch [70/300], Step [20200/27733], Loss: 2.8087\n",
      "Epoch [70/300], Step [20300/27733], Loss: 2.0204\n",
      "Epoch [70/300], Step [20400/27733], Loss: 2.3498\n",
      "Epoch [70/300], Step [20500/27733], Loss: 1.7275\n",
      "Epoch [70/300], Step [20600/27733], Loss: 2.2673\n",
      "Epoch [70/300], Step [20700/27733], Loss: 3.1608\n",
      "Epoch [70/300], Step [20800/27733], Loss: 2.6888\n",
      "Epoch [70/300], Step [20900/27733], Loss: 2.1363\n",
      "Epoch [70/300], Step [21000/27733], Loss: 3.1898\n",
      "Epoch [70/300], Step [21100/27733], Loss: 2.4807\n",
      "Epoch [70/300], Step [21200/27733], Loss: 2.6946\n",
      "Epoch [70/300], Step [21300/27733], Loss: 2.8992\n",
      "Epoch [70/300], Step [21400/27733], Loss: 2.9586\n",
      "Epoch [70/300], Step [21500/27733], Loss: 2.5799\n",
      "Epoch [70/300], Step [21600/27733], Loss: 2.3378\n",
      "Epoch [70/300], Step [21700/27733], Loss: 3.6005\n",
      "Epoch [70/300], Step [21800/27733], Loss: 2.4588\n",
      "Epoch [70/300], Step [21900/27733], Loss: 3.4611\n",
      "Epoch [70/300], Step [22000/27733], Loss: 2.9249\n",
      "Epoch [70/300], Step [22100/27733], Loss: 3.0649\n",
      "Epoch [70/300], Step [22200/27733], Loss: 2.0854\n",
      "Epoch [70/300], Step [22300/27733], Loss: 3.7763\n",
      "Epoch [70/300], Step [22400/27733], Loss: 3.5366\n",
      "Epoch [70/300], Step [22500/27733], Loss: 2.8133\n",
      "Epoch [70/300], Step [22600/27733], Loss: 2.9960\n",
      "Epoch [70/300], Step [22700/27733], Loss: 2.7378\n",
      "Epoch [70/300], Step [22800/27733], Loss: 2.9331\n",
      "Epoch [70/300], Step [22900/27733], Loss: 2.4631\n",
      "Epoch [70/300], Step [23000/27733], Loss: 3.6737\n",
      "Epoch [70/300], Step [23100/27733], Loss: 2.7042\n",
      "Epoch [70/300], Step [23200/27733], Loss: 3.0757\n",
      "Epoch [70/300], Step [23300/27733], Loss: 2.7657\n",
      "Epoch [70/300], Step [23400/27733], Loss: 2.4606\n",
      "Epoch [70/300], Step [23500/27733], Loss: 3.3059\n",
      "Epoch [70/300], Step [23600/27733], Loss: 2.2635\n",
      "Epoch [70/300], Step [23700/27733], Loss: 3.1444\n",
      "Epoch [70/300], Step [23800/27733], Loss: 2.5018\n",
      "Epoch [70/300], Step [23900/27733], Loss: 3.2381\n",
      "Epoch [70/300], Step [24000/27733], Loss: 2.3085\n",
      "Epoch [70/300], Step [24100/27733], Loss: 2.7092\n",
      "Epoch [70/300], Step [24200/27733], Loss: 2.4794\n",
      "Epoch [70/300], Step [24300/27733], Loss: 2.8652\n",
      "Epoch [70/300], Step [24400/27733], Loss: 2.9878\n",
      "Epoch [70/300], Step [24500/27733], Loss: 2.8879\n",
      "Epoch [70/300], Step [24600/27733], Loss: 3.2774\n",
      "Epoch [70/300], Step [24700/27733], Loss: 3.4940\n",
      "Epoch [70/300], Step [24800/27733], Loss: 2.8378\n",
      "Epoch [70/300], Step [24900/27733], Loss: 3.2059\n",
      "Epoch [70/300], Step [25000/27733], Loss: 2.6870\n",
      "Epoch [70/300], Step [25100/27733], Loss: 3.2468\n",
      "Epoch [70/300], Step [25200/27733], Loss: 3.1569\n",
      "Epoch [70/300], Step [25300/27733], Loss: 2.7726\n",
      "Epoch [70/300], Step [25400/27733], Loss: 2.5656\n",
      "Epoch [70/300], Step [25500/27733], Loss: 2.4116\n",
      "Epoch [70/300], Step [25600/27733], Loss: 2.3824\n",
      "Epoch [70/300], Step [25700/27733], Loss: 3.9401\n",
      "Epoch [70/300], Step [25800/27733], Loss: 2.3680\n",
      "Epoch [70/300], Step [25900/27733], Loss: 3.1785\n",
      "Epoch [70/300], Step [26000/27733], Loss: 2.1460\n",
      "Epoch [70/300], Step [26100/27733], Loss: 2.0576\n",
      "Epoch [70/300], Step [26200/27733], Loss: 2.9679\n",
      "Epoch [70/300], Step [26300/27733], Loss: 3.1549\n",
      "Epoch [70/300], Step [26400/27733], Loss: 2.7704\n",
      "Epoch [70/300], Step [26500/27733], Loss: 2.0465\n",
      "Epoch [70/300], Step [26600/27733], Loss: 2.7788\n",
      "Epoch [70/300], Step [26700/27733], Loss: 2.3937\n",
      "Epoch [70/300], Step [26800/27733], Loss: 2.7893\n",
      "Epoch [70/300], Step [26900/27733], Loss: 3.3315\n",
      "Epoch [70/300], Step [27000/27733], Loss: 2.6195\n",
      "Epoch [70/300], Step [27100/27733], Loss: 2.5144\n",
      "Epoch [70/300], Step [27200/27733], Loss: 3.4171\n",
      "Epoch [70/300], Step [27300/27733], Loss: 2.8334\n",
      "Epoch [70/300], Step [27400/27733], Loss: 2.6935\n",
      "Epoch [70/300], Step [27500/27733], Loss: 3.2933\n",
      "Epoch [70/300], Step [27600/27733], Loss: 2.8685\n",
      "Epoch [70/300], Step [27700/27733], Loss: 3.0237\n",
      "Epoch [71/300], Step [100/27733], Loss: 2.0163\n",
      "Epoch [71/300], Step [200/27733], Loss: 3.1133\n",
      "Epoch [71/300], Step [300/27733], Loss: 2.7865\n",
      "Epoch [71/300], Step [400/27733], Loss: 2.6647\n",
      "Epoch [71/300], Step [500/27733], Loss: 1.9381\n",
      "Epoch [71/300], Step [600/27733], Loss: 1.7466\n",
      "Epoch [71/300], Step [700/27733], Loss: 2.3893\n",
      "Epoch [71/300], Step [800/27733], Loss: 2.2375\n",
      "Epoch [71/300], Step [900/27733], Loss: 3.0097\n",
      "Epoch [71/300], Step [1000/27733], Loss: 1.7728\n",
      "Epoch [71/300], Step [1100/27733], Loss: 2.6661\n",
      "Epoch [71/300], Step [1200/27733], Loss: 2.4840\n",
      "Epoch [71/300], Step [1300/27733], Loss: 2.5717\n",
      "Epoch [71/300], Step [1400/27733], Loss: 2.5489\n",
      "Epoch [71/300], Step [1500/27733], Loss: 1.9505\n",
      "Epoch [71/300], Step [1600/27733], Loss: 2.2572\n",
      "Epoch [71/300], Step [1700/27733], Loss: 2.5403\n",
      "Epoch [71/300], Step [1800/27733], Loss: 2.9949\n",
      "Epoch [71/300], Step [1900/27733], Loss: 1.6876\n",
      "Epoch [71/300], Step [2000/27733], Loss: 2.0866\n",
      "Epoch [71/300], Step [2100/27733], Loss: 1.9459\n",
      "Epoch [71/300], Step [2200/27733], Loss: 2.2145\n",
      "Epoch [71/300], Step [2300/27733], Loss: 2.5404\n",
      "Epoch [71/300], Step [2400/27733], Loss: 2.6958\n",
      "Epoch [71/300], Step [2500/27733], Loss: 3.1619\n",
      "Epoch [71/300], Step [2600/27733], Loss: 2.1254\n",
      "Epoch [71/300], Step [2700/27733], Loss: 2.8530\n",
      "Epoch [71/300], Step [2800/27733], Loss: 2.6494\n",
      "Epoch [71/300], Step [2900/27733], Loss: 2.2238\n",
      "Epoch [71/300], Step [3000/27733], Loss: 1.2458\n",
      "Epoch [71/300], Step [3100/27733], Loss: 2.4810\n",
      "Epoch [71/300], Step [3200/27733], Loss: 2.4556\n",
      "Epoch [71/300], Step [3300/27733], Loss: 3.4950\n",
      "Epoch [71/300], Step [3400/27733], Loss: 3.0139\n",
      "Epoch [71/300], Step [3500/27733], Loss: 2.8300\n",
      "Epoch [71/300], Step [3600/27733], Loss: 1.6863\n",
      "Epoch [71/300], Step [3700/27733], Loss: 2.6920\n",
      "Epoch [71/300], Step [3800/27733], Loss: 2.0541\n",
      "Epoch [71/300], Step [3900/27733], Loss: 2.2558\n",
      "Epoch [71/300], Step [4000/27733], Loss: 2.4811\n",
      "Epoch [71/300], Step [4100/27733], Loss: 1.9097\n",
      "Epoch [71/300], Step [4200/27733], Loss: 2.1530\n",
      "Epoch [71/300], Step [4300/27733], Loss: 1.7758\n",
      "Epoch [71/300], Step [4400/27733], Loss: 2.8642\n",
      "Epoch [71/300], Step [4500/27733], Loss: 2.8718\n",
      "Epoch [71/300], Step [4600/27733], Loss: 1.9244\n",
      "Epoch [71/300], Step [4700/27733], Loss: 2.1405\n",
      "Epoch [71/300], Step [4800/27733], Loss: 2.9199\n",
      "Epoch [71/300], Step [4900/27733], Loss: 2.4147\n",
      "Epoch [71/300], Step [5000/27733], Loss: 2.8022\n",
      "Epoch [71/300], Step [5100/27733], Loss: 1.7771\n",
      "Epoch [71/300], Step [5200/27733], Loss: 2.3798\n",
      "Epoch [71/300], Step [5300/27733], Loss: 3.0873\n",
      "Epoch [71/300], Step [5400/27733], Loss: 2.5007\n",
      "Epoch [71/300], Step [5500/27733], Loss: 2.4820\n",
      "Epoch [71/300], Step [5600/27733], Loss: 2.7851\n",
      "Epoch [71/300], Step [5700/27733], Loss: 2.7427\n",
      "Epoch [71/300], Step [5800/27733], Loss: 2.4659\n",
      "Epoch [71/300], Step [5900/27733], Loss: 2.9898\n",
      "Epoch [71/300], Step [6000/27733], Loss: 3.2672\n",
      "Epoch [71/300], Step [6100/27733], Loss: 2.0723\n",
      "Epoch [71/300], Step [6200/27733], Loss: 2.8796\n",
      "Epoch [71/300], Step [6300/27733], Loss: 3.5543\n",
      "Epoch [71/300], Step [6400/27733], Loss: 2.8644\n",
      "Epoch [71/300], Step [6500/27733], Loss: 2.5353\n",
      "Epoch [71/300], Step [6600/27733], Loss: 3.1305\n",
      "Epoch [71/300], Step [6700/27733], Loss: 2.7949\n",
      "Epoch [71/300], Step [6800/27733], Loss: 2.3242\n",
      "Epoch [71/300], Step [6900/27733], Loss: 2.9299\n",
      "Epoch [71/300], Step [7000/27733], Loss: 2.2795\n",
      "Epoch [71/300], Step [7100/27733], Loss: 2.4965\n",
      "Epoch [71/300], Step [7200/27733], Loss: 2.4483\n",
      "Epoch [71/300], Step [7300/27733], Loss: 2.9742\n",
      "Epoch [71/300], Step [7400/27733], Loss: 3.0065\n",
      "Epoch [71/300], Step [7500/27733], Loss: 2.4353\n",
      "Epoch [71/300], Step [7600/27733], Loss: 2.0097\n",
      "Epoch [71/300], Step [7700/27733], Loss: 2.5908\n",
      "Epoch [71/300], Step [7800/27733], Loss: 2.2889\n",
      "Epoch [71/300], Step [7900/27733], Loss: 2.6918\n",
      "Epoch [71/300], Step [8000/27733], Loss: 2.2353\n",
      "Epoch [71/300], Step [8100/27733], Loss: 3.1621\n",
      "Epoch [71/300], Step [8200/27733], Loss: 2.7087\n",
      "Epoch [71/300], Step [8300/27733], Loss: 3.4285\n",
      "Epoch [71/300], Step [8400/27733], Loss: 1.9226\n",
      "Epoch [71/300], Step [8500/27733], Loss: 2.6005\n",
      "Epoch [71/300], Step [8600/27733], Loss: 2.7026\n",
      "Epoch [71/300], Step [8700/27733], Loss: 2.4060\n",
      "Epoch [71/300], Step [8800/27733], Loss: 2.8763\n",
      "Epoch [71/300], Step [8900/27733], Loss: 2.3946\n",
      "Epoch [71/300], Step [9000/27733], Loss: 2.7366\n",
      "Epoch [71/300], Step [9100/27733], Loss: 2.4959\n",
      "Epoch [71/300], Step [9200/27733], Loss: 2.7929\n",
      "Epoch [71/300], Step [9300/27733], Loss: 2.9560\n",
      "Epoch [71/300], Step [9400/27733], Loss: 3.2598\n",
      "Epoch [71/300], Step [9500/27733], Loss: 2.8132\n",
      "Epoch [71/300], Step [9600/27733], Loss: 2.8705\n",
      "Epoch [71/300], Step [9700/27733], Loss: 2.2860\n",
      "Epoch [71/300], Step [9800/27733], Loss: 2.7875\n",
      "Epoch [71/300], Step [9900/27733], Loss: 2.1693\n",
      "Epoch [71/300], Step [10000/27733], Loss: 2.8300\n",
      "Epoch [71/300], Step [10100/27733], Loss: 3.1269\n",
      "Epoch [71/300], Step [10200/27733], Loss: 2.1868\n",
      "Epoch [71/300], Step [10300/27733], Loss: 2.9449\n",
      "Epoch [71/300], Step [10400/27733], Loss: 2.4808\n",
      "Epoch [71/300], Step [10500/27733], Loss: 3.0373\n",
      "Epoch [71/300], Step [10600/27733], Loss: 2.9614\n",
      "Epoch [71/300], Step [10700/27733], Loss: 2.6429\n",
      "Epoch [71/300], Step [10800/27733], Loss: 3.4086\n",
      "Epoch [71/300], Step [10900/27733], Loss: 2.1665\n",
      "Epoch [71/300], Step [11000/27733], Loss: 2.9778\n",
      "Epoch [71/300], Step [11100/27733], Loss: 3.2054\n",
      "Epoch [71/300], Step [11200/27733], Loss: 3.1089\n",
      "Epoch [71/300], Step [11300/27733], Loss: 3.5661\n",
      "Epoch [71/300], Step [11400/27733], Loss: 2.3462\n",
      "Epoch [71/300], Step [11500/27733], Loss: 2.4416\n",
      "Epoch [71/300], Step [11600/27733], Loss: 2.1768\n",
      "Epoch [71/300], Step [11700/27733], Loss: 3.6562\n",
      "Epoch [71/300], Step [11800/27733], Loss: 2.3913\n",
      "Epoch [71/300], Step [11900/27733], Loss: 2.3170\n",
      "Epoch [71/300], Step [12000/27733], Loss: 2.4311\n",
      "Epoch [71/300], Step [12100/27733], Loss: 2.5044\n",
      "Epoch [71/300], Step [12200/27733], Loss: 2.4799\n",
      "Epoch [71/300], Step [12300/27733], Loss: 2.5001\n",
      "Epoch [71/300], Step [12400/27733], Loss: 2.7632\n",
      "Epoch [71/300], Step [12500/27733], Loss: 1.9868\n",
      "Epoch [71/300], Step [12600/27733], Loss: 3.1722\n",
      "Epoch [71/300], Step [12700/27733], Loss: 3.1805\n",
      "Epoch [71/300], Step [12800/27733], Loss: 1.6604\n",
      "Epoch [71/300], Step [12900/27733], Loss: 2.0566\n",
      "Epoch [71/300], Step [13000/27733], Loss: 3.8962\n",
      "Epoch [71/300], Step [13100/27733], Loss: 2.5083\n",
      "Epoch [71/300], Step [13200/27733], Loss: 3.2383\n",
      "Epoch [71/300], Step [13300/27733], Loss: 3.0986\n",
      "Epoch [71/300], Step [13400/27733], Loss: 2.6336\n",
      "Epoch [71/300], Step [13500/27733], Loss: 2.9929\n",
      "Epoch [71/300], Step [13600/27733], Loss: 2.8210\n",
      "Epoch [71/300], Step [13700/27733], Loss: 2.8109\n",
      "Epoch [71/300], Step [13800/27733], Loss: 2.0985\n",
      "Epoch [71/300], Step [13900/27733], Loss: 3.8464\n",
      "Epoch [71/300], Step [14000/27733], Loss: 2.5842\n",
      "Epoch [71/300], Step [14100/27733], Loss: 2.8515\n",
      "Epoch [71/300], Step [14200/27733], Loss: 2.3372\n",
      "Epoch [71/300], Step [14300/27733], Loss: 2.9604\n",
      "Epoch [71/300], Step [14400/27733], Loss: 2.5338\n",
      "Epoch [71/300], Step [14500/27733], Loss: 2.7721\n",
      "Epoch [71/300], Step [14600/27733], Loss: 2.9146\n",
      "Epoch [71/300], Step [14700/27733], Loss: 2.0616\n",
      "Epoch [71/300], Step [14800/27733], Loss: 2.2667\n",
      "Epoch [71/300], Step [14900/27733], Loss: 2.8117\n",
      "Epoch [71/300], Step [15000/27733], Loss: 2.3418\n",
      "Epoch [71/300], Step [15100/27733], Loss: 2.6993\n",
      "Epoch [71/300], Step [15200/27733], Loss: 2.7809\n",
      "Epoch [71/300], Step [15300/27733], Loss: 2.2302\n",
      "Epoch [71/300], Step [15400/27733], Loss: 2.3472\n",
      "Epoch [71/300], Step [15500/27733], Loss: 2.5511\n",
      "Epoch [71/300], Step [15600/27733], Loss: 2.8826\n",
      "Epoch [71/300], Step [15700/27733], Loss: 2.6867\n",
      "Epoch [71/300], Step [15800/27733], Loss: 2.7080\n",
      "Epoch [71/300], Step [15900/27733], Loss: 2.9805\n",
      "Epoch [71/300], Step [16000/27733], Loss: 2.6766\n",
      "Epoch [71/300], Step [16100/27733], Loss: 2.5578\n",
      "Epoch [71/300], Step [16200/27733], Loss: 2.3664\n",
      "Epoch [71/300], Step [16300/27733], Loss: 2.2191\n",
      "Epoch [71/300], Step [16400/27733], Loss: 3.0323\n",
      "Epoch [71/300], Step [16500/27733], Loss: 3.1530\n",
      "Epoch [71/300], Step [16600/27733], Loss: 3.4173\n",
      "Epoch [71/300], Step [16700/27733], Loss: 2.7001\n",
      "Epoch [71/300], Step [16800/27733], Loss: 3.3866\n",
      "Epoch [71/300], Step [16900/27733], Loss: 3.4110\n",
      "Epoch [71/300], Step [17000/27733], Loss: 2.5683\n",
      "Epoch [71/300], Step [17100/27733], Loss: 2.4052\n",
      "Epoch [71/300], Step [17200/27733], Loss: 3.6480\n",
      "Epoch [71/300], Step [17300/27733], Loss: 2.7690\n",
      "Epoch [71/300], Step [17400/27733], Loss: 3.5240\n",
      "Epoch [71/300], Step [17500/27733], Loss: 2.6799\n",
      "Epoch [71/300], Step [17600/27733], Loss: 2.4786\n",
      "Epoch [71/300], Step [17700/27733], Loss: 2.9875\n",
      "Epoch [71/300], Step [17800/27733], Loss: 2.5663\n",
      "Epoch [71/300], Step [17900/27733], Loss: 2.9920\n",
      "Epoch [71/300], Step [18000/27733], Loss: 3.0131\n",
      "Epoch [71/300], Step [18100/27733], Loss: 2.9729\n",
      "Epoch [71/300], Step [18200/27733], Loss: 2.8481\n",
      "Epoch [71/300], Step [18300/27733], Loss: 2.5524\n",
      "Epoch [71/300], Step [18400/27733], Loss: 3.4173\n",
      "Epoch [71/300], Step [18500/27733], Loss: 2.8841\n",
      "Epoch [71/300], Step [18600/27733], Loss: 2.9179\n",
      "Epoch [71/300], Step [18700/27733], Loss: 2.0133\n",
      "Epoch [71/300], Step [18800/27733], Loss: 3.0013\n",
      "Epoch [71/300], Step [18900/27733], Loss: 2.4050\n",
      "Epoch [71/300], Step [19000/27733], Loss: 2.6751\n",
      "Epoch [71/300], Step [19100/27733], Loss: 2.7432\n",
      "Epoch [71/300], Step [19200/27733], Loss: 2.6794\n",
      "Epoch [71/300], Step [19300/27733], Loss: 2.3783\n",
      "Epoch [71/300], Step [19400/27733], Loss: 2.9979\n",
      "Epoch [71/300], Step [19500/27733], Loss: 2.6303\n",
      "Epoch [71/300], Step [19600/27733], Loss: 2.3701\n",
      "Epoch [71/300], Step [19700/27733], Loss: 2.7557\n",
      "Epoch [71/300], Step [19800/27733], Loss: 3.1738\n",
      "Epoch [71/300], Step [19900/27733], Loss: 2.1801\n",
      "Epoch [71/300], Step [20000/27733], Loss: 2.8739\n",
      "Epoch [71/300], Step [20100/27733], Loss: 3.4586\n",
      "Epoch [71/300], Step [20200/27733], Loss: 3.0246\n",
      "Epoch [71/300], Step [20300/27733], Loss: 2.5579\n",
      "Epoch [71/300], Step [20400/27733], Loss: 2.9526\n",
      "Epoch [71/300], Step [20500/27733], Loss: 2.9527\n",
      "Epoch [71/300], Step [20600/27733], Loss: 2.3198\n",
      "Epoch [71/300], Step [20700/27733], Loss: 3.7045\n",
      "Epoch [71/300], Step [20800/27733], Loss: 3.1204\n",
      "Epoch [71/300], Step [20900/27733], Loss: 2.5717\n",
      "Epoch [71/300], Step [21000/27733], Loss: 2.9239\n",
      "Epoch [71/300], Step [21100/27733], Loss: 2.7529\n",
      "Epoch [71/300], Step [21200/27733], Loss: 2.6274\n",
      "Epoch [71/300], Step [21300/27733], Loss: 2.2900\n",
      "Epoch [71/300], Step [21400/27733], Loss: 4.3895\n",
      "Epoch [71/300], Step [21500/27733], Loss: 3.3961\n",
      "Epoch [71/300], Step [21600/27733], Loss: 3.0017\n",
      "Epoch [71/300], Step [21700/27733], Loss: 3.2113\n",
      "Epoch [71/300], Step [21800/27733], Loss: 2.0600\n",
      "Epoch [71/300], Step [21900/27733], Loss: 2.7345\n",
      "Epoch [71/300], Step [22000/27733], Loss: 3.3241\n",
      "Epoch [71/300], Step [22100/27733], Loss: 2.5404\n",
      "Epoch [71/300], Step [22200/27733], Loss: 3.1287\n",
      "Epoch [71/300], Step [22300/27733], Loss: 2.5969\n",
      "Epoch [71/300], Step [22400/27733], Loss: 3.3969\n",
      "Epoch [71/300], Step [22500/27733], Loss: 2.7436\n",
      "Epoch [71/300], Step [22600/27733], Loss: 3.2500\n",
      "Epoch [71/300], Step [22700/27733], Loss: 2.3914\n",
      "Epoch [71/300], Step [22800/27733], Loss: 2.3294\n",
      "Epoch [71/300], Step [22900/27733], Loss: 3.5882\n",
      "Epoch [71/300], Step [23000/27733], Loss: 2.5135\n",
      "Epoch [71/300], Step [23100/27733], Loss: 2.7847\n",
      "Epoch [71/300], Step [23200/27733], Loss: 3.4876\n",
      "Epoch [71/300], Step [23300/27733], Loss: 3.1874\n",
      "Epoch [71/300], Step [23400/27733], Loss: 2.5253\n",
      "Epoch [71/300], Step [23500/27733], Loss: 2.8860\n",
      "Epoch [71/300], Step [23600/27733], Loss: 3.3295\n",
      "Epoch [71/300], Step [23700/27733], Loss: 2.5546\n",
      "Epoch [71/300], Step [23800/27733], Loss: 2.6159\n",
      "Epoch [71/300], Step [23900/27733], Loss: 2.7575\n",
      "Epoch [71/300], Step [24000/27733], Loss: 2.8149\n",
      "Epoch [71/300], Step [24100/27733], Loss: 3.5520\n",
      "Epoch [71/300], Step [24200/27733], Loss: 3.1150\n",
      "Epoch [71/300], Step [24300/27733], Loss: 2.5906\n",
      "Epoch [71/300], Step [24400/27733], Loss: 3.1159\n",
      "Epoch [71/300], Step [24500/27733], Loss: 3.3663\n",
      "Epoch [71/300], Step [24600/27733], Loss: 3.4875\n",
      "Epoch [71/300], Step [24700/27733], Loss: 3.3431\n",
      "Epoch [71/300], Step [24800/27733], Loss: 2.7601\n",
      "Epoch [71/300], Step [24900/27733], Loss: 3.2736\n",
      "Epoch [71/300], Step [25000/27733], Loss: 2.3495\n",
      "Epoch [71/300], Step [25100/27733], Loss: 3.0019\n",
      "Epoch [71/300], Step [25200/27733], Loss: 3.2647\n",
      "Epoch [71/300], Step [25300/27733], Loss: 2.7693\n",
      "Epoch [71/300], Step [25400/27733], Loss: 3.0395\n",
      "Epoch [71/300], Step [25500/27733], Loss: 3.6999\n",
      "Epoch [71/300], Step [25600/27733], Loss: 2.8105\n",
      "Epoch [71/300], Step [25700/27733], Loss: 2.7190\n",
      "Epoch [71/300], Step [25800/27733], Loss: 3.5924\n",
      "Epoch [71/300], Step [25900/27733], Loss: 2.5590\n",
      "Epoch [71/300], Step [26000/27733], Loss: 2.8188\n",
      "Epoch [71/300], Step [26100/27733], Loss: 2.7417\n",
      "Epoch [71/300], Step [26200/27733], Loss: 2.5758\n",
      "Epoch [71/300], Step [26300/27733], Loss: 2.5315\n",
      "Epoch [71/300], Step [26400/27733], Loss: 3.1335\n",
      "Epoch [71/300], Step [26500/27733], Loss: 2.4165\n",
      "Epoch [71/300], Step [26600/27733], Loss: 3.3416\n",
      "Epoch [71/300], Step [26700/27733], Loss: 2.7373\n",
      "Epoch [71/300], Step [26800/27733], Loss: 3.1302\n",
      "Epoch [71/300], Step [26900/27733], Loss: 3.7139\n",
      "Epoch [71/300], Step [27000/27733], Loss: 3.2835\n",
      "Epoch [71/300], Step [27100/27733], Loss: 2.9345\n",
      "Epoch [71/300], Step [27200/27733], Loss: 2.7824\n",
      "Epoch [71/300], Step [27300/27733], Loss: 2.6939\n",
      "Epoch [71/300], Step [27400/27733], Loss: 2.9625\n",
      "Epoch [71/300], Step [27500/27733], Loss: 3.2354\n",
      "Epoch [71/300], Step [27600/27733], Loss: 3.6910\n",
      "Epoch [71/300], Step [27700/27733], Loss: 2.9164\n",
      "Epoch [72/300], Step [100/27733], Loss: 3.5722\n",
      "Epoch [72/300], Step [200/27733], Loss: 2.1646\n",
      "Epoch [72/300], Step [300/27733], Loss: 2.3543\n",
      "Epoch [72/300], Step [400/27733], Loss: 2.1593\n",
      "Epoch [72/300], Step [500/27733], Loss: 2.4608\n",
      "Epoch [72/300], Step [600/27733], Loss: 3.0569\n",
      "Epoch [72/300], Step [700/27733], Loss: 2.7420\n",
      "Epoch [72/300], Step [800/27733], Loss: 2.5901\n",
      "Epoch [72/300], Step [900/27733], Loss: 2.7145\n",
      "Epoch [72/300], Step [1000/27733], Loss: 3.3973\n",
      "Epoch [72/300], Step [1100/27733], Loss: 1.9050\n",
      "Epoch [72/300], Step [1200/27733], Loss: 2.1093\n",
      "Epoch [72/300], Step [1300/27733], Loss: 2.3070\n",
      "Epoch [72/300], Step [1400/27733], Loss: 2.9868\n",
      "Epoch [72/300], Step [1500/27733], Loss: 1.8969\n",
      "Epoch [72/300], Step [1600/27733], Loss: 2.9414\n",
      "Epoch [72/300], Step [1700/27733], Loss: 1.7866\n",
      "Epoch [72/300], Step [1800/27733], Loss: 2.3409\n",
      "Epoch [72/300], Step [1900/27733], Loss: 2.6258\n",
      "Epoch [72/300], Step [2000/27733], Loss: 2.6897\n",
      "Epoch [72/300], Step [2100/27733], Loss: 2.2402\n",
      "Epoch [72/300], Step [2200/27733], Loss: 3.2229\n",
      "Epoch [72/300], Step [2300/27733], Loss: 3.1119\n",
      "Epoch [72/300], Step [2400/27733], Loss: 2.5784\n",
      "Epoch [72/300], Step [2500/27733], Loss: 2.1640\n",
      "Epoch [72/300], Step [2600/27733], Loss: 2.8943\n",
      "Epoch [72/300], Step [2700/27733], Loss: 2.6223\n",
      "Epoch [72/300], Step [2800/27733], Loss: 2.0253\n",
      "Epoch [72/300], Step [2900/27733], Loss: 2.3386\n",
      "Epoch [72/300], Step [3000/27733], Loss: 2.0854\n",
      "Epoch [72/300], Step [3100/27733], Loss: 2.4409\n",
      "Epoch [72/300], Step [3200/27733], Loss: 2.5533\n",
      "Epoch [72/300], Step [3300/27733], Loss: 2.1579\n",
      "Epoch [72/300], Step [3400/27733], Loss: 2.9605\n",
      "Epoch [72/300], Step [3500/27733], Loss: 2.6040\n",
      "Epoch [72/300], Step [3600/27733], Loss: 2.5920\n",
      "Epoch [72/300], Step [3700/27733], Loss: 2.6851\n",
      "Epoch [72/300], Step [3800/27733], Loss: 1.9783\n",
      "Epoch [72/300], Step [3900/27733], Loss: 2.3232\n",
      "Epoch [72/300], Step [4000/27733], Loss: 1.7983\n",
      "Epoch [72/300], Step [4100/27733], Loss: 1.7087\n",
      "Epoch [72/300], Step [4200/27733], Loss: 3.1157\n",
      "Epoch [72/300], Step [4300/27733], Loss: 2.2091\n",
      "Epoch [72/300], Step [4400/27733], Loss: 3.0183\n",
      "Epoch [72/300], Step [4500/27733], Loss: 3.0310\n",
      "Epoch [72/300], Step [4600/27733], Loss: 2.1393\n",
      "Epoch [72/300], Step [4700/27733], Loss: 2.7182\n",
      "Epoch [72/300], Step [4800/27733], Loss: 2.2044\n",
      "Epoch [72/300], Step [4900/27733], Loss: 2.1092\n",
      "Epoch [72/300], Step [5000/27733], Loss: 1.8392\n",
      "Epoch [72/300], Step [5100/27733], Loss: 2.7402\n",
      "Epoch [72/300], Step [5200/27733], Loss: 2.4099\n",
      "Epoch [72/300], Step [5300/27733], Loss: 1.1947\n",
      "Epoch [72/300], Step [5400/27733], Loss: 2.6144\n",
      "Epoch [72/300], Step [5500/27733], Loss: 1.9922\n",
      "Epoch [72/300], Step [5600/27733], Loss: 2.1460\n",
      "Epoch [72/300], Step [5700/27733], Loss: 3.1320\n",
      "Epoch [72/300], Step [5800/27733], Loss: 2.3938\n",
      "Epoch [72/300], Step [5900/27733], Loss: 2.7822\n",
      "Epoch [72/300], Step [6000/27733], Loss: 2.3846\n",
      "Epoch [72/300], Step [6100/27733], Loss: 1.8060\n",
      "Epoch [72/300], Step [6200/27733], Loss: 1.8990\n",
      "Epoch [72/300], Step [6300/27733], Loss: 2.5363\n",
      "Epoch [72/300], Step [6400/27733], Loss: 2.6342\n",
      "Epoch [72/300], Step [6500/27733], Loss: 2.2409\n",
      "Epoch [72/300], Step [6600/27733], Loss: 2.4698\n",
      "Epoch [72/300], Step [6700/27733], Loss: 2.5523\n",
      "Epoch [72/300], Step [6800/27733], Loss: 2.3916\n",
      "Epoch [72/300], Step [6900/27733], Loss: 3.4644\n",
      "Epoch [72/300], Step [7000/27733], Loss: 2.3209\n",
      "Epoch [72/300], Step [7100/27733], Loss: 3.3171\n",
      "Epoch [72/300], Step [7200/27733], Loss: 2.5475\n",
      "Epoch [72/300], Step [7300/27733], Loss: 2.3654\n",
      "Epoch [72/300], Step [7400/27733], Loss: 3.1203\n",
      "Epoch [72/300], Step [7500/27733], Loss: 2.2177\n",
      "Epoch [72/300], Step [7600/27733], Loss: 2.6557\n",
      "Epoch [72/300], Step [7700/27733], Loss: 2.2796\n",
      "Epoch [72/300], Step [7800/27733], Loss: 2.2312\n",
      "Epoch [72/300], Step [7900/27733], Loss: 2.6504\n",
      "Epoch [72/300], Step [8000/27733], Loss: 1.8128\n",
      "Epoch [72/300], Step [8100/27733], Loss: 2.0687\n",
      "Epoch [72/300], Step [8200/27733], Loss: 2.3513\n",
      "Epoch [72/300], Step [8300/27733], Loss: 3.0280\n",
      "Epoch [72/300], Step [8400/27733], Loss: 2.1701\n",
      "Epoch [72/300], Step [8500/27733], Loss: 2.0938\n",
      "Epoch [72/300], Step [8600/27733], Loss: 2.1713\n",
      "Epoch [72/300], Step [8700/27733], Loss: 2.6959\n",
      "Epoch [72/300], Step [8800/27733], Loss: 2.3379\n",
      "Epoch [72/300], Step [8900/27733], Loss: 3.1930\n",
      "Epoch [72/300], Step [9000/27733], Loss: 2.1541\n",
      "Epoch [72/300], Step [9100/27733], Loss: 2.9425\n",
      "Epoch [72/300], Step [9200/27733], Loss: 2.3065\n",
      "Epoch [72/300], Step [9300/27733], Loss: 2.6990\n",
      "Epoch [72/300], Step [9400/27733], Loss: 2.3276\n",
      "Epoch [72/300], Step [9500/27733], Loss: 2.9865\n",
      "Epoch [72/300], Step [9600/27733], Loss: 2.5280\n",
      "Epoch [72/300], Step [9700/27733], Loss: 2.4432\n",
      "Epoch [72/300], Step [9800/27733], Loss: 2.4646\n",
      "Epoch [72/300], Step [9900/27733], Loss: 3.0683\n",
      "Epoch [72/300], Step [10000/27733], Loss: 2.2142\n",
      "Epoch [72/300], Step [10100/27733], Loss: 3.2611\n",
      "Epoch [72/300], Step [10200/27733], Loss: 2.8194\n",
      "Epoch [72/300], Step [10300/27733], Loss: 2.5807\n",
      "Epoch [72/300], Step [10400/27733], Loss: 2.5589\n",
      "Epoch [72/300], Step [10500/27733], Loss: 2.3059\n",
      "Epoch [72/300], Step [10600/27733], Loss: 2.5618\n",
      "Epoch [72/300], Step [10700/27733], Loss: 2.0091\n",
      "Epoch [72/300], Step [10800/27733], Loss: 1.6717\n",
      "Epoch [72/300], Step [10900/27733], Loss: 2.6536\n",
      "Epoch [72/300], Step [11000/27733], Loss: 3.0894\n",
      "Epoch [72/300], Step [11100/27733], Loss: 2.4484\n",
      "Epoch [72/300], Step [11200/27733], Loss: 2.6930\n",
      "Epoch [72/300], Step [11300/27733], Loss: 2.0030\n",
      "Epoch [72/300], Step [11400/27733], Loss: 2.2767\n",
      "Epoch [72/300], Step [11500/27733], Loss: 3.0215\n",
      "Epoch [72/300], Step [11600/27733], Loss: 2.6429\n",
      "Epoch [72/300], Step [11700/27733], Loss: 3.5117\n",
      "Epoch [72/300], Step [11800/27733], Loss: 3.0575\n",
      "Epoch [72/300], Step [11900/27733], Loss: 2.3984\n",
      "Epoch [72/300], Step [12000/27733], Loss: 2.8577\n",
      "Epoch [72/300], Step [12100/27733], Loss: 2.0823\n",
      "Epoch [72/300], Step [12200/27733], Loss: 2.2790\n",
      "Epoch [72/300], Step [12300/27733], Loss: 2.6081\n",
      "Epoch [72/300], Step [12400/27733], Loss: 3.1033\n",
      "Epoch [72/300], Step [12500/27733], Loss: 2.8864\n",
      "Epoch [72/300], Step [12600/27733], Loss: 2.5989\n",
      "Epoch [72/300], Step [12700/27733], Loss: 2.8672\n",
      "Epoch [72/300], Step [12800/27733], Loss: 2.2122\n",
      "Epoch [72/300], Step [12900/27733], Loss: 2.3936\n",
      "Epoch [72/300], Step [13000/27733], Loss: 3.3302\n",
      "Epoch [72/300], Step [13100/27733], Loss: 3.2238\n",
      "Epoch [72/300], Step [13200/27733], Loss: 3.5286\n",
      "Epoch [72/300], Step [13300/27733], Loss: 3.4782\n",
      "Epoch [72/300], Step [13400/27733], Loss: 2.0804\n",
      "Epoch [72/300], Step [13500/27733], Loss: 2.9191\n",
      "Epoch [72/300], Step [13600/27733], Loss: 2.1737\n",
      "Epoch [72/300], Step [13700/27733], Loss: 2.3954\n",
      "Epoch [72/300], Step [13800/27733], Loss: 2.3571\n",
      "Epoch [72/300], Step [13900/27733], Loss: 2.7815\n",
      "Epoch [72/300], Step [14000/27733], Loss: 2.8799\n",
      "Epoch [72/300], Step [14100/27733], Loss: 2.6008\n",
      "Epoch [72/300], Step [14200/27733], Loss: 2.6890\n",
      "Epoch [72/300], Step [14300/27733], Loss: 3.1049\n",
      "Epoch [72/300], Step [14400/27733], Loss: 2.5806\n",
      "Epoch [72/300], Step [14500/27733], Loss: 3.5666\n",
      "Epoch [72/300], Step [14600/27733], Loss: 3.1538\n",
      "Epoch [72/300], Step [14700/27733], Loss: 1.9500\n",
      "Epoch [72/300], Step [14800/27733], Loss: 3.0265\n",
      "Epoch [72/300], Step [14900/27733], Loss: 2.7097\n",
      "Epoch [72/300], Step [15000/27733], Loss: 2.2044\n",
      "Epoch [72/300], Step [15100/27733], Loss: 2.6850\n",
      "Epoch [72/300], Step [15200/27733], Loss: 3.2690\n",
      "Epoch [72/300], Step [15300/27733], Loss: 3.1678\n",
      "Epoch [72/300], Step [15400/27733], Loss: 2.8574\n",
      "Epoch [72/300], Step [15500/27733], Loss: 2.2980\n",
      "Epoch [72/300], Step [15600/27733], Loss: 3.5322\n",
      "Epoch [72/300], Step [15700/27733], Loss: 3.9252\n",
      "Epoch [72/300], Step [15800/27733], Loss: 2.7926\n",
      "Epoch [72/300], Step [15900/27733], Loss: 1.9642\n",
      "Epoch [72/300], Step [16000/27733], Loss: 2.7292\n",
      "Epoch [72/300], Step [16100/27733], Loss: 3.5685\n",
      "Epoch [72/300], Step [16200/27733], Loss: 2.5877\n",
      "Epoch [72/300], Step [16300/27733], Loss: 2.7265\n",
      "Epoch [72/300], Step [16400/27733], Loss: 2.9416\n",
      "Epoch [72/300], Step [16500/27733], Loss: 2.4892\n",
      "Epoch [72/300], Step [16600/27733], Loss: 2.2038\n",
      "Epoch [72/300], Step [16700/27733], Loss: 2.9722\n",
      "Epoch [72/300], Step [16800/27733], Loss: 2.8321\n",
      "Epoch [72/300], Step [16900/27733], Loss: 2.9833\n",
      "Epoch [72/300], Step [17000/27733], Loss: 3.2385\n",
      "Epoch [72/300], Step [17100/27733], Loss: 3.1290\n",
      "Epoch [72/300], Step [17200/27733], Loss: 3.6531\n",
      "Epoch [72/300], Step [17300/27733], Loss: 2.3627\n",
      "Epoch [72/300], Step [17400/27733], Loss: 2.6745\n",
      "Epoch [72/300], Step [17500/27733], Loss: 2.9351\n",
      "Epoch [72/300], Step [17600/27733], Loss: 2.7935\n",
      "Epoch [72/300], Step [17700/27733], Loss: 2.4393\n",
      "Epoch [72/300], Step [17800/27733], Loss: 3.7457\n",
      "Epoch [72/300], Step [17900/27733], Loss: 2.6082\n",
      "Epoch [72/300], Step [18000/27733], Loss: 3.4281\n",
      "Epoch [72/300], Step [18100/27733], Loss: 3.6691\n",
      "Epoch [72/300], Step [18200/27733], Loss: 2.4101\n",
      "Epoch [72/300], Step [18300/27733], Loss: 3.1951\n",
      "Epoch [72/300], Step [18400/27733], Loss: 3.0989\n",
      "Epoch [72/300], Step [18500/27733], Loss: 2.6433\n",
      "Epoch [72/300], Step [18600/27733], Loss: 2.9078\n",
      "Epoch [72/300], Step [18700/27733], Loss: 2.1316\n",
      "Epoch [72/300], Step [18800/27733], Loss: 2.2713\n",
      "Epoch [72/300], Step [18900/27733], Loss: 2.6612\n",
      "Epoch [72/300], Step [19000/27733], Loss: 2.4747\n",
      "Epoch [72/300], Step [19100/27733], Loss: 2.2539\n",
      "Epoch [72/300], Step [19200/27733], Loss: 2.5226\n",
      "Epoch [72/300], Step [19300/27733], Loss: 3.0526\n",
      "Epoch [72/300], Step [19400/27733], Loss: 2.6930\n",
      "Epoch [72/300], Step [19500/27733], Loss: 2.7693\n",
      "Epoch [72/300], Step [19600/27733], Loss: 3.0965\n",
      "Epoch [72/300], Step [19700/27733], Loss: 2.8628\n",
      "Epoch [72/300], Step [19800/27733], Loss: 2.9081\n",
      "Epoch [72/300], Step [19900/27733], Loss: 3.2100\n",
      "Epoch [72/300], Step [20000/27733], Loss: 3.3775\n",
      "Epoch [72/300], Step [20100/27733], Loss: 2.6966\n",
      "Epoch [72/300], Step [20200/27733], Loss: 3.4762\n",
      "Epoch [72/300], Step [20300/27733], Loss: 2.2887\n",
      "Epoch [72/300], Step [20400/27733], Loss: 2.6233\n",
      "Epoch [72/300], Step [20500/27733], Loss: 2.4367\n",
      "Epoch [72/300], Step [20600/27733], Loss: 3.1398\n",
      "Epoch [72/300], Step [20700/27733], Loss: 3.4334\n",
      "Epoch [72/300], Step [20800/27733], Loss: 3.0904\n",
      "Epoch [72/300], Step [20900/27733], Loss: 2.2621\n",
      "Epoch [72/300], Step [21000/27733], Loss: 3.0220\n",
      "Epoch [72/300], Step [21100/27733], Loss: 3.2853\n",
      "Epoch [72/300], Step [21200/27733], Loss: 3.1939\n",
      "Epoch [72/300], Step [21300/27733], Loss: 3.2261\n",
      "Epoch [72/300], Step [21400/27733], Loss: 3.0327\n",
      "Epoch [72/300], Step [21500/27733], Loss: 2.3670\n",
      "Epoch [72/300], Step [21600/27733], Loss: 3.0974\n",
      "Epoch [72/300], Step [21700/27733], Loss: 2.7114\n",
      "Epoch [72/300], Step [21800/27733], Loss: 3.5885\n",
      "Epoch [72/300], Step [21900/27733], Loss: 3.0860\n",
      "Epoch [72/300], Step [22000/27733], Loss: 2.3413\n",
      "Epoch [72/300], Step [22100/27733], Loss: 2.6499\n",
      "Epoch [72/300], Step [22200/27733], Loss: 2.1768\n",
      "Epoch [72/300], Step [22300/27733], Loss: 2.7902\n",
      "Epoch [72/300], Step [22400/27733], Loss: 2.8482\n",
      "Epoch [72/300], Step [22500/27733], Loss: 3.1331\n",
      "Epoch [72/300], Step [22600/27733], Loss: 2.5930\n",
      "Epoch [72/300], Step [22700/27733], Loss: 2.9695\n",
      "Epoch [72/300], Step [22800/27733], Loss: 2.7685\n",
      "Epoch [72/300], Step [22900/27733], Loss: 2.7441\n",
      "Epoch [72/300], Step [23000/27733], Loss: 2.2984\n",
      "Epoch [72/300], Step [23100/27733], Loss: 2.5676\n",
      "Epoch [72/300], Step [23200/27733], Loss: 2.8649\n",
      "Epoch [72/300], Step [23300/27733], Loss: 2.7115\n",
      "Epoch [72/300], Step [23400/27733], Loss: 2.5607\n",
      "Epoch [72/300], Step [23500/27733], Loss: 2.9603\n",
      "Epoch [72/300], Step [23600/27733], Loss: 2.6876\n",
      "Epoch [72/300], Step [23700/27733], Loss: 2.5533\n",
      "Epoch [72/300], Step [23800/27733], Loss: 2.8103\n",
      "Epoch [72/300], Step [23900/27733], Loss: 2.7676\n",
      "Epoch [72/300], Step [24000/27733], Loss: 3.3834\n",
      "Epoch [72/300], Step [24100/27733], Loss: 2.3316\n",
      "Epoch [72/300], Step [24200/27733], Loss: 3.4617\n",
      "Epoch [72/300], Step [24300/27733], Loss: 2.9522\n",
      "Epoch [72/300], Step [24400/27733], Loss: 2.2872\n",
      "Epoch [72/300], Step [24500/27733], Loss: 2.4886\n",
      "Epoch [72/300], Step [24600/27733], Loss: 3.4945\n",
      "Epoch [72/300], Step [24700/27733], Loss: 2.2099\n",
      "Epoch [72/300], Step [24800/27733], Loss: 2.3381\n",
      "Epoch [72/300], Step [24900/27733], Loss: 3.3230\n",
      "Epoch [72/300], Step [25000/27733], Loss: 3.2853\n",
      "Epoch [72/300], Step [25100/27733], Loss: 3.4966\n",
      "Epoch [72/300], Step [25200/27733], Loss: 2.8689\n",
      "Epoch [72/300], Step [25300/27733], Loss: 2.7445\n",
      "Epoch [72/300], Step [25400/27733], Loss: 3.4183\n",
      "Epoch [72/300], Step [25500/27733], Loss: 2.2525\n",
      "Epoch [72/300], Step [25600/27733], Loss: 3.1336\n",
      "Epoch [72/300], Step [25700/27733], Loss: 3.9779\n",
      "Epoch [72/300], Step [25800/27733], Loss: 2.9473\n",
      "Epoch [72/300], Step [25900/27733], Loss: 2.2687\n",
      "Epoch [72/300], Step [26000/27733], Loss: 3.1108\n",
      "Epoch [72/300], Step [26100/27733], Loss: 2.4146\n",
      "Epoch [72/300], Step [26200/27733], Loss: 4.0396\n",
      "Epoch [72/300], Step [26300/27733], Loss: 3.5246\n",
      "Epoch [72/300], Step [26400/27733], Loss: 2.4265\n",
      "Epoch [72/300], Step [26500/27733], Loss: 3.7384\n",
      "Epoch [72/300], Step [26600/27733], Loss: 2.3467\n",
      "Epoch [72/300], Step [26700/27733], Loss: 2.2708\n",
      "Epoch [72/300], Step [26800/27733], Loss: 3.1582\n",
      "Epoch [72/300], Step [26900/27733], Loss: 3.7194\n",
      "Epoch [72/300], Step [27000/27733], Loss: 4.3431\n",
      "Epoch [72/300], Step [27100/27733], Loss: 3.0220\n",
      "Epoch [72/300], Step [27200/27733], Loss: 2.5698\n",
      "Epoch [72/300], Step [27300/27733], Loss: 3.3027\n",
      "Epoch [72/300], Step [27400/27733], Loss: 2.9254\n",
      "Epoch [72/300], Step [27500/27733], Loss: 3.2425\n",
      "Epoch [72/300], Step [27600/27733], Loss: 3.5437\n",
      "Epoch [72/300], Step [27700/27733], Loss: 3.4453\n",
      "Epoch [73/300], Step [100/27733], Loss: 2.6490\n",
      "Epoch [73/300], Step [200/27733], Loss: 2.4266\n",
      "Epoch [73/300], Step [300/27733], Loss: 2.4282\n",
      "Epoch [73/300], Step [400/27733], Loss: 2.6997\n",
      "Epoch [73/300], Step [500/27733], Loss: 1.9323\n",
      "Epoch [73/300], Step [600/27733], Loss: 2.1758\n",
      "Epoch [73/300], Step [700/27733], Loss: 1.9676\n",
      "Epoch [73/300], Step [800/27733], Loss: 3.0600\n",
      "Epoch [73/300], Step [900/27733], Loss: 1.7971\n",
      "Epoch [73/300], Step [1000/27733], Loss: 2.5925\n",
      "Epoch [73/300], Step [1100/27733], Loss: 2.1769\n",
      "Epoch [73/300], Step [1200/27733], Loss: 2.7287\n",
      "Epoch [73/300], Step [1300/27733], Loss: 2.4381\n",
      "Epoch [73/300], Step [1400/27733], Loss: 2.6330\n",
      "Epoch [73/300], Step [1500/27733], Loss: 2.6209\n",
      "Epoch [73/300], Step [1600/27733], Loss: 2.4960\n",
      "Epoch [73/300], Step [1700/27733], Loss: 2.7840\n",
      "Epoch [73/300], Step [1800/27733], Loss: 2.2132\n",
      "Epoch [73/300], Step [1900/27733], Loss: 2.3232\n",
      "Epoch [73/300], Step [2000/27733], Loss: 1.4713\n",
      "Epoch [73/300], Step [2100/27733], Loss: 2.2529\n",
      "Epoch [73/300], Step [2200/27733], Loss: 3.9870\n",
      "Epoch [73/300], Step [2300/27733], Loss: 2.3977\n",
      "Epoch [73/300], Step [2400/27733], Loss: 2.1309\n",
      "Epoch [73/300], Step [2500/27733], Loss: 2.7560\n",
      "Epoch [73/300], Step [2600/27733], Loss: 2.6986\n",
      "Epoch [73/300], Step [2700/27733], Loss: 2.5178\n",
      "Epoch [73/300], Step [2800/27733], Loss: 2.4165\n",
      "Epoch [73/300], Step [2900/27733], Loss: 2.5079\n",
      "Epoch [73/300], Step [3000/27733], Loss: 2.0730\n",
      "Epoch [73/300], Step [3100/27733], Loss: 3.2208\n",
      "Epoch [73/300], Step [3200/27733], Loss: 2.3198\n",
      "Epoch [73/300], Step [3300/27733], Loss: 2.7713\n",
      "Epoch [73/300], Step [3400/27733], Loss: 2.8881\n",
      "Epoch [73/300], Step [3500/27733], Loss: 2.6154\n",
      "Epoch [73/300], Step [3600/27733], Loss: 2.1654\n",
      "Epoch [73/300], Step [3700/27733], Loss: 2.8992\n",
      "Epoch [73/300], Step [3800/27733], Loss: 2.5542\n",
      "Epoch [73/300], Step [3900/27733], Loss: 2.7249\n",
      "Epoch [73/300], Step [4000/27733], Loss: 1.6220\n",
      "Epoch [73/300], Step [4100/27733], Loss: 3.0737\n",
      "Epoch [73/300], Step [4200/27733], Loss: 2.0137\n",
      "Epoch [73/300], Step [4300/27733], Loss: 2.2685\n",
      "Epoch [73/300], Step [4400/27733], Loss: 2.2979\n",
      "Epoch [73/300], Step [4500/27733], Loss: 1.9306\n",
      "Epoch [73/300], Step [4600/27733], Loss: 2.3872\n",
      "Epoch [73/300], Step [4700/27733], Loss: 2.1126\n",
      "Epoch [73/300], Step [4800/27733], Loss: 2.1954\n",
      "Epoch [73/300], Step [4900/27733], Loss: 1.9851\n",
      "Epoch [73/300], Step [5000/27733], Loss: 3.5000\n",
      "Epoch [73/300], Step [5100/27733], Loss: 2.9203\n",
      "Epoch [73/300], Step [5200/27733], Loss: 2.7982\n",
      "Epoch [73/300], Step [5300/27733], Loss: 1.6867\n",
      "Epoch [73/300], Step [5400/27733], Loss: 2.1390\n",
      "Epoch [73/300], Step [5500/27733], Loss: 1.9969\n",
      "Epoch [73/300], Step [5600/27733], Loss: 2.6412\n",
      "Epoch [73/300], Step [5700/27733], Loss: 2.2535\n",
      "Epoch [73/300], Step [5800/27733], Loss: 3.1662\n",
      "Epoch [73/300], Step [5900/27733], Loss: 2.1475\n",
      "Epoch [73/300], Step [6000/27733], Loss: 2.5066\n",
      "Epoch [73/300], Step [6100/27733], Loss: 3.4138\n",
      "Epoch [73/300], Step [6200/27733], Loss: 1.9254\n",
      "Epoch [73/300], Step [6300/27733], Loss: 2.6256\n",
      "Epoch [73/300], Step [6400/27733], Loss: 2.1706\n",
      "Epoch [73/300], Step [6500/27733], Loss: 2.3856\n",
      "Epoch [73/300], Step [6600/27733], Loss: 1.9369\n",
      "Epoch [73/300], Step [6700/27733], Loss: 3.2695\n",
      "Epoch [73/300], Step [6800/27733], Loss: 1.9838\n",
      "Epoch [73/300], Step [6900/27733], Loss: 2.0074\n",
      "Epoch [73/300], Step [7000/27733], Loss: 2.9420\n",
      "Epoch [73/300], Step [7100/27733], Loss: 2.4778\n",
      "Epoch [73/300], Step [7200/27733], Loss: 2.9452\n",
      "Epoch [73/300], Step [7300/27733], Loss: 2.1489\n",
      "Epoch [73/300], Step [7400/27733], Loss: 2.1540\n",
      "Epoch [73/300], Step [7500/27733], Loss: 2.5756\n",
      "Epoch [73/300], Step [7600/27733], Loss: 2.9257\n",
      "Epoch [73/300], Step [7700/27733], Loss: 2.1611\n",
      "Epoch [73/300], Step [7800/27733], Loss: 3.5169\n",
      "Epoch [73/300], Step [7900/27733], Loss: 3.1790\n",
      "Epoch [73/300], Step [8000/27733], Loss: 2.3448\n",
      "Epoch [73/300], Step [8100/27733], Loss: 2.2516\n",
      "Epoch [73/300], Step [8200/27733], Loss: 2.5235\n",
      "Epoch [73/300], Step [8300/27733], Loss: 2.3713\n",
      "Epoch [73/300], Step [8400/27733], Loss: 3.0784\n",
      "Epoch [73/300], Step [8500/27733], Loss: 2.7867\n",
      "Epoch [73/300], Step [8600/27733], Loss: 2.1313\n",
      "Epoch [73/300], Step [8700/27733], Loss: 3.1748\n",
      "Epoch [73/300], Step [8800/27733], Loss: 2.9510\n",
      "Epoch [73/300], Step [8900/27733], Loss: 2.4604\n",
      "Epoch [73/300], Step [9000/27733], Loss: 2.3277\n",
      "Epoch [73/300], Step [9100/27733], Loss: 3.0544\n",
      "Epoch [73/300], Step [9200/27733], Loss: 2.1148\n",
      "Epoch [73/300], Step [9300/27733], Loss: 2.7548\n",
      "Epoch [73/300], Step [9400/27733], Loss: 2.5339\n",
      "Epoch [73/300], Step [9500/27733], Loss: 3.4751\n",
      "Epoch [73/300], Step [9600/27733], Loss: 2.3209\n",
      "Epoch [73/300], Step [9700/27733], Loss: 2.6897\n",
      "Epoch [73/300], Step [9800/27733], Loss: 2.2031\n",
      "Epoch [73/300], Step [9900/27733], Loss: 2.5814\n",
      "Epoch [73/300], Step [10000/27733], Loss: 2.8435\n",
      "Epoch [73/300], Step [10100/27733], Loss: 2.9194\n",
      "Epoch [73/300], Step [10200/27733], Loss: 2.9222\n",
      "Epoch [73/300], Step [10300/27733], Loss: 2.5960\n",
      "Epoch [73/300], Step [10400/27733], Loss: 2.8563\n",
      "Epoch [73/300], Step [10500/27733], Loss: 2.6041\n",
      "Epoch [73/300], Step [10600/27733], Loss: 2.2692\n",
      "Epoch [73/300], Step [10700/27733], Loss: 2.5393\n",
      "Epoch [73/300], Step [10800/27733], Loss: 2.7799\n",
      "Epoch [73/300], Step [10900/27733], Loss: 2.7049\n",
      "Epoch [73/300], Step [11000/27733], Loss: 3.2109\n",
      "Epoch [73/300], Step [11100/27733], Loss: 3.3614\n",
      "Epoch [73/300], Step [11200/27733], Loss: 3.1183\n",
      "Epoch [73/300], Step [11300/27733], Loss: 2.8234\n",
      "Epoch [73/300], Step [11400/27733], Loss: 2.7849\n",
      "Epoch [73/300], Step [11500/27733], Loss: 2.4643\n",
      "Epoch [73/300], Step [11600/27733], Loss: 2.6241\n",
      "Epoch [73/300], Step [11700/27733], Loss: 2.6531\n",
      "Epoch [73/300], Step [11800/27733], Loss: 2.3042\n",
      "Epoch [73/300], Step [11900/27733], Loss: 2.6121\n",
      "Epoch [73/300], Step [12000/27733], Loss: 2.3304\n",
      "Epoch [73/300], Step [12100/27733], Loss: 3.4211\n",
      "Epoch [73/300], Step [12200/27733], Loss: 2.1791\n",
      "Epoch [73/300], Step [12300/27733], Loss: 2.3971\n",
      "Epoch [73/300], Step [12400/27733], Loss: 2.1433\n",
      "Epoch [73/300], Step [12500/27733], Loss: 2.2249\n",
      "Epoch [73/300], Step [12600/27733], Loss: 2.0833\n",
      "Epoch [73/300], Step [12700/27733], Loss: 3.6617\n",
      "Epoch [73/300], Step [12800/27733], Loss: 2.1537\n",
      "Epoch [73/300], Step [12900/27733], Loss: 1.8702\n",
      "Epoch [73/300], Step [13000/27733], Loss: 2.1922\n",
      "Epoch [73/300], Step [13100/27733], Loss: 2.9930\n",
      "Epoch [73/300], Step [13200/27733], Loss: 2.6040\n",
      "Epoch [73/300], Step [13300/27733], Loss: 2.9286\n",
      "Epoch [73/300], Step [13400/27733], Loss: 2.6912\n",
      "Epoch [73/300], Step [13500/27733], Loss: 2.8267\n",
      "Epoch [73/300], Step [13600/27733], Loss: 2.6155\n",
      "Epoch [73/300], Step [13700/27733], Loss: 2.8208\n",
      "Epoch [73/300], Step [13800/27733], Loss: 2.7026\n",
      "Epoch [73/300], Step [13900/27733], Loss: 2.6949\n",
      "Epoch [73/300], Step [14000/27733], Loss: 2.9873\n",
      "Epoch [73/300], Step [14100/27733], Loss: 2.2974\n",
      "Epoch [73/300], Step [14200/27733], Loss: 2.3588\n",
      "Epoch [73/300], Step [14300/27733], Loss: 2.9356\n",
      "Epoch [73/300], Step [14400/27733], Loss: 2.1305\n",
      "Epoch [73/300], Step [14500/27733], Loss: 3.0100\n",
      "Epoch [73/300], Step [14600/27733], Loss: 3.1243\n",
      "Epoch [73/300], Step [14700/27733], Loss: 2.7539\n",
      "Epoch [73/300], Step [14800/27733], Loss: 3.2238\n",
      "Epoch [73/300], Step [14900/27733], Loss: 3.3399\n",
      "Epoch [73/300], Step [15000/27733], Loss: 2.7596\n",
      "Epoch [73/300], Step [15100/27733], Loss: 2.5252\n",
      "Epoch [73/300], Step [15200/27733], Loss: 2.5931\n",
      "Epoch [73/300], Step [15300/27733], Loss: 2.4789\n",
      "Epoch [73/300], Step [15400/27733], Loss: 3.3628\n",
      "Epoch [73/300], Step [15500/27733], Loss: 2.9785\n",
      "Epoch [73/300], Step [15600/27733], Loss: 2.8745\n",
      "Epoch [73/300], Step [15700/27733], Loss: 2.3693\n",
      "Epoch [73/300], Step [15800/27733], Loss: 2.9838\n",
      "Epoch [73/300], Step [15900/27733], Loss: 1.7936\n",
      "Epoch [73/300], Step [16000/27733], Loss: 2.7353\n",
      "Epoch [73/300], Step [16100/27733], Loss: 3.0717\n",
      "Epoch [73/300], Step [16200/27733], Loss: 2.4471\n",
      "Epoch [73/300], Step [16300/27733], Loss: 2.4881\n",
      "Epoch [73/300], Step [16400/27733], Loss: 2.4909\n",
      "Epoch [73/300], Step [16500/27733], Loss: 3.1300\n",
      "Epoch [73/300], Step [16600/27733], Loss: 2.6152\n",
      "Epoch [73/300], Step [16700/27733], Loss: 2.4328\n",
      "Epoch [73/300], Step [16800/27733], Loss: 2.3019\n",
      "Epoch [73/300], Step [16900/27733], Loss: 2.8213\n",
      "Epoch [73/300], Step [17000/27733], Loss: 2.9012\n",
      "Epoch [73/300], Step [17100/27733], Loss: 2.9218\n",
      "Epoch [73/300], Step [17200/27733], Loss: 2.2388\n",
      "Epoch [73/300], Step [17300/27733], Loss: 2.5051\n",
      "Epoch [73/300], Step [17400/27733], Loss: 3.1715\n",
      "Epoch [73/300], Step [17500/27733], Loss: 2.8448\n",
      "Epoch [73/300], Step [17600/27733], Loss: 2.2791\n",
      "Epoch [73/300], Step [17700/27733], Loss: 3.3079\n",
      "Epoch [73/300], Step [17800/27733], Loss: 4.1024\n",
      "Epoch [73/300], Step [17900/27733], Loss: 2.1204\n",
      "Epoch [73/300], Step [18000/27733], Loss: 2.6986\n",
      "Epoch [73/300], Step [18100/27733], Loss: 2.2581\n",
      "Epoch [73/300], Step [18200/27733], Loss: 2.6756\n",
      "Epoch [73/300], Step [18300/27733], Loss: 2.9593\n",
      "Epoch [73/300], Step [18400/27733], Loss: 3.1399\n",
      "Epoch [73/300], Step [18500/27733], Loss: 2.3872\n",
      "Epoch [73/300], Step [18600/27733], Loss: 2.3071\n",
      "Epoch [73/300], Step [18700/27733], Loss: 2.9780\n",
      "Epoch [73/300], Step [18800/27733], Loss: 2.7147\n",
      "Epoch [73/300], Step [18900/27733], Loss: 2.3512\n",
      "Epoch [73/300], Step [19000/27733], Loss: 3.6831\n",
      "Epoch [73/300], Step [19100/27733], Loss: 2.9241\n",
      "Epoch [73/300], Step [19200/27733], Loss: 3.2356\n",
      "Epoch [73/300], Step [19300/27733], Loss: 2.8329\n",
      "Epoch [73/300], Step [19400/27733], Loss: 3.2134\n",
      "Epoch [73/300], Step [19500/27733], Loss: 2.5194\n",
      "Epoch [73/300], Step [19600/27733], Loss: 2.6218\n",
      "Epoch [73/300], Step [19700/27733], Loss: 2.0961\n",
      "Epoch [73/300], Step [19800/27733], Loss: 2.5916\n",
      "Epoch [73/300], Step [19900/27733], Loss: 2.6081\n",
      "Epoch [73/300], Step [20000/27733], Loss: 2.9762\n",
      "Epoch [73/300], Step [20100/27733], Loss: 2.7039\n",
      "Epoch [73/300], Step [20200/27733], Loss: 3.1293\n",
      "Epoch [73/300], Step [20300/27733], Loss: 1.9605\n",
      "Epoch [73/300], Step [20400/27733], Loss: 3.1280\n",
      "Epoch [73/300], Step [20500/27733], Loss: 2.9176\n",
      "Epoch [73/300], Step [20600/27733], Loss: 3.1622\n",
      "Epoch [73/300], Step [20700/27733], Loss: 3.3232\n",
      "Epoch [73/300], Step [20800/27733], Loss: 2.1590\n",
      "Epoch [73/300], Step [20900/27733], Loss: 2.9324\n",
      "Epoch [73/300], Step [21000/27733], Loss: 2.4889\n",
      "Epoch [73/300], Step [21100/27733], Loss: 2.8503\n",
      "Epoch [73/300], Step [21200/27733], Loss: 2.1924\n",
      "Epoch [73/300], Step [21300/27733], Loss: 3.3828\n",
      "Epoch [73/300], Step [21400/27733], Loss: 3.3935\n",
      "Epoch [73/300], Step [21500/27733], Loss: 2.5910\n",
      "Epoch [73/300], Step [21600/27733], Loss: 3.1013\n",
      "Epoch [73/300], Step [21700/27733], Loss: 2.3476\n",
      "Epoch [73/300], Step [21800/27733], Loss: 2.6151\n",
      "Epoch [73/300], Step [21900/27733], Loss: 2.7105\n",
      "Epoch [73/300], Step [22000/27733], Loss: 2.6153\n",
      "Epoch [73/300], Step [22100/27733], Loss: 2.8114\n",
      "Epoch [73/300], Step [22200/27733], Loss: 2.3687\n",
      "Epoch [73/300], Step [22300/27733], Loss: 3.0606\n",
      "Epoch [73/300], Step [22400/27733], Loss: 2.4185\n",
      "Epoch [73/300], Step [22500/27733], Loss: 3.7928\n",
      "Epoch [73/300], Step [22600/27733], Loss: 2.7953\n",
      "Epoch [73/300], Step [22700/27733], Loss: 3.3003\n",
      "Epoch [73/300], Step [22800/27733], Loss: 2.3094\n",
      "Epoch [73/300], Step [22900/27733], Loss: 4.0841\n",
      "Epoch [73/300], Step [23000/27733], Loss: 2.4927\n",
      "Epoch [73/300], Step [23100/27733], Loss: 4.4236\n",
      "Epoch [73/300], Step [23200/27733], Loss: 2.4031\n",
      "Epoch [73/300], Step [23300/27733], Loss: 2.9569\n",
      "Epoch [73/300], Step [23400/27733], Loss: 2.8457\n",
      "Epoch [73/300], Step [23500/27733], Loss: 2.8210\n",
      "Epoch [73/300], Step [23600/27733], Loss: 2.0159\n",
      "Epoch [73/300], Step [23700/27733], Loss: 2.1428\n",
      "Epoch [73/300], Step [23800/27733], Loss: 2.4966\n",
      "Epoch [73/300], Step [23900/27733], Loss: 2.7608\n",
      "Epoch [73/300], Step [24000/27733], Loss: 2.8393\n",
      "Epoch [73/300], Step [24100/27733], Loss: 2.1561\n",
      "Epoch [73/300], Step [24200/27733], Loss: 2.8348\n",
      "Epoch [73/300], Step [24300/27733], Loss: 2.4553\n",
      "Epoch [73/300], Step [24400/27733], Loss: 2.8518\n",
      "Epoch [73/300], Step [24500/27733], Loss: 3.0241\n",
      "Epoch [73/300], Step [24600/27733], Loss: 3.5749\n",
      "Epoch [73/300], Step [24700/27733], Loss: 2.7194\n",
      "Epoch [73/300], Step [24800/27733], Loss: 2.0782\n",
      "Epoch [73/300], Step [24900/27733], Loss: 2.4306\n",
      "Epoch [73/300], Step [25000/27733], Loss: 3.4274\n",
      "Epoch [73/300], Step [25100/27733], Loss: 3.4327\n",
      "Epoch [73/300], Step [25200/27733], Loss: 3.3174\n",
      "Epoch [73/300], Step [25300/27733], Loss: 2.9545\n",
      "Epoch [73/300], Step [25400/27733], Loss: 2.6907\n",
      "Epoch [73/300], Step [25500/27733], Loss: 3.3932\n",
      "Epoch [73/300], Step [25600/27733], Loss: 3.4153\n",
      "Epoch [73/300], Step [25700/27733], Loss: 2.5911\n",
      "Epoch [73/300], Step [25800/27733], Loss: 3.3241\n",
      "Epoch [73/300], Step [25900/27733], Loss: 2.7068\n",
      "Epoch [73/300], Step [26000/27733], Loss: 3.1119\n",
      "Epoch [73/300], Step [26100/27733], Loss: 3.0504\n",
      "Epoch [73/300], Step [26200/27733], Loss: 3.8254\n",
      "Epoch [73/300], Step [26300/27733], Loss: 2.1392\n",
      "Epoch [73/300], Step [26400/27733], Loss: 2.8456\n",
      "Epoch [73/300], Step [26500/27733], Loss: 2.9291\n",
      "Epoch [73/300], Step [26600/27733], Loss: 2.4203\n",
      "Epoch [73/300], Step [26700/27733], Loss: 3.1258\n",
      "Epoch [73/300], Step [26800/27733], Loss: 2.6749\n",
      "Epoch [73/300], Step [26900/27733], Loss: 2.3194\n",
      "Epoch [73/300], Step [27000/27733], Loss: 2.5535\n",
      "Epoch [73/300], Step [27100/27733], Loss: 3.1204\n",
      "Epoch [73/300], Step [27200/27733], Loss: 3.0399\n",
      "Epoch [73/300], Step [27300/27733], Loss: 3.9753\n",
      "Epoch [73/300], Step [27400/27733], Loss: 3.1690\n",
      "Epoch [73/300], Step [27500/27733], Loss: 2.5405\n",
      "Epoch [73/300], Step [27600/27733], Loss: 3.0732\n",
      "Epoch [73/300], Step [27700/27733], Loss: 2.5607\n",
      "Epoch [74/300], Step [100/27733], Loss: 2.9106\n",
      "Epoch [74/300], Step [200/27733], Loss: 2.5533\n",
      "Epoch [74/300], Step [300/27733], Loss: 3.1596\n",
      "Epoch [74/300], Step [400/27733], Loss: 2.4134\n",
      "Epoch [74/300], Step [500/27733], Loss: 3.5643\n",
      "Epoch [74/300], Step [600/27733], Loss: 2.4495\n",
      "Epoch [74/300], Step [700/27733], Loss: 2.5436\n",
      "Epoch [74/300], Step [800/27733], Loss: 1.7796\n",
      "Epoch [74/300], Step [900/27733], Loss: 2.4817\n",
      "Epoch [74/300], Step [1000/27733], Loss: 2.3275\n",
      "Epoch [74/300], Step [1100/27733], Loss: 2.3384\n",
      "Epoch [74/300], Step [1200/27733], Loss: 2.2705\n",
      "Epoch [74/300], Step [1300/27733], Loss: 2.2310\n",
      "Epoch [74/300], Step [1400/27733], Loss: 2.3514\n",
      "Epoch [74/300], Step [1500/27733], Loss: 3.5193\n",
      "Epoch [74/300], Step [1600/27733], Loss: 2.2926\n",
      "Epoch [74/300], Step [1700/27733], Loss: 2.4630\n",
      "Epoch [74/300], Step [1800/27733], Loss: 2.4399\n",
      "Epoch [74/300], Step [1900/27733], Loss: 2.7279\n",
      "Epoch [74/300], Step [2000/27733], Loss: 2.9754\n",
      "Epoch [74/300], Step [2100/27733], Loss: 2.5015\n",
      "Epoch [74/300], Step [2200/27733], Loss: 1.8863\n",
      "Epoch [74/300], Step [2300/27733], Loss: 2.5857\n",
      "Epoch [74/300], Step [2400/27733], Loss: 2.6944\n",
      "Epoch [74/300], Step [2500/27733], Loss: 2.4953\n",
      "Epoch [74/300], Step [2600/27733], Loss: 2.6712\n",
      "Epoch [74/300], Step [2700/27733], Loss: 1.9277\n",
      "Epoch [74/300], Step [2800/27733], Loss: 2.4178\n",
      "Epoch [74/300], Step [2900/27733], Loss: 1.4655\n",
      "Epoch [74/300], Step [3000/27733], Loss: 2.0153\n",
      "Epoch [74/300], Step [3100/27733], Loss: 2.7094\n",
      "Epoch [74/300], Step [3200/27733], Loss: 3.6190\n",
      "Epoch [74/300], Step [3300/27733], Loss: 2.4622\n",
      "Epoch [74/300], Step [3400/27733], Loss: 2.2283\n",
      "Epoch [74/300], Step [3500/27733], Loss: 3.0377\n",
      "Epoch [74/300], Step [3600/27733], Loss: 2.4858\n",
      "Epoch [74/300], Step [3700/27733], Loss: 2.0782\n",
      "Epoch [74/300], Step [3800/27733], Loss: 2.6889\n",
      "Epoch [74/300], Step [3900/27733], Loss: 2.7219\n",
      "Epoch [74/300], Step [4000/27733], Loss: 3.2233\n",
      "Epoch [74/300], Step [4100/27733], Loss: 2.2014\n",
      "Epoch [74/300], Step [4200/27733], Loss: 2.4692\n",
      "Epoch [74/300], Step [4300/27733], Loss: 2.3447\n",
      "Epoch [74/300], Step [4400/27733], Loss: 2.2176\n",
      "Epoch [74/300], Step [4500/27733], Loss: 2.4690\n",
      "Epoch [74/300], Step [4600/27733], Loss: 2.6410\n",
      "Epoch [74/300], Step [4700/27733], Loss: 1.4841\n",
      "Epoch [74/300], Step [4800/27733], Loss: 2.4037\n",
      "Epoch [74/300], Step [4900/27733], Loss: 2.7533\n",
      "Epoch [74/300], Step [5000/27733], Loss: 2.9156\n",
      "Epoch [74/300], Step [5100/27733], Loss: 1.8112\n",
      "Epoch [74/300], Step [5200/27733], Loss: 2.5967\n",
      "Epoch [74/300], Step [5300/27733], Loss: 2.2698\n",
      "Epoch [74/300], Step [5400/27733], Loss: 3.0850\n",
      "Epoch [74/300], Step [5500/27733], Loss: 2.9812\n",
      "Epoch [74/300], Step [5600/27733], Loss: 2.5698\n",
      "Epoch [74/300], Step [5700/27733], Loss: 2.5667\n",
      "Epoch [74/300], Step [5800/27733], Loss: 2.7441\n",
      "Epoch [74/300], Step [5900/27733], Loss: 2.8480\n",
      "Epoch [74/300], Step [6000/27733], Loss: 2.2763\n",
      "Epoch [74/300], Step [6100/27733], Loss: 2.0504\n",
      "Epoch [74/300], Step [6200/27733], Loss: 3.4496\n",
      "Epoch [74/300], Step [6300/27733], Loss: 2.2873\n",
      "Epoch [74/300], Step [6400/27733], Loss: 2.4776\n",
      "Epoch [74/300], Step [6500/27733], Loss: 2.4390\n",
      "Epoch [74/300], Step [6600/27733], Loss: 2.7222\n",
      "Epoch [74/300], Step [6700/27733], Loss: 2.4230\n",
      "Epoch [74/300], Step [6800/27733], Loss: 3.1431\n",
      "Epoch [74/300], Step [6900/27733], Loss: 2.7273\n",
      "Epoch [74/300], Step [7000/27733], Loss: 2.2774\n",
      "Epoch [74/300], Step [7100/27733], Loss: 2.4562\n",
      "Epoch [74/300], Step [7200/27733], Loss: 2.1816\n",
      "Epoch [74/300], Step [7300/27733], Loss: 2.6206\n",
      "Epoch [74/300], Step [7400/27733], Loss: 2.3827\n",
      "Epoch [74/300], Step [7500/27733], Loss: 2.7418\n",
      "Epoch [74/300], Step [7600/27733], Loss: 2.8254\n",
      "Epoch [74/300], Step [7700/27733], Loss: 2.6868\n",
      "Epoch [74/300], Step [7800/27733], Loss: 2.2917\n",
      "Epoch [74/300], Step [7900/27733], Loss: 1.6955\n",
      "Epoch [74/300], Step [8000/27733], Loss: 2.5231\n",
      "Epoch [74/300], Step [8100/27733], Loss: 2.9464\n",
      "Epoch [74/300], Step [8200/27733], Loss: 3.6035\n",
      "Epoch [74/300], Step [8300/27733], Loss: 2.7762\n",
      "Epoch [74/300], Step [8400/27733], Loss: 3.3221\n",
      "Epoch [74/300], Step [8500/27733], Loss: 2.2967\n",
      "Epoch [74/300], Step [8600/27733], Loss: 3.2015\n",
      "Epoch [74/300], Step [8700/27733], Loss: 2.7246\n",
      "Epoch [74/300], Step [8800/27733], Loss: 2.6141\n",
      "Epoch [74/300], Step [8900/27733], Loss: 2.0646\n",
      "Epoch [74/300], Step [9000/27733], Loss: 2.6541\n",
      "Epoch [74/300], Step [9100/27733], Loss: 2.1058\n",
      "Epoch [74/300], Step [9200/27733], Loss: 3.1690\n",
      "Epoch [74/300], Step [9300/27733], Loss: 3.1277\n",
      "Epoch [74/300], Step [9400/27733], Loss: 2.9231\n",
      "Epoch [74/300], Step [9500/27733], Loss: 2.2319\n",
      "Epoch [74/300], Step [9600/27733], Loss: 3.4602\n",
      "Epoch [74/300], Step [9700/27733], Loss: 2.5166\n",
      "Epoch [74/300], Step [9800/27733], Loss: 2.6901\n",
      "Epoch [74/300], Step [9900/27733], Loss: 3.4163\n",
      "Epoch [74/300], Step [10000/27733], Loss: 2.9464\n",
      "Epoch [74/300], Step [10100/27733], Loss: 2.2747\n",
      "Epoch [74/300], Step [10200/27733], Loss: 3.0710\n",
      "Epoch [74/300], Step [10300/27733], Loss: 1.9163\n",
      "Epoch [74/300], Step [10400/27733], Loss: 2.5294\n",
      "Epoch [74/300], Step [10500/27733], Loss: 2.8113\n",
      "Epoch [74/300], Step [10600/27733], Loss: 2.6721\n",
      "Epoch [74/300], Step [10700/27733], Loss: 2.6082\n",
      "Epoch [74/300], Step [10800/27733], Loss: 2.5936\n",
      "Epoch [74/300], Step [10900/27733], Loss: 2.4037\n",
      "Epoch [74/300], Step [11000/27733], Loss: 3.0842\n",
      "Epoch [74/300], Step [11100/27733], Loss: 3.0106\n",
      "Epoch [74/300], Step [11200/27733], Loss: 1.8061\n",
      "Epoch [74/300], Step [11300/27733], Loss: 2.3067\n",
      "Epoch [74/300], Step [11400/27733], Loss: 2.3803\n",
      "Epoch [74/300], Step [11500/27733], Loss: 1.8034\n",
      "Epoch [74/300], Step [11600/27733], Loss: 2.8607\n",
      "Epoch [74/300], Step [11700/27733], Loss: 2.4635\n",
      "Epoch [74/300], Step [11800/27733], Loss: 2.1474\n",
      "Epoch [74/300], Step [11900/27733], Loss: 2.7749\n",
      "Epoch [74/300], Step [12000/27733], Loss: 2.6947\n",
      "Epoch [74/300], Step [12100/27733], Loss: 2.7861\n",
      "Epoch [74/300], Step [12200/27733], Loss: 2.2595\n",
      "Epoch [74/300], Step [12300/27733], Loss: 2.1602\n",
      "Epoch [74/300], Step [12400/27733], Loss: 3.4995\n",
      "Epoch [74/300], Step [12500/27733], Loss: 3.0563\n",
      "Epoch [74/300], Step [12600/27733], Loss: 2.0104\n",
      "Epoch [74/300], Step [12700/27733], Loss: 2.9639\n",
      "Epoch [74/300], Step [12800/27733], Loss: 2.5019\n",
      "Epoch [74/300], Step [12900/27733], Loss: 2.2742\n",
      "Epoch [74/300], Step [13000/27733], Loss: 2.7513\n",
      "Epoch [74/300], Step [13100/27733], Loss: 3.4978\n",
      "Epoch [74/300], Step [13200/27733], Loss: 2.5426\n",
      "Epoch [74/300], Step [13300/27733], Loss: 3.1121\n",
      "Epoch [74/300], Step [13400/27733], Loss: 2.2524\n",
      "Epoch [74/300], Step [13500/27733], Loss: 2.9768\n",
      "Epoch [74/300], Step [13600/27733], Loss: 2.5484\n",
      "Epoch [74/300], Step [13700/27733], Loss: 3.0280\n",
      "Epoch [74/300], Step [13800/27733], Loss: 2.9973\n",
      "Epoch [74/300], Step [13900/27733], Loss: 2.6478\n",
      "Epoch [74/300], Step [14000/27733], Loss: 2.5233\n",
      "Epoch [74/300], Step [14100/27733], Loss: 3.0820\n",
      "Epoch [74/300], Step [14200/27733], Loss: 2.9558\n",
      "Epoch [74/300], Step [14300/27733], Loss: 2.4799\n",
      "Epoch [74/300], Step [14400/27733], Loss: 2.8155\n",
      "Epoch [74/300], Step [14500/27733], Loss: 2.6611\n",
      "Epoch [74/300], Step [14600/27733], Loss: 3.5561\n",
      "Epoch [74/300], Step [14700/27733], Loss: 3.0817\n",
      "Epoch [74/300], Step [14800/27733], Loss: 2.3509\n",
      "Epoch [74/300], Step [14900/27733], Loss: 3.4535\n",
      "Epoch [74/300], Step [15000/27733], Loss: 2.5057\n",
      "Epoch [74/300], Step [15100/27733], Loss: 3.0166\n",
      "Epoch [74/300], Step [15200/27733], Loss: 2.6198\n",
      "Epoch [74/300], Step [15300/27733], Loss: 3.4486\n",
      "Epoch [74/300], Step [15400/27733], Loss: 2.5654\n",
      "Epoch [74/300], Step [15500/27733], Loss: 2.9194\n",
      "Epoch [74/300], Step [15600/27733], Loss: 2.0187\n",
      "Epoch [74/300], Step [15700/27733], Loss: 2.6146\n",
      "Epoch [74/300], Step [15800/27733], Loss: 2.7730\n",
      "Epoch [74/300], Step [15900/27733], Loss: 3.1690\n",
      "Epoch [74/300], Step [16000/27733], Loss: 2.6226\n",
      "Epoch [74/300], Step [16100/27733], Loss: 2.9032\n",
      "Epoch [74/300], Step [16200/27733], Loss: 2.3729\n",
      "Epoch [74/300], Step [16300/27733], Loss: 2.3182\n",
      "Epoch [74/300], Step [16400/27733], Loss: 1.7993\n",
      "Epoch [74/300], Step [16500/27733], Loss: 3.2380\n",
      "Epoch [74/300], Step [16600/27733], Loss: 2.8022\n",
      "Epoch [74/300], Step [16700/27733], Loss: 2.6113\n",
      "Epoch [74/300], Step [16800/27733], Loss: 2.5858\n",
      "Epoch [74/300], Step [16900/27733], Loss: 2.9450\n",
      "Epoch [74/300], Step [17000/27733], Loss: 2.5928\n",
      "Epoch [74/300], Step [17100/27733], Loss: 2.4200\n",
      "Epoch [74/300], Step [17200/27733], Loss: 3.1880\n",
      "Epoch [74/300], Step [17300/27733], Loss: 2.7931\n",
      "Epoch [74/300], Step [17400/27733], Loss: 2.5584\n",
      "Epoch [74/300], Step [17500/27733], Loss: 3.4305\n",
      "Epoch [74/300], Step [17600/27733], Loss: 2.6920\n",
      "Epoch [74/300], Step [17700/27733], Loss: 1.9424\n",
      "Epoch [74/300], Step [17800/27733], Loss: 2.5071\n",
      "Epoch [74/300], Step [17900/27733], Loss: 1.9805\n",
      "Epoch [74/300], Step [18000/27733], Loss: 2.0722\n",
      "Epoch [74/300], Step [18100/27733], Loss: 2.6414\n",
      "Epoch [74/300], Step [18200/27733], Loss: 3.5995\n",
      "Epoch [74/300], Step [18300/27733], Loss: 2.3974\n",
      "Epoch [74/300], Step [18400/27733], Loss: 3.1221\n",
      "Epoch [74/300], Step [18500/27733], Loss: 3.2549\n",
      "Epoch [74/300], Step [18600/27733], Loss: 2.0459\n",
      "Epoch [74/300], Step [18700/27733], Loss: 2.4436\n",
      "Epoch [74/300], Step [18800/27733], Loss: 2.5792\n",
      "Epoch [74/300], Step [18900/27733], Loss: 2.2789\n",
      "Epoch [74/300], Step [19000/27733], Loss: 2.7871\n",
      "Epoch [74/300], Step [19100/27733], Loss: 2.2263\n",
      "Epoch [74/300], Step [19200/27733], Loss: 3.6015\n",
      "Epoch [74/300], Step [19300/27733], Loss: 3.5054\n",
      "Epoch [74/300], Step [19400/27733], Loss: 3.1419\n",
      "Epoch [74/300], Step [19500/27733], Loss: 2.3755\n",
      "Epoch [74/300], Step [19600/27733], Loss: 3.0772\n",
      "Epoch [74/300], Step [19700/27733], Loss: 2.2067\n",
      "Epoch [74/300], Step [19800/27733], Loss: 2.2393\n",
      "Epoch [74/300], Step [19900/27733], Loss: 2.3198\n",
      "Epoch [74/300], Step [20000/27733], Loss: 2.3449\n",
      "Epoch [74/300], Step [20100/27733], Loss: 2.3534\n",
      "Epoch [74/300], Step [20200/27733], Loss: 2.6340\n",
      "Epoch [74/300], Step [20300/27733], Loss: 3.1774\n",
      "Epoch [74/300], Step [20400/27733], Loss: 2.5121\n",
      "Epoch [74/300], Step [20500/27733], Loss: 2.8827\n",
      "Epoch [74/300], Step [20600/27733], Loss: 2.2010\n",
      "Epoch [74/300], Step [20700/27733], Loss: 2.2198\n",
      "Epoch [74/300], Step [20800/27733], Loss: 3.2633\n",
      "Epoch [74/300], Step [20900/27733], Loss: 2.7677\n",
      "Epoch [74/300], Step [21000/27733], Loss: 2.9367\n",
      "Epoch [74/300], Step [21100/27733], Loss: 2.3210\n",
      "Epoch [74/300], Step [21200/27733], Loss: 2.2923\n",
      "Epoch [74/300], Step [21300/27733], Loss: 2.4425\n",
      "Epoch [74/300], Step [21400/27733], Loss: 3.2252\n",
      "Epoch [74/300], Step [21500/27733], Loss: 2.7934\n",
      "Epoch [74/300], Step [21600/27733], Loss: 2.5183\n",
      "Epoch [74/300], Step [21700/27733], Loss: 3.1251\n",
      "Epoch [74/300], Step [21800/27733], Loss: 2.8102\n",
      "Epoch [74/300], Step [21900/27733], Loss: 3.4093\n",
      "Epoch [74/300], Step [22000/27733], Loss: 2.7836\n",
      "Epoch [74/300], Step [22100/27733], Loss: 2.6661\n",
      "Epoch [74/300], Step [22200/27733], Loss: 2.1881\n",
      "Epoch [74/300], Step [22300/27733], Loss: 2.4504\n",
      "Epoch [74/300], Step [22400/27733], Loss: 3.1836\n",
      "Epoch [74/300], Step [22500/27733], Loss: 2.7859\n",
      "Epoch [74/300], Step [22600/27733], Loss: 2.5343\n",
      "Epoch [74/300], Step [22700/27733], Loss: 3.3466\n",
      "Epoch [74/300], Step [22800/27733], Loss: 2.7249\n",
      "Epoch [74/300], Step [22900/27733], Loss: 2.5755\n",
      "Epoch [74/300], Step [23000/27733], Loss: 2.5613\n",
      "Epoch [74/300], Step [23100/27733], Loss: 2.3410\n",
      "Epoch [74/300], Step [23200/27733], Loss: 2.7468\n",
      "Epoch [74/300], Step [23300/27733], Loss: 3.0131\n",
      "Epoch [74/300], Step [23400/27733], Loss: 2.9717\n",
      "Epoch [74/300], Step [23500/27733], Loss: 2.4533\n",
      "Epoch [74/300], Step [23600/27733], Loss: 2.5253\n",
      "Epoch [74/300], Step [23700/27733], Loss: 3.0656\n",
      "Epoch [74/300], Step [23800/27733], Loss: 3.1821\n",
      "Epoch [74/300], Step [23900/27733], Loss: 2.5369\n",
      "Epoch [74/300], Step [24000/27733], Loss: 3.2245\n",
      "Epoch [74/300], Step [24100/27733], Loss: 2.3720\n",
      "Epoch [74/300], Step [24200/27733], Loss: 2.9260\n",
      "Epoch [74/300], Step [24300/27733], Loss: 2.3488\n",
      "Epoch [74/300], Step [24400/27733], Loss: 3.1404\n",
      "Epoch [74/300], Step [24500/27733], Loss: 3.1886\n",
      "Epoch [74/300], Step [24600/27733], Loss: 2.8285\n",
      "Epoch [74/300], Step [24700/27733], Loss: 2.2783\n",
      "Epoch [74/300], Step [24800/27733], Loss: 2.9427\n",
      "Epoch [74/300], Step [24900/27733], Loss: 3.5547\n",
      "Epoch [74/300], Step [25000/27733], Loss: 2.8168\n",
      "Epoch [74/300], Step [25100/27733], Loss: 3.0441\n",
      "Epoch [74/300], Step [25200/27733], Loss: 3.4604\n",
      "Epoch [74/300], Step [25300/27733], Loss: 2.6759\n",
      "Epoch [74/300], Step [25400/27733], Loss: 3.6563\n",
      "Epoch [74/300], Step [25500/27733], Loss: 3.1180\n",
      "Epoch [74/300], Step [25600/27733], Loss: 2.8802\n",
      "Epoch [74/300], Step [25700/27733], Loss: 2.6235\n",
      "Epoch [74/300], Step [25800/27733], Loss: 2.8353\n",
      "Epoch [74/300], Step [25900/27733], Loss: 3.0603\n",
      "Epoch [74/300], Step [26000/27733], Loss: 3.1563\n",
      "Epoch [74/300], Step [26100/27733], Loss: 2.3994\n",
      "Epoch [74/300], Step [26200/27733], Loss: 2.7769\n",
      "Epoch [74/300], Step [26300/27733], Loss: 2.0793\n",
      "Epoch [74/300], Step [26400/27733], Loss: 3.2728\n",
      "Epoch [74/300], Step [26500/27733], Loss: 3.1728\n",
      "Epoch [74/300], Step [26600/27733], Loss: 2.4311\n",
      "Epoch [74/300], Step [26700/27733], Loss: 2.3160\n",
      "Epoch [74/300], Step [26800/27733], Loss: 2.9495\n",
      "Epoch [74/300], Step [26900/27733], Loss: 3.0406\n",
      "Epoch [74/300], Step [27000/27733], Loss: 3.4355\n",
      "Epoch [74/300], Step [27100/27733], Loss: 2.7536\n",
      "Epoch [74/300], Step [27200/27733], Loss: 2.9919\n",
      "Epoch [74/300], Step [27300/27733], Loss: 3.3504\n",
      "Epoch [74/300], Step [27400/27733], Loss: 2.6955\n",
      "Epoch [74/300], Step [27500/27733], Loss: 3.0273\n",
      "Epoch [74/300], Step [27600/27733], Loss: 3.3146\n",
      "Epoch [74/300], Step [27700/27733], Loss: 2.8653\n",
      "Epoch [75/300], Step [100/27733], Loss: 2.2037\n",
      "Epoch [75/300], Step [200/27733], Loss: 2.2860\n",
      "Epoch [75/300], Step [300/27733], Loss: 2.6904\n",
      "Epoch [75/300], Step [400/27733], Loss: 3.1666\n",
      "Epoch [75/300], Step [500/27733], Loss: 2.4469\n",
      "Epoch [75/300], Step [600/27733], Loss: 2.3053\n",
      "Epoch [75/300], Step [700/27733], Loss: 1.9601\n",
      "Epoch [75/300], Step [800/27733], Loss: 2.4198\n",
      "Epoch [75/300], Step [900/27733], Loss: 2.3818\n",
      "Epoch [75/300], Step [1000/27733], Loss: 1.9066\n",
      "Epoch [75/300], Step [1100/27733], Loss: 2.5176\n",
      "Epoch [75/300], Step [1200/27733], Loss: 2.2471\n",
      "Epoch [75/300], Step [1300/27733], Loss: 2.3574\n",
      "Epoch [75/300], Step [1400/27733], Loss: 2.0064\n",
      "Epoch [75/300], Step [1500/27733], Loss: 2.3497\n",
      "Epoch [75/300], Step [1600/27733], Loss: 2.5959\n",
      "Epoch [75/300], Step [1700/27733], Loss: 2.6207\n",
      "Epoch [75/300], Step [1800/27733], Loss: 3.3810\n",
      "Epoch [75/300], Step [1900/27733], Loss: 3.0066\n",
      "Epoch [75/300], Step [2000/27733], Loss: 2.4695\n",
      "Epoch [75/300], Step [2100/27733], Loss: 2.3491\n",
      "Epoch [75/300], Step [2200/27733], Loss: 2.5474\n",
      "Epoch [75/300], Step [2300/27733], Loss: 1.4889\n",
      "Epoch [75/300], Step [2400/27733], Loss: 1.9410\n",
      "Epoch [75/300], Step [2500/27733], Loss: 2.1856\n",
      "Epoch [75/300], Step [2600/27733], Loss: 3.2248\n",
      "Epoch [75/300], Step [2700/27733], Loss: 1.8720\n",
      "Epoch [75/300], Step [2800/27733], Loss: 3.2209\n",
      "Epoch [75/300], Step [2900/27733], Loss: 2.2947\n",
      "Epoch [75/300], Step [3000/27733], Loss: 2.0149\n",
      "Epoch [75/300], Step [3100/27733], Loss: 2.3319\n",
      "Epoch [75/300], Step [3200/27733], Loss: 2.2948\n",
      "Epoch [75/300], Step [3300/27733], Loss: 2.5511\n",
      "Epoch [75/300], Step [3400/27733], Loss: 2.3881\n",
      "Epoch [75/300], Step [3500/27733], Loss: 2.4489\n",
      "Epoch [75/300], Step [3600/27733], Loss: 2.8501\n",
      "Epoch [75/300], Step [3700/27733], Loss: 2.4600\n",
      "Epoch [75/300], Step [3800/27733], Loss: 2.6840\n",
      "Epoch [75/300], Step [3900/27733], Loss: 2.1095\n",
      "Epoch [75/300], Step [4000/27733], Loss: 2.6253\n",
      "Epoch [75/300], Step [4100/27733], Loss: 3.2132\n",
      "Epoch [75/300], Step [4200/27733], Loss: 2.8908\n",
      "Epoch [75/300], Step [4300/27733], Loss: 1.6109\n",
      "Epoch [75/300], Step [4400/27733], Loss: 2.1318\n",
      "Epoch [75/300], Step [4500/27733], Loss: 2.3005\n",
      "Epoch [75/300], Step [4600/27733], Loss: 1.9655\n",
      "Epoch [75/300], Step [4700/27733], Loss: 2.6819\n",
      "Epoch [75/300], Step [4800/27733], Loss: 2.9103\n",
      "Epoch [75/300], Step [4900/27733], Loss: 2.2017\n",
      "Epoch [75/300], Step [5000/27733], Loss: 2.4707\n",
      "Epoch [75/300], Step [5100/27733], Loss: 2.3275\n",
      "Epoch [75/300], Step [5200/27733], Loss: 2.7866\n",
      "Epoch [75/300], Step [5300/27733], Loss: 3.1199\n",
      "Epoch [75/300], Step [5400/27733], Loss: 2.5072\n",
      "Epoch [75/300], Step [5500/27733], Loss: 2.7570\n",
      "Epoch [75/300], Step [5600/27733], Loss: 1.9021\n",
      "Epoch [75/300], Step [5700/27733], Loss: 2.9753\n",
      "Epoch [75/300], Step [5800/27733], Loss: 2.9573\n",
      "Epoch [75/300], Step [5900/27733], Loss: 3.3053\n",
      "Epoch [75/300], Step [6000/27733], Loss: 2.1038\n",
      "Epoch [75/300], Step [6100/27733], Loss: 2.9653\n",
      "Epoch [75/300], Step [6200/27733], Loss: 2.8431\n",
      "Epoch [75/300], Step [6300/27733], Loss: 2.8672\n",
      "Epoch [75/300], Step [6400/27733], Loss: 2.7083\n",
      "Epoch [75/300], Step [6500/27733], Loss: 2.3182\n",
      "Epoch [75/300], Step [6600/27733], Loss: 2.7287\n",
      "Epoch [75/300], Step [6700/27733], Loss: 1.6200\n",
      "Epoch [75/300], Step [6800/27733], Loss: 2.7063\n",
      "Epoch [75/300], Step [6900/27733], Loss: 2.8774\n",
      "Epoch [75/300], Step [7000/27733], Loss: 2.6768\n",
      "Epoch [75/300], Step [7100/27733], Loss: 2.3640\n",
      "Epoch [75/300], Step [7200/27733], Loss: 2.4712\n",
      "Epoch [75/300], Step [7300/27733], Loss: 2.1000\n",
      "Epoch [75/300], Step [7400/27733], Loss: 3.4710\n",
      "Epoch [75/300], Step [7500/27733], Loss: 2.3315\n",
      "Epoch [75/300], Step [7600/27733], Loss: 2.5783\n",
      "Epoch [75/300], Step [7700/27733], Loss: 2.3248\n",
      "Epoch [75/300], Step [7800/27733], Loss: 3.5838\n",
      "Epoch [75/300], Step [7900/27733], Loss: 2.9697\n",
      "Epoch [75/300], Step [8000/27733], Loss: 2.5431\n",
      "Epoch [75/300], Step [8100/27733], Loss: 2.1840\n",
      "Epoch [75/300], Step [8200/27733], Loss: 2.8832\n",
      "Epoch [75/300], Step [8300/27733], Loss: 2.3201\n",
      "Epoch [75/300], Step [8400/27733], Loss: 2.2227\n",
      "Epoch [75/300], Step [8500/27733], Loss: 2.1706\n",
      "Epoch [75/300], Step [8600/27733], Loss: 2.2177\n",
      "Epoch [75/300], Step [8700/27733], Loss: 3.0309\n",
      "Epoch [75/300], Step [8800/27733], Loss: 2.8100\n",
      "Epoch [75/300], Step [8900/27733], Loss: 2.1708\n",
      "Epoch [75/300], Step [9000/27733], Loss: 3.0751\n",
      "Epoch [75/300], Step [9100/27733], Loss: 3.4402\n",
      "Epoch [75/300], Step [9200/27733], Loss: 3.0691\n",
      "Epoch [75/300], Step [9300/27733], Loss: 2.2842\n",
      "Epoch [75/300], Step [9400/27733], Loss: 2.7131\n",
      "Epoch [75/300], Step [9500/27733], Loss: 3.4779\n",
      "Epoch [75/300], Step [9600/27733], Loss: 1.9213\n",
      "Epoch [75/300], Step [9700/27733], Loss: 2.6683\n",
      "Epoch [75/300], Step [9800/27733], Loss: 2.0435\n",
      "Epoch [75/300], Step [9900/27733], Loss: 2.3037\n",
      "Epoch [75/300], Step [10000/27733], Loss: 3.5702\n",
      "Epoch [75/300], Step [10100/27733], Loss: 2.6045\n",
      "Epoch [75/300], Step [10200/27733], Loss: 1.9961\n",
      "Epoch [75/300], Step [10300/27733], Loss: 2.3822\n",
      "Epoch [75/300], Step [10400/27733], Loss: 2.7479\n",
      "Epoch [75/300], Step [10500/27733], Loss: 2.6672\n",
      "Epoch [75/300], Step [10600/27733], Loss: 2.8285\n",
      "Epoch [75/300], Step [10700/27733], Loss: 2.8963\n",
      "Epoch [75/300], Step [10800/27733], Loss: 2.8407\n",
      "Epoch [75/300], Step [10900/27733], Loss: 2.8717\n",
      "Epoch [75/300], Step [11000/27733], Loss: 2.8748\n",
      "Epoch [75/300], Step [11100/27733], Loss: 2.5301\n",
      "Epoch [75/300], Step [11200/27733], Loss: 4.0352\n",
      "Epoch [75/300], Step [11300/27733], Loss: 3.1222\n",
      "Epoch [75/300], Step [11400/27733], Loss: 2.7920\n",
      "Epoch [75/300], Step [11500/27733], Loss: 2.4079\n",
      "Epoch [75/300], Step [11600/27733], Loss: 2.5093\n",
      "Epoch [75/300], Step [11700/27733], Loss: 2.6141\n",
      "Epoch [75/300], Step [11800/27733], Loss: 1.2832\n",
      "Epoch [75/300], Step [11900/27733], Loss: 2.6932\n",
      "Epoch [75/300], Step [12000/27733], Loss: 3.4347\n",
      "Epoch [75/300], Step [12100/27733], Loss: 2.8100\n",
      "Epoch [75/300], Step [12200/27733], Loss: 2.5497\n",
      "Epoch [75/300], Step [12300/27733], Loss: 3.3620\n",
      "Epoch [75/300], Step [12400/27733], Loss: 4.1214\n",
      "Epoch [75/300], Step [12500/27733], Loss: 2.2079\n",
      "Epoch [75/300], Step [12600/27733], Loss: 2.2518\n",
      "Epoch [75/300], Step [12700/27733], Loss: 2.2062\n",
      "Epoch [75/300], Step [12800/27733], Loss: 2.6833\n",
      "Epoch [75/300], Step [12900/27733], Loss: 4.0551\n",
      "Epoch [75/300], Step [13000/27733], Loss: 2.5534\n",
      "Epoch [75/300], Step [13100/27733], Loss: 2.8811\n",
      "Epoch [75/300], Step [13200/27733], Loss: 2.0351\n",
      "Epoch [75/300], Step [13300/27733], Loss: 2.7073\n",
      "Epoch [75/300], Step [13400/27733], Loss: 2.7677\n",
      "Epoch [75/300], Step [13500/27733], Loss: 2.4804\n",
      "Epoch [75/300], Step [13600/27733], Loss: 3.1605\n",
      "Epoch [75/300], Step [13700/27733], Loss: 2.9089\n",
      "Epoch [75/300], Step [13800/27733], Loss: 2.3740\n",
      "Epoch [75/300], Step [13900/27733], Loss: 2.7884\n",
      "Epoch [75/300], Step [14000/27733], Loss: 2.8509\n",
      "Epoch [75/300], Step [14100/27733], Loss: 2.8597\n",
      "Epoch [75/300], Step [14200/27733], Loss: 2.1638\n",
      "Epoch [75/300], Step [14300/27733], Loss: 2.5373\n",
      "Epoch [75/300], Step [14400/27733], Loss: 3.7278\n",
      "Epoch [75/300], Step [14500/27733], Loss: 3.0575\n",
      "Epoch [75/300], Step [14600/27733], Loss: 1.9547\n",
      "Epoch [75/300], Step [14700/27733], Loss: 3.5814\n",
      "Epoch [75/300], Step [14800/27733], Loss: 2.8418\n",
      "Epoch [75/300], Step [14900/27733], Loss: 2.7456\n",
      "Epoch [75/300], Step [15000/27733], Loss: 3.4603\n",
      "Epoch [75/300], Step [15100/27733], Loss: 3.5940\n",
      "Epoch [75/300], Step [15200/27733], Loss: 2.4985\n",
      "Epoch [75/300], Step [15300/27733], Loss: 2.8890\n",
      "Epoch [75/300], Step [15400/27733], Loss: 2.8716\n",
      "Epoch [75/300], Step [15500/27733], Loss: 2.0712\n",
      "Epoch [75/300], Step [15600/27733], Loss: 2.8553\n",
      "Epoch [75/300], Step [15700/27733], Loss: 2.6931\n",
      "Epoch [75/300], Step [15800/27733], Loss: 4.1530\n",
      "Epoch [75/300], Step [15900/27733], Loss: 2.3720\n",
      "Epoch [75/300], Step [16000/27733], Loss: 2.8131\n",
      "Epoch [75/300], Step [16100/27733], Loss: 3.1372\n",
      "Epoch [75/300], Step [16200/27733], Loss: 2.5660\n",
      "Epoch [75/300], Step [16300/27733], Loss: 2.4966\n",
      "Epoch [75/300], Step [16400/27733], Loss: 2.2141\n",
      "Epoch [75/300], Step [16500/27733], Loss: 3.5270\n",
      "Epoch [75/300], Step [16600/27733], Loss: 2.6912\n",
      "Epoch [75/300], Step [16700/27733], Loss: 2.7429\n",
      "Epoch [75/300], Step [16800/27733], Loss: 2.9167\n",
      "Epoch [75/300], Step [16900/27733], Loss: 2.8265\n",
      "Epoch [75/300], Step [17000/27733], Loss: 2.8772\n",
      "Epoch [75/300], Step [17100/27733], Loss: 2.9731\n",
      "Epoch [75/300], Step [17200/27733], Loss: 2.4541\n",
      "Epoch [75/300], Step [17300/27733], Loss: 2.8625\n",
      "Epoch [75/300], Step [17400/27733], Loss: 2.2406\n",
      "Epoch [75/300], Step [17500/27733], Loss: 2.9417\n",
      "Epoch [75/300], Step [17600/27733], Loss: 2.2123\n",
      "Epoch [75/300], Step [17700/27733], Loss: 2.9078\n",
      "Epoch [75/300], Step [17800/27733], Loss: 2.8956\n",
      "Epoch [75/300], Step [17900/27733], Loss: 2.5327\n",
      "Epoch [75/300], Step [18000/27733], Loss: 2.8396\n",
      "Epoch [75/300], Step [18100/27733], Loss: 3.1656\n",
      "Epoch [75/300], Step [18200/27733], Loss: 3.4689\n",
      "Epoch [75/300], Step [18300/27733], Loss: 2.4041\n",
      "Epoch [75/300], Step [18400/27733], Loss: 2.3571\n",
      "Epoch [75/300], Step [18500/27733], Loss: 3.1290\n",
      "Epoch [75/300], Step [18600/27733], Loss: 2.9608\n",
      "Epoch [75/300], Step [18700/27733], Loss: 3.5191\n",
      "Epoch [75/300], Step [18800/27733], Loss: 2.9309\n",
      "Epoch [75/300], Step [18900/27733], Loss: 2.8069\n",
      "Epoch [75/300], Step [19000/27733], Loss: 3.1490\n",
      "Epoch [75/300], Step [19100/27733], Loss: 3.4478\n",
      "Epoch [75/300], Step [19200/27733], Loss: 2.0709\n",
      "Epoch [75/300], Step [19300/27733], Loss: 2.5381\n",
      "Epoch [75/300], Step [19400/27733], Loss: 2.3121\n",
      "Epoch [75/300], Step [19500/27733], Loss: 2.3985\n",
      "Epoch [75/300], Step [19600/27733], Loss: 2.8235\n",
      "Epoch [75/300], Step [19700/27733], Loss: 3.1592\n",
      "Epoch [75/300], Step [19800/27733], Loss: 2.7887\n",
      "Epoch [75/300], Step [19900/27733], Loss: 2.8552\n",
      "Epoch [75/300], Step [20000/27733], Loss: 3.2467\n",
      "Epoch [75/300], Step [20100/27733], Loss: 2.8730\n",
      "Epoch [75/300], Step [20200/27733], Loss: 2.6942\n",
      "Epoch [75/300], Step [20300/27733], Loss: 3.1463\n",
      "Epoch [75/300], Step [20400/27733], Loss: 2.2606\n",
      "Epoch [75/300], Step [20500/27733], Loss: 2.3220\n",
      "Epoch [75/300], Step [20600/27733], Loss: 2.7016\n",
      "Epoch [75/300], Step [20700/27733], Loss: 2.7793\n",
      "Epoch [75/300], Step [20800/27733], Loss: 2.3689\n",
      "Epoch [75/300], Step [20900/27733], Loss: 2.8715\n",
      "Epoch [75/300], Step [21000/27733], Loss: 1.7287\n",
      "Epoch [75/300], Step [21100/27733], Loss: 3.8846\n",
      "Epoch [75/300], Step [21200/27733], Loss: 3.3908\n",
      "Epoch [75/300], Step [21300/27733], Loss: 3.5328\n",
      "Epoch [75/300], Step [21400/27733], Loss: 2.7511\n",
      "Epoch [75/300], Step [21500/27733], Loss: 3.4323\n",
      "Epoch [75/300], Step [21600/27733], Loss: 2.3803\n",
      "Epoch [75/300], Step [21700/27733], Loss: 2.9233\n",
      "Epoch [75/300], Step [21800/27733], Loss: 2.8417\n",
      "Epoch [75/300], Step [21900/27733], Loss: 2.9405\n",
      "Epoch [75/300], Step [22000/27733], Loss: 3.4120\n",
      "Epoch [75/300], Step [22100/27733], Loss: 2.6552\n",
      "Epoch [75/300], Step [22200/27733], Loss: 2.4284\n",
      "Epoch [75/300], Step [22300/27733], Loss: 2.7678\n",
      "Epoch [75/300], Step [22400/27733], Loss: 2.6478\n",
      "Epoch [75/300], Step [22500/27733], Loss: 2.9837\n",
      "Epoch [75/300], Step [22600/27733], Loss: 3.0301\n",
      "Epoch [75/300], Step [22700/27733], Loss: 3.2587\n",
      "Epoch [75/300], Step [22800/27733], Loss: 3.2898\n",
      "Epoch [75/300], Step [22900/27733], Loss: 2.6267\n",
      "Epoch [75/300], Step [23000/27733], Loss: 2.3449\n",
      "Epoch [75/300], Step [23100/27733], Loss: 3.3790\n",
      "Epoch [75/300], Step [23200/27733], Loss: 3.6385\n",
      "Epoch [75/300], Step [23300/27733], Loss: 2.7633\n",
      "Epoch [75/300], Step [23400/27733], Loss: 3.2002\n",
      "Epoch [75/300], Step [23500/27733], Loss: 2.5988\n",
      "Epoch [75/300], Step [23600/27733], Loss: 2.4925\n",
      "Epoch [75/300], Step [23700/27733], Loss: 2.4856\n",
      "Epoch [75/300], Step [23800/27733], Loss: 2.4264\n",
      "Epoch [75/300], Step [23900/27733], Loss: 3.1448\n",
      "Epoch [75/300], Step [24000/27733], Loss: 3.6140\n",
      "Epoch [75/300], Step [24100/27733], Loss: 2.9388\n",
      "Epoch [75/300], Step [24200/27733], Loss: 3.0267\n",
      "Epoch [75/300], Step [24300/27733], Loss: 2.8599\n",
      "Epoch [75/300], Step [24400/27733], Loss: 3.5358\n",
      "Epoch [75/300], Step [24500/27733], Loss: 2.3156\n",
      "Epoch [75/300], Step [24600/27733], Loss: 2.7890\n",
      "Epoch [75/300], Step [24700/27733], Loss: 2.2625\n",
      "Epoch [75/300], Step [24800/27733], Loss: 2.7460\n",
      "Epoch [75/300], Step [24900/27733], Loss: 3.6656\n",
      "Epoch [75/300], Step [25000/27733], Loss: 2.8749\n",
      "Epoch [75/300], Step [25100/27733], Loss: 2.7436\n",
      "Epoch [75/300], Step [25200/27733], Loss: 3.0117\n",
      "Epoch [75/300], Step [25300/27733], Loss: 3.1539\n",
      "Epoch [75/300], Step [25400/27733], Loss: 3.2966\n",
      "Epoch [75/300], Step [25500/27733], Loss: 3.5894\n",
      "Epoch [75/300], Step [25600/27733], Loss: 2.7093\n",
      "Epoch [75/300], Step [25700/27733], Loss: 2.8118\n",
      "Epoch [75/300], Step [25800/27733], Loss: 3.5033\n",
      "Epoch [75/300], Step [25900/27733], Loss: 2.2684\n",
      "Epoch [75/300], Step [26000/27733], Loss: 2.4908\n",
      "Epoch [75/300], Step [26100/27733], Loss: 2.7380\n",
      "Epoch [75/300], Step [26200/27733], Loss: 3.7118\n",
      "Epoch [75/300], Step [26300/27733], Loss: 3.1830\n",
      "Epoch [75/300], Step [26400/27733], Loss: 2.6737\n",
      "Epoch [75/300], Step [26500/27733], Loss: 2.9606\n",
      "Epoch [75/300], Step [26600/27733], Loss: 2.9029\n",
      "Epoch [75/300], Step [26700/27733], Loss: 2.6303\n",
      "Epoch [75/300], Step [26800/27733], Loss: 2.5449\n",
      "Epoch [75/300], Step [26900/27733], Loss: 3.6530\n",
      "Epoch [75/300], Step [27000/27733], Loss: 2.8156\n",
      "Epoch [75/300], Step [27100/27733], Loss: 2.9348\n",
      "Epoch [75/300], Step [27200/27733], Loss: 2.6479\n",
      "Epoch [75/300], Step [27300/27733], Loss: 3.1776\n",
      "Epoch [75/300], Step [27400/27733], Loss: 2.6583\n",
      "Epoch [75/300], Step [27500/27733], Loss: 3.2053\n",
      "Epoch [75/300], Step [27600/27733], Loss: 3.5289\n",
      "Epoch [75/300], Step [27700/27733], Loss: 2.4482\n",
      "Epoch [76/300], Step [100/27733], Loss: 2.6057\n",
      "Epoch [76/300], Step [200/27733], Loss: 1.9020\n",
      "Epoch [76/300], Step [300/27733], Loss: 3.6724\n",
      "Epoch [76/300], Step [400/27733], Loss: 2.0859\n",
      "Epoch [76/300], Step [500/27733], Loss: 2.0285\n",
      "Epoch [76/300], Step [600/27733], Loss: 2.5897\n",
      "Epoch [76/300], Step [700/27733], Loss: 2.3223\n",
      "Epoch [76/300], Step [800/27733], Loss: 2.3950\n",
      "Epoch [76/300], Step [900/27733], Loss: 2.2884\n",
      "Epoch [76/300], Step [1000/27733], Loss: 2.6936\n",
      "Epoch [76/300], Step [1100/27733], Loss: 2.3182\n",
      "Epoch [76/300], Step [1200/27733], Loss: 2.1398\n",
      "Epoch [76/300], Step [1300/27733], Loss: 2.6932\n",
      "Epoch [76/300], Step [1400/27733], Loss: 1.6230\n",
      "Epoch [76/300], Step [1500/27733], Loss: 2.6193\n",
      "Epoch [76/300], Step [1600/27733], Loss: 2.0709\n",
      "Epoch [76/300], Step [1700/27733], Loss: 3.2546\n",
      "Epoch [76/300], Step [1800/27733], Loss: 2.1421\n",
      "Epoch [76/300], Step [1900/27733], Loss: 2.5675\n",
      "Epoch [76/300], Step [2000/27733], Loss: 3.7571\n",
      "Epoch [76/300], Step [2100/27733], Loss: 2.2644\n",
      "Epoch [76/300], Step [2200/27733], Loss: 2.3837\n",
      "Epoch [76/300], Step [2300/27733], Loss: 2.6577\n",
      "Epoch [76/300], Step [2400/27733], Loss: 2.1209\n",
      "Epoch [76/300], Step [2500/27733], Loss: 2.8527\n",
      "Epoch [76/300], Step [2600/27733], Loss: 2.4340\n",
      "Epoch [76/300], Step [2700/27733], Loss: 1.6679\n",
      "Epoch [76/300], Step [2800/27733], Loss: 1.8643\n",
      "Epoch [76/300], Step [2900/27733], Loss: 2.5465\n",
      "Epoch [76/300], Step [3000/27733], Loss: 2.7143\n",
      "Epoch [76/300], Step [3100/27733], Loss: 2.7246\n",
      "Epoch [76/300], Step [3200/27733], Loss: 2.3298\n",
      "Epoch [76/300], Step [3300/27733], Loss: 2.4053\n",
      "Epoch [76/300], Step [3400/27733], Loss: 3.5526\n",
      "Epoch [76/300], Step [3500/27733], Loss: 2.6742\n",
      "Epoch [76/300], Step [3600/27733], Loss: 3.0534\n",
      "Epoch [76/300], Step [3700/27733], Loss: 2.4871\n",
      "Epoch [76/300], Step [3800/27733], Loss: 2.4052\n",
      "Epoch [76/300], Step [3900/27733], Loss: 2.4816\n",
      "Epoch [76/300], Step [4000/27733], Loss: 3.0060\n",
      "Epoch [76/300], Step [4100/27733], Loss: 2.5315\n",
      "Epoch [76/300], Step [4200/27733], Loss: 2.4606\n",
      "Epoch [76/300], Step [4300/27733], Loss: 1.9962\n",
      "Epoch [76/300], Step [4400/27733], Loss: 2.9704\n",
      "Epoch [76/300], Step [4500/27733], Loss: 2.6187\n",
      "Epoch [76/300], Step [4600/27733], Loss: 2.7881\n",
      "Epoch [76/300], Step [4700/27733], Loss: 2.0518\n",
      "Epoch [76/300], Step [4800/27733], Loss: 2.4472\n",
      "Epoch [76/300], Step [4900/27733], Loss: 2.5232\n",
      "Epoch [76/300], Step [5000/27733], Loss: 2.3743\n",
      "Epoch [76/300], Step [5100/27733], Loss: 3.0692\n",
      "Epoch [76/300], Step [5200/27733], Loss: 2.8797\n",
      "Epoch [76/300], Step [5300/27733], Loss: 3.0671\n",
      "Epoch [76/300], Step [5400/27733], Loss: 1.8972\n",
      "Epoch [76/300], Step [5500/27733], Loss: 3.3645\n",
      "Epoch [76/300], Step [5600/27733], Loss: 2.4113\n",
      "Epoch [76/300], Step [5700/27733], Loss: 2.8781\n",
      "Epoch [76/300], Step [5800/27733], Loss: 2.0604\n",
      "Epoch [76/300], Step [5900/27733], Loss: 2.6454\n",
      "Epoch [76/300], Step [6000/27733], Loss: 3.1592\n",
      "Epoch [76/300], Step [6100/27733], Loss: 2.1168\n",
      "Epoch [76/300], Step [6200/27733], Loss: 2.6911\n",
      "Epoch [76/300], Step [6300/27733], Loss: 1.8497\n",
      "Epoch [76/300], Step [6400/27733], Loss: 1.6664\n",
      "Epoch [76/300], Step [6500/27733], Loss: 2.6572\n",
      "Epoch [76/300], Step [6600/27733], Loss: 2.7147\n",
      "Epoch [76/300], Step [6700/27733], Loss: 2.8702\n",
      "Epoch [76/300], Step [6800/27733], Loss: 2.4514\n",
      "Epoch [76/300], Step [6900/27733], Loss: 2.2555\n",
      "Epoch [76/300], Step [7000/27733], Loss: 2.2954\n",
      "Epoch [76/300], Step [7100/27733], Loss: 2.9819\n",
      "Epoch [76/300], Step [7200/27733], Loss: 2.2304\n",
      "Epoch [76/300], Step [7300/27733], Loss: 2.8188\n",
      "Epoch [76/300], Step [7400/27733], Loss: 2.3379\n",
      "Epoch [76/300], Step [7500/27733], Loss: 2.3915\n",
      "Epoch [76/300], Step [7600/27733], Loss: 2.2792\n",
      "Epoch [76/300], Step [7700/27733], Loss: 2.9480\n",
      "Epoch [76/300], Step [7800/27733], Loss: 2.3157\n",
      "Epoch [76/300], Step [7900/27733], Loss: 2.0552\n",
      "Epoch [76/300], Step [8000/27733], Loss: 1.6810\n",
      "Epoch [76/300], Step [8100/27733], Loss: 3.1934\n",
      "Epoch [76/300], Step [8200/27733], Loss: 2.9371\n",
      "Epoch [76/300], Step [8300/27733], Loss: 3.0071\n",
      "Epoch [76/300], Step [8400/27733], Loss: 3.0809\n",
      "Epoch [76/300], Step [8500/27733], Loss: 3.2231\n",
      "Epoch [76/300], Step [8600/27733], Loss: 2.3542\n",
      "Epoch [76/300], Step [8700/27733], Loss: 3.0938\n",
      "Epoch [76/300], Step [8800/27733], Loss: 3.3211\n",
      "Epoch [76/300], Step [8900/27733], Loss: 2.5626\n",
      "Epoch [76/300], Step [9000/27733], Loss: 2.3563\n",
      "Epoch [76/300], Step [9100/27733], Loss: 2.9380\n",
      "Epoch [76/300], Step [9200/27733], Loss: 3.3172\n",
      "Epoch [76/300], Step [9300/27733], Loss: 2.5221\n",
      "Epoch [76/300], Step [9400/27733], Loss: 2.5274\n",
      "Epoch [76/300], Step [9500/27733], Loss: 2.7370\n",
      "Epoch [76/300], Step [9600/27733], Loss: 2.4586\n",
      "Epoch [76/300], Step [9700/27733], Loss: 2.5982\n",
      "Epoch [76/300], Step [9800/27733], Loss: 2.5146\n",
      "Epoch [76/300], Step [9900/27733], Loss: 2.7753\n",
      "Epoch [76/300], Step [10000/27733], Loss: 2.6228\n",
      "Epoch [76/300], Step [10100/27733], Loss: 3.0684\n",
      "Epoch [76/300], Step [10200/27733], Loss: 2.6462\n",
      "Epoch [76/300], Step [10300/27733], Loss: 2.9271\n",
      "Epoch [76/300], Step [10400/27733], Loss: 2.4839\n",
      "Epoch [76/300], Step [10500/27733], Loss: 3.0424\n",
      "Epoch [76/300], Step [10600/27733], Loss: 3.4171\n",
      "Epoch [76/300], Step [10700/27733], Loss: 1.9709\n",
      "Epoch [76/300], Step [10800/27733], Loss: 2.3461\n",
      "Epoch [76/300], Step [10900/27733], Loss: 2.7893\n",
      "Epoch [76/300], Step [11000/27733], Loss: 2.1618\n",
      "Epoch [76/300], Step [11100/27733], Loss: 2.5956\n",
      "Epoch [76/300], Step [11200/27733], Loss: 3.9225\n",
      "Epoch [76/300], Step [11300/27733], Loss: 2.6748\n",
      "Epoch [76/300], Step [11400/27733], Loss: 2.9709\n",
      "Epoch [76/300], Step [11500/27733], Loss: 2.6110\n",
      "Epoch [76/300], Step [11600/27733], Loss: 2.4077\n",
      "Epoch [76/300], Step [11700/27733], Loss: 2.3919\n",
      "Epoch [76/300], Step [11800/27733], Loss: 3.2493\n",
      "Epoch [76/300], Step [11900/27733], Loss: 2.7806\n",
      "Epoch [76/300], Step [12000/27733], Loss: 3.3653\n",
      "Epoch [76/300], Step [12100/27733], Loss: 2.9686\n",
      "Epoch [76/300], Step [12200/27733], Loss: 2.5903\n",
      "Epoch [76/300], Step [12300/27733], Loss: 2.6944\n",
      "Epoch [76/300], Step [12400/27733], Loss: 2.7516\n",
      "Epoch [76/300], Step [12500/27733], Loss: 2.4524\n",
      "Epoch [76/300], Step [12600/27733], Loss: 2.4151\n",
      "Epoch [76/300], Step [12700/27733], Loss: 1.9030\n",
      "Epoch [76/300], Step [12800/27733], Loss: 2.4017\n",
      "Epoch [76/300], Step [12900/27733], Loss: 3.4160\n",
      "Epoch [76/300], Step [13000/27733], Loss: 2.7856\n",
      "Epoch [76/300], Step [13100/27733], Loss: 3.1335\n",
      "Epoch [76/300], Step [13200/27733], Loss: 2.1522\n",
      "Epoch [76/300], Step [13300/27733], Loss: 3.3486\n",
      "Epoch [76/300], Step [13400/27733], Loss: 2.0663\n",
      "Epoch [76/300], Step [13500/27733], Loss: 3.2973\n",
      "Epoch [76/300], Step [13600/27733], Loss: 3.7386\n",
      "Epoch [76/300], Step [13700/27733], Loss: 2.7430\n",
      "Epoch [76/300], Step [13800/27733], Loss: 3.4024\n",
      "Epoch [76/300], Step [13900/27733], Loss: 2.3692\n",
      "Epoch [76/300], Step [14000/27733], Loss: 3.1229\n",
      "Epoch [76/300], Step [14100/27733], Loss: 1.9397\n",
      "Epoch [76/300], Step [14200/27733], Loss: 3.5183\n",
      "Epoch [76/300], Step [14300/27733], Loss: 1.9394\n",
      "Epoch [76/300], Step [14400/27733], Loss: 2.6130\n",
      "Epoch [76/300], Step [14500/27733], Loss: 2.5103\n",
      "Epoch [76/300], Step [14600/27733], Loss: 2.5171\n",
      "Epoch [76/300], Step [14700/27733], Loss: 2.9145\n",
      "Epoch [76/300], Step [14800/27733], Loss: 3.9720\n",
      "Epoch [76/300], Step [14900/27733], Loss: 2.5918\n",
      "Epoch [76/300], Step [15000/27733], Loss: 3.1394\n",
      "Epoch [76/300], Step [15100/27733], Loss: 2.8927\n",
      "Epoch [76/300], Step [15200/27733], Loss: 2.8788\n",
      "Epoch [76/300], Step [15300/27733], Loss: 1.8100\n",
      "Epoch [76/300], Step [15400/27733], Loss: 2.2533\n",
      "Epoch [76/300], Step [15500/27733], Loss: 3.3001\n",
      "Epoch [76/300], Step [15600/27733], Loss: 2.1675\n",
      "Epoch [76/300], Step [15700/27733], Loss: 2.4759\n",
      "Epoch [76/300], Step [15800/27733], Loss: 2.3574\n",
      "Epoch [76/300], Step [15900/27733], Loss: 2.9075\n",
      "Epoch [76/300], Step [16000/27733], Loss: 2.0255\n",
      "Epoch [76/300], Step [16100/27733], Loss: 2.4748\n",
      "Epoch [76/300], Step [16200/27733], Loss: 2.6484\n",
      "Epoch [76/300], Step [16300/27733], Loss: 2.2312\n",
      "Epoch [76/300], Step [16400/27733], Loss: 2.7457\n",
      "Epoch [76/300], Step [16500/27733], Loss: 2.7323\n",
      "Epoch [76/300], Step [16600/27733], Loss: 3.2280\n",
      "Epoch [76/300], Step [16700/27733], Loss: 2.5222\n",
      "Epoch [76/300], Step [16800/27733], Loss: 3.1153\n",
      "Epoch [76/300], Step [16900/27733], Loss: 2.3391\n",
      "Epoch [76/300], Step [17000/27733], Loss: 2.3676\n",
      "Epoch [76/300], Step [17100/27733], Loss: 3.7346\n",
      "Epoch [76/300], Step [17200/27733], Loss: 2.9122\n",
      "Epoch [76/300], Step [17300/27733], Loss: 2.5407\n",
      "Epoch [76/300], Step [17400/27733], Loss: 3.4680\n",
      "Epoch [76/300], Step [17500/27733], Loss: 2.6893\n",
      "Epoch [76/300], Step [17600/27733], Loss: 2.0363\n",
      "Epoch [76/300], Step [17700/27733], Loss: 2.7811\n",
      "Epoch [76/300], Step [17800/27733], Loss: 3.2547\n",
      "Epoch [76/300], Step [17900/27733], Loss: 2.4503\n",
      "Epoch [76/300], Step [18000/27733], Loss: 2.5761\n",
      "Epoch [76/300], Step [18100/27733], Loss: 3.2851\n",
      "Epoch [76/300], Step [18200/27733], Loss: 2.5304\n",
      "Epoch [76/300], Step [18300/27733], Loss: 3.1935\n",
      "Epoch [76/300], Step [18400/27733], Loss: 2.3951\n",
      "Epoch [76/300], Step [18500/27733], Loss: 2.4846\n",
      "Epoch [76/300], Step [18600/27733], Loss: 3.3736\n",
      "Epoch [76/300], Step [18700/27733], Loss: 2.9519\n",
      "Epoch [76/300], Step [18800/27733], Loss: 2.8336\n",
      "Epoch [76/300], Step [18900/27733], Loss: 2.5691\n",
      "Epoch [76/300], Step [19000/27733], Loss: 2.4364\n",
      "Epoch [76/300], Step [19100/27733], Loss: 2.4971\n",
      "Epoch [76/300], Step [19200/27733], Loss: 2.3395\n",
      "Epoch [76/300], Step [19300/27733], Loss: 3.1469\n",
      "Epoch [76/300], Step [19400/27733], Loss: 3.4461\n",
      "Epoch [76/300], Step [19500/27733], Loss: 1.9153\n",
      "Epoch [76/300], Step [19600/27733], Loss: 2.9785\n",
      "Epoch [76/300], Step [19700/27733], Loss: 3.2496\n",
      "Epoch [76/300], Step [19800/27733], Loss: 3.5829\n",
      "Epoch [76/300], Step [19900/27733], Loss: 3.4953\n",
      "Epoch [76/300], Step [20000/27733], Loss: 3.1458\n",
      "Epoch [76/300], Step [20100/27733], Loss: 3.5436\n",
      "Epoch [76/300], Step [20200/27733], Loss: 3.0815\n",
      "Epoch [76/300], Step [20300/27733], Loss: 3.6531\n",
      "Epoch [76/300], Step [20400/27733], Loss: 2.7022\n",
      "Epoch [76/300], Step [20500/27733], Loss: 2.0612\n",
      "Epoch [76/300], Step [20600/27733], Loss: 3.1163\n",
      "Epoch [76/300], Step [20700/27733], Loss: 3.7594\n",
      "Epoch [76/300], Step [20800/27733], Loss: 3.1317\n",
      "Epoch [76/300], Step [20900/27733], Loss: 3.0841\n",
      "Epoch [76/300], Step [21000/27733], Loss: 3.3987\n",
      "Epoch [76/300], Step [21100/27733], Loss: 3.5230\n",
      "Epoch [76/300], Step [21200/27733], Loss: 3.1842\n",
      "Epoch [76/300], Step [21300/27733], Loss: 3.1773\n",
      "Epoch [76/300], Step [21400/27733], Loss: 2.9822\n",
      "Epoch [76/300], Step [21500/27733], Loss: 2.8573\n",
      "Epoch [76/300], Step [21600/27733], Loss: 3.0320\n",
      "Epoch [76/300], Step [21700/27733], Loss: 3.4201\n",
      "Epoch [76/300], Step [21800/27733], Loss: 2.8310\n",
      "Epoch [76/300], Step [21900/27733], Loss: 3.2454\n",
      "Epoch [76/300], Step [22000/27733], Loss: 2.2970\n",
      "Epoch [76/300], Step [22100/27733], Loss: 2.6424\n",
      "Epoch [76/300], Step [22200/27733], Loss: 3.0422\n",
      "Epoch [76/300], Step [22300/27733], Loss: 4.0578\n",
      "Epoch [76/300], Step [22400/27733], Loss: 3.2621\n",
      "Epoch [76/300], Step [22500/27733], Loss: 2.7808\n",
      "Epoch [76/300], Step [22600/27733], Loss: 2.7015\n",
      "Epoch [76/300], Step [22700/27733], Loss: 2.5874\n",
      "Epoch [76/300], Step [22800/27733], Loss: 2.9247\n",
      "Epoch [76/300], Step [22900/27733], Loss: 3.0832\n",
      "Epoch [76/300], Step [23000/27733], Loss: 3.2374\n",
      "Epoch [76/300], Step [23100/27733], Loss: 3.1378\n",
      "Epoch [76/300], Step [23200/27733], Loss: 3.0434\n",
      "Epoch [76/300], Step [23300/27733], Loss: 3.5217\n",
      "Epoch [76/300], Step [23400/27733], Loss: 2.8158\n",
      "Epoch [76/300], Step [23500/27733], Loss: 2.4790\n",
      "Epoch [76/300], Step [23600/27733], Loss: 3.4525\n",
      "Epoch [76/300], Step [23700/27733], Loss: 2.9265\n",
      "Epoch [76/300], Step [23800/27733], Loss: 3.1669\n",
      "Epoch [76/300], Step [23900/27733], Loss: 2.8039\n",
      "Epoch [76/300], Step [24000/27733], Loss: 3.7119\n",
      "Epoch [76/300], Step [24100/27733], Loss: 3.0530\n",
      "Epoch [76/300], Step [24200/27733], Loss: 3.4015\n",
      "Epoch [76/300], Step [24300/27733], Loss: 2.6381\n",
      "Epoch [76/300], Step [24400/27733], Loss: 3.8256\n",
      "Epoch [76/300], Step [24500/27733], Loss: 2.6355\n",
      "Epoch [76/300], Step [24600/27733], Loss: 2.5958\n",
      "Epoch [76/300], Step [24700/27733], Loss: 2.2022\n",
      "Epoch [76/300], Step [24800/27733], Loss: 3.1871\n",
      "Epoch [76/300], Step [24900/27733], Loss: 2.5784\n",
      "Epoch [76/300], Step [25000/27733], Loss: 2.7036\n",
      "Epoch [76/300], Step [25100/27733], Loss: 3.0541\n",
      "Epoch [76/300], Step [25200/27733], Loss: 2.7777\n",
      "Epoch [76/300], Step [25300/27733], Loss: 2.9136\n",
      "Epoch [76/300], Step [25400/27733], Loss: 2.5998\n",
      "Epoch [76/300], Step [25500/27733], Loss: 3.2376\n",
      "Epoch [76/300], Step [25600/27733], Loss: 2.6190\n",
      "Epoch [76/300], Step [25700/27733], Loss: 2.6234\n",
      "Epoch [76/300], Step [25800/27733], Loss: 2.6670\n",
      "Epoch [76/300], Step [25900/27733], Loss: 3.6296\n",
      "Epoch [76/300], Step [26000/27733], Loss: 2.6179\n",
      "Epoch [76/300], Step [26100/27733], Loss: 2.6004\n",
      "Epoch [76/300], Step [26200/27733], Loss: 3.9535\n",
      "Epoch [76/300], Step [26300/27733], Loss: 2.5652\n",
      "Epoch [76/300], Step [26400/27733], Loss: 3.2502\n",
      "Epoch [76/300], Step [26500/27733], Loss: 2.3483\n",
      "Epoch [76/300], Step [26600/27733], Loss: 2.6736\n",
      "Epoch [76/300], Step [26700/27733], Loss: 3.3721\n",
      "Epoch [76/300], Step [26800/27733], Loss: 2.8074\n",
      "Epoch [76/300], Step [26900/27733], Loss: 2.3934\n",
      "Epoch [76/300], Step [27000/27733], Loss: 3.9700\n",
      "Epoch [76/300], Step [27100/27733], Loss: 2.1032\n",
      "Epoch [76/300], Step [27200/27733], Loss: 2.9023\n",
      "Epoch [76/300], Step [27300/27733], Loss: 2.4377\n",
      "Epoch [76/300], Step [27400/27733], Loss: 3.1408\n",
      "Epoch [76/300], Step [27500/27733], Loss: 2.7379\n",
      "Epoch [76/300], Step [27600/27733], Loss: 4.3572\n",
      "Epoch [76/300], Step [27700/27733], Loss: 2.5857\n",
      "Epoch [77/300], Step [100/27733], Loss: 2.0948\n",
      "Epoch [77/300], Step [200/27733], Loss: 2.1692\n",
      "Epoch [77/300], Step [300/27733], Loss: 2.4760\n",
      "Epoch [77/300], Step [400/27733], Loss: 2.4442\n",
      "Epoch [77/300], Step [500/27733], Loss: 2.0555\n",
      "Epoch [77/300], Step [600/27733], Loss: 2.0711\n",
      "Epoch [77/300], Step [700/27733], Loss: 1.8139\n",
      "Epoch [77/300], Step [800/27733], Loss: 2.1076\n",
      "Epoch [77/300], Step [900/27733], Loss: 2.8677\n",
      "Epoch [77/300], Step [1000/27733], Loss: 1.8778\n",
      "Epoch [77/300], Step [1100/27733], Loss: 2.8623\n",
      "Epoch [77/300], Step [1200/27733], Loss: 3.4328\n",
      "Epoch [77/300], Step [1300/27733], Loss: 1.8585\n",
      "Epoch [77/300], Step [1400/27733], Loss: 2.6396\n",
      "Epoch [77/300], Step [1500/27733], Loss: 2.1840\n",
      "Epoch [77/300], Step [1600/27733], Loss: 2.3617\n",
      "Epoch [77/300], Step [1700/27733], Loss: 2.7540\n",
      "Epoch [77/300], Step [1800/27733], Loss: 1.6351\n",
      "Epoch [77/300], Step [1900/27733], Loss: 1.8718\n",
      "Epoch [77/300], Step [2000/27733], Loss: 1.8459\n",
      "Epoch [77/300], Step [2100/27733], Loss: 2.1569\n",
      "Epoch [77/300], Step [2200/27733], Loss: 2.2466\n",
      "Epoch [77/300], Step [2300/27733], Loss: 2.1687\n",
      "Epoch [77/300], Step [2400/27733], Loss: 2.2605\n",
      "Epoch [77/300], Step [2500/27733], Loss: 2.7305\n",
      "Epoch [77/300], Step [2600/27733], Loss: 2.3265\n",
      "Epoch [77/300], Step [2700/27733], Loss: 2.7117\n",
      "Epoch [77/300], Step [2800/27733], Loss: 2.2702\n",
      "Epoch [77/300], Step [2900/27733], Loss: 2.7674\n",
      "Epoch [77/300], Step [3000/27733], Loss: 2.4768\n",
      "Epoch [77/300], Step [3100/27733], Loss: 2.9172\n",
      "Epoch [77/300], Step [3200/27733], Loss: 3.0500\n",
      "Epoch [77/300], Step [3300/27733], Loss: 2.1934\n",
      "Epoch [77/300], Step [3400/27733], Loss: 2.8247\n",
      "Epoch [77/300], Step [3500/27733], Loss: 3.5139\n",
      "Epoch [77/300], Step [3600/27733], Loss: 2.6927\n",
      "Epoch [77/300], Step [3700/27733], Loss: 2.8075\n",
      "Epoch [77/300], Step [3800/27733], Loss: 2.7577\n",
      "Epoch [77/300], Step [3900/27733], Loss: 2.0776\n",
      "Epoch [77/300], Step [4000/27733], Loss: 2.0552\n",
      "Epoch [77/300], Step [4100/27733], Loss: 3.3276\n",
      "Epoch [77/300], Step [4200/27733], Loss: 2.5690\n",
      "Epoch [77/300], Step [4300/27733], Loss: 2.4241\n",
      "Epoch [77/300], Step [4400/27733], Loss: 2.7828\n",
      "Epoch [77/300], Step [4500/27733], Loss: 2.5044\n",
      "Epoch [77/300], Step [4600/27733], Loss: 2.8505\n",
      "Epoch [77/300], Step [4700/27733], Loss: 2.5572\n",
      "Epoch [77/300], Step [4800/27733], Loss: 2.9468\n",
      "Epoch [77/300], Step [4900/27733], Loss: 2.1647\n",
      "Epoch [77/300], Step [5000/27733], Loss: 2.2093\n",
      "Epoch [77/300], Step [5100/27733], Loss: 2.0610\n",
      "Epoch [77/300], Step [5200/27733], Loss: 2.3867\n",
      "Epoch [77/300], Step [5300/27733], Loss: 1.9718\n",
      "Epoch [77/300], Step [5400/27733], Loss: 2.2285\n",
      "Epoch [77/300], Step [5500/27733], Loss: 1.9931\n",
      "Epoch [77/300], Step [5600/27733], Loss: 2.2274\n",
      "Epoch [77/300], Step [5700/27733], Loss: 2.5193\n",
      "Epoch [77/300], Step [5800/27733], Loss: 2.5142\n",
      "Epoch [77/300], Step [5900/27733], Loss: 3.1744\n",
      "Epoch [77/300], Step [6000/27733], Loss: 2.8677\n",
      "Epoch [77/300], Step [6100/27733], Loss: 2.3741\n",
      "Epoch [77/300], Step [6200/27733], Loss: 2.1736\n",
      "Epoch [77/300], Step [6300/27733], Loss: 2.5047\n",
      "Epoch [77/300], Step [6400/27733], Loss: 2.1059\n",
      "Epoch [77/300], Step [6500/27733], Loss: 2.0349\n",
      "Epoch [77/300], Step [6600/27733], Loss: 1.6805\n",
      "Epoch [77/300], Step [6700/27733], Loss: 2.9796\n",
      "Epoch [77/300], Step [6800/27733], Loss: 3.0024\n",
      "Epoch [77/300], Step [6900/27733], Loss: 2.4058\n",
      "Epoch [77/300], Step [7000/27733], Loss: 2.3925\n",
      "Epoch [77/300], Step [7100/27733], Loss: 2.1500\n",
      "Epoch [77/300], Step [7200/27733], Loss: 2.2616\n",
      "Epoch [77/300], Step [7300/27733], Loss: 2.8543\n",
      "Epoch [77/300], Step [7400/27733], Loss: 2.3522\n",
      "Epoch [77/300], Step [7500/27733], Loss: 2.4484\n",
      "Epoch [77/300], Step [7600/27733], Loss: 1.8993\n",
      "Epoch [77/300], Step [7700/27733], Loss: 2.7668\n",
      "Epoch [77/300], Step [7800/27733], Loss: 2.5257\n",
      "Epoch [77/300], Step [7900/27733], Loss: 2.7194\n",
      "Epoch [77/300], Step [8000/27733], Loss: 2.6073\n",
      "Epoch [77/300], Step [8100/27733], Loss: 2.7446\n",
      "Epoch [77/300], Step [8200/27733], Loss: 2.5540\n",
      "Epoch [77/300], Step [8300/27733], Loss: 3.2394\n",
      "Epoch [77/300], Step [8400/27733], Loss: 2.4782\n",
      "Epoch [77/300], Step [8500/27733], Loss: 2.3766\n",
      "Epoch [77/300], Step [8600/27733], Loss: 3.1992\n",
      "Epoch [77/300], Step [8700/27733], Loss: 2.3152\n",
      "Epoch [77/300], Step [8800/27733], Loss: 2.8468\n",
      "Epoch [77/300], Step [8900/27733], Loss: 2.0170\n",
      "Epoch [77/300], Step [9000/27733], Loss: 2.9074\n",
      "Epoch [77/300], Step [9100/27733], Loss: 3.3065\n",
      "Epoch [77/300], Step [9200/27733], Loss: 2.7627\n",
      "Epoch [77/300], Step [9300/27733], Loss: 2.7292\n",
      "Epoch [77/300], Step [9400/27733], Loss: 1.5995\n",
      "Epoch [77/300], Step [9500/27733], Loss: 2.2698\n",
      "Epoch [77/300], Step [9600/27733], Loss: 3.0909\n",
      "Epoch [77/300], Step [9700/27733], Loss: 2.4088\n",
      "Epoch [77/300], Step [9800/27733], Loss: 3.1621\n",
      "Epoch [77/300], Step [9900/27733], Loss: 2.4976\n",
      "Epoch [77/300], Step [10000/27733], Loss: 2.8229\n",
      "Epoch [77/300], Step [10100/27733], Loss: 2.8588\n",
      "Epoch [77/300], Step [10200/27733], Loss: 2.3972\n",
      "Epoch [77/300], Step [10300/27733], Loss: 3.0719\n",
      "Epoch [77/300], Step [10400/27733], Loss: 3.1787\n",
      "Epoch [77/300], Step [10500/27733], Loss: 2.4940\n",
      "Epoch [77/300], Step [10600/27733], Loss: 2.1210\n",
      "Epoch [77/300], Step [10700/27733], Loss: 2.9287\n",
      "Epoch [77/300], Step [10800/27733], Loss: 2.6208\n",
      "Epoch [77/300], Step [10900/27733], Loss: 2.6614\n",
      "Epoch [77/300], Step [11000/27733], Loss: 2.1588\n",
      "Epoch [77/300], Step [11100/27733], Loss: 2.8245\n",
      "Epoch [77/300], Step [11200/27733], Loss: 2.7263\n",
      "Epoch [77/300], Step [11300/27733], Loss: 2.5857\n",
      "Epoch [77/300], Step [11400/27733], Loss: 2.1094\n",
      "Epoch [77/300], Step [11500/27733], Loss: 2.0509\n",
      "Epoch [77/300], Step [11600/27733], Loss: 2.1219\n",
      "Epoch [77/300], Step [11700/27733], Loss: 1.9237\n",
      "Epoch [77/300], Step [11800/27733], Loss: 3.1838\n",
      "Epoch [77/300], Step [11900/27733], Loss: 2.5514\n",
      "Epoch [77/300], Step [12000/27733], Loss: 2.6003\n",
      "Epoch [77/300], Step [12100/27733], Loss: 2.6446\n",
      "Epoch [77/300], Step [12200/27733], Loss: 2.6459\n",
      "Epoch [77/300], Step [12300/27733], Loss: 2.8016\n",
      "Epoch [77/300], Step [12400/27733], Loss: 2.3650\n",
      "Epoch [77/300], Step [12500/27733], Loss: 3.1524\n",
      "Epoch [77/300], Step [12600/27733], Loss: 2.8704\n",
      "Epoch [77/300], Step [12700/27733], Loss: 2.5831\n",
      "Epoch [77/300], Step [12800/27733], Loss: 2.6982\n",
      "Epoch [77/300], Step [12900/27733], Loss: 2.1598\n",
      "Epoch [77/300], Step [13000/27733], Loss: 2.4524\n",
      "Epoch [77/300], Step [13100/27733], Loss: 3.6665\n",
      "Epoch [77/300], Step [13200/27733], Loss: 2.9453\n",
      "Epoch [77/300], Step [13300/27733], Loss: 2.5965\n",
      "Epoch [77/300], Step [13400/27733], Loss: 2.6294\n",
      "Epoch [77/300], Step [13500/27733], Loss: 2.6616\n",
      "Epoch [77/300], Step [13600/27733], Loss: 2.7134\n",
      "Epoch [77/300], Step [13700/27733], Loss: 2.4367\n",
      "Epoch [77/300], Step [13800/27733], Loss: 2.8810\n",
      "Epoch [77/300], Step [13900/27733], Loss: 3.3292\n",
      "Epoch [77/300], Step [14000/27733], Loss: 2.0378\n",
      "Epoch [77/300], Step [14100/27733], Loss: 1.8922\n",
      "Epoch [77/300], Step [14200/27733], Loss: 3.2952\n",
      "Epoch [77/300], Step [14300/27733], Loss: 2.9899\n",
      "Epoch [77/300], Step [14400/27733], Loss: 3.1962\n",
      "Epoch [77/300], Step [14500/27733], Loss: 2.1280\n",
      "Epoch [77/300], Step [14600/27733], Loss: 3.2474\n",
      "Epoch [77/300], Step [14700/27733], Loss: 1.9951\n",
      "Epoch [77/300], Step [14800/27733], Loss: 3.0839\n",
      "Epoch [77/300], Step [14900/27733], Loss: 3.3005\n",
      "Epoch [77/300], Step [15000/27733], Loss: 3.3855\n",
      "Epoch [77/300], Step [15100/27733], Loss: 3.4979\n",
      "Epoch [77/300], Step [15200/27733], Loss: 2.5262\n",
      "Epoch [77/300], Step [15300/27733], Loss: 2.7018\n",
      "Epoch [77/300], Step [15400/27733], Loss: 2.5862\n",
      "Epoch [77/300], Step [15500/27733], Loss: 2.4419\n",
      "Epoch [77/300], Step [15600/27733], Loss: 3.0965\n",
      "Epoch [77/300], Step [15700/27733], Loss: 3.3569\n",
      "Epoch [77/300], Step [15800/27733], Loss: 3.7542\n",
      "Epoch [77/300], Step [15900/27733], Loss: 2.4984\n",
      "Epoch [77/300], Step [16000/27733], Loss: 2.5058\n",
      "Epoch [77/300], Step [16100/27733], Loss: 3.3204\n",
      "Epoch [77/300], Step [16200/27733], Loss: 2.8181\n",
      "Epoch [77/300], Step [16300/27733], Loss: 3.0704\n",
      "Epoch [77/300], Step [16400/27733], Loss: 2.8975\n",
      "Epoch [77/300], Step [16500/27733], Loss: 2.9613\n",
      "Epoch [77/300], Step [16600/27733], Loss: 3.8910\n",
      "Epoch [77/300], Step [16700/27733], Loss: 2.7977\n",
      "Epoch [77/300], Step [16800/27733], Loss: 2.9920\n",
      "Epoch [77/300], Step [16900/27733], Loss: 2.8730\n",
      "Epoch [77/300], Step [17000/27733], Loss: 2.0057\n",
      "Epoch [77/300], Step [17100/27733], Loss: 2.6722\n",
      "Epoch [77/300], Step [17200/27733], Loss: 3.1511\n",
      "Epoch [77/300], Step [17300/27733], Loss: 2.4420\n",
      "Epoch [77/300], Step [17400/27733], Loss: 2.2854\n",
      "Epoch [77/300], Step [17500/27733], Loss: 3.4476\n",
      "Epoch [77/300], Step [17600/27733], Loss: 2.7542\n",
      "Epoch [77/300], Step [17700/27733], Loss: 2.0724\n",
      "Epoch [77/300], Step [17800/27733], Loss: 2.7464\n",
      "Epoch [77/300], Step [17900/27733], Loss: 2.1888\n",
      "Epoch [77/300], Step [18000/27733], Loss: 3.1710\n",
      "Epoch [77/300], Step [18100/27733], Loss: 2.7099\n",
      "Epoch [77/300], Step [18200/27733], Loss: 3.1680\n",
      "Epoch [77/300], Step [18300/27733], Loss: 3.3786\n",
      "Epoch [77/300], Step [18400/27733], Loss: 2.1361\n",
      "Epoch [77/300], Step [18500/27733], Loss: 2.7384\n",
      "Epoch [77/300], Step [18600/27733], Loss: 2.5116\n",
      "Epoch [77/300], Step [18700/27733], Loss: 2.7794\n",
      "Epoch [77/300], Step [18800/27733], Loss: 3.3751\n",
      "Epoch [77/300], Step [18900/27733], Loss: 2.6196\n",
      "Epoch [77/300], Step [19000/27733], Loss: 2.6631\n",
      "Epoch [77/300], Step [19100/27733], Loss: 3.0267\n",
      "Epoch [77/300], Step [19200/27733], Loss: 2.6449\n",
      "Epoch [77/300], Step [19300/27733], Loss: 2.6522\n",
      "Epoch [77/300], Step [19400/27733], Loss: 2.9716\n",
      "Epoch [77/300], Step [19500/27733], Loss: 2.7599\n",
      "Epoch [77/300], Step [19600/27733], Loss: 3.1860\n",
      "Epoch [77/300], Step [19700/27733], Loss: 2.3544\n",
      "Epoch [77/300], Step [19800/27733], Loss: 3.2520\n",
      "Epoch [77/300], Step [19900/27733], Loss: 3.0734\n",
      "Epoch [77/300], Step [20000/27733], Loss: 3.7453\n",
      "Epoch [77/300], Step [20100/27733], Loss: 3.5751\n",
      "Epoch [77/300], Step [20200/27733], Loss: 3.1244\n",
      "Epoch [77/300], Step [20300/27733], Loss: 2.8810\n",
      "Epoch [77/300], Step [20400/27733], Loss: 2.8655\n",
      "Epoch [77/300], Step [20500/27733], Loss: 2.4600\n",
      "Epoch [77/300], Step [20600/27733], Loss: 2.7399\n",
      "Epoch [77/300], Step [20700/27733], Loss: 2.7521\n",
      "Epoch [77/300], Step [20800/27733], Loss: 2.7850\n",
      "Epoch [77/300], Step [20900/27733], Loss: 2.6583\n",
      "Epoch [77/300], Step [21000/27733], Loss: 2.5814\n",
      "Epoch [77/300], Step [21100/27733], Loss: 2.5171\n",
      "Epoch [77/300], Step [21200/27733], Loss: 3.2622\n",
      "Epoch [77/300], Step [21300/27733], Loss: 3.3947\n",
      "Epoch [77/300], Step [21400/27733], Loss: 2.2635\n",
      "Epoch [77/300], Step [21500/27733], Loss: 2.1391\n",
      "Epoch [77/300], Step [21600/27733], Loss: 3.1293\n",
      "Epoch [77/300], Step [21700/27733], Loss: 2.4628\n",
      "Epoch [77/300], Step [21800/27733], Loss: 2.3570\n",
      "Epoch [77/300], Step [21900/27733], Loss: 2.6547\n",
      "Epoch [77/300], Step [22000/27733], Loss: 2.9933\n",
      "Epoch [77/300], Step [22100/27733], Loss: 2.8889\n",
      "Epoch [77/300], Step [22200/27733], Loss: 3.3872\n",
      "Epoch [77/300], Step [22300/27733], Loss: 2.5195\n",
      "Epoch [77/300], Step [22400/27733], Loss: 3.0217\n",
      "Epoch [77/300], Step [22500/27733], Loss: 3.5285\n",
      "Epoch [77/300], Step [22600/27733], Loss: 3.5737\n",
      "Epoch [77/300], Step [22700/27733], Loss: 2.7108\n",
      "Epoch [77/300], Step [22800/27733], Loss: 2.6410\n",
      "Epoch [77/300], Step [22900/27733], Loss: 2.5385\n",
      "Epoch [77/300], Step [23000/27733], Loss: 2.8629\n",
      "Epoch [77/300], Step [23100/27733], Loss: 2.5744\n",
      "Epoch [77/300], Step [23200/27733], Loss: 2.9504\n",
      "Epoch [77/300], Step [23300/27733], Loss: 2.4830\n",
      "Epoch [77/300], Step [23400/27733], Loss: 2.9092\n",
      "Epoch [77/300], Step [23500/27733], Loss: 2.9527\n",
      "Epoch [77/300], Step [23600/27733], Loss: 2.5758\n",
      "Epoch [77/300], Step [23700/27733], Loss: 2.8238\n",
      "Epoch [77/300], Step [23800/27733], Loss: 3.0269\n",
      "Epoch [77/300], Step [23900/27733], Loss: 2.1826\n",
      "Epoch [77/300], Step [24000/27733], Loss: 2.7526\n",
      "Epoch [77/300], Step [24100/27733], Loss: 2.7061\n",
      "Epoch [77/300], Step [24200/27733], Loss: 3.3633\n",
      "Epoch [77/300], Step [24300/27733], Loss: 2.3800\n",
      "Epoch [77/300], Step [24400/27733], Loss: 2.5461\n",
      "Epoch [77/300], Step [24500/27733], Loss: 2.6941\n",
      "Epoch [77/300], Step [24600/27733], Loss: 2.7712\n",
      "Epoch [77/300], Step [24700/27733], Loss: 3.1353\n",
      "Epoch [77/300], Step [24800/27733], Loss: 3.4122\n",
      "Epoch [77/300], Step [24900/27733], Loss: 2.5780\n",
      "Epoch [77/300], Step [25000/27733], Loss: 2.3920\n",
      "Epoch [77/300], Step [25100/27733], Loss: 3.6414\n",
      "Epoch [77/300], Step [25200/27733], Loss: 3.1038\n",
      "Epoch [77/300], Step [25300/27733], Loss: 2.6173\n",
      "Epoch [77/300], Step [25400/27733], Loss: 2.3289\n",
      "Epoch [77/300], Step [25500/27733], Loss: 3.1586\n",
      "Epoch [77/300], Step [25600/27733], Loss: 3.4870\n",
      "Epoch [77/300], Step [25700/27733], Loss: 2.7999\n",
      "Epoch [77/300], Step [25800/27733], Loss: 2.9342\n",
      "Epoch [77/300], Step [25900/27733], Loss: 2.5357\n",
      "Epoch [77/300], Step [26000/27733], Loss: 2.2005\n",
      "Epoch [77/300], Step [26100/27733], Loss: 2.0886\n",
      "Epoch [77/300], Step [26200/27733], Loss: 3.2716\n",
      "Epoch [77/300], Step [26300/27733], Loss: 3.3010\n",
      "Epoch [77/300], Step [26400/27733], Loss: 2.7569\n",
      "Epoch [77/300], Step [26500/27733], Loss: 3.4193\n",
      "Epoch [77/300], Step [26600/27733], Loss: 3.7151\n",
      "Epoch [77/300], Step [26700/27733], Loss: 2.9126\n",
      "Epoch [77/300], Step [26800/27733], Loss: 3.2403\n",
      "Epoch [77/300], Step [26900/27733], Loss: 1.9255\n",
      "Epoch [77/300], Step [27000/27733], Loss: 3.7842\n",
      "Epoch [77/300], Step [27100/27733], Loss: 3.4960\n",
      "Epoch [77/300], Step [27200/27733], Loss: 3.5027\n",
      "Epoch [77/300], Step [27300/27733], Loss: 2.7672\n",
      "Epoch [77/300], Step [27400/27733], Loss: 2.6590\n",
      "Epoch [77/300], Step [27500/27733], Loss: 2.3085\n",
      "Epoch [77/300], Step [27600/27733], Loss: 2.8037\n",
      "Epoch [77/300], Step [27700/27733], Loss: 4.1042\n",
      "Epoch [78/300], Step [100/27733], Loss: 2.2601\n",
      "Epoch [78/300], Step [200/27733], Loss: 2.6195\n",
      "Epoch [78/300], Step [300/27733], Loss: 2.0039\n",
      "Epoch [78/300], Step [400/27733], Loss: 2.9137\n",
      "Epoch [78/300], Step [500/27733], Loss: 2.8754\n",
      "Epoch [78/300], Step [600/27733], Loss: 1.7566\n",
      "Epoch [78/300], Step [700/27733], Loss: 2.1197\n",
      "Epoch [78/300], Step [800/27733], Loss: 2.1966\n",
      "Epoch [78/300], Step [900/27733], Loss: 2.5021\n",
      "Epoch [78/300], Step [1000/27733], Loss: 2.2102\n",
      "Epoch [78/300], Step [1100/27733], Loss: 1.7403\n",
      "Epoch [78/300], Step [1200/27733], Loss: 1.9913\n",
      "Epoch [78/300], Step [1300/27733], Loss: 1.7692\n",
      "Epoch [78/300], Step [1400/27733], Loss: 2.7773\n",
      "Epoch [78/300], Step [1500/27733], Loss: 2.8035\n",
      "Epoch [78/300], Step [1600/27733], Loss: 2.3886\n",
      "Epoch [78/300], Step [1700/27733], Loss: 3.1994\n",
      "Epoch [78/300], Step [1800/27733], Loss: 2.0766\n",
      "Epoch [78/300], Step [1900/27733], Loss: 2.8113\n",
      "Epoch [78/300], Step [2000/27733], Loss: 2.1255\n",
      "Epoch [78/300], Step [2100/27733], Loss: 2.8102\n",
      "Epoch [78/300], Step [2200/27733], Loss: 2.3002\n",
      "Epoch [78/300], Step [2300/27733], Loss: 2.3231\n",
      "Epoch [78/300], Step [2400/27733], Loss: 2.6062\n",
      "Epoch [78/300], Step [2500/27733], Loss: 2.0478\n",
      "Epoch [78/300], Step [2600/27733], Loss: 3.1096\n",
      "Epoch [78/300], Step [2700/27733], Loss: 2.3644\n",
      "Epoch [78/300], Step [2800/27733], Loss: 1.8766\n",
      "Epoch [78/300], Step [2900/27733], Loss: 2.8759\n",
      "Epoch [78/300], Step [3000/27733], Loss: 1.8169\n",
      "Epoch [78/300], Step [3100/27733], Loss: 1.9955\n",
      "Epoch [78/300], Step [3200/27733], Loss: 2.7189\n",
      "Epoch [78/300], Step [3300/27733], Loss: 2.1011\n",
      "Epoch [78/300], Step [3400/27733], Loss: 1.9498\n",
      "Epoch [78/300], Step [3500/27733], Loss: 2.5073\n",
      "Epoch [78/300], Step [3600/27733], Loss: 2.0744\n",
      "Epoch [78/300], Step [3700/27733], Loss: 2.1459\n",
      "Epoch [78/300], Step [3800/27733], Loss: 1.8702\n",
      "Epoch [78/300], Step [3900/27733], Loss: 2.6157\n",
      "Epoch [78/300], Step [4000/27733], Loss: 2.3919\n",
      "Epoch [78/300], Step [4100/27733], Loss: 2.2601\n",
      "Epoch [78/300], Step [4200/27733], Loss: 2.6306\n",
      "Epoch [78/300], Step [4300/27733], Loss: 1.7962\n",
      "Epoch [78/300], Step [4400/27733], Loss: 2.4700\n",
      "Epoch [78/300], Step [4500/27733], Loss: 2.2238\n",
      "Epoch [78/300], Step [4600/27733], Loss: 2.8841\n",
      "Epoch [78/300], Step [4700/27733], Loss: 2.2327\n",
      "Epoch [78/300], Step [4800/27733], Loss: 1.8125\n",
      "Epoch [78/300], Step [4900/27733], Loss: 2.6222\n",
      "Epoch [78/300], Step [5000/27733], Loss: 2.8235\n",
      "Epoch [78/300], Step [5100/27733], Loss: 2.4668\n",
      "Epoch [78/300], Step [5200/27733], Loss: 2.9010\n",
      "Epoch [78/300], Step [5300/27733], Loss: 2.0969\n",
      "Epoch [78/300], Step [5400/27733], Loss: 2.5949\n",
      "Epoch [78/300], Step [5500/27733], Loss: 3.9556\n",
      "Epoch [78/300], Step [5600/27733], Loss: 3.2734\n",
      "Epoch [78/300], Step [5700/27733], Loss: 2.4464\n",
      "Epoch [78/300], Step [5800/27733], Loss: 2.6177\n",
      "Epoch [78/300], Step [5900/27733], Loss: 2.4936\n",
      "Epoch [78/300], Step [6000/27733], Loss: 2.7867\n",
      "Epoch [78/300], Step [6100/27733], Loss: 2.8602\n",
      "Epoch [78/300], Step [6200/27733], Loss: 2.4319\n",
      "Epoch [78/300], Step [6300/27733], Loss: 2.9983\n",
      "Epoch [78/300], Step [6400/27733], Loss: 2.3882\n",
      "Epoch [78/300], Step [6500/27733], Loss: 2.9453\n",
      "Epoch [78/300], Step [6600/27733], Loss: 2.6134\n",
      "Epoch [78/300], Step [6700/27733], Loss: 2.0927\n",
      "Epoch [78/300], Step [6800/27733], Loss: 2.7634\n",
      "Epoch [78/300], Step [6900/27733], Loss: 2.2046\n",
      "Epoch [78/300], Step [7000/27733], Loss: 2.4921\n",
      "Epoch [78/300], Step [7100/27733], Loss: 2.4959\n",
      "Epoch [78/300], Step [7200/27733], Loss: 2.8324\n",
      "Epoch [78/300], Step [7300/27733], Loss: 3.4827\n",
      "Epoch [78/300], Step [7400/27733], Loss: 2.5698\n",
      "Epoch [78/300], Step [7500/27733], Loss: 2.8694\n",
      "Epoch [78/300], Step [7600/27733], Loss: 2.8854\n",
      "Epoch [78/300], Step [7700/27733], Loss: 2.4917\n",
      "Epoch [78/300], Step [7800/27733], Loss: 2.6647\n",
      "Epoch [78/300], Step [7900/27733], Loss: 2.0795\n",
      "Epoch [78/300], Step [8000/27733], Loss: 2.0160\n",
      "Epoch [78/300], Step [8100/27733], Loss: 3.5441\n",
      "Epoch [78/300], Step [8200/27733], Loss: 2.6044\n",
      "Epoch [78/300], Step [8300/27733], Loss: 1.9264\n",
      "Epoch [78/300], Step [8400/27733], Loss: 1.9223\n",
      "Epoch [78/300], Step [8500/27733], Loss: 1.6597\n",
      "Epoch [78/300], Step [8600/27733], Loss: 2.3791\n",
      "Epoch [78/300], Step [8700/27733], Loss: 3.6800\n",
      "Epoch [78/300], Step [8800/27733], Loss: 2.5226\n",
      "Epoch [78/300], Step [8900/27733], Loss: 2.4687\n",
      "Epoch [78/300], Step [9000/27733], Loss: 2.3420\n",
      "Epoch [78/300], Step [9100/27733], Loss: 2.5687\n",
      "Epoch [78/300], Step [9200/27733], Loss: 1.9120\n",
      "Epoch [78/300], Step [9300/27733], Loss: 3.6252\n",
      "Epoch [78/300], Step [9400/27733], Loss: 2.9358\n",
      "Epoch [78/300], Step [9500/27733], Loss: 2.2754\n",
      "Epoch [78/300], Step [9600/27733], Loss: 2.8813\n",
      "Epoch [78/300], Step [9700/27733], Loss: 1.7276\n",
      "Epoch [78/300], Step [9800/27733], Loss: 2.6212\n",
      "Epoch [78/300], Step [9900/27733], Loss: 2.8717\n",
      "Epoch [78/300], Step [10000/27733], Loss: 2.4826\n",
      "Epoch [78/300], Step [10100/27733], Loss: 1.6136\n",
      "Epoch [78/300], Step [10200/27733], Loss: 3.2554\n",
      "Epoch [78/300], Step [10300/27733], Loss: 2.8033\n",
      "Epoch [78/300], Step [10400/27733], Loss: 2.9217\n",
      "Epoch [78/300], Step [10500/27733], Loss: 2.7736\n",
      "Epoch [78/300], Step [10600/27733], Loss: 2.3535\n",
      "Epoch [78/300], Step [10700/27733], Loss: 2.6703\n",
      "Epoch [78/300], Step [10800/27733], Loss: 2.5627\n",
      "Epoch [78/300], Step [10900/27733], Loss: 2.7024\n",
      "Epoch [78/300], Step [11000/27733], Loss: 2.0551\n",
      "Epoch [78/300], Step [11100/27733], Loss: 2.5194\n",
      "Epoch [78/300], Step [11200/27733], Loss: 2.0615\n",
      "Epoch [78/300], Step [11300/27733], Loss: 1.9869\n",
      "Epoch [78/300], Step [11400/27733], Loss: 2.5618\n",
      "Epoch [78/300], Step [11500/27733], Loss: 2.6118\n",
      "Epoch [78/300], Step [11600/27733], Loss: 2.4723\n",
      "Epoch [78/300], Step [11700/27733], Loss: 2.1887\n",
      "Epoch [78/300], Step [11800/27733], Loss: 2.3890\n",
      "Epoch [78/300], Step [11900/27733], Loss: 2.6703\n",
      "Epoch [78/300], Step [12000/27733], Loss: 2.4328\n",
      "Epoch [78/300], Step [12100/27733], Loss: 2.6713\n",
      "Epoch [78/300], Step [12200/27733], Loss: 3.1899\n",
      "Epoch [78/300], Step [12300/27733], Loss: 2.4924\n",
      "Epoch [78/300], Step [12400/27733], Loss: 3.4343\n",
      "Epoch [78/300], Step [12500/27733], Loss: 3.1890\n",
      "Epoch [78/300], Step [12600/27733], Loss: 3.0566\n",
      "Epoch [78/300], Step [12700/27733], Loss: 2.1411\n",
      "Epoch [78/300], Step [12800/27733], Loss: 2.8901\n",
      "Epoch [78/300], Step [12900/27733], Loss: 3.3746\n",
      "Epoch [78/300], Step [13000/27733], Loss: 3.1378\n",
      "Epoch [78/300], Step [13100/27733], Loss: 3.4255\n",
      "Epoch [78/300], Step [13200/27733], Loss: 2.5417\n",
      "Epoch [78/300], Step [13300/27733], Loss: 3.6268\n",
      "Epoch [78/300], Step [13400/27733], Loss: 2.3098\n",
      "Epoch [78/300], Step [13500/27733], Loss: 2.5650\n",
      "Epoch [78/300], Step [13600/27733], Loss: 2.1442\n",
      "Epoch [78/300], Step [13700/27733], Loss: 2.8156\n",
      "Epoch [78/300], Step [13800/27733], Loss: 3.2120\n",
      "Epoch [78/300], Step [13900/27733], Loss: 2.5801\n",
      "Epoch [78/300], Step [14000/27733], Loss: 3.0248\n",
      "Epoch [78/300], Step [14100/27733], Loss: 3.3899\n",
      "Epoch [78/300], Step [14200/27733], Loss: 4.0522\n",
      "Epoch [78/300], Step [14300/27733], Loss: 2.4972\n",
      "Epoch [78/300], Step [14400/27733], Loss: 2.9456\n",
      "Epoch [78/300], Step [14500/27733], Loss: 3.5637\n",
      "Epoch [78/300], Step [14600/27733], Loss: 2.1876\n",
      "Epoch [78/300], Step [14700/27733], Loss: 3.2300\n",
      "Epoch [78/300], Step [14800/27733], Loss: 2.7768\n",
      "Epoch [78/300], Step [14900/27733], Loss: 2.4210\n",
      "Epoch [78/300], Step [15000/27733], Loss: 2.2481\n",
      "Epoch [78/300], Step [15100/27733], Loss: 2.5629\n",
      "Epoch [78/300], Step [15200/27733], Loss: 3.2817\n",
      "Epoch [78/300], Step [15300/27733], Loss: 3.3929\n",
      "Epoch [78/300], Step [15400/27733], Loss: 2.5745\n",
      "Epoch [78/300], Step [15500/27733], Loss: 1.8664\n",
      "Epoch [78/300], Step [15600/27733], Loss: 3.0033\n",
      "Epoch [78/300], Step [15700/27733], Loss: 1.8729\n",
      "Epoch [78/300], Step [15800/27733], Loss: 2.1243\n",
      "Epoch [78/300], Step [15900/27733], Loss: 2.6548\n",
      "Epoch [78/300], Step [16000/27733], Loss: 2.9708\n",
      "Epoch [78/300], Step [16100/27733], Loss: 2.6980\n",
      "Epoch [78/300], Step [16200/27733], Loss: 3.0509\n",
      "Epoch [78/300], Step [16300/27733], Loss: 3.2549\n",
      "Epoch [78/300], Step [16400/27733], Loss: 3.0738\n",
      "Epoch [78/300], Step [16500/27733], Loss: 2.8563\n",
      "Epoch [78/300], Step [16600/27733], Loss: 2.4508\n",
      "Epoch [78/300], Step [16700/27733], Loss: 2.7825\n",
      "Epoch [78/300], Step [16800/27733], Loss: 2.2616\n",
      "Epoch [78/300], Step [16900/27733], Loss: 2.5620\n",
      "Epoch [78/300], Step [17000/27733], Loss: 3.4066\n",
      "Epoch [78/300], Step [17100/27733], Loss: 3.0789\n",
      "Epoch [78/300], Step [17200/27733], Loss: 2.3845\n",
      "Epoch [78/300], Step [17300/27733], Loss: 2.7615\n",
      "Epoch [78/300], Step [17400/27733], Loss: 3.1373\n",
      "Epoch [78/300], Step [17500/27733], Loss: 3.9582\n",
      "Epoch [78/300], Step [17600/27733], Loss: 2.9709\n",
      "Epoch [78/300], Step [17700/27733], Loss: 2.9728\n",
      "Epoch [78/300], Step [17800/27733], Loss: 3.2930\n",
      "Epoch [78/300], Step [17900/27733], Loss: 1.9688\n",
      "Epoch [78/300], Step [18000/27733], Loss: 2.3193\n",
      "Epoch [78/300], Step [18100/27733], Loss: 2.5492\n",
      "Epoch [78/300], Step [18200/27733], Loss: 3.6228\n",
      "Epoch [78/300], Step [18300/27733], Loss: 3.5361\n",
      "Epoch [78/300], Step [18400/27733], Loss: 3.1603\n",
      "Epoch [78/300], Step [18500/27733], Loss: 2.5219\n",
      "Epoch [78/300], Step [18600/27733], Loss: 2.8390\n",
      "Epoch [78/300], Step [18700/27733], Loss: 2.3467\n",
      "Epoch [78/300], Step [18800/27733], Loss: 2.0919\n",
      "Epoch [78/300], Step [18900/27733], Loss: 1.8893\n",
      "Epoch [78/300], Step [19000/27733], Loss: 2.7087\n",
      "Epoch [78/300], Step [19100/27733], Loss: 2.5680\n",
      "Epoch [78/300], Step [19200/27733], Loss: 2.2295\n",
      "Epoch [78/300], Step [19300/27733], Loss: 3.6566\n",
      "Epoch [78/300], Step [19400/27733], Loss: 3.7305\n",
      "Epoch [78/300], Step [19500/27733], Loss: 3.3329\n",
      "Epoch [78/300], Step [19600/27733], Loss: 2.0466\n",
      "Epoch [78/300], Step [19700/27733], Loss: 2.7833\n",
      "Epoch [78/300], Step [19800/27733], Loss: 3.3252\n",
      "Epoch [78/300], Step [19900/27733], Loss: 2.3858\n",
      "Epoch [78/300], Step [20000/27733], Loss: 2.5051\n",
      "Epoch [78/300], Step [20100/27733], Loss: 2.0863\n",
      "Epoch [78/300], Step [20200/27733], Loss: 3.8454\n",
      "Epoch [78/300], Step [20300/27733], Loss: 4.7689\n",
      "Epoch [78/300], Step [20400/27733], Loss: 3.5602\n",
      "Epoch [78/300], Step [20500/27733], Loss: 3.0294\n",
      "Epoch [78/300], Step [20600/27733], Loss: 3.1998\n",
      "Epoch [78/300], Step [20700/27733], Loss: 2.0544\n",
      "Epoch [78/300], Step [20800/27733], Loss: 2.6060\n",
      "Epoch [78/300], Step [20900/27733], Loss: 2.5423\n",
      "Epoch [78/300], Step [21000/27733], Loss: 2.0879\n",
      "Epoch [78/300], Step [21100/27733], Loss: 2.5624\n",
      "Epoch [78/300], Step [21200/27733], Loss: 3.1210\n",
      "Epoch [78/300], Step [21300/27733], Loss: 2.7498\n",
      "Epoch [78/300], Step [21400/27733], Loss: 3.1736\n",
      "Epoch [78/300], Step [21500/27733], Loss: 3.3660\n",
      "Epoch [78/300], Step [21600/27733], Loss: 2.7680\n",
      "Epoch [78/300], Step [21700/27733], Loss: 2.5612\n",
      "Epoch [78/300], Step [21800/27733], Loss: 3.1013\n",
      "Epoch [78/300], Step [21900/27733], Loss: 3.0759\n",
      "Epoch [78/300], Step [22000/27733], Loss: 4.0487\n",
      "Epoch [78/300], Step [22100/27733], Loss: 3.5770\n",
      "Epoch [78/300], Step [22200/27733], Loss: 3.2140\n",
      "Epoch [78/300], Step [22300/27733], Loss: 2.6231\n",
      "Epoch [78/300], Step [22400/27733], Loss: 3.1587\n",
      "Epoch [78/300], Step [22500/27733], Loss: 2.9841\n",
      "Epoch [78/300], Step [22600/27733], Loss: 3.1888\n",
      "Epoch [78/300], Step [22700/27733], Loss: 3.4835\n",
      "Epoch [78/300], Step [22800/27733], Loss: 3.6727\n",
      "Epoch [78/300], Step [22900/27733], Loss: 2.4573\n",
      "Epoch [78/300], Step [23000/27733], Loss: 2.1279\n",
      "Epoch [78/300], Step [23100/27733], Loss: 2.7374\n",
      "Epoch [78/300], Step [23200/27733], Loss: 2.9165\n",
      "Epoch [78/300], Step [23300/27733], Loss: 3.4099\n",
      "Epoch [78/300], Step [23400/27733], Loss: 2.6709\n",
      "Epoch [78/300], Step [23500/27733], Loss: 2.2718\n",
      "Epoch [78/300], Step [23600/27733], Loss: 2.9949\n",
      "Epoch [78/300], Step [23700/27733], Loss: 2.7094\n",
      "Epoch [78/300], Step [23800/27733], Loss: 3.0060\n",
      "Epoch [78/300], Step [23900/27733], Loss: 1.8053\n",
      "Epoch [78/300], Step [24000/27733], Loss: 2.2623\n",
      "Epoch [78/300], Step [24100/27733], Loss: 2.8802\n",
      "Epoch [78/300], Step [24200/27733], Loss: 3.4696\n",
      "Epoch [78/300], Step [24300/27733], Loss: 3.1196\n",
      "Epoch [78/300], Step [24400/27733], Loss: 3.0488\n",
      "Epoch [78/300], Step [24500/27733], Loss: 2.8289\n",
      "Epoch [78/300], Step [24600/27733], Loss: 2.2586\n",
      "Epoch [78/300], Step [24700/27733], Loss: 2.9244\n",
      "Epoch [78/300], Step [24800/27733], Loss: 2.4473\n",
      "Epoch [78/300], Step [24900/27733], Loss: 3.0386\n",
      "Epoch [78/300], Step [25000/27733], Loss: 2.8464\n",
      "Epoch [78/300], Step [25100/27733], Loss: 2.9673\n",
      "Epoch [78/300], Step [25200/27733], Loss: 2.9740\n",
      "Epoch [78/300], Step [25300/27733], Loss: 3.1973\n",
      "Epoch [78/300], Step [25400/27733], Loss: 2.3370\n",
      "Epoch [78/300], Step [25500/27733], Loss: 2.6008\n",
      "Epoch [78/300], Step [25600/27733], Loss: 3.4234\n",
      "Epoch [78/300], Step [25700/27733], Loss: 3.0148\n",
      "Epoch [78/300], Step [25800/27733], Loss: 4.0532\n",
      "Epoch [78/300], Step [25900/27733], Loss: 2.9773\n",
      "Epoch [78/300], Step [26000/27733], Loss: 1.7927\n",
      "Epoch [78/300], Step [26100/27733], Loss: 2.7357\n",
      "Epoch [78/300], Step [26200/27733], Loss: 2.8497\n",
      "Epoch [78/300], Step [26300/27733], Loss: 2.2944\n",
      "Epoch [78/300], Step [26400/27733], Loss: 4.0393\n",
      "Epoch [78/300], Step [26500/27733], Loss: 2.5698\n",
      "Epoch [78/300], Step [26600/27733], Loss: 2.7504\n",
      "Epoch [78/300], Step [26700/27733], Loss: 3.1476\n",
      "Epoch [78/300], Step [26800/27733], Loss: 2.5555\n",
      "Epoch [78/300], Step [26900/27733], Loss: 3.0209\n",
      "Epoch [78/300], Step [27000/27733], Loss: 2.7798\n",
      "Epoch [78/300], Step [27100/27733], Loss: 3.4255\n",
      "Epoch [78/300], Step [27200/27733], Loss: 2.8944\n",
      "Epoch [78/300], Step [27300/27733], Loss: 2.6772\n",
      "Epoch [78/300], Step [27400/27733], Loss: 2.7908\n",
      "Epoch [78/300], Step [27500/27733], Loss: 3.6482\n",
      "Epoch [78/300], Step [27600/27733], Loss: 3.4186\n",
      "Epoch [78/300], Step [27700/27733], Loss: 3.2314\n",
      "Epoch [79/300], Step [100/27733], Loss: 2.5585\n",
      "Epoch [79/300], Step [200/27733], Loss: 2.2405\n",
      "Epoch [79/300], Step [300/27733], Loss: 2.7893\n",
      "Epoch [79/300], Step [400/27733], Loss: 2.3050\n",
      "Epoch [79/300], Step [500/27733], Loss: 1.8029\n",
      "Epoch [79/300], Step [600/27733], Loss: 2.7885\n",
      "Epoch [79/300], Step [700/27733], Loss: 1.8764\n",
      "Epoch [79/300], Step [800/27733], Loss: 2.1402\n",
      "Epoch [79/300], Step [900/27733], Loss: 2.0912\n",
      "Epoch [79/300], Step [1000/27733], Loss: 1.9496\n",
      "Epoch [79/300], Step [1100/27733], Loss: 2.0719\n",
      "Epoch [79/300], Step [1200/27733], Loss: 2.3024\n",
      "Epoch [79/300], Step [1300/27733], Loss: 2.4036\n",
      "Epoch [79/300], Step [1400/27733], Loss: 2.3363\n",
      "Epoch [79/300], Step [1500/27733], Loss: 1.7564\n",
      "Epoch [79/300], Step [1600/27733], Loss: 1.7973\n",
      "Epoch [79/300], Step [1700/27733], Loss: 1.8703\n",
      "Epoch [79/300], Step [1800/27733], Loss: 2.3290\n",
      "Epoch [79/300], Step [1900/27733], Loss: 2.5401\n",
      "Epoch [79/300], Step [2000/27733], Loss: 2.4264\n",
      "Epoch [79/300], Step [2100/27733], Loss: 2.6035\n",
      "Epoch [79/300], Step [2200/27733], Loss: 2.9536\n",
      "Epoch [79/300], Step [2300/27733], Loss: 2.6474\n",
      "Epoch [79/300], Step [2400/27733], Loss: 2.4962\n",
      "Epoch [79/300], Step [2500/27733], Loss: 2.3124\n",
      "Epoch [79/300], Step [2600/27733], Loss: 2.6615\n",
      "Epoch [79/300], Step [2700/27733], Loss: 2.1438\n",
      "Epoch [79/300], Step [2800/27733], Loss: 3.1094\n",
      "Epoch [79/300], Step [2900/27733], Loss: 2.0948\n",
      "Epoch [79/300], Step [3000/27733], Loss: 2.3745\n",
      "Epoch [79/300], Step [3100/27733], Loss: 2.7245\n",
      "Epoch [79/300], Step [3200/27733], Loss: 1.8637\n",
      "Epoch [79/300], Step [3300/27733], Loss: 1.9104\n",
      "Epoch [79/300], Step [3400/27733], Loss: 2.3424\n",
      "Epoch [79/300], Step [3500/27733], Loss: 2.1833\n",
      "Epoch [79/300], Step [3600/27733], Loss: 2.5161\n",
      "Epoch [79/300], Step [3700/27733], Loss: 2.6075\n",
      "Epoch [79/300], Step [3800/27733], Loss: 3.0403\n",
      "Epoch [79/300], Step [3900/27733], Loss: 2.3969\n",
      "Epoch [79/300], Step [4000/27733], Loss: 2.3537\n",
      "Epoch [79/300], Step [4100/27733], Loss: 2.6892\n",
      "Epoch [79/300], Step [4200/27733], Loss: 2.5473\n",
      "Epoch [79/300], Step [4300/27733], Loss: 2.7081\n",
      "Epoch [79/300], Step [4400/27733], Loss: 2.2920\n",
      "Epoch [79/300], Step [4500/27733], Loss: 2.8395\n",
      "Epoch [79/300], Step [4600/27733], Loss: 2.7079\n",
      "Epoch [79/300], Step [4700/27733], Loss: 2.9695\n",
      "Epoch [79/300], Step [4800/27733], Loss: 2.7687\n",
      "Epoch [79/300], Step [4900/27733], Loss: 2.8861\n",
      "Epoch [79/300], Step [5000/27733], Loss: 2.4591\n",
      "Epoch [79/300], Step [5100/27733], Loss: 2.6185\n",
      "Epoch [79/300], Step [5200/27733], Loss: 2.3460\n",
      "Epoch [79/300], Step [5300/27733], Loss: 2.0931\n",
      "Epoch [79/300], Step [5400/27733], Loss: 2.7182\n",
      "Epoch [79/300], Step [5500/27733], Loss: 2.7797\n",
      "Epoch [79/300], Step [5600/27733], Loss: 2.8231\n",
      "Epoch [79/300], Step [5700/27733], Loss: 2.9818\n",
      "Epoch [79/300], Step [5800/27733], Loss: 2.6014\n",
      "Epoch [79/300], Step [5900/27733], Loss: 2.1283\n",
      "Epoch [79/300], Step [6000/27733], Loss: 2.5090\n",
      "Epoch [79/300], Step [6100/27733], Loss: 3.0427\n",
      "Epoch [79/300], Step [6200/27733], Loss: 2.3956\n",
      "Epoch [79/300], Step [6300/27733], Loss: 2.9112\n",
      "Epoch [79/300], Step [6400/27733], Loss: 2.6905\n",
      "Epoch [79/300], Step [6500/27733], Loss: 2.9200\n",
      "Epoch [79/300], Step [6600/27733], Loss: 3.3424\n",
      "Epoch [79/300], Step [6700/27733], Loss: 2.7433\n",
      "Epoch [79/300], Step [6800/27733], Loss: 3.7364\n",
      "Epoch [79/300], Step [6900/27733], Loss: 2.3720\n",
      "Epoch [79/300], Step [7000/27733], Loss: 2.3182\n",
      "Epoch [79/300], Step [7100/27733], Loss: 1.8914\n",
      "Epoch [79/300], Step [7200/27733], Loss: 1.9558\n",
      "Epoch [79/300], Step [7300/27733], Loss: 3.2954\n",
      "Epoch [79/300], Step [7400/27733], Loss: 2.9442\n",
      "Epoch [79/300], Step [7500/27733], Loss: 2.3349\n",
      "Epoch [79/300], Step [7600/27733], Loss: 2.7979\n",
      "Epoch [79/300], Step [7700/27733], Loss: 2.2332\n",
      "Epoch [79/300], Step [7800/27733], Loss: 3.1279\n",
      "Epoch [79/300], Step [7900/27733], Loss: 2.6374\n",
      "Epoch [79/300], Step [8000/27733], Loss: 3.1724\n",
      "Epoch [79/300], Step [8100/27733], Loss: 3.1045\n",
      "Epoch [79/300], Step [8200/27733], Loss: 2.5677\n",
      "Epoch [79/300], Step [8300/27733], Loss: 1.9794\n",
      "Epoch [79/300], Step [8400/27733], Loss: 2.3416\n",
      "Epoch [79/300], Step [8500/27733], Loss: 2.9488\n",
      "Epoch [79/300], Step [8600/27733], Loss: 2.8584\n",
      "Epoch [79/300], Step [8700/27733], Loss: 2.6621\n",
      "Epoch [79/300], Step [8800/27733], Loss: 2.3748\n",
      "Epoch [79/300], Step [8900/27733], Loss: 2.7943\n",
      "Epoch [79/300], Step [9000/27733], Loss: 2.3484\n",
      "Epoch [79/300], Step [9100/27733], Loss: 2.2154\n",
      "Epoch [79/300], Step [9200/27733], Loss: 2.3356\n",
      "Epoch [79/300], Step [9300/27733], Loss: 1.7864\n",
      "Epoch [79/300], Step [9400/27733], Loss: 2.8477\n",
      "Epoch [79/300], Step [9500/27733], Loss: 3.5126\n",
      "Epoch [79/300], Step [9600/27733], Loss: 3.5198\n",
      "Epoch [79/300], Step [9700/27733], Loss: 2.6991\n",
      "Epoch [79/300], Step [9800/27733], Loss: 2.7852\n",
      "Epoch [79/300], Step [9900/27733], Loss: 2.4136\n",
      "Epoch [79/300], Step [10000/27733], Loss: 3.2419\n",
      "Epoch [79/300], Step [10100/27733], Loss: 2.3471\n",
      "Epoch [79/300], Step [10200/27733], Loss: 2.3459\n",
      "Epoch [79/300], Step [10300/27733], Loss: 2.5310\n",
      "Epoch [79/300], Step [10400/27733], Loss: 3.5578\n",
      "Epoch [79/300], Step [10500/27733], Loss: 2.3755\n",
      "Epoch [79/300], Step [10600/27733], Loss: 2.2366\n",
      "Epoch [79/300], Step [10700/27733], Loss: 3.1325\n",
      "Epoch [79/300], Step [10800/27733], Loss: 2.8303\n",
      "Epoch [79/300], Step [10900/27733], Loss: 2.8216\n",
      "Epoch [79/300], Step [11000/27733], Loss: 2.6878\n",
      "Epoch [79/300], Step [11100/27733], Loss: 2.6729\n",
      "Epoch [79/300], Step [11200/27733], Loss: 2.2032\n",
      "Epoch [79/300], Step [11300/27733], Loss: 3.1209\n",
      "Epoch [79/300], Step [11400/27733], Loss: 2.7566\n",
      "Epoch [79/300], Step [11500/27733], Loss: 2.7984\n",
      "Epoch [79/300], Step [11600/27733], Loss: 3.1104\n",
      "Epoch [79/300], Step [11700/27733], Loss: 2.0034\n",
      "Epoch [79/300], Step [11800/27733], Loss: 3.4284\n",
      "Epoch [79/300], Step [11900/27733], Loss: 2.3637\n",
      "Epoch [79/300], Step [12000/27733], Loss: 2.5875\n",
      "Epoch [79/300], Step [12100/27733], Loss: 3.2727\n",
      "Epoch [79/300], Step [12200/27733], Loss: 3.2130\n",
      "Epoch [79/300], Step [12300/27733], Loss: 2.3587\n",
      "Epoch [79/300], Step [12400/27733], Loss: 3.0094\n",
      "Epoch [79/300], Step [12500/27733], Loss: 2.7987\n",
      "Epoch [79/300], Step [12600/27733], Loss: 2.3937\n",
      "Epoch [79/300], Step [12700/27733], Loss: 3.2198\n",
      "Epoch [79/300], Step [12800/27733], Loss: 2.8050\n",
      "Epoch [79/300], Step [12900/27733], Loss: 2.3360\n",
      "Epoch [79/300], Step [13000/27733], Loss: 2.7529\n",
      "Epoch [79/300], Step [13100/27733], Loss: 2.9506\n",
      "Epoch [79/300], Step [13200/27733], Loss: 2.5111\n",
      "Epoch [79/300], Step [13300/27733], Loss: 3.1601\n",
      "Epoch [79/300], Step [13400/27733], Loss: 2.1262\n",
      "Epoch [79/300], Step [13500/27733], Loss: 2.7450\n",
      "Epoch [79/300], Step [13600/27733], Loss: 2.5575\n",
      "Epoch [79/300], Step [13700/27733], Loss: 2.4965\n",
      "Epoch [79/300], Step [13800/27733], Loss: 2.5476\n",
      "Epoch [79/300], Step [13900/27733], Loss: 2.7638\n",
      "Epoch [79/300], Step [14000/27733], Loss: 2.3327\n",
      "Epoch [79/300], Step [14100/27733], Loss: 3.2157\n",
      "Epoch [79/300], Step [14200/27733], Loss: 3.2955\n",
      "Epoch [79/300], Step [14300/27733], Loss: 2.1858\n",
      "Epoch [79/300], Step [14400/27733], Loss: 2.6754\n",
      "Epoch [79/300], Step [14500/27733], Loss: 2.3103\n",
      "Epoch [79/300], Step [14600/27733], Loss: 3.0820\n",
      "Epoch [79/300], Step [14700/27733], Loss: 2.4429\n",
      "Epoch [79/300], Step [14800/27733], Loss: 3.3995\n",
      "Epoch [79/300], Step [14900/27733], Loss: 3.5005\n",
      "Epoch [79/300], Step [15000/27733], Loss: 3.0870\n",
      "Epoch [79/300], Step [15100/27733], Loss: 3.2757\n",
      "Epoch [79/300], Step [15200/27733], Loss: 4.0024\n",
      "Epoch [79/300], Step [15300/27733], Loss: 2.8437\n",
      "Epoch [79/300], Step [15400/27733], Loss: 2.8632\n",
      "Epoch [79/300], Step [15500/27733], Loss: 2.8118\n",
      "Epoch [79/300], Step [15600/27733], Loss: 3.3967\n",
      "Epoch [79/300], Step [15700/27733], Loss: 2.3662\n",
      "Epoch [79/300], Step [15800/27733], Loss: 3.6594\n",
      "Epoch [79/300], Step [15900/27733], Loss: 3.1008\n",
      "Epoch [79/300], Step [16000/27733], Loss: 2.7915\n",
      "Epoch [79/300], Step [16100/27733], Loss: 2.6695\n",
      "Epoch [79/300], Step [16200/27733], Loss: 2.3901\n",
      "Epoch [79/300], Step [16300/27733], Loss: 1.9440\n",
      "Epoch [79/300], Step [16400/27733], Loss: 2.8913\n",
      "Epoch [79/300], Step [16500/27733], Loss: 3.0591\n",
      "Epoch [79/300], Step [16600/27733], Loss: 2.2167\n",
      "Epoch [79/300], Step [16700/27733], Loss: 2.1834\n",
      "Epoch [79/300], Step [16800/27733], Loss: 3.0480\n",
      "Epoch [79/300], Step [16900/27733], Loss: 2.1877\n",
      "Epoch [79/300], Step [17000/27733], Loss: 2.7656\n",
      "Epoch [79/300], Step [17100/27733], Loss: 3.3672\n",
      "Epoch [79/300], Step [17200/27733], Loss: 3.0363\n",
      "Epoch [79/300], Step [17300/27733], Loss: 2.6164\n",
      "Epoch [79/300], Step [17400/27733], Loss: 2.8868\n",
      "Epoch [79/300], Step [17500/27733], Loss: 2.9279\n",
      "Epoch [79/300], Step [17600/27733], Loss: 2.9845\n",
      "Epoch [79/300], Step [17700/27733], Loss: 2.4412\n",
      "Epoch [79/300], Step [17800/27733], Loss: 3.1378\n",
      "Epoch [79/300], Step [17900/27733], Loss: 2.5141\n",
      "Epoch [79/300], Step [18000/27733], Loss: 3.2610\n",
      "Epoch [79/300], Step [18100/27733], Loss: 2.6543\n",
      "Epoch [79/300], Step [18200/27733], Loss: 3.0288\n",
      "Epoch [79/300], Step [18300/27733], Loss: 2.2748\n",
      "Epoch [79/300], Step [18400/27733], Loss: 2.8917\n",
      "Epoch [79/300], Step [18500/27733], Loss: 2.9978\n",
      "Epoch [79/300], Step [18600/27733], Loss: 2.6280\n",
      "Epoch [79/300], Step [18700/27733], Loss: 2.7551\n",
      "Epoch [79/300], Step [18800/27733], Loss: 3.7037\n",
      "Epoch [79/300], Step [18900/27733], Loss: 2.6169\n",
      "Epoch [79/300], Step [19000/27733], Loss: 2.2155\n",
      "Epoch [79/300], Step [19100/27733], Loss: 2.3161\n",
      "Epoch [79/300], Step [19200/27733], Loss: 2.1055\n",
      "Epoch [79/300], Step [19300/27733], Loss: 3.0069\n",
      "Epoch [79/300], Step [19400/27733], Loss: 2.5341\n",
      "Epoch [79/300], Step [19500/27733], Loss: 2.9343\n",
      "Epoch [79/300], Step [19600/27733], Loss: 2.8023\n",
      "Epoch [79/300], Step [19700/27733], Loss: 1.9911\n",
      "Epoch [79/300], Step [19800/27733], Loss: 3.4581\n",
      "Epoch [79/300], Step [19900/27733], Loss: 3.4200\n",
      "Epoch [79/300], Step [20000/27733], Loss: 3.1242\n",
      "Epoch [79/300], Step [20100/27733], Loss: 2.5027\n",
      "Epoch [79/300], Step [20200/27733], Loss: 2.6386\n",
      "Epoch [79/300], Step [20300/27733], Loss: 2.8489\n",
      "Epoch [79/300], Step [20400/27733], Loss: 4.0780\n",
      "Epoch [79/300], Step [20500/27733], Loss: 2.3694\n",
      "Epoch [79/300], Step [20600/27733], Loss: 3.6050\n",
      "Epoch [79/300], Step [20700/27733], Loss: 3.7629\n",
      "Epoch [79/300], Step [20800/27733], Loss: 3.5687\n",
      "Epoch [79/300], Step [20900/27733], Loss: 3.0332\n",
      "Epoch [79/300], Step [21000/27733], Loss: 2.8326\n",
      "Epoch [79/300], Step [21100/27733], Loss: 2.6125\n",
      "Epoch [79/300], Step [21200/27733], Loss: 2.3155\n",
      "Epoch [79/300], Step [21300/27733], Loss: 3.5305\n",
      "Epoch [79/300], Step [21400/27733], Loss: 3.0250\n",
      "Epoch [79/300], Step [21500/27733], Loss: 2.7247\n",
      "Epoch [79/300], Step [21600/27733], Loss: 3.1889\n",
      "Epoch [79/300], Step [21700/27733], Loss: 2.5501\n",
      "Epoch [79/300], Step [21800/27733], Loss: 2.7497\n",
      "Epoch [79/300], Step [21900/27733], Loss: 2.5664\n",
      "Epoch [79/300], Step [22000/27733], Loss: 2.8526\n",
      "Epoch [79/300], Step [22100/27733], Loss: 2.4738\n",
      "Epoch [79/300], Step [22200/27733], Loss: 2.5729\n",
      "Epoch [79/300], Step [22300/27733], Loss: 3.6251\n",
      "Epoch [79/300], Step [22400/27733], Loss: 3.1175\n",
      "Epoch [79/300], Step [22500/27733], Loss: 2.4519\n",
      "Epoch [79/300], Step [22600/27733], Loss: 1.9759\n",
      "Epoch [79/300], Step [22700/27733], Loss: 3.0893\n",
      "Epoch [79/300], Step [22800/27733], Loss: 2.4948\n",
      "Epoch [79/300], Step [22900/27733], Loss: 2.9729\n",
      "Epoch [79/300], Step [23000/27733], Loss: 3.2982\n",
      "Epoch [79/300], Step [23100/27733], Loss: 3.2899\n",
      "Epoch [79/300], Step [23200/27733], Loss: 2.4692\n",
      "Epoch [79/300], Step [23300/27733], Loss: 2.9976\n",
      "Epoch [79/300], Step [23400/27733], Loss: 3.4526\n",
      "Epoch [79/300], Step [23500/27733], Loss: 2.8325\n",
      "Epoch [79/300], Step [23600/27733], Loss: 2.4895\n",
      "Epoch [79/300], Step [23700/27733], Loss: 3.1008\n",
      "Epoch [79/300], Step [23800/27733], Loss: 2.9017\n",
      "Epoch [79/300], Step [23900/27733], Loss: 2.5633\n",
      "Epoch [79/300], Step [24000/27733], Loss: 3.3689\n",
      "Epoch [79/300], Step [24100/27733], Loss: 3.2571\n",
      "Epoch [79/300], Step [24200/27733], Loss: 2.4214\n",
      "Epoch [79/300], Step [24300/27733], Loss: 3.1034\n",
      "Epoch [79/300], Step [24400/27733], Loss: 2.5578\n",
      "Epoch [79/300], Step [24500/27733], Loss: 3.5509\n",
      "Epoch [79/300], Step [24600/27733], Loss: 2.7685\n",
      "Epoch [79/300], Step [24700/27733], Loss: 2.5773\n",
      "Epoch [79/300], Step [24800/27733], Loss: 3.1380\n",
      "Epoch [79/300], Step [24900/27733], Loss: 3.0238\n",
      "Epoch [79/300], Step [25000/27733], Loss: 3.2490\n",
      "Epoch [79/300], Step [25100/27733], Loss: 3.4808\n",
      "Epoch [79/300], Step [25200/27733], Loss: 2.2802\n",
      "Epoch [79/300], Step [25300/27733], Loss: 3.4527\n",
      "Epoch [79/300], Step [25400/27733], Loss: 3.0613\n",
      "Epoch [79/300], Step [25500/27733], Loss: 3.9380\n",
      "Epoch [79/300], Step [25600/27733], Loss: 3.1922\n",
      "Epoch [79/300], Step [25700/27733], Loss: 3.3676\n",
      "Epoch [79/300], Step [25800/27733], Loss: 2.9390\n",
      "Epoch [79/300], Step [25900/27733], Loss: 3.5349\n",
      "Epoch [79/300], Step [26000/27733], Loss: 3.1415\n",
      "Epoch [79/300], Step [26100/27733], Loss: 2.4653\n",
      "Epoch [79/300], Step [26200/27733], Loss: 3.9421\n",
      "Epoch [79/300], Step [26300/27733], Loss: 1.9609\n",
      "Epoch [79/300], Step [26400/27733], Loss: 3.5129\n",
      "Epoch [79/300], Step [26500/27733], Loss: 3.3071\n",
      "Epoch [79/300], Step [26600/27733], Loss: 2.6445\n",
      "Epoch [79/300], Step [26700/27733], Loss: 2.2666\n",
      "Epoch [79/300], Step [26800/27733], Loss: 3.1362\n",
      "Epoch [79/300], Step [26900/27733], Loss: 3.3834\n",
      "Epoch [79/300], Step [27000/27733], Loss: 2.7861\n",
      "Epoch [79/300], Step [27100/27733], Loss: 2.2064\n",
      "Epoch [79/300], Step [27200/27733], Loss: 3.1684\n",
      "Epoch [79/300], Step [27300/27733], Loss: 2.4820\n",
      "Epoch [79/300], Step [27400/27733], Loss: 2.6913\n",
      "Epoch [79/300], Step [27500/27733], Loss: 3.6147\n",
      "Epoch [79/300], Step [27600/27733], Loss: 3.7763\n",
      "Epoch [79/300], Step [27700/27733], Loss: 2.2395\n",
      "Epoch [80/300], Step [100/27733], Loss: 1.8535\n",
      "Epoch [80/300], Step [200/27733], Loss: 2.3178\n",
      "Epoch [80/300], Step [300/27733], Loss: 2.9058\n",
      "Epoch [80/300], Step [400/27733], Loss: 1.7455\n",
      "Epoch [80/300], Step [500/27733], Loss: 2.8747\n",
      "Epoch [80/300], Step [600/27733], Loss: 2.1176\n",
      "Epoch [80/300], Step [700/27733], Loss: 2.5601\n",
      "Epoch [80/300], Step [800/27733], Loss: 2.1943\n",
      "Epoch [80/300], Step [900/27733], Loss: 1.6626\n",
      "Epoch [80/300], Step [1000/27733], Loss: 2.8024\n",
      "Epoch [80/300], Step [1100/27733], Loss: 2.5465\n",
      "Epoch [80/300], Step [1200/27733], Loss: 2.4213\n",
      "Epoch [80/300], Step [1300/27733], Loss: 2.4416\n",
      "Epoch [80/300], Step [1400/27733], Loss: 2.2158\n",
      "Epoch [80/300], Step [1500/27733], Loss: 2.0016\n",
      "Epoch [80/300], Step [1600/27733], Loss: 2.4271\n",
      "Epoch [80/300], Step [1700/27733], Loss: 2.5936\n",
      "Epoch [80/300], Step [1800/27733], Loss: 2.3037\n",
      "Epoch [80/300], Step [1900/27733], Loss: 3.1256\n",
      "Epoch [80/300], Step [2000/27733], Loss: 2.5429\n",
      "Epoch [80/300], Step [2100/27733], Loss: 2.0571\n",
      "Epoch [80/300], Step [2200/27733], Loss: 2.2615\n",
      "Epoch [80/300], Step [2300/27733], Loss: 2.1292\n",
      "Epoch [80/300], Step [2400/27733], Loss: 2.5062\n",
      "Epoch [80/300], Step [2500/27733], Loss: 2.5410\n",
      "Epoch [80/300], Step [2600/27733], Loss: 2.5841\n",
      "Epoch [80/300], Step [2700/27733], Loss: 3.0732\n",
      "Epoch [80/300], Step [2800/27733], Loss: 2.7026\n",
      "Epoch [80/300], Step [2900/27733], Loss: 2.7635\n",
      "Epoch [80/300], Step [3000/27733], Loss: 2.2490\n",
      "Epoch [80/300], Step [3100/27733], Loss: 1.9670\n",
      "Epoch [80/300], Step [3200/27733], Loss: 2.7374\n",
      "Epoch [80/300], Step [3300/27733], Loss: 2.8424\n",
      "Epoch [80/300], Step [3400/27733], Loss: 2.1509\n",
      "Epoch [80/300], Step [3500/27733], Loss: 2.6946\n",
      "Epoch [80/300], Step [3600/27733], Loss: 2.6373\n",
      "Epoch [80/300], Step [3700/27733], Loss: 2.7781\n",
      "Epoch [80/300], Step [3800/27733], Loss: 2.5808\n",
      "Epoch [80/300], Step [3900/27733], Loss: 2.3397\n",
      "Epoch [80/300], Step [4000/27733], Loss: 2.3360\n",
      "Epoch [80/300], Step [4100/27733], Loss: 3.4386\n",
      "Epoch [80/300], Step [4200/27733], Loss: 2.5448\n",
      "Epoch [80/300], Step [4300/27733], Loss: 2.3128\n",
      "Epoch [80/300], Step [4400/27733], Loss: 1.9802\n",
      "Epoch [80/300], Step [4500/27733], Loss: 1.9298\n",
      "Epoch [80/300], Step [4600/27733], Loss: 2.7599\n",
      "Epoch [80/300], Step [4700/27733], Loss: 2.1070\n",
      "Epoch [80/300], Step [4800/27733], Loss: 2.3881\n",
      "Epoch [80/300], Step [4900/27733], Loss: 3.3778\n",
      "Epoch [80/300], Step [5000/27733], Loss: 3.4465\n",
      "Epoch [80/300], Step [5100/27733], Loss: 1.7965\n",
      "Epoch [80/300], Step [5200/27733], Loss: 3.6536\n",
      "Epoch [80/300], Step [5300/27733], Loss: 3.0377\n",
      "Epoch [80/300], Step [5400/27733], Loss: 2.7990\n",
      "Epoch [80/300], Step [5500/27733], Loss: 2.8994\n",
      "Epoch [80/300], Step [5600/27733], Loss: 2.4846\n",
      "Epoch [80/300], Step [5700/27733], Loss: 2.6732\n",
      "Epoch [80/300], Step [5800/27733], Loss: 2.9867\n",
      "Epoch [80/300], Step [5900/27733], Loss: 2.6629\n",
      "Epoch [80/300], Step [6000/27733], Loss: 2.9497\n",
      "Epoch [80/300], Step [6100/27733], Loss: 2.9535\n",
      "Epoch [80/300], Step [6200/27733], Loss: 2.8010\n",
      "Epoch [80/300], Step [6300/27733], Loss: 2.5393\n",
      "Epoch [80/300], Step [6400/27733], Loss: 3.0213\n",
      "Epoch [80/300], Step [6500/27733], Loss: 2.2308\n",
      "Epoch [80/300], Step [6600/27733], Loss: 2.3438\n",
      "Epoch [80/300], Step [6700/27733], Loss: 2.1719\n",
      "Epoch [80/300], Step [6800/27733], Loss: 2.2475\n",
      "Epoch [80/300], Step [6900/27733], Loss: 2.4701\n",
      "Epoch [80/300], Step [7000/27733], Loss: 2.2265\n",
      "Epoch [80/300], Step [7100/27733], Loss: 2.1292\n",
      "Epoch [80/300], Step [7200/27733], Loss: 1.8330\n",
      "Epoch [80/300], Step [7300/27733], Loss: 2.5424\n",
      "Epoch [80/300], Step [7400/27733], Loss: 2.5735\n",
      "Epoch [80/300], Step [7500/27733], Loss: 2.7343\n",
      "Epoch [80/300], Step [7600/27733], Loss: 2.2738\n",
      "Epoch [80/300], Step [7700/27733], Loss: 2.3186\n",
      "Epoch [80/300], Step [7800/27733], Loss: 2.4795\n",
      "Epoch [80/300], Step [7900/27733], Loss: 3.1819\n",
      "Epoch [80/300], Step [8000/27733], Loss: 2.6555\n",
      "Epoch [80/300], Step [8100/27733], Loss: 2.0307\n",
      "Epoch [80/300], Step [8200/27733], Loss: 2.2072\n",
      "Epoch [80/300], Step [8300/27733], Loss: 3.3741\n",
      "Epoch [80/300], Step [8400/27733], Loss: 3.3185\n",
      "Epoch [80/300], Step [8500/27733], Loss: 3.4313\n",
      "Epoch [80/300], Step [8600/27733], Loss: 2.6164\n",
      "Epoch [80/300], Step [8700/27733], Loss: 2.8042\n",
      "Epoch [80/300], Step [8800/27733], Loss: 2.8810\n",
      "Epoch [80/300], Step [8900/27733], Loss: 3.4019\n",
      "Epoch [80/300], Step [9000/27733], Loss: 2.6441\n",
      "Epoch [80/300], Step [9100/27733], Loss: 2.5973\n",
      "Epoch [80/300], Step [9200/27733], Loss: 2.9703\n",
      "Epoch [80/300], Step [9300/27733], Loss: 2.7659\n",
      "Epoch [80/300], Step [9400/27733], Loss: 1.5758\n",
      "Epoch [80/300], Step [9500/27733], Loss: 3.4333\n",
      "Epoch [80/300], Step [9600/27733], Loss: 3.8097\n",
      "Epoch [80/300], Step [9700/27733], Loss: 2.2721\n",
      "Epoch [80/300], Step [9800/27733], Loss: 2.1840\n",
      "Epoch [80/300], Step [9900/27733], Loss: 3.2459\n",
      "Epoch [80/300], Step [10000/27733], Loss: 2.3553\n",
      "Epoch [80/300], Step [10100/27733], Loss: 3.2599\n",
      "Epoch [80/300], Step [10200/27733], Loss: 2.2275\n",
      "Epoch [80/300], Step [10300/27733], Loss: 2.5905\n",
      "Epoch [80/300], Step [10400/27733], Loss: 2.5865\n",
      "Epoch [80/300], Step [10500/27733], Loss: 2.8889\n",
      "Epoch [80/300], Step [10600/27733], Loss: 2.0912\n",
      "Epoch [80/300], Step [10700/27733], Loss: 2.3885\n",
      "Epoch [80/300], Step [10800/27733], Loss: 3.3774\n",
      "Epoch [80/300], Step [10900/27733], Loss: 2.5303\n",
      "Epoch [80/300], Step [11000/27733], Loss: 2.9102\n",
      "Epoch [80/300], Step [11100/27733], Loss: 2.9044\n",
      "Epoch [80/300], Step [11200/27733], Loss: 2.7202\n",
      "Epoch [80/300], Step [11300/27733], Loss: 3.3358\n",
      "Epoch [80/300], Step [11400/27733], Loss: 3.3614\n",
      "Epoch [80/300], Step [11500/27733], Loss: 2.6068\n",
      "Epoch [80/300], Step [11600/27733], Loss: 2.4765\n",
      "Epoch [80/300], Step [11700/27733], Loss: 2.4548\n",
      "Epoch [80/300], Step [11800/27733], Loss: 3.5802\n",
      "Epoch [80/300], Step [11900/27733], Loss: 3.0936\n",
      "Epoch [80/300], Step [12000/27733], Loss: 3.4379\n",
      "Epoch [80/300], Step [12100/27733], Loss: 2.3578\n",
      "Epoch [80/300], Step [12200/27733], Loss: 2.4512\n",
      "Epoch [80/300], Step [12300/27733], Loss: 2.9699\n",
      "Epoch [80/300], Step [12400/27733], Loss: 2.2761\n",
      "Epoch [80/300], Step [12500/27733], Loss: 2.4495\n",
      "Epoch [80/300], Step [12600/27733], Loss: 2.3588\n",
      "Epoch [80/300], Step [12700/27733], Loss: 3.2158\n",
      "Epoch [80/300], Step [12800/27733], Loss: 2.9529\n",
      "Epoch [80/300], Step [12900/27733], Loss: 2.7585\n",
      "Epoch [80/300], Step [13000/27733], Loss: 2.8969\n",
      "Epoch [80/300], Step [13100/27733], Loss: 2.8901\n",
      "Epoch [80/300], Step [13200/27733], Loss: 2.9456\n",
      "Epoch [80/300], Step [13300/27733], Loss: 3.6808\n",
      "Epoch [80/300], Step [13400/27733], Loss: 2.0782\n",
      "Epoch [80/300], Step [13500/27733], Loss: 2.7955\n",
      "Epoch [80/300], Step [13600/27733], Loss: 2.1729\n",
      "Epoch [80/300], Step [13700/27733], Loss: 2.6413\n",
      "Epoch [80/300], Step [13800/27733], Loss: 2.7310\n",
      "Epoch [80/300], Step [13900/27733], Loss: 2.3954\n",
      "Epoch [80/300], Step [14000/27733], Loss: 3.4221\n",
      "Epoch [80/300], Step [14100/27733], Loss: 3.6322\n",
      "Epoch [80/300], Step [14200/27733], Loss: 2.1917\n",
      "Epoch [80/300], Step [14300/27733], Loss: 2.6175\n",
      "Epoch [80/300], Step [14400/27733], Loss: 3.0251\n",
      "Epoch [80/300], Step [14500/27733], Loss: 2.7330\n",
      "Epoch [80/300], Step [14600/27733], Loss: 2.4616\n",
      "Epoch [80/300], Step [14700/27733], Loss: 2.7368\n",
      "Epoch [80/300], Step [14800/27733], Loss: 2.6909\n",
      "Epoch [80/300], Step [14900/27733], Loss: 2.7704\n",
      "Epoch [80/300], Step [15000/27733], Loss: 3.1826\n",
      "Epoch [80/300], Step [15100/27733], Loss: 2.8022\n",
      "Epoch [80/300], Step [15200/27733], Loss: 2.9489\n",
      "Epoch [80/300], Step [15300/27733], Loss: 2.3523\n",
      "Epoch [80/300], Step [15400/27733], Loss: 3.6822\n",
      "Epoch [80/300], Step [15500/27733], Loss: 2.8694\n",
      "Epoch [80/300], Step [15600/27733], Loss: 2.7123\n",
      "Epoch [80/300], Step [15700/27733], Loss: 2.5455\n",
      "Epoch [80/300], Step [15800/27733], Loss: 2.4119\n",
      "Epoch [80/300], Step [15900/27733], Loss: 2.9949\n",
      "Epoch [80/300], Step [16000/27733], Loss: 2.3573\n",
      "Epoch [80/300], Step [16100/27733], Loss: 1.7589\n",
      "Epoch [80/300], Step [16200/27733], Loss: 2.8691\n",
      "Epoch [80/300], Step [16300/27733], Loss: 2.6088\n",
      "Epoch [80/300], Step [16400/27733], Loss: 2.4392\n",
      "Epoch [80/300], Step [16500/27733], Loss: 2.7261\n",
      "Epoch [80/300], Step [16600/27733], Loss: 3.3400\n",
      "Epoch [80/300], Step [16700/27733], Loss: 2.8668\n",
      "Epoch [80/300], Step [16800/27733], Loss: 3.1187\n",
      "Epoch [80/300], Step [16900/27733], Loss: 3.0633\n",
      "Epoch [80/300], Step [17000/27733], Loss: 2.2592\n",
      "Epoch [80/300], Step [17100/27733], Loss: 2.6351\n",
      "Epoch [80/300], Step [17200/27733], Loss: 2.9437\n",
      "Epoch [80/300], Step [17300/27733], Loss: 3.4964\n",
      "Epoch [80/300], Step [17400/27733], Loss: 3.1607\n",
      "Epoch [80/300], Step [17500/27733], Loss: 2.7903\n",
      "Epoch [80/300], Step [17600/27733], Loss: 2.4076\n",
      "Epoch [80/300], Step [17700/27733], Loss: 2.1891\n",
      "Epoch [80/300], Step [17800/27733], Loss: 1.7143\n",
      "Epoch [80/300], Step [17900/27733], Loss: 2.7021\n",
      "Epoch [80/300], Step [18000/27733], Loss: 2.8141\n",
      "Epoch [80/300], Step [18100/27733], Loss: 3.2807\n",
      "Epoch [80/300], Step [18200/27733], Loss: 3.4571\n",
      "Epoch [80/300], Step [18300/27733], Loss: 3.1223\n",
      "Epoch [80/300], Step [18400/27733], Loss: 2.5750\n",
      "Epoch [80/300], Step [18500/27733], Loss: 2.9961\n",
      "Epoch [80/300], Step [18600/27733], Loss: 2.7670\n",
      "Epoch [80/300], Step [18700/27733], Loss: 3.1567\n",
      "Epoch [80/300], Step [18800/27733], Loss: 3.1517\n",
      "Epoch [80/300], Step [18900/27733], Loss: 2.4356\n",
      "Epoch [80/300], Step [19000/27733], Loss: 3.2838\n",
      "Epoch [80/300], Step [19100/27733], Loss: 3.2596\n",
      "Epoch [80/300], Step [19200/27733], Loss: 3.4388\n",
      "Epoch [80/300], Step [19300/27733], Loss: 3.4235\n",
      "Epoch [80/300], Step [19400/27733], Loss: 2.8185\n",
      "Epoch [80/300], Step [19500/27733], Loss: 1.7234\n",
      "Epoch [80/300], Step [19600/27733], Loss: 2.8677\n",
      "Epoch [80/300], Step [19700/27733], Loss: 2.5704\n",
      "Epoch [80/300], Step [19800/27733], Loss: 2.2397\n",
      "Epoch [80/300], Step [19900/27733], Loss: 2.8396\n",
      "Epoch [80/300], Step [20000/27733], Loss: 3.3371\n",
      "Epoch [80/300], Step [20100/27733], Loss: 2.9227\n",
      "Epoch [80/300], Step [20200/27733], Loss: 2.5154\n",
      "Epoch [80/300], Step [20300/27733], Loss: 3.1580\n",
      "Epoch [80/300], Step [20400/27733], Loss: 2.3721\n",
      "Epoch [80/300], Step [20500/27733], Loss: 3.3993\n",
      "Epoch [80/300], Step [20600/27733], Loss: 2.4942\n",
      "Epoch [80/300], Step [20700/27733], Loss: 3.2309\n",
      "Epoch [80/300], Step [20800/27733], Loss: 2.3410\n",
      "Epoch [80/300], Step [20900/27733], Loss: 3.2336\n",
      "Epoch [80/300], Step [21000/27733], Loss: 3.1557\n",
      "Epoch [80/300], Step [21100/27733], Loss: 3.4270\n",
      "Epoch [80/300], Step [21200/27733], Loss: 3.0492\n",
      "Epoch [80/300], Step [21300/27733], Loss: 2.9110\n",
      "Epoch [80/300], Step [21400/27733], Loss: 2.8464\n",
      "Epoch [80/300], Step [21500/27733], Loss: 2.5679\n",
      "Epoch [80/300], Step [21600/27733], Loss: 2.1980\n",
      "Epoch [80/300], Step [21700/27733], Loss: 3.3032\n",
      "Epoch [80/300], Step [21800/27733], Loss: 3.4428\n",
      "Epoch [80/300], Step [21900/27733], Loss: 2.7619\n",
      "Epoch [80/300], Step [22000/27733], Loss: 2.6004\n",
      "Epoch [80/300], Step [22100/27733], Loss: 3.2944\n",
      "Epoch [80/300], Step [22200/27733], Loss: 2.1049\n",
      "Epoch [80/300], Step [22300/27733], Loss: 3.9079\n",
      "Epoch [80/300], Step [22400/27733], Loss: 4.0072\n",
      "Epoch [80/300], Step [22500/27733], Loss: 2.6818\n",
      "Epoch [80/300], Step [22600/27733], Loss: 3.8084\n",
      "Epoch [80/300], Step [22700/27733], Loss: 2.0685\n",
      "Epoch [80/300], Step [22800/27733], Loss: 2.2746\n",
      "Epoch [80/300], Step [22900/27733], Loss: 2.9719\n",
      "Epoch [80/300], Step [23000/27733], Loss: 2.5679\n",
      "Epoch [80/300], Step [23100/27733], Loss: 1.8813\n",
      "Epoch [80/300], Step [23200/27733], Loss: 3.2890\n",
      "Epoch [80/300], Step [23300/27733], Loss: 3.2027\n",
      "Epoch [80/300], Step [23400/27733], Loss: 2.3920\n",
      "Epoch [80/300], Step [23500/27733], Loss: 2.9149\n",
      "Epoch [80/300], Step [23600/27733], Loss: 2.8087\n",
      "Epoch [80/300], Step [23700/27733], Loss: 3.1841\n",
      "Epoch [80/300], Step [23800/27733], Loss: 2.6579\n",
      "Epoch [80/300], Step [23900/27733], Loss: 2.2940\n",
      "Epoch [80/300], Step [24000/27733], Loss: 2.7212\n",
      "Epoch [80/300], Step [24100/27733], Loss: 2.9604\n",
      "Epoch [80/300], Step [24200/27733], Loss: 2.9176\n",
      "Epoch [80/300], Step [24300/27733], Loss: 3.7956\n",
      "Epoch [80/300], Step [24400/27733], Loss: 2.5168\n",
      "Epoch [80/300], Step [24500/27733], Loss: 2.9681\n",
      "Epoch [80/300], Step [24600/27733], Loss: 2.7985\n",
      "Epoch [80/300], Step [24700/27733], Loss: 2.7490\n",
      "Epoch [80/300], Step [24800/27733], Loss: 2.8722\n",
      "Epoch [80/300], Step [24900/27733], Loss: 2.9275\n",
      "Epoch [80/300], Step [25000/27733], Loss: 1.7355\n",
      "Epoch [80/300], Step [25100/27733], Loss: 2.3567\n",
      "Epoch [80/300], Step [25200/27733], Loss: 2.9680\n",
      "Epoch [80/300], Step [25300/27733], Loss: 2.2534\n",
      "Epoch [80/300], Step [25400/27733], Loss: 3.5305\n",
      "Epoch [80/300], Step [25500/27733], Loss: 3.0163\n",
      "Epoch [80/300], Step [25600/27733], Loss: 2.0936\n",
      "Epoch [80/300], Step [25700/27733], Loss: 2.8576\n",
      "Epoch [80/300], Step [25800/27733], Loss: 3.3498\n",
      "Epoch [80/300], Step [25900/27733], Loss: 2.9430\n",
      "Epoch [80/300], Step [26000/27733], Loss: 3.5551\n",
      "Epoch [80/300], Step [26100/27733], Loss: 2.6022\n",
      "Epoch [80/300], Step [26200/27733], Loss: 2.1920\n",
      "Epoch [80/300], Step [26300/27733], Loss: 2.7720\n",
      "Epoch [80/300], Step [26400/27733], Loss: 2.3679\n",
      "Epoch [80/300], Step [26500/27733], Loss: 2.7713\n",
      "Epoch [80/300], Step [26600/27733], Loss: 4.3628\n",
      "Epoch [80/300], Step [26700/27733], Loss: 3.1051\n",
      "Epoch [80/300], Step [26800/27733], Loss: 2.9932\n",
      "Epoch [80/300], Step [26900/27733], Loss: 3.2626\n",
      "Epoch [80/300], Step [27000/27733], Loss: 2.6881\n",
      "Epoch [80/300], Step [27100/27733], Loss: 3.1781\n",
      "Epoch [80/300], Step [27200/27733], Loss: 2.3330\n",
      "Epoch [80/300], Step [27300/27733], Loss: 3.3892\n",
      "Epoch [80/300], Step [27400/27733], Loss: 2.7174\n",
      "Epoch [80/300], Step [27500/27733], Loss: 2.8283\n",
      "Epoch [80/300], Step [27600/27733], Loss: 2.8044\n",
      "Epoch [80/300], Step [27700/27733], Loss: 2.2437\n",
      "Epoch [81/300], Step [100/27733], Loss: 2.6430\n",
      "Epoch [81/300], Step [200/27733], Loss: 2.2414\n",
      "Epoch [81/300], Step [300/27733], Loss: 2.7055\n",
      "Epoch [81/300], Step [400/27733], Loss: 2.5247\n",
      "Epoch [81/300], Step [500/27733], Loss: 2.5248\n",
      "Epoch [81/300], Step [600/27733], Loss: 2.5227\n",
      "Epoch [81/300], Step [700/27733], Loss: 2.6296\n",
      "Epoch [81/300], Step [800/27733], Loss: 2.4272\n",
      "Epoch [81/300], Step [900/27733], Loss: 2.2410\n",
      "Epoch [81/300], Step [1000/27733], Loss: 2.7564\n",
      "Epoch [81/300], Step [1100/27733], Loss: 2.5277\n",
      "Epoch [81/300], Step [1200/27733], Loss: 2.0502\n",
      "Epoch [81/300], Step [1300/27733], Loss: 2.5537\n",
      "Epoch [81/300], Step [1400/27733], Loss: 3.3235\n",
      "Epoch [81/300], Step [1500/27733], Loss: 2.3451\n",
      "Epoch [81/300], Step [1600/27733], Loss: 2.1538\n",
      "Epoch [81/300], Step [1700/27733], Loss: 2.3195\n",
      "Epoch [81/300], Step [1800/27733], Loss: 2.5925\n",
      "Epoch [81/300], Step [1900/27733], Loss: 2.0607\n",
      "Epoch [81/300], Step [2000/27733], Loss: 2.3685\n",
      "Epoch [81/300], Step [2100/27733], Loss: 1.5433\n",
      "Epoch [81/300], Step [2200/27733], Loss: 1.8551\n",
      "Epoch [81/300], Step [2300/27733], Loss: 1.9999\n",
      "Epoch [81/300], Step [2400/27733], Loss: 3.3782\n",
      "Epoch [81/300], Step [2500/27733], Loss: 2.8549\n",
      "Epoch [81/300], Step [2600/27733], Loss: 2.6164\n",
      "Epoch [81/300], Step [2700/27733], Loss: 2.7467\n",
      "Epoch [81/300], Step [2800/27733], Loss: 2.1496\n",
      "Epoch [81/300], Step [2900/27733], Loss: 2.8713\n",
      "Epoch [81/300], Step [3000/27733], Loss: 1.8002\n",
      "Epoch [81/300], Step [3100/27733], Loss: 2.1288\n",
      "Epoch [81/300], Step [3200/27733], Loss: 2.8923\n",
      "Epoch [81/300], Step [3300/27733], Loss: 2.8860\n",
      "Epoch [81/300], Step [3400/27733], Loss: 2.7020\n",
      "Epoch [81/300], Step [3500/27733], Loss: 2.0959\n",
      "Epoch [81/300], Step [3600/27733], Loss: 2.2781\n",
      "Epoch [81/300], Step [3700/27733], Loss: 2.6264\n",
      "Epoch [81/300], Step [3800/27733], Loss: 2.6548\n",
      "Epoch [81/300], Step [3900/27733], Loss: 3.1926\n",
      "Epoch [81/300], Step [4000/27733], Loss: 2.0144\n",
      "Epoch [81/300], Step [4100/27733], Loss: 2.5773\n",
      "Epoch [81/300], Step [4200/27733], Loss: 2.1714\n",
      "Epoch [81/300], Step [4300/27733], Loss: 2.8802\n",
      "Epoch [81/300], Step [4400/27733], Loss: 3.0439\n",
      "Epoch [81/300], Step [4500/27733], Loss: 3.1881\n",
      "Epoch [81/300], Step [4600/27733], Loss: 1.8364\n",
      "Epoch [81/300], Step [4700/27733], Loss: 1.9414\n",
      "Epoch [81/300], Step [4800/27733], Loss: 2.0996\n",
      "Epoch [81/300], Step [4900/27733], Loss: 2.9353\n",
      "Epoch [81/300], Step [5000/27733], Loss: 3.1813\n",
      "Epoch [81/300], Step [5100/27733], Loss: 2.5751\n",
      "Epoch [81/300], Step [5200/27733], Loss: 2.2257\n",
      "Epoch [81/300], Step [5300/27733], Loss: 3.3408\n",
      "Epoch [81/300], Step [5400/27733], Loss: 3.1001\n",
      "Epoch [81/300], Step [5500/27733], Loss: 2.1326\n",
      "Epoch [81/300], Step [5600/27733], Loss: 2.5296\n",
      "Epoch [81/300], Step [5700/27733], Loss: 2.2415\n",
      "Epoch [81/300], Step [5800/27733], Loss: 2.5089\n",
      "Epoch [81/300], Step [5900/27733], Loss: 3.1315\n",
      "Epoch [81/300], Step [6000/27733], Loss: 2.5811\n",
      "Epoch [81/300], Step [6100/27733], Loss: 2.2350\n",
      "Epoch [81/300], Step [6200/27733], Loss: 2.6800\n",
      "Epoch [81/300], Step [6300/27733], Loss: 2.7753\n",
      "Epoch [81/300], Step [6400/27733], Loss: 2.5063\n",
      "Epoch [81/300], Step [6500/27733], Loss: 2.0459\n",
      "Epoch [81/300], Step [6600/27733], Loss: 1.6627\n",
      "Epoch [81/300], Step [6700/27733], Loss: 2.5998\n",
      "Epoch [81/300], Step [6800/27733], Loss: 2.1602\n",
      "Epoch [81/300], Step [6900/27733], Loss: 2.7521\n",
      "Epoch [81/300], Step [7000/27733], Loss: 2.4566\n",
      "Epoch [81/300], Step [7100/27733], Loss: 2.9889\n",
      "Epoch [81/300], Step [7200/27733], Loss: 3.2324\n",
      "Epoch [81/300], Step [7300/27733], Loss: 2.2509\n",
      "Epoch [81/300], Step [7400/27733], Loss: 2.6295\n",
      "Epoch [81/300], Step [7500/27733], Loss: 2.0887\n",
      "Epoch [81/300], Step [7600/27733], Loss: 2.7840\n",
      "Epoch [81/300], Step [7700/27733], Loss: 2.5350\n",
      "Epoch [81/300], Step [7800/27733], Loss: 2.8982\n",
      "Epoch [81/300], Step [7900/27733], Loss: 2.6369\n",
      "Epoch [81/300], Step [8000/27733], Loss: 2.5053\n",
      "Epoch [81/300], Step [8100/27733], Loss: 2.1882\n",
      "Epoch [81/300], Step [8200/27733], Loss: 1.8119\n",
      "Epoch [81/300], Step [8300/27733], Loss: 3.1443\n",
      "Epoch [81/300], Step [8400/27733], Loss: 3.0085\n",
      "Epoch [81/300], Step [8500/27733], Loss: 2.7965\n",
      "Epoch [81/300], Step [8600/27733], Loss: 2.2881\n",
      "Epoch [81/300], Step [8700/27733], Loss: 2.2923\n",
      "Epoch [81/300], Step [8800/27733], Loss: 2.4082\n",
      "Epoch [81/300], Step [8900/27733], Loss: 2.2505\n",
      "Epoch [81/300], Step [9000/27733], Loss: 2.3100\n",
      "Epoch [81/300], Step [9100/27733], Loss: 1.9500\n",
      "Epoch [81/300], Step [9200/27733], Loss: 1.7668\n",
      "Epoch [81/300], Step [9300/27733], Loss: 3.2397\n",
      "Epoch [81/300], Step [9400/27733], Loss: 3.1345\n",
      "Epoch [81/300], Step [9500/27733], Loss: 2.2287\n",
      "Epoch [81/300], Step [9600/27733], Loss: 2.8776\n",
      "Epoch [81/300], Step [9700/27733], Loss: 3.4089\n",
      "Epoch [81/300], Step [9800/27733], Loss: 2.7275\n",
      "Epoch [81/300], Step [9900/27733], Loss: 3.1649\n",
      "Epoch [81/300], Step [10000/27733], Loss: 2.4698\n",
      "Epoch [81/300], Step [10100/27733], Loss: 2.1342\n",
      "Epoch [81/300], Step [10200/27733], Loss: 2.1311\n",
      "Epoch [81/300], Step [10300/27733], Loss: 3.0840\n",
      "Epoch [81/300], Step [10400/27733], Loss: 3.5994\n",
      "Epoch [81/300], Step [10500/27733], Loss: 2.9148\n",
      "Epoch [81/300], Step [10600/27733], Loss: 3.1785\n",
      "Epoch [81/300], Step [10700/27733], Loss: 2.5431\n",
      "Epoch [81/300], Step [10800/27733], Loss: 2.2759\n",
      "Epoch [81/300], Step [10900/27733], Loss: 3.0087\n",
      "Epoch [81/300], Step [11000/27733], Loss: 2.0105\n",
      "Epoch [81/300], Step [11100/27733], Loss: 3.2733\n",
      "Epoch [81/300], Step [11200/27733], Loss: 2.9700\n",
      "Epoch [81/300], Step [11300/27733], Loss: 2.3009\n",
      "Epoch [81/300], Step [11400/27733], Loss: 3.1079\n",
      "Epoch [81/300], Step [11500/27733], Loss: 3.0325\n",
      "Epoch [81/300], Step [11600/27733], Loss: 2.0309\n",
      "Epoch [81/300], Step [11700/27733], Loss: 3.7349\n",
      "Epoch [81/300], Step [11800/27733], Loss: 2.3495\n",
      "Epoch [81/300], Step [11900/27733], Loss: 2.1212\n",
      "Epoch [81/300], Step [12000/27733], Loss: 2.8578\n",
      "Epoch [81/300], Step [12100/27733], Loss: 3.0262\n",
      "Epoch [81/300], Step [12200/27733], Loss: 2.5559\n",
      "Epoch [81/300], Step [12300/27733], Loss: 2.9432\n",
      "Epoch [81/300], Step [12400/27733], Loss: 2.1444\n",
      "Epoch [81/300], Step [12500/27733], Loss: 2.3624\n",
      "Epoch [81/300], Step [12600/27733], Loss: 2.5155\n",
      "Epoch [81/300], Step [12700/27733], Loss: 3.0027\n",
      "Epoch [81/300], Step [12800/27733], Loss: 2.2958\n",
      "Epoch [81/300], Step [12900/27733], Loss: 2.1750\n",
      "Epoch [81/300], Step [13000/27733], Loss: 3.3501\n",
      "Epoch [81/300], Step [13100/27733], Loss: 2.8986\n",
      "Epoch [81/300], Step [13200/27733], Loss: 2.9394\n",
      "Epoch [81/300], Step [13300/27733], Loss: 3.3685\n",
      "Epoch [81/300], Step [13400/27733], Loss: 3.2538\n",
      "Epoch [81/300], Step [13500/27733], Loss: 2.7109\n",
      "Epoch [81/300], Step [13600/27733], Loss: 2.2927\n",
      "Epoch [81/300], Step [13700/27733], Loss: 3.0414\n",
      "Epoch [81/300], Step [13800/27733], Loss: 2.7779\n",
      "Epoch [81/300], Step [13900/27733], Loss: 3.2032\n",
      "Epoch [81/300], Step [14000/27733], Loss: 2.9743\n",
      "Epoch [81/300], Step [14100/27733], Loss: 2.4065\n",
      "Epoch [81/300], Step [14200/27733], Loss: 3.1591\n",
      "Epoch [81/300], Step [14300/27733], Loss: 2.6507\n",
      "Epoch [81/300], Step [14400/27733], Loss: 1.6735\n",
      "Epoch [81/300], Step [14500/27733], Loss: 2.6519\n",
      "Epoch [81/300], Step [14600/27733], Loss: 2.8258\n",
      "Epoch [81/300], Step [14700/27733], Loss: 2.4282\n",
      "Epoch [81/300], Step [14800/27733], Loss: 2.2996\n",
      "Epoch [81/300], Step [14900/27733], Loss: 2.1677\n",
      "Epoch [81/300], Step [15000/27733], Loss: 2.3125\n",
      "Epoch [81/300], Step [15100/27733], Loss: 2.4539\n",
      "Epoch [81/300], Step [15200/27733], Loss: 2.2082\n",
      "Epoch [81/300], Step [15300/27733], Loss: 2.8746\n",
      "Epoch [81/300], Step [15400/27733], Loss: 3.2703\n",
      "Epoch [81/300], Step [15500/27733], Loss: 2.5061\n",
      "Epoch [81/300], Step [15600/27733], Loss: 2.4323\n",
      "Epoch [81/300], Step [15700/27733], Loss: 3.5423\n",
      "Epoch [81/300], Step [15800/27733], Loss: 2.6880\n",
      "Epoch [81/300], Step [15900/27733], Loss: 2.7853\n",
      "Epoch [81/300], Step [16000/27733], Loss: 2.8489\n",
      "Epoch [81/300], Step [16100/27733], Loss: 3.3290\n",
      "Epoch [81/300], Step [16200/27733], Loss: 2.8754\n",
      "Epoch [81/300], Step [16300/27733], Loss: 2.6923\n",
      "Epoch [81/300], Step [16400/27733], Loss: 3.2062\n",
      "Epoch [81/300], Step [16500/27733], Loss: 2.6717\n",
      "Epoch [81/300], Step [16600/27733], Loss: 2.7659\n",
      "Epoch [81/300], Step [16700/27733], Loss: 2.2298\n",
      "Epoch [81/300], Step [16800/27733], Loss: 2.7206\n",
      "Epoch [81/300], Step [16900/27733], Loss: 2.4981\n",
      "Epoch [81/300], Step [17000/27733], Loss: 2.8360\n",
      "Epoch [81/300], Step [17100/27733], Loss: 2.2418\n",
      "Epoch [81/300], Step [17200/27733], Loss: 3.0056\n",
      "Epoch [81/300], Step [17300/27733], Loss: 3.3652\n",
      "Epoch [81/300], Step [17400/27733], Loss: 1.8368\n",
      "Epoch [81/300], Step [17500/27733], Loss: 3.0553\n",
      "Epoch [81/300], Step [17600/27733], Loss: 3.6425\n",
      "Epoch [81/300], Step [17700/27733], Loss: 2.9998\n",
      "Epoch [81/300], Step [17800/27733], Loss: 2.9670\n",
      "Epoch [81/300], Step [17900/27733], Loss: 3.2612\n",
      "Epoch [81/300], Step [18000/27733], Loss: 2.7139\n",
      "Epoch [81/300], Step [18100/27733], Loss: 2.5298\n",
      "Epoch [81/300], Step [18200/27733], Loss: 3.2733\n",
      "Epoch [81/300], Step [18300/27733], Loss: 2.9194\n",
      "Epoch [81/300], Step [18400/27733], Loss: 2.6122\n",
      "Epoch [81/300], Step [18500/27733], Loss: 2.5655\n",
      "Epoch [81/300], Step [18600/27733], Loss: 3.3143\n",
      "Epoch [81/300], Step [18700/27733], Loss: 2.6752\n",
      "Epoch [81/300], Step [18800/27733], Loss: 2.2045\n",
      "Epoch [81/300], Step [18900/27733], Loss: 3.0707\n",
      "Epoch [81/300], Step [19000/27733], Loss: 2.8008\n",
      "Epoch [81/300], Step [19100/27733], Loss: 2.6446\n",
      "Epoch [81/300], Step [19200/27733], Loss: 3.5931\n",
      "Epoch [81/300], Step [19300/27733], Loss: 2.4967\n",
      "Epoch [81/300], Step [19400/27733], Loss: 3.0534\n",
      "Epoch [81/300], Step [19500/27733], Loss: 3.0106\n",
      "Epoch [81/300], Step [19600/27733], Loss: 2.6359\n",
      "Epoch [81/300], Step [19700/27733], Loss: 3.2423\n",
      "Epoch [81/300], Step [19800/27733], Loss: 2.7944\n",
      "Epoch [81/300], Step [19900/27733], Loss: 3.5108\n",
      "Epoch [81/300], Step [20000/27733], Loss: 3.0076\n",
      "Epoch [81/300], Step [20100/27733], Loss: 2.8228\n",
      "Epoch [81/300], Step [20200/27733], Loss: 2.6896\n",
      "Epoch [81/300], Step [20300/27733], Loss: 2.6429\n",
      "Epoch [81/300], Step [20400/27733], Loss: 2.8350\n",
      "Epoch [81/300], Step [20500/27733], Loss: 2.1944\n",
      "Epoch [81/300], Step [20600/27733], Loss: 2.2791\n",
      "Epoch [81/300], Step [20700/27733], Loss: 2.1160\n",
      "Epoch [81/300], Step [20800/27733], Loss: 2.9552\n",
      "Epoch [81/300], Step [20900/27733], Loss: 2.4797\n",
      "Epoch [81/300], Step [21000/27733], Loss: 2.9846\n",
      "Epoch [81/300], Step [21100/27733], Loss: 3.4491\n",
      "Epoch [81/300], Step [21200/27733], Loss: 3.4080\n",
      "Epoch [81/300], Step [21300/27733], Loss: 2.5840\n",
      "Epoch [81/300], Step [21400/27733], Loss: 3.2409\n",
      "Epoch [81/300], Step [21500/27733], Loss: 3.5737\n",
      "Epoch [81/300], Step [21600/27733], Loss: 2.3062\n",
      "Epoch [81/300], Step [21700/27733], Loss: 2.6350\n",
      "Epoch [81/300], Step [21800/27733], Loss: 2.9431\n",
      "Epoch [81/300], Step [21900/27733], Loss: 3.1243\n",
      "Epoch [81/300], Step [22000/27733], Loss: 3.0057\n",
      "Epoch [81/300], Step [22100/27733], Loss: 2.2082\n",
      "Epoch [81/300], Step [22200/27733], Loss: 2.7478\n",
      "Epoch [81/300], Step [22300/27733], Loss: 2.5414\n",
      "Epoch [81/300], Step [22400/27733], Loss: 2.5795\n",
      "Epoch [81/300], Step [22500/27733], Loss: 3.6131\n",
      "Epoch [81/300], Step [22600/27733], Loss: 3.1052\n",
      "Epoch [81/300], Step [22700/27733], Loss: 2.8993\n",
      "Epoch [81/300], Step [22800/27733], Loss: 3.6949\n",
      "Epoch [81/300], Step [22900/27733], Loss: 3.0494\n",
      "Epoch [81/300], Step [23000/27733], Loss: 3.2586\n",
      "Epoch [81/300], Step [23100/27733], Loss: 3.4236\n",
      "Epoch [81/300], Step [23200/27733], Loss: 2.9424\n",
      "Epoch [81/300], Step [23300/27733], Loss: 2.9101\n",
      "Epoch [81/300], Step [23400/27733], Loss: 2.5577\n",
      "Epoch [81/300], Step [23500/27733], Loss: 2.7321\n",
      "Epoch [81/300], Step [23600/27733], Loss: 2.7224\n",
      "Epoch [81/300], Step [23700/27733], Loss: 2.4043\n",
      "Epoch [81/300], Step [23800/27733], Loss: 2.3684\n",
      "Epoch [81/300], Step [23900/27733], Loss: 3.8464\n",
      "Epoch [81/300], Step [24000/27733], Loss: 2.5539\n",
      "Epoch [81/300], Step [24100/27733], Loss: 2.9429\n",
      "Epoch [81/300], Step [24200/27733], Loss: 3.4763\n",
      "Epoch [81/300], Step [24300/27733], Loss: 2.6589\n",
      "Epoch [81/300], Step [24400/27733], Loss: 2.6044\n",
      "Epoch [81/300], Step [24500/27733], Loss: 3.3403\n",
      "Epoch [81/300], Step [24600/27733], Loss: 3.1338\n",
      "Epoch [81/300], Step [24700/27733], Loss: 3.5417\n",
      "Epoch [81/300], Step [24800/27733], Loss: 2.5530\n",
      "Epoch [81/300], Step [24900/27733], Loss: 4.0736\n",
      "Epoch [81/300], Step [25000/27733], Loss: 3.2211\n",
      "Epoch [81/300], Step [25100/27733], Loss: 3.6261\n",
      "Epoch [81/300], Step [25200/27733], Loss: 2.1533\n",
      "Epoch [81/300], Step [25300/27733], Loss: 2.8106\n",
      "Epoch [81/300], Step [25400/27733], Loss: 3.5846\n",
      "Epoch [81/300], Step [25500/27733], Loss: 1.9402\n",
      "Epoch [81/300], Step [25600/27733], Loss: 2.7504\n",
      "Epoch [81/300], Step [25700/27733], Loss: 2.9924\n",
      "Epoch [81/300], Step [25800/27733], Loss: 2.3048\n",
      "Epoch [81/300], Step [25900/27733], Loss: 3.7614\n",
      "Epoch [81/300], Step [26000/27733], Loss: 3.4460\n",
      "Epoch [81/300], Step [26100/27733], Loss: 2.4190\n",
      "Epoch [81/300], Step [26200/27733], Loss: 2.6760\n",
      "Epoch [81/300], Step [26300/27733], Loss: 2.5193\n",
      "Epoch [81/300], Step [26400/27733], Loss: 3.4201\n",
      "Epoch [81/300], Step [26500/27733], Loss: 3.2213\n",
      "Epoch [81/300], Step [26600/27733], Loss: 2.8072\n",
      "Epoch [81/300], Step [26700/27733], Loss: 2.8029\n",
      "Epoch [81/300], Step [26800/27733], Loss: 2.7919\n",
      "Epoch [81/300], Step [26900/27733], Loss: 3.6148\n",
      "Epoch [81/300], Step [27000/27733], Loss: 3.3262\n",
      "Epoch [81/300], Step [27100/27733], Loss: 3.4498\n",
      "Epoch [81/300], Step [27200/27733], Loss: 3.6688\n",
      "Epoch [81/300], Step [27300/27733], Loss: 3.0005\n",
      "Epoch [81/300], Step [27400/27733], Loss: 3.1376\n",
      "Epoch [81/300], Step [27500/27733], Loss: 3.0839\n",
      "Epoch [81/300], Step [27600/27733], Loss: 3.6474\n",
      "Epoch [81/300], Step [27700/27733], Loss: 2.1982\n",
      "Epoch [82/300], Step [100/27733], Loss: 1.9449\n",
      "Epoch [82/300], Step [200/27733], Loss: 2.5248\n",
      "Epoch [82/300], Step [300/27733], Loss: 2.0669\n",
      "Epoch [82/300], Step [400/27733], Loss: 2.3616\n",
      "Epoch [82/300], Step [500/27733], Loss: 2.5249\n",
      "Epoch [82/300], Step [600/27733], Loss: 2.4495\n",
      "Epoch [82/300], Step [700/27733], Loss: 2.8903\n",
      "Epoch [82/300], Step [800/27733], Loss: 2.9570\n",
      "Epoch [82/300], Step [900/27733], Loss: 2.4592\n",
      "Epoch [82/300], Step [1000/27733], Loss: 2.0914\n",
      "Epoch [82/300], Step [1100/27733], Loss: 2.3933\n",
      "Epoch [82/300], Step [1200/27733], Loss: 2.9462\n",
      "Epoch [82/300], Step [1300/27733], Loss: 2.1168\n",
      "Epoch [82/300], Step [1400/27733], Loss: 2.1019\n",
      "Epoch [82/300], Step [1500/27733], Loss: 2.2066\n",
      "Epoch [82/300], Step [1600/27733], Loss: 2.5007\n",
      "Epoch [82/300], Step [1700/27733], Loss: 2.1697\n",
      "Epoch [82/300], Step [1800/27733], Loss: 2.1971\n",
      "Epoch [82/300], Step [1900/27733], Loss: 1.9075\n",
      "Epoch [82/300], Step [2000/27733], Loss: 1.6359\n",
      "Epoch [82/300], Step [2100/27733], Loss: 3.4503\n",
      "Epoch [82/300], Step [2200/27733], Loss: 2.6978\n",
      "Epoch [82/300], Step [2300/27733], Loss: 2.3117\n",
      "Epoch [82/300], Step [2400/27733], Loss: 2.4749\n",
      "Epoch [82/300], Step [2500/27733], Loss: 2.3726\n",
      "Epoch [82/300], Step [2600/27733], Loss: 1.9509\n",
      "Epoch [82/300], Step [2700/27733], Loss: 2.1384\n",
      "Epoch [82/300], Step [2800/27733], Loss: 2.5569\n",
      "Epoch [82/300], Step [2900/27733], Loss: 2.8281\n",
      "Epoch [82/300], Step [3000/27733], Loss: 2.7311\n",
      "Epoch [82/300], Step [3100/27733], Loss: 2.3591\n",
      "Epoch [82/300], Step [3200/27733], Loss: 2.4860\n",
      "Epoch [82/300], Step [3300/27733], Loss: 3.2480\n",
      "Epoch [82/300], Step [3400/27733], Loss: 2.1326\n",
      "Epoch [82/300], Step [3500/27733], Loss: 2.5728\n",
      "Epoch [82/300], Step [3600/27733], Loss: 3.0222\n",
      "Epoch [82/300], Step [3700/27733], Loss: 3.1887\n",
      "Epoch [82/300], Step [3800/27733], Loss: 3.0811\n",
      "Epoch [82/300], Step [3900/27733], Loss: 3.0201\n",
      "Epoch [82/300], Step [4000/27733], Loss: 2.4071\n",
      "Epoch [82/300], Step [4100/27733], Loss: 3.0974\n",
      "Epoch [82/300], Step [4200/27733], Loss: 2.9369\n",
      "Epoch [82/300], Step [4300/27733], Loss: 2.7053\n",
      "Epoch [82/300], Step [4400/27733], Loss: 2.9051\n",
      "Epoch [82/300], Step [4500/27733], Loss: 2.7554\n",
      "Epoch [82/300], Step [4600/27733], Loss: 2.6765\n",
      "Epoch [82/300], Step [4700/27733], Loss: 2.3598\n",
      "Epoch [82/300], Step [4800/27733], Loss: 2.8043\n",
      "Epoch [82/300], Step [4900/27733], Loss: 3.5108\n",
      "Epoch [82/300], Step [5000/27733], Loss: 3.6492\n",
      "Epoch [82/300], Step [5100/27733], Loss: 1.9426\n",
      "Epoch [82/300], Step [5200/27733], Loss: 2.5936\n",
      "Epoch [82/300], Step [5300/27733], Loss: 3.0819\n",
      "Epoch [82/300], Step [5400/27733], Loss: 2.8482\n",
      "Epoch [82/300], Step [5500/27733], Loss: 2.8056\n",
      "Epoch [82/300], Step [5600/27733], Loss: 3.6725\n",
      "Epoch [82/300], Step [5700/27733], Loss: 2.4565\n",
      "Epoch [82/300], Step [5800/27733], Loss: 2.8909\n",
      "Epoch [82/300], Step [5900/27733], Loss: 3.5356\n",
      "Epoch [82/300], Step [6000/27733], Loss: 2.3109\n",
      "Epoch [82/300], Step [6100/27733], Loss: 3.2218\n",
      "Epoch [82/300], Step [6200/27733], Loss: 2.5091\n",
      "Epoch [82/300], Step [6300/27733], Loss: 2.8707\n",
      "Epoch [82/300], Step [6400/27733], Loss: 1.9418\n",
      "Epoch [82/300], Step [6500/27733], Loss: 2.0976\n",
      "Epoch [82/300], Step [6600/27733], Loss: 2.7595\n",
      "Epoch [82/300], Step [6700/27733], Loss: 2.9552\n",
      "Epoch [82/300], Step [6800/27733], Loss: 3.2866\n",
      "Epoch [82/300], Step [6900/27733], Loss: 3.0224\n",
      "Epoch [82/300], Step [7000/27733], Loss: 2.9237\n",
      "Epoch [82/300], Step [7100/27733], Loss: 2.9504\n",
      "Epoch [82/300], Step [7200/27733], Loss: 2.8534\n",
      "Epoch [82/300], Step [7300/27733], Loss: 3.3956\n",
      "Epoch [82/300], Step [7400/27733], Loss: 2.3958\n",
      "Epoch [82/300], Step [7500/27733], Loss: 2.1073\n",
      "Epoch [82/300], Step [7600/27733], Loss: 2.1298\n",
      "Epoch [82/300], Step [7700/27733], Loss: 2.5558\n",
      "Epoch [82/300], Step [7800/27733], Loss: 3.0233\n",
      "Epoch [82/300], Step [7900/27733], Loss: 2.3373\n",
      "Epoch [82/300], Step [8000/27733], Loss: 2.5544\n",
      "Epoch [82/300], Step [8100/27733], Loss: 2.7873\n",
      "Epoch [82/300], Step [8200/27733], Loss: 2.6566\n",
      "Epoch [82/300], Step [8300/27733], Loss: 2.7722\n",
      "Epoch [82/300], Step [8400/27733], Loss: 3.1987\n",
      "Epoch [82/300], Step [8500/27733], Loss: 2.6555\n",
      "Epoch [82/300], Step [8600/27733], Loss: 2.0941\n",
      "Epoch [82/300], Step [8700/27733], Loss: 2.8755\n",
      "Epoch [82/300], Step [8800/27733], Loss: 2.2363\n",
      "Epoch [82/300], Step [8900/27733], Loss: 2.6637\n",
      "Epoch [82/300], Step [9000/27733], Loss: 2.7310\n",
      "Epoch [82/300], Step [9100/27733], Loss: 3.5274\n",
      "Epoch [82/300], Step [9200/27733], Loss: 2.5484\n",
      "Epoch [82/300], Step [9300/27733], Loss: 2.2010\n",
      "Epoch [82/300], Step [9400/27733], Loss: 2.7019\n",
      "Epoch [82/300], Step [9500/27733], Loss: 2.9994\n",
      "Epoch [82/300], Step [9600/27733], Loss: 3.5861\n",
      "Epoch [82/300], Step [9700/27733], Loss: 3.3935\n",
      "Epoch [82/300], Step [9800/27733], Loss: 1.9755\n",
      "Epoch [82/300], Step [9900/27733], Loss: 3.0030\n",
      "Epoch [82/300], Step [10000/27733], Loss: 3.0037\n",
      "Epoch [82/300], Step [10100/27733], Loss: 2.8288\n",
      "Epoch [82/300], Step [10200/27733], Loss: 1.9942\n",
      "Epoch [82/300], Step [10300/27733], Loss: 2.5010\n",
      "Epoch [82/300], Step [10400/27733], Loss: 2.6334\n",
      "Epoch [82/300], Step [10500/27733], Loss: 2.6340\n",
      "Epoch [82/300], Step [10600/27733], Loss: 3.3940\n",
      "Epoch [82/300], Step [10700/27733], Loss: 3.1738\n",
      "Epoch [82/300], Step [10800/27733], Loss: 2.8439\n",
      "Epoch [82/300], Step [10900/27733], Loss: 3.0306\n",
      "Epoch [82/300], Step [11000/27733], Loss: 3.3182\n",
      "Epoch [82/300], Step [11100/27733], Loss: 2.2632\n",
      "Epoch [82/300], Step [11200/27733], Loss: 2.8535\n",
      "Epoch [82/300], Step [11300/27733], Loss: 2.3462\n",
      "Epoch [82/300], Step [11400/27733], Loss: 2.7514\n",
      "Epoch [82/300], Step [11500/27733], Loss: 2.9321\n",
      "Epoch [82/300], Step [11600/27733], Loss: 2.2650\n",
      "Epoch [82/300], Step [11700/27733], Loss: 2.4225\n",
      "Epoch [82/300], Step [11800/27733], Loss: 2.8200\n",
      "Epoch [82/300], Step [11900/27733], Loss: 2.3482\n",
      "Epoch [82/300], Step [12000/27733], Loss: 2.2842\n",
      "Epoch [82/300], Step [12100/27733], Loss: 2.6053\n",
      "Epoch [82/300], Step [12200/27733], Loss: 2.4327\n",
      "Epoch [82/300], Step [12300/27733], Loss: 3.6336\n",
      "Epoch [82/300], Step [12400/27733], Loss: 3.1747\n",
      "Epoch [82/300], Step [12500/27733], Loss: 3.2101\n",
      "Epoch [82/300], Step [12600/27733], Loss: 3.0356\n",
      "Epoch [82/300], Step [12700/27733], Loss: 3.6006\n",
      "Epoch [82/300], Step [12800/27733], Loss: 2.9096\n",
      "Epoch [82/300], Step [12900/27733], Loss: 3.8237\n",
      "Epoch [82/300], Step [13000/27733], Loss: 3.0780\n",
      "Epoch [82/300], Step [13100/27733], Loss: 2.9912\n",
      "Epoch [82/300], Step [13200/27733], Loss: 2.9650\n",
      "Epoch [82/300], Step [13300/27733], Loss: 2.9379\n",
      "Epoch [82/300], Step [13400/27733], Loss: 2.9324\n",
      "Epoch [82/300], Step [13500/27733], Loss: 2.4627\n",
      "Epoch [82/300], Step [13600/27733], Loss: 3.1137\n",
      "Epoch [82/300], Step [13700/27733], Loss: 3.0587\n",
      "Epoch [82/300], Step [13800/27733], Loss: 2.2388\n",
      "Epoch [82/300], Step [13900/27733], Loss: 2.8543\n",
      "Epoch [82/300], Step [14000/27733], Loss: 3.1433\n",
      "Epoch [82/300], Step [14100/27733], Loss: 3.1473\n",
      "Epoch [82/300], Step [14200/27733], Loss: 2.3264\n",
      "Epoch [82/300], Step [14300/27733], Loss: 3.1164\n",
      "Epoch [82/300], Step [14400/27733], Loss: 3.1821\n",
      "Epoch [82/300], Step [14500/27733], Loss: 1.8957\n",
      "Epoch [82/300], Step [14600/27733], Loss: 2.3208\n",
      "Epoch [82/300], Step [14700/27733], Loss: 2.4074\n",
      "Epoch [82/300], Step [14800/27733], Loss: 2.4650\n",
      "Epoch [82/300], Step [14900/27733], Loss: 3.7497\n",
      "Epoch [82/300], Step [15000/27733], Loss: 3.0509\n",
      "Epoch [82/300], Step [15100/27733], Loss: 2.0994\n",
      "Epoch [82/300], Step [15200/27733], Loss: 2.6141\n",
      "Epoch [82/300], Step [15300/27733], Loss: 3.0455\n",
      "Epoch [82/300], Step [15400/27733], Loss: 2.2744\n",
      "Epoch [82/300], Step [15500/27733], Loss: 2.8807\n",
      "Epoch [82/300], Step [15600/27733], Loss: 2.6794\n",
      "Epoch [82/300], Step [15700/27733], Loss: 2.8133\n",
      "Epoch [82/300], Step [15800/27733], Loss: 2.7564\n",
      "Epoch [82/300], Step [15900/27733], Loss: 3.0197\n",
      "Epoch [82/300], Step [16000/27733], Loss: 2.5242\n",
      "Epoch [82/300], Step [16100/27733], Loss: 2.5896\n",
      "Epoch [82/300], Step [16200/27733], Loss: 2.5560\n",
      "Epoch [82/300], Step [16300/27733], Loss: 3.3522\n",
      "Epoch [82/300], Step [16400/27733], Loss: 3.1619\n",
      "Epoch [82/300], Step [16500/27733], Loss: 3.7448\n",
      "Epoch [82/300], Step [16600/27733], Loss: 3.0345\n",
      "Epoch [82/300], Step [16700/27733], Loss: 3.2085\n",
      "Epoch [82/300], Step [16800/27733], Loss: 3.0670\n",
      "Epoch [82/300], Step [16900/27733], Loss: 2.0953\n",
      "Epoch [82/300], Step [17000/27733], Loss: 2.6353\n",
      "Epoch [82/300], Step [17100/27733], Loss: 2.6034\n",
      "Epoch [82/300], Step [17200/27733], Loss: 3.8111\n",
      "Epoch [82/300], Step [17300/27733], Loss: 3.4925\n",
      "Epoch [82/300], Step [17400/27733], Loss: 3.2537\n",
      "Epoch [82/300], Step [17500/27733], Loss: 1.9195\n",
      "Epoch [82/300], Step [17600/27733], Loss: 2.2719\n",
      "Epoch [82/300], Step [17700/27733], Loss: 2.6624\n",
      "Epoch [82/300], Step [17800/27733], Loss: 3.0944\n",
      "Epoch [82/300], Step [17900/27733], Loss: 2.7990\n",
      "Epoch [82/300], Step [18000/27733], Loss: 2.8588\n",
      "Epoch [82/300], Step [18100/27733], Loss: 2.4482\n",
      "Epoch [82/300], Step [18200/27733], Loss: 2.7024\n",
      "Epoch [82/300], Step [18300/27733], Loss: 3.3020\n",
      "Epoch [82/300], Step [18400/27733], Loss: 2.4090\n",
      "Epoch [82/300], Step [18500/27733], Loss: 2.9448\n",
      "Epoch [82/300], Step [18600/27733], Loss: 2.1961\n",
      "Epoch [82/300], Step [18700/27733], Loss: 2.9640\n",
      "Epoch [82/300], Step [18800/27733], Loss: 2.9260\n",
      "Epoch [82/300], Step [18900/27733], Loss: 3.1483\n",
      "Epoch [82/300], Step [19000/27733], Loss: 2.4569\n",
      "Epoch [82/300], Step [19100/27733], Loss: 2.9315\n",
      "Epoch [82/300], Step [19200/27733], Loss: 2.2044\n",
      "Epoch [82/300], Step [19300/27733], Loss: 2.6798\n",
      "Epoch [82/300], Step [19400/27733], Loss: 2.8380\n",
      "Epoch [82/300], Step [19500/27733], Loss: 2.8690\n",
      "Epoch [82/300], Step [19600/27733], Loss: 3.6382\n",
      "Epoch [82/300], Step [19700/27733], Loss: 3.2405\n",
      "Epoch [82/300], Step [19800/27733], Loss: 2.9927\n",
      "Epoch [82/300], Step [19900/27733], Loss: 2.8945\n",
      "Epoch [82/300], Step [20000/27733], Loss: 3.3657\n",
      "Epoch [82/300], Step [20100/27733], Loss: 2.9531\n",
      "Epoch [82/300], Step [20200/27733], Loss: 3.2320\n",
      "Epoch [82/300], Step [20300/27733], Loss: 3.4740\n",
      "Epoch [82/300], Step [20400/27733], Loss: 3.8941\n",
      "Epoch [82/300], Step [20500/27733], Loss: 3.7647\n",
      "Epoch [82/300], Step [20600/27733], Loss: 2.9252\n",
      "Epoch [82/300], Step [20700/27733], Loss: 2.8244\n",
      "Epoch [82/300], Step [20800/27733], Loss: 3.3968\n",
      "Epoch [82/300], Step [20900/27733], Loss: 2.8395\n",
      "Epoch [82/300], Step [21000/27733], Loss: 2.7345\n",
      "Epoch [82/300], Step [21100/27733], Loss: 3.7765\n",
      "Epoch [82/300], Step [21200/27733], Loss: 2.0916\n",
      "Epoch [82/300], Step [21300/27733], Loss: 2.6651\n",
      "Epoch [82/300], Step [21400/27733], Loss: 2.1039\n",
      "Epoch [82/300], Step [21500/27733], Loss: 2.9060\n",
      "Epoch [82/300], Step [21600/27733], Loss: 2.6855\n",
      "Epoch [82/300], Step [21700/27733], Loss: 2.8776\n",
      "Epoch [82/300], Step [21800/27733], Loss: 2.3192\n",
      "Epoch [82/300], Step [21900/27733], Loss: 3.4552\n",
      "Epoch [82/300], Step [22000/27733], Loss: 2.7495\n",
      "Epoch [82/300], Step [22100/27733], Loss: 3.0967\n",
      "Epoch [82/300], Step [22200/27733], Loss: 3.5704\n",
      "Epoch [82/300], Step [22300/27733], Loss: 2.5224\n",
      "Epoch [82/300], Step [22400/27733], Loss: 3.8438\n",
      "Epoch [82/300], Step [22500/27733], Loss: 2.6834\n",
      "Epoch [82/300], Step [22600/27733], Loss: 2.2894\n",
      "Epoch [82/300], Step [22700/27733], Loss: 3.1116\n",
      "Epoch [82/300], Step [22800/27733], Loss: 2.8903\n",
      "Epoch [82/300], Step [22900/27733], Loss: 2.9272\n",
      "Epoch [82/300], Step [23000/27733], Loss: 2.9545\n",
      "Epoch [82/300], Step [23100/27733], Loss: 3.0946\n",
      "Epoch [82/300], Step [23200/27733], Loss: 2.7815\n",
      "Epoch [82/300], Step [23300/27733], Loss: 2.5779\n",
      "Epoch [82/300], Step [23400/27733], Loss: 2.4378\n",
      "Epoch [82/300], Step [23500/27733], Loss: 2.2562\n",
      "Epoch [82/300], Step [23600/27733], Loss: 3.0180\n",
      "Epoch [82/300], Step [23700/27733], Loss: 2.6994\n",
      "Epoch [82/300], Step [23800/27733], Loss: 3.1544\n",
      "Epoch [82/300], Step [23900/27733], Loss: 2.1454\n",
      "Epoch [82/300], Step [24000/27733], Loss: 3.0898\n",
      "Epoch [82/300], Step [24100/27733], Loss: 3.4004\n",
      "Epoch [82/300], Step [24200/27733], Loss: 2.5359\n",
      "Epoch [82/300], Step [24300/27733], Loss: 3.5402\n",
      "Epoch [82/300], Step [24400/27733], Loss: 3.0752\n",
      "Epoch [82/300], Step [24500/27733], Loss: 3.4612\n",
      "Epoch [82/300], Step [24600/27733], Loss: 3.0051\n",
      "Epoch [82/300], Step [24700/27733], Loss: 2.3682\n",
      "Epoch [82/300], Step [24800/27733], Loss: 2.4997\n",
      "Epoch [82/300], Step [24900/27733], Loss: 3.5769\n",
      "Epoch [82/300], Step [25000/27733], Loss: 3.2272\n",
      "Epoch [82/300], Step [25100/27733], Loss: 3.0105\n",
      "Epoch [82/300], Step [25200/27733], Loss: 3.1244\n",
      "Epoch [82/300], Step [25300/27733], Loss: 2.1135\n",
      "Epoch [82/300], Step [25400/27733], Loss: 2.6929\n",
      "Epoch [82/300], Step [25500/27733], Loss: 1.6294\n",
      "Epoch [82/300], Step [25600/27733], Loss: 2.7406\n",
      "Epoch [82/300], Step [25700/27733], Loss: 2.8992\n",
      "Epoch [82/300], Step [25800/27733], Loss: 3.2520\n",
      "Epoch [82/300], Step [25900/27733], Loss: 1.9778\n",
      "Epoch [82/300], Step [26000/27733], Loss: 2.6739\n",
      "Epoch [82/300], Step [26100/27733], Loss: 2.7998\n",
      "Epoch [82/300], Step [26200/27733], Loss: 2.9170\n",
      "Epoch [82/300], Step [26300/27733], Loss: 2.8191\n",
      "Epoch [82/300], Step [26400/27733], Loss: 3.2242\n",
      "Epoch [82/300], Step [26500/27733], Loss: 3.1375\n",
      "Epoch [82/300], Step [26600/27733], Loss: 3.2062\n",
      "Epoch [82/300], Step [26700/27733], Loss: 3.2457\n",
      "Epoch [82/300], Step [26800/27733], Loss: 2.8821\n",
      "Epoch [82/300], Step [26900/27733], Loss: 3.3993\n",
      "Epoch [82/300], Step [27000/27733], Loss: 3.1163\n",
      "Epoch [82/300], Step [27100/27733], Loss: 3.3919\n",
      "Epoch [82/300], Step [27200/27733], Loss: 2.9028\n",
      "Epoch [82/300], Step [27300/27733], Loss: 1.9552\n",
      "Epoch [82/300], Step [27400/27733], Loss: 3.4393\n",
      "Epoch [82/300], Step [27500/27733], Loss: 3.8583\n",
      "Epoch [82/300], Step [27600/27733], Loss: 3.5188\n",
      "Epoch [82/300], Step [27700/27733], Loss: 2.7771\n",
      "Epoch [83/300], Step [100/27733], Loss: 2.2957\n",
      "Epoch [83/300], Step [200/27733], Loss: 2.7257\n",
      "Epoch [83/300], Step [300/27733], Loss: 2.1235\n",
      "Epoch [83/300], Step [400/27733], Loss: 2.4734\n",
      "Epoch [83/300], Step [500/27733], Loss: 2.6469\n",
      "Epoch [83/300], Step [600/27733], Loss: 2.6762\n",
      "Epoch [83/300], Step [700/27733], Loss: 2.9401\n",
      "Epoch [83/300], Step [800/27733], Loss: 1.9242\n",
      "Epoch [83/300], Step [900/27733], Loss: 1.9974\n",
      "Epoch [83/300], Step [1000/27733], Loss: 2.6810\n",
      "Epoch [83/300], Step [1100/27733], Loss: 2.0746\n",
      "Epoch [83/300], Step [1200/27733], Loss: 2.3691\n",
      "Epoch [83/300], Step [1300/27733], Loss: 2.5052\n",
      "Epoch [83/300], Step [1400/27733], Loss: 1.8245\n",
      "Epoch [83/300], Step [1500/27733], Loss: 1.8344\n",
      "Epoch [83/300], Step [1600/27733], Loss: 2.3245\n",
      "Epoch [83/300], Step [1700/27733], Loss: 1.9575\n",
      "Epoch [83/300], Step [1800/27733], Loss: 1.8025\n",
      "Epoch [83/300], Step [1900/27733], Loss: 2.2540\n",
      "Epoch [83/300], Step [2000/27733], Loss: 2.6257\n",
      "Epoch [83/300], Step [2100/27733], Loss: 3.1755\n",
      "Epoch [83/300], Step [2200/27733], Loss: 2.8380\n",
      "Epoch [83/300], Step [2300/27733], Loss: 2.1507\n",
      "Epoch [83/300], Step [2400/27733], Loss: 2.7940\n",
      "Epoch [83/300], Step [2500/27733], Loss: 3.0146\n",
      "Epoch [83/300], Step [2600/27733], Loss: 2.7018\n",
      "Epoch [83/300], Step [2700/27733], Loss: 2.7313\n",
      "Epoch [83/300], Step [2800/27733], Loss: 2.9496\n",
      "Epoch [83/300], Step [2900/27733], Loss: 3.2191\n",
      "Epoch [83/300], Step [3000/27733], Loss: 2.9198\n",
      "Epoch [83/300], Step [3100/27733], Loss: 2.3493\n",
      "Epoch [83/300], Step [3200/27733], Loss: 2.7842\n",
      "Epoch [83/300], Step [3300/27733], Loss: 2.6231\n",
      "Epoch [83/300], Step [3400/27733], Loss: 2.1776\n",
      "Epoch [83/300], Step [3500/27733], Loss: 2.5043\n",
      "Epoch [83/300], Step [3600/27733], Loss: 2.2772\n",
      "Epoch [83/300], Step [3700/27733], Loss: 2.5853\n",
      "Epoch [83/300], Step [3800/27733], Loss: 2.4766\n",
      "Epoch [83/300], Step [3900/27733], Loss: 2.4510\n",
      "Epoch [83/300], Step [4000/27733], Loss: 2.1770\n",
      "Epoch [83/300], Step [4100/27733], Loss: 2.6995\n",
      "Epoch [83/300], Step [4200/27733], Loss: 2.2291\n",
      "Epoch [83/300], Step [4300/27733], Loss: 2.8032\n",
      "Epoch [83/300], Step [4400/27733], Loss: 2.8322\n",
      "Epoch [83/300], Step [4500/27733], Loss: 1.6803\n",
      "Epoch [83/300], Step [4600/27733], Loss: 3.0109\n",
      "Epoch [83/300], Step [4700/27733], Loss: 1.9709\n",
      "Epoch [83/300], Step [4800/27733], Loss: 3.0304\n",
      "Epoch [83/300], Step [4900/27733], Loss: 2.7573\n",
      "Epoch [83/300], Step [5000/27733], Loss: 2.2103\n",
      "Epoch [83/300], Step [5100/27733], Loss: 2.9558\n",
      "Epoch [83/300], Step [5200/27733], Loss: 2.8086\n",
      "Epoch [83/300], Step [5300/27733], Loss: 2.7448\n",
      "Epoch [83/300], Step [5400/27733], Loss: 2.9434\n",
      "Epoch [83/300], Step [5500/27733], Loss: 3.0724\n",
      "Epoch [83/300], Step [5600/27733], Loss: 3.7847\n",
      "Epoch [83/300], Step [5700/27733], Loss: 2.6683\n",
      "Epoch [83/300], Step [5800/27733], Loss: 2.8552\n",
      "Epoch [83/300], Step [5900/27733], Loss: 2.6091\n",
      "Epoch [83/300], Step [6000/27733], Loss: 2.7460\n",
      "Epoch [83/300], Step [6100/27733], Loss: 3.1113\n",
      "Epoch [83/300], Step [6200/27733], Loss: 2.6653\n",
      "Epoch [83/300], Step [6300/27733], Loss: 2.1111\n",
      "Epoch [83/300], Step [6400/27733], Loss: 3.1221\n",
      "Epoch [83/300], Step [6500/27733], Loss: 2.3583\n",
      "Epoch [83/300], Step [6600/27733], Loss: 2.1060\n",
      "Epoch [83/300], Step [6700/27733], Loss: 2.4180\n",
      "Epoch [83/300], Step [6800/27733], Loss: 2.7968\n",
      "Epoch [83/300], Step [6900/27733], Loss: 2.4276\n",
      "Epoch [83/300], Step [7000/27733], Loss: 3.1534\n",
      "Epoch [83/300], Step [7100/27733], Loss: 2.4033\n",
      "Epoch [83/300], Step [7200/27733], Loss: 2.2668\n",
      "Epoch [83/300], Step [7300/27733], Loss: 3.6703\n",
      "Epoch [83/300], Step [7400/27733], Loss: 2.3745\n",
      "Epoch [83/300], Step [7500/27733], Loss: 1.6902\n",
      "Epoch [83/300], Step [7600/27733], Loss: 2.8630\n",
      "Epoch [83/300], Step [7700/27733], Loss: 3.1590\n",
      "Epoch [83/300], Step [7800/27733], Loss: 2.8123\n",
      "Epoch [83/300], Step [7900/27733], Loss: 2.1770\n",
      "Epoch [83/300], Step [8000/27733], Loss: 3.1776\n",
      "Epoch [83/300], Step [8100/27733], Loss: 2.7023\n",
      "Epoch [83/300], Step [8200/27733], Loss: 2.2872\n",
      "Epoch [83/300], Step [8300/27733], Loss: 2.3993\n",
      "Epoch [83/300], Step [8400/27733], Loss: 2.9850\n",
      "Epoch [83/300], Step [8500/27733], Loss: 1.8601\n",
      "Epoch [83/300], Step [8600/27733], Loss: 3.0572\n",
      "Epoch [83/300], Step [8700/27733], Loss: 2.3471\n",
      "Epoch [83/300], Step [8800/27733], Loss: 2.6346\n",
      "Epoch [83/300], Step [8900/27733], Loss: 3.3289\n",
      "Epoch [83/300], Step [9000/27733], Loss: 2.3209\n",
      "Epoch [83/300], Step [9100/27733], Loss: 3.2903\n",
      "Epoch [83/300], Step [9200/27733], Loss: 2.2175\n",
      "Epoch [83/300], Step [9300/27733], Loss: 2.9629\n",
      "Epoch [83/300], Step [9400/27733], Loss: 2.7922\n",
      "Epoch [83/300], Step [9500/27733], Loss: 2.5596\n",
      "Epoch [83/300], Step [9600/27733], Loss: 3.5739\n",
      "Epoch [83/300], Step [9700/27733], Loss: 2.5804\n",
      "Epoch [83/300], Step [9800/27733], Loss: 3.4254\n",
      "Epoch [83/300], Step [9900/27733], Loss: 3.3121\n",
      "Epoch [83/300], Step [10000/27733], Loss: 2.4738\n",
      "Epoch [83/300], Step [10100/27733], Loss: 2.6675\n",
      "Epoch [83/300], Step [10200/27733], Loss: 2.7323\n",
      "Epoch [83/300], Step [10300/27733], Loss: 2.6267\n",
      "Epoch [83/300], Step [10400/27733], Loss: 2.5326\n",
      "Epoch [83/300], Step [10500/27733], Loss: 2.4436\n",
      "Epoch [83/300], Step [10600/27733], Loss: 2.3535\n",
      "Epoch [83/300], Step [10700/27733], Loss: 2.3962\n",
      "Epoch [83/300], Step [10800/27733], Loss: 2.5611\n",
      "Epoch [83/300], Step [10900/27733], Loss: 2.6541\n",
      "Epoch [83/300], Step [11000/27733], Loss: 2.9333\n",
      "Epoch [83/300], Step [11100/27733], Loss: 2.3705\n",
      "Epoch [83/300], Step [11200/27733], Loss: 2.5580\n",
      "Epoch [83/300], Step [11300/27733], Loss: 2.2995\n",
      "Epoch [83/300], Step [11400/27733], Loss: 2.6909\n",
      "Epoch [83/300], Step [11500/27733], Loss: 2.5473\n",
      "Epoch [83/300], Step [11600/27733], Loss: 2.4001\n",
      "Epoch [83/300], Step [11700/27733], Loss: 2.8288\n",
      "Epoch [83/300], Step [11800/27733], Loss: 2.7635\n",
      "Epoch [83/300], Step [11900/27733], Loss: 2.0582\n",
      "Epoch [83/300], Step [12000/27733], Loss: 3.1712\n",
      "Epoch [83/300], Step [12100/27733], Loss: 2.1427\n",
      "Epoch [83/300], Step [12200/27733], Loss: 1.7520\n",
      "Epoch [83/300], Step [12300/27733], Loss: 2.7820\n",
      "Epoch [83/300], Step [12400/27733], Loss: 2.7466\n",
      "Epoch [83/300], Step [12500/27733], Loss: 2.7595\n",
      "Epoch [83/300], Step [12600/27733], Loss: 3.0119\n",
      "Epoch [83/300], Step [12700/27733], Loss: 2.5012\n",
      "Epoch [83/300], Step [12800/27733], Loss: 2.4386\n",
      "Epoch [83/300], Step [12900/27733], Loss: 3.0566\n",
      "Epoch [83/300], Step [13000/27733], Loss: 2.6412\n",
      "Epoch [83/300], Step [13100/27733], Loss: 3.4322\n",
      "Epoch [83/300], Step [13200/27733], Loss: 3.0888\n",
      "Epoch [83/300], Step [13300/27733], Loss: 2.7011\n",
      "Epoch [83/300], Step [13400/27733], Loss: 2.0893\n",
      "Epoch [83/300], Step [13500/27733], Loss: 2.7994\n",
      "Epoch [83/300], Step [13600/27733], Loss: 2.5599\n",
      "Epoch [83/300], Step [13700/27733], Loss: 3.1154\n",
      "Epoch [83/300], Step [13800/27733], Loss: 1.5493\n",
      "Epoch [83/300], Step [13900/27733], Loss: 3.7874\n",
      "Epoch [83/300], Step [14000/27733], Loss: 2.7911\n",
      "Epoch [83/300], Step [14100/27733], Loss: 2.7175\n",
      "Epoch [83/300], Step [14200/27733], Loss: 4.1680\n",
      "Epoch [83/300], Step [14300/27733], Loss: 2.9406\n",
      "Epoch [83/300], Step [14400/27733], Loss: 2.1618\n",
      "Epoch [83/300], Step [14500/27733], Loss: 2.9296\n",
      "Epoch [83/300], Step [14600/27733], Loss: 3.6721\n",
      "Epoch [83/300], Step [14700/27733], Loss: 3.1676\n",
      "Epoch [83/300], Step [14800/27733], Loss: 2.3908\n",
      "Epoch [83/300], Step [14900/27733], Loss: 2.6828\n",
      "Epoch [83/300], Step [15000/27733], Loss: 2.4618\n",
      "Epoch [83/300], Step [15100/27733], Loss: 2.4289\n",
      "Epoch [83/300], Step [15200/27733], Loss: 2.4968\n",
      "Epoch [83/300], Step [15300/27733], Loss: 2.5860\n",
      "Epoch [83/300], Step [15400/27733], Loss: 3.4893\n",
      "Epoch [83/300], Step [15500/27733], Loss: 2.8927\n",
      "Epoch [83/300], Step [15600/27733], Loss: 2.3752\n",
      "Epoch [83/300], Step [15700/27733], Loss: 2.6514\n",
      "Epoch [83/300], Step [15800/27733], Loss: 3.3248\n",
      "Epoch [83/300], Step [15900/27733], Loss: 2.5583\n",
      "Epoch [83/300], Step [16000/27733], Loss: 2.8536\n",
      "Epoch [83/300], Step [16100/27733], Loss: 2.3745\n",
      "Epoch [83/300], Step [16200/27733], Loss: 2.8375\n",
      "Epoch [83/300], Step [16300/27733], Loss: 1.8101\n",
      "Epoch [83/300], Step [16400/27733], Loss: 2.9813\n",
      "Epoch [83/300], Step [16500/27733], Loss: 3.1154\n",
      "Epoch [83/300], Step [16600/27733], Loss: 2.9157\n",
      "Epoch [83/300], Step [16700/27733], Loss: 2.7777\n",
      "Epoch [83/300], Step [16800/27733], Loss: 2.4408\n",
      "Epoch [83/300], Step [16900/27733], Loss: 2.7092\n",
      "Epoch [83/300], Step [17000/27733], Loss: 3.5914\n",
      "Epoch [83/300], Step [17100/27733], Loss: 3.3030\n",
      "Epoch [83/300], Step [17200/27733], Loss: 2.9904\n",
      "Epoch [83/300], Step [17300/27733], Loss: 1.9777\n",
      "Epoch [83/300], Step [17400/27733], Loss: 2.7657\n",
      "Epoch [83/300], Step [17500/27733], Loss: 2.7478\n",
      "Epoch [83/300], Step [17600/27733], Loss: 2.8986\n",
      "Epoch [83/300], Step [17700/27733], Loss: 3.4339\n",
      "Epoch [83/300], Step [17800/27733], Loss: 2.2386\n",
      "Epoch [83/300], Step [17900/27733], Loss: 3.3897\n",
      "Epoch [83/300], Step [18000/27733], Loss: 3.8388\n",
      "Epoch [83/300], Step [18100/27733], Loss: 3.8283\n",
      "Epoch [83/300], Step [18200/27733], Loss: 4.4137\n",
      "Epoch [83/300], Step [18300/27733], Loss: 3.0243\n",
      "Epoch [83/300], Step [18400/27733], Loss: 2.0567\n",
      "Epoch [83/300], Step [18500/27733], Loss: 2.6098\n",
      "Epoch [83/300], Step [18600/27733], Loss: 3.0847\n",
      "Epoch [83/300], Step [18700/27733], Loss: 3.8049\n",
      "Epoch [83/300], Step [18800/27733], Loss: 2.5724\n",
      "Epoch [83/300], Step [18900/27733], Loss: 3.1021\n",
      "Epoch [83/300], Step [19000/27733], Loss: 2.7718\n",
      "Epoch [83/300], Step [19100/27733], Loss: 2.1367\n",
      "Epoch [83/300], Step [19200/27733], Loss: 2.9088\n",
      "Epoch [83/300], Step [19300/27733], Loss: 2.6002\n",
      "Epoch [83/300], Step [19400/27733], Loss: 2.6208\n",
      "Epoch [83/300], Step [19500/27733], Loss: 3.8026\n",
      "Epoch [83/300], Step [19600/27733], Loss: 3.1294\n",
      "Epoch [83/300], Step [19700/27733], Loss: 2.7116\n",
      "Epoch [83/300], Step [19800/27733], Loss: 3.3634\n",
      "Epoch [83/300], Step [19900/27733], Loss: 3.1937\n",
      "Epoch [83/300], Step [20000/27733], Loss: 3.2212\n",
      "Epoch [83/300], Step [20100/27733], Loss: 2.7208\n",
      "Epoch [83/300], Step [20200/27733], Loss: 3.4547\n",
      "Epoch [83/300], Step [20300/27733], Loss: 3.2848\n",
      "Epoch [83/300], Step [20400/27733], Loss: 3.2450\n",
      "Epoch [83/300], Step [20500/27733], Loss: 2.6947\n",
      "Epoch [83/300], Step [20600/27733], Loss: 2.2510\n",
      "Epoch [83/300], Step [20700/27733], Loss: 2.7069\n",
      "Epoch [83/300], Step [20800/27733], Loss: 2.4888\n",
      "Epoch [83/300], Step [20900/27733], Loss: 2.2435\n",
      "Epoch [83/300], Step [21000/27733], Loss: 3.0057\n",
      "Epoch [83/300], Step [21100/27733], Loss: 2.6760\n",
      "Epoch [83/300], Step [21200/27733], Loss: 2.8044\n",
      "Epoch [83/300], Step [21300/27733], Loss: 3.2462\n",
      "Epoch [83/300], Step [21400/27733], Loss: 2.2417\n",
      "Epoch [83/300], Step [21500/27733], Loss: 2.7106\n",
      "Epoch [83/300], Step [21600/27733], Loss: 3.2237\n",
      "Epoch [83/300], Step [21700/27733], Loss: 2.9726\n",
      "Epoch [83/300], Step [21800/27733], Loss: 2.6387\n",
      "Epoch [83/300], Step [21900/27733], Loss: 3.4146\n",
      "Epoch [83/300], Step [22000/27733], Loss: 3.1718\n",
      "Epoch [83/300], Step [22100/27733], Loss: 2.8651\n",
      "Epoch [83/300], Step [22200/27733], Loss: 3.1506\n",
      "Epoch [83/300], Step [22300/27733], Loss: 2.6679\n",
      "Epoch [83/300], Step [22400/27733], Loss: 3.5716\n",
      "Epoch [83/300], Step [22500/27733], Loss: 2.8916\n",
      "Epoch [83/300], Step [22600/27733], Loss: 3.1450\n",
      "Epoch [83/300], Step [22700/27733], Loss: 2.3675\n",
      "Epoch [83/300], Step [22800/27733], Loss: 2.7997\n",
      "Epoch [83/300], Step [22900/27733], Loss: 2.8443\n",
      "Epoch [83/300], Step [23000/27733], Loss: 2.9680\n",
      "Epoch [83/300], Step [23100/27733], Loss: 3.3360\n",
      "Epoch [83/300], Step [23200/27733], Loss: 3.1986\n",
      "Epoch [83/300], Step [23300/27733], Loss: 3.2707\n",
      "Epoch [83/300], Step [23400/27733], Loss: 2.9100\n",
      "Epoch [83/300], Step [23500/27733], Loss: 2.6206\n",
      "Epoch [83/300], Step [23600/27733], Loss: 2.8054\n",
      "Epoch [83/300], Step [23700/27733], Loss: 3.0255\n",
      "Epoch [83/300], Step [23800/27733], Loss: 3.5226\n",
      "Epoch [83/300], Step [23900/27733], Loss: 3.1407\n",
      "Epoch [83/300], Step [24000/27733], Loss: 2.5842\n",
      "Epoch [83/300], Step [24100/27733], Loss: 2.6829\n",
      "Epoch [83/300], Step [24200/27733], Loss: 2.7350\n",
      "Epoch [83/300], Step [24300/27733], Loss: 2.3489\n",
      "Epoch [83/300], Step [24400/27733], Loss: 2.7389\n",
      "Epoch [83/300], Step [24500/27733], Loss: 3.2384\n",
      "Epoch [83/300], Step [24600/27733], Loss: 2.7049\n",
      "Epoch [83/300], Step [24700/27733], Loss: 3.1026\n",
      "Epoch [83/300], Step [24800/27733], Loss: 3.1013\n",
      "Epoch [83/300], Step [24900/27733], Loss: 3.2341\n",
      "Epoch [83/300], Step [25000/27733], Loss: 3.2053\n",
      "Epoch [83/300], Step [25100/27733], Loss: 3.1705\n",
      "Epoch [83/300], Step [25200/27733], Loss: 2.4135\n",
      "Epoch [83/300], Step [25300/27733], Loss: 2.9253\n",
      "Epoch [83/300], Step [25400/27733], Loss: 2.8404\n",
      "Epoch [83/300], Step [25500/27733], Loss: 2.6574\n",
      "Epoch [83/300], Step [25600/27733], Loss: 3.5175\n",
      "Epoch [83/300], Step [25700/27733], Loss: 2.8631\n",
      "Epoch [83/300], Step [25800/27733], Loss: 2.7727\n",
      "Epoch [83/300], Step [25900/27733], Loss: 2.7681\n",
      "Epoch [83/300], Step [26000/27733], Loss: 2.9248\n",
      "Epoch [83/300], Step [26100/27733], Loss: 4.0106\n",
      "Epoch [83/300], Step [26200/27733], Loss: 2.9862\n",
      "Epoch [83/300], Step [26300/27733], Loss: 3.9470\n",
      "Epoch [83/300], Step [26400/27733], Loss: 2.5728\n",
      "Epoch [83/300], Step [26500/27733], Loss: 3.1990\n",
      "Epoch [83/300], Step [26600/27733], Loss: 2.6748\n",
      "Epoch [83/300], Step [26700/27733], Loss: 2.9463\n",
      "Epoch [83/300], Step [26800/27733], Loss: 3.2603\n",
      "Epoch [83/300], Step [26900/27733], Loss: 2.8484\n",
      "Epoch [83/300], Step [27000/27733], Loss: 3.9136\n",
      "Epoch [83/300], Step [27100/27733], Loss: 4.0111\n",
      "Epoch [83/300], Step [27200/27733], Loss: 3.2196\n",
      "Epoch [83/300], Step [27300/27733], Loss: 3.1918\n",
      "Epoch [83/300], Step [27400/27733], Loss: 3.3318\n",
      "Epoch [83/300], Step [27500/27733], Loss: 3.8635\n",
      "Epoch [83/300], Step [27600/27733], Loss: 2.9630\n",
      "Epoch [83/300], Step [27700/27733], Loss: 3.0864\n",
      "Epoch [84/300], Step [100/27733], Loss: 2.9102\n",
      "Epoch [84/300], Step [200/27733], Loss: 2.2401\n",
      "Epoch [84/300], Step [300/27733], Loss: 3.3432\n",
      "Epoch [84/300], Step [400/27733], Loss: 2.6433\n",
      "Epoch [84/300], Step [500/27733], Loss: 2.6313\n",
      "Epoch [84/300], Step [600/27733], Loss: 2.6111\n",
      "Epoch [84/300], Step [700/27733], Loss: 2.0079\n",
      "Epoch [84/300], Step [800/27733], Loss: 2.4884\n",
      "Epoch [84/300], Step [900/27733], Loss: 2.5929\n",
      "Epoch [84/300], Step [1000/27733], Loss: 2.2842\n",
      "Epoch [84/300], Step [1100/27733], Loss: 2.3065\n",
      "Epoch [84/300], Step [1200/27733], Loss: 2.3760\n",
      "Epoch [84/300], Step [1300/27733], Loss: 2.5411\n",
      "Epoch [84/300], Step [1400/27733], Loss: 2.4675\n",
      "Epoch [84/300], Step [1500/27733], Loss: 2.2997\n",
      "Epoch [84/300], Step [1600/27733], Loss: 2.4388\n",
      "Epoch [84/300], Step [1700/27733], Loss: 2.2997\n",
      "Epoch [84/300], Step [1800/27733], Loss: 2.0657\n",
      "Epoch [84/300], Step [1900/27733], Loss: 2.1728\n",
      "Epoch [84/300], Step [2000/27733], Loss: 2.8996\n",
      "Epoch [84/300], Step [2100/27733], Loss: 2.4637\n",
      "Epoch [84/300], Step [2200/27733], Loss: 1.8030\n",
      "Epoch [84/300], Step [2300/27733], Loss: 3.2810\n",
      "Epoch [84/300], Step [2400/27733], Loss: 2.2236\n",
      "Epoch [84/300], Step [2500/27733], Loss: 2.3642\n",
      "Epoch [84/300], Step [2600/27733], Loss: 2.2948\n",
      "Epoch [84/300], Step [2700/27733], Loss: 2.2757\n",
      "Epoch [84/300], Step [2800/27733], Loss: 2.4321\n",
      "Epoch [84/300], Step [2900/27733], Loss: 2.5832\n",
      "Epoch [84/300], Step [3000/27733], Loss: 2.5133\n",
      "Epoch [84/300], Step [3100/27733], Loss: 2.8889\n",
      "Epoch [84/300], Step [3200/27733], Loss: 2.8440\n",
      "Epoch [84/300], Step [3300/27733], Loss: 3.1100\n",
      "Epoch [84/300], Step [3400/27733], Loss: 2.7574\n",
      "Epoch [84/300], Step [3500/27733], Loss: 2.3321\n",
      "Epoch [84/300], Step [3600/27733], Loss: 2.6463\n",
      "Epoch [84/300], Step [3700/27733], Loss: 2.0590\n",
      "Epoch [84/300], Step [3800/27733], Loss: 1.9921\n",
      "Epoch [84/300], Step [3900/27733], Loss: 3.2239\n",
      "Epoch [84/300], Step [4000/27733], Loss: 2.5398\n",
      "Epoch [84/300], Step [4100/27733], Loss: 1.6743\n",
      "Epoch [84/300], Step [4200/27733], Loss: 2.3354\n",
      "Epoch [84/300], Step [4300/27733], Loss: 2.3324\n",
      "Epoch [84/300], Step [4400/27733], Loss: 2.9313\n",
      "Epoch [84/300], Step [4500/27733], Loss: 2.5926\n",
      "Epoch [84/300], Step [4600/27733], Loss: 1.8453\n",
      "Epoch [84/300], Step [4700/27733], Loss: 2.9770\n",
      "Epoch [84/300], Step [4800/27733], Loss: 2.6248\n",
      "Epoch [84/300], Step [4900/27733], Loss: 2.2902\n",
      "Epoch [84/300], Step [5000/27733], Loss: 2.4694\n",
      "Epoch [84/300], Step [5100/27733], Loss: 3.4252\n",
      "Epoch [84/300], Step [5200/27733], Loss: 2.3936\n",
      "Epoch [84/300], Step [5300/27733], Loss: 3.2929\n",
      "Epoch [84/300], Step [5400/27733], Loss: 2.2291\n",
      "Epoch [84/300], Step [5500/27733], Loss: 2.7984\n",
      "Epoch [84/300], Step [5600/27733], Loss: 1.9814\n",
      "Epoch [84/300], Step [5700/27733], Loss: 3.3062\n",
      "Epoch [84/300], Step [5800/27733], Loss: 2.7656\n",
      "Epoch [84/300], Step [5900/27733], Loss: 2.7452\n",
      "Epoch [84/300], Step [6000/27733], Loss: 2.5330\n",
      "Epoch [84/300], Step [6100/27733], Loss: 2.6398\n",
      "Epoch [84/300], Step [6200/27733], Loss: 2.5112\n",
      "Epoch [84/300], Step [6300/27733], Loss: 2.2272\n",
      "Epoch [84/300], Step [6400/27733], Loss: 2.3979\n",
      "Epoch [84/300], Step [6500/27733], Loss: 1.8655\n",
      "Epoch [84/300], Step [6600/27733], Loss: 2.3840\n",
      "Epoch [84/300], Step [6700/27733], Loss: 2.0440\n",
      "Epoch [84/300], Step [6800/27733], Loss: 2.3246\n",
      "Epoch [84/300], Step [6900/27733], Loss: 2.6291\n",
      "Epoch [84/300], Step [7000/27733], Loss: 2.7363\n",
      "Epoch [84/300], Step [7100/27733], Loss: 2.6371\n",
      "Epoch [84/300], Step [7200/27733], Loss: 2.4535\n",
      "Epoch [84/300], Step [7300/27733], Loss: 2.7130\n",
      "Epoch [84/300], Step [7400/27733], Loss: 2.8181\n",
      "Epoch [84/300], Step [7500/27733], Loss: 3.1031\n",
      "Epoch [84/300], Step [7600/27733], Loss: 3.0689\n",
      "Epoch [84/300], Step [7700/27733], Loss: 2.8506\n",
      "Epoch [84/300], Step [7800/27733], Loss: 2.3182\n",
      "Epoch [84/300], Step [7900/27733], Loss: 2.1829\n",
      "Epoch [84/300], Step [8000/27733], Loss: 2.4884\n",
      "Epoch [84/300], Step [8100/27733], Loss: 3.4843\n",
      "Epoch [84/300], Step [8200/27733], Loss: 3.1651\n",
      "Epoch [84/300], Step [8300/27733], Loss: 2.5540\n",
      "Epoch [84/300], Step [8400/27733], Loss: 2.7066\n",
      "Epoch [84/300], Step [8500/27733], Loss: 2.8152\n",
      "Epoch [84/300], Step [8600/27733], Loss: 2.4118\n",
      "Epoch [84/300], Step [8700/27733], Loss: 2.7338\n",
      "Epoch [84/300], Step [8800/27733], Loss: 2.7032\n",
      "Epoch [84/300], Step [8900/27733], Loss: 2.4735\n",
      "Epoch [84/300], Step [9000/27733], Loss: 2.8860\n",
      "Epoch [84/300], Step [9100/27733], Loss: 2.3410\n",
      "Epoch [84/300], Step [9200/27733], Loss: 2.4216\n",
      "Epoch [84/300], Step [9300/27733], Loss: 3.0486\n",
      "Epoch [84/300], Step [9400/27733], Loss: 2.3980\n",
      "Epoch [84/300], Step [9500/27733], Loss: 2.0100\n",
      "Epoch [84/300], Step [9600/27733], Loss: 2.4996\n",
      "Epoch [84/300], Step [9700/27733], Loss: 3.2670\n",
      "Epoch [84/300], Step [9800/27733], Loss: 2.8936\n",
      "Epoch [84/300], Step [9900/27733], Loss: 2.3691\n",
      "Epoch [84/300], Step [10000/27733], Loss: 3.1663\n",
      "Epoch [84/300], Step [10100/27733], Loss: 2.5977\n",
      "Epoch [84/300], Step [10200/27733], Loss: 2.2685\n",
      "Epoch [84/300], Step [10300/27733], Loss: 2.8393\n",
      "Epoch [84/300], Step [10400/27733], Loss: 2.5446\n",
      "Epoch [84/300], Step [10500/27733], Loss: 3.0793\n",
      "Epoch [84/300], Step [10600/27733], Loss: 2.0315\n",
      "Epoch [84/300], Step [10700/27733], Loss: 2.3696\n",
      "Epoch [84/300], Step [10800/27733], Loss: 2.7079\n",
      "Epoch [84/300], Step [10900/27733], Loss: 3.6224\n",
      "Epoch [84/300], Step [11000/27733], Loss: 3.2199\n",
      "Epoch [84/300], Step [11100/27733], Loss: 3.7088\n",
      "Epoch [84/300], Step [11200/27733], Loss: 2.5236\n",
      "Epoch [84/300], Step [11300/27733], Loss: 3.6497\n",
      "Epoch [84/300], Step [11400/27733], Loss: 2.7480\n",
      "Epoch [84/300], Step [11500/27733], Loss: 3.3281\n",
      "Epoch [84/300], Step [11600/27733], Loss: 2.6705\n",
      "Epoch [84/300], Step [11700/27733], Loss: 2.3727\n",
      "Epoch [84/300], Step [11800/27733], Loss: 2.6690\n",
      "Epoch [84/300], Step [11900/27733], Loss: 2.9263\n",
      "Epoch [84/300], Step [12000/27733], Loss: 2.2455\n",
      "Epoch [84/300], Step [12100/27733], Loss: 2.7009\n",
      "Epoch [84/300], Step [12200/27733], Loss: 2.7932\n",
      "Epoch [84/300], Step [12300/27733], Loss: 2.8569\n",
      "Epoch [84/300], Step [12400/27733], Loss: 2.6258\n",
      "Epoch [84/300], Step [12500/27733], Loss: 2.3843\n",
      "Epoch [84/300], Step [12600/27733], Loss: 2.2199\n",
      "Epoch [84/300], Step [12700/27733], Loss: 2.6612\n",
      "Epoch [84/300], Step [12800/27733], Loss: 2.5502\n",
      "Epoch [84/300], Step [12900/27733], Loss: 2.5577\n",
      "Epoch [84/300], Step [13000/27733], Loss: 3.2207\n",
      "Epoch [84/300], Step [13100/27733], Loss: 2.6364\n",
      "Epoch [84/300], Step [13200/27733], Loss: 2.3106\n",
      "Epoch [84/300], Step [13300/27733], Loss: 2.1144\n",
      "Epoch [84/300], Step [13400/27733], Loss: 2.3184\n",
      "Epoch [84/300], Step [13500/27733], Loss: 2.4456\n",
      "Epoch [84/300], Step [13600/27733], Loss: 3.0110\n",
      "Epoch [84/300], Step [13700/27733], Loss: 2.3718\n",
      "Epoch [84/300], Step [13800/27733], Loss: 2.1302\n",
      "Epoch [84/300], Step [13900/27733], Loss: 2.7072\n",
      "Epoch [84/300], Step [14000/27733], Loss: 2.4607\n",
      "Epoch [84/300], Step [14100/27733], Loss: 3.0371\n",
      "Epoch [84/300], Step [14200/27733], Loss: 2.0645\n",
      "Epoch [84/300], Step [14300/27733], Loss: 3.9504\n",
      "Epoch [84/300], Step [14400/27733], Loss: 2.8655\n",
      "Epoch [84/300], Step [14500/27733], Loss: 2.7655\n",
      "Epoch [84/300], Step [14600/27733], Loss: 3.5315\n",
      "Epoch [84/300], Step [14700/27733], Loss: 2.4283\n",
      "Epoch [84/300], Step [14800/27733], Loss: 3.5602\n",
      "Epoch [84/300], Step [14900/27733], Loss: 3.2627\n",
      "Epoch [84/300], Step [15000/27733], Loss: 2.8803\n",
      "Epoch [84/300], Step [15100/27733], Loss: 2.9656\n",
      "Epoch [84/300], Step [15200/27733], Loss: 1.9169\n",
      "Epoch [84/300], Step [15300/27733], Loss: 1.5166\n",
      "Epoch [84/300], Step [15400/27733], Loss: 2.6366\n",
      "Epoch [84/300], Step [15500/27733], Loss: 2.5319\n",
      "Epoch [84/300], Step [15600/27733], Loss: 2.6211\n",
      "Epoch [84/300], Step [15700/27733], Loss: 2.4953\n",
      "Epoch [84/300], Step [15800/27733], Loss: 2.6841\n",
      "Epoch [84/300], Step [15900/27733], Loss: 3.2434\n",
      "Epoch [84/300], Step [16000/27733], Loss: 2.7880\n",
      "Epoch [84/300], Step [16100/27733], Loss: 2.4531\n",
      "Epoch [84/300], Step [16200/27733], Loss: 3.0797\n",
      "Epoch [84/300], Step [16300/27733], Loss: 2.8201\n",
      "Epoch [84/300], Step [16400/27733], Loss: 2.5032\n",
      "Epoch [84/300], Step [16500/27733], Loss: 2.8760\n",
      "Epoch [84/300], Step [16600/27733], Loss: 2.0731\n",
      "Epoch [84/300], Step [16700/27733], Loss: 3.0055\n",
      "Epoch [84/300], Step [16800/27733], Loss: 3.6292\n",
      "Epoch [84/300], Step [16900/27733], Loss: 2.9048\n",
      "Epoch [84/300], Step [17000/27733], Loss: 3.6181\n",
      "Epoch [84/300], Step [17100/27733], Loss: 3.0027\n",
      "Epoch [84/300], Step [17200/27733], Loss: 2.8973\n",
      "Epoch [84/300], Step [17300/27733], Loss: 2.6009\n",
      "Epoch [84/300], Step [17400/27733], Loss: 2.8042\n",
      "Epoch [84/300], Step [17500/27733], Loss: 2.6622\n",
      "Epoch [84/300], Step [17600/27733], Loss: 3.2444\n",
      "Epoch [84/300], Step [17700/27733], Loss: 2.2884\n",
      "Epoch [84/300], Step [17800/27733], Loss: 2.0992\n",
      "Epoch [84/300], Step [17900/27733], Loss: 2.4684\n",
      "Epoch [84/300], Step [18000/27733], Loss: 2.7242\n",
      "Epoch [84/300], Step [18100/27733], Loss: 2.8863\n",
      "Epoch [84/300], Step [18200/27733], Loss: 2.6320\n",
      "Epoch [84/300], Step [18300/27733], Loss: 2.6653\n",
      "Epoch [84/300], Step [18400/27733], Loss: 3.2572\n",
      "Epoch [84/300], Step [18500/27733], Loss: 2.8636\n",
      "Epoch [84/300], Step [18600/27733], Loss: 2.6117\n",
      "Epoch [84/300], Step [18700/27733], Loss: 2.6736\n",
      "Epoch [84/300], Step [18800/27733], Loss: 2.7137\n",
      "Epoch [84/300], Step [18900/27733], Loss: 2.5390\n",
      "Epoch [84/300], Step [19000/27733], Loss: 2.5102\n",
      "Epoch [84/300], Step [19100/27733], Loss: 2.7020\n",
      "Epoch [84/300], Step [19200/27733], Loss: 2.6527\n",
      "Epoch [84/300], Step [19300/27733], Loss: 1.9173\n",
      "Epoch [84/300], Step [19400/27733], Loss: 4.0812\n",
      "Epoch [84/300], Step [19500/27733], Loss: 3.0503\n",
      "Epoch [84/300], Step [19600/27733], Loss: 2.3525\n",
      "Epoch [84/300], Step [19700/27733], Loss: 3.4858\n",
      "Epoch [84/300], Step [19800/27733], Loss: 2.8060\n",
      "Epoch [84/300], Step [19900/27733], Loss: 2.6924\n",
      "Epoch [84/300], Step [20000/27733], Loss: 3.4897\n",
      "Epoch [84/300], Step [20100/27733], Loss: 2.8683\n",
      "Epoch [84/300], Step [20200/27733], Loss: 2.4935\n",
      "Epoch [84/300], Step [20300/27733], Loss: 2.3705\n",
      "Epoch [84/300], Step [20400/27733], Loss: 2.7942\n",
      "Epoch [84/300], Step [20500/27733], Loss: 2.1999\n",
      "Epoch [84/300], Step [20600/27733], Loss: 2.6388\n",
      "Epoch [84/300], Step [20700/27733], Loss: 2.9790\n",
      "Epoch [84/300], Step [20800/27733], Loss: 2.3902\n",
      "Epoch [84/300], Step [20900/27733], Loss: 3.3913\n",
      "Epoch [84/300], Step [21000/27733], Loss: 4.1016\n",
      "Epoch [84/300], Step [21100/27733], Loss: 2.5828\n",
      "Epoch [84/300], Step [21200/27733], Loss: 2.7480\n",
      "Epoch [84/300], Step [21300/27733], Loss: 4.4028\n",
      "Epoch [84/300], Step [21400/27733], Loss: 2.6296\n",
      "Epoch [84/300], Step [21500/27733], Loss: 2.7858\n",
      "Epoch [84/300], Step [21600/27733], Loss: 2.2751\n",
      "Epoch [84/300], Step [21700/27733], Loss: 3.0180\n",
      "Epoch [84/300], Step [21800/27733], Loss: 2.7949\n",
      "Epoch [84/300], Step [21900/27733], Loss: 2.6977\n",
      "Epoch [84/300], Step [22000/27733], Loss: 1.9064\n",
      "Epoch [84/300], Step [22100/27733], Loss: 2.4293\n",
      "Epoch [84/300], Step [22200/27733], Loss: 2.7002\n",
      "Epoch [84/300], Step [22300/27733], Loss: 4.2292\n",
      "Epoch [84/300], Step [22400/27733], Loss: 2.2256\n",
      "Epoch [84/300], Step [22500/27733], Loss: 2.6142\n",
      "Epoch [84/300], Step [22600/27733], Loss: 2.4451\n",
      "Epoch [84/300], Step [22700/27733], Loss: 3.2414\n",
      "Epoch [84/300], Step [22800/27733], Loss: 2.9676\n",
      "Epoch [84/300], Step [22900/27733], Loss: 2.6719\n",
      "Epoch [84/300], Step [23000/27733], Loss: 2.5551\n",
      "Epoch [84/300], Step [23100/27733], Loss: 2.8535\n",
      "Epoch [84/300], Step [23200/27733], Loss: 3.6572\n",
      "Epoch [84/300], Step [23300/27733], Loss: 2.9211\n",
      "Epoch [84/300], Step [23400/27733], Loss: 2.0620\n",
      "Epoch [84/300], Step [23500/27733], Loss: 2.5050\n",
      "Epoch [84/300], Step [23600/27733], Loss: 2.9862\n",
      "Epoch [84/300], Step [23700/27733], Loss: 3.7098\n",
      "Epoch [84/300], Step [23800/27733], Loss: 2.0999\n",
      "Epoch [84/300], Step [23900/27733], Loss: 3.2523\n",
      "Epoch [84/300], Step [24000/27733], Loss: 2.9049\n",
      "Epoch [84/300], Step [24100/27733], Loss: 2.5385\n",
      "Epoch [84/300], Step [24200/27733], Loss: 2.3862\n",
      "Epoch [84/300], Step [24300/27733], Loss: 2.6397\n",
      "Epoch [84/300], Step [24400/27733], Loss: 3.4761\n",
      "Epoch [84/300], Step [24500/27733], Loss: 2.9108\n",
      "Epoch [84/300], Step [24600/27733], Loss: 2.7443\n",
      "Epoch [84/300], Step [24700/27733], Loss: 2.5164\n",
      "Epoch [84/300], Step [24800/27733], Loss: 2.2590\n",
      "Epoch [84/300], Step [24900/27733], Loss: 3.0923\n",
      "Epoch [84/300], Step [25000/27733], Loss: 2.7977\n",
      "Epoch [84/300], Step [25100/27733], Loss: 3.0747\n",
      "Epoch [84/300], Step [25200/27733], Loss: 2.6151\n",
      "Epoch [84/300], Step [25300/27733], Loss: 2.9931\n",
      "Epoch [84/300], Step [25400/27733], Loss: 3.4216\n",
      "Epoch [84/300], Step [25500/27733], Loss: 2.6751\n",
      "Epoch [84/300], Step [25600/27733], Loss: 2.9171\n",
      "Epoch [84/300], Step [25700/27733], Loss: 4.1512\n",
      "Epoch [84/300], Step [25800/27733], Loss: 3.1384\n",
      "Epoch [84/300], Step [25900/27733], Loss: 2.6303\n",
      "Epoch [84/300], Step [26000/27733], Loss: 2.8223\n",
      "Epoch [84/300], Step [26100/27733], Loss: 3.2015\n",
      "Epoch [84/300], Step [26200/27733], Loss: 3.4594\n",
      "Epoch [84/300], Step [26300/27733], Loss: 3.6753\n",
      "Epoch [84/300], Step [26400/27733], Loss: 2.6626\n",
      "Epoch [84/300], Step [26500/27733], Loss: 3.2629\n",
      "Epoch [84/300], Step [26600/27733], Loss: 3.6736\n",
      "Epoch [84/300], Step [26700/27733], Loss: 2.4715\n",
      "Epoch [84/300], Step [26800/27733], Loss: 3.1937\n",
      "Epoch [84/300], Step [26900/27733], Loss: 2.9258\n",
      "Epoch [84/300], Step [27000/27733], Loss: 3.5268\n",
      "Epoch [84/300], Step [27100/27733], Loss: 3.1933\n",
      "Epoch [84/300], Step [27200/27733], Loss: 2.1777\n",
      "Epoch [84/300], Step [27300/27733], Loss: 2.9363\n",
      "Epoch [84/300], Step [27400/27733], Loss: 3.4999\n",
      "Epoch [84/300], Step [27500/27733], Loss: 2.7347\n",
      "Epoch [84/300], Step [27600/27733], Loss: 3.4015\n",
      "Epoch [84/300], Step [27700/27733], Loss: 3.1277\n",
      "Epoch [85/300], Step [100/27733], Loss: 2.1882\n",
      "Epoch [85/300], Step [200/27733], Loss: 3.5389\n",
      "Epoch [85/300], Step [300/27733], Loss: 2.2394\n",
      "Epoch [85/300], Step [400/27733], Loss: 2.6224\n",
      "Epoch [85/300], Step [500/27733], Loss: 1.9144\n",
      "Epoch [85/300], Step [600/27733], Loss: 2.1443\n",
      "Epoch [85/300], Step [700/27733], Loss: 2.3149\n",
      "Epoch [85/300], Step [800/27733], Loss: 2.2884\n",
      "Epoch [85/300], Step [900/27733], Loss: 1.7474\n",
      "Epoch [85/300], Step [1000/27733], Loss: 2.2637\n",
      "Epoch [85/300], Step [1100/27733], Loss: 2.1328\n",
      "Epoch [85/300], Step [1200/27733], Loss: 1.6296\n",
      "Epoch [85/300], Step [1300/27733], Loss: 2.3400\n",
      "Epoch [85/300], Step [1400/27733], Loss: 2.8591\n",
      "Epoch [85/300], Step [1500/27733], Loss: 2.5753\n",
      "Epoch [85/300], Step [1600/27733], Loss: 2.6011\n",
      "Epoch [85/300], Step [1700/27733], Loss: 3.1104\n",
      "Epoch [85/300], Step [1800/27733], Loss: 2.8183\n",
      "Epoch [85/300], Step [1900/27733], Loss: 2.9647\n",
      "Epoch [85/300], Step [2000/27733], Loss: 2.2755\n",
      "Epoch [85/300], Step [2100/27733], Loss: 2.6972\n",
      "Epoch [85/300], Step [2200/27733], Loss: 2.6142\n",
      "Epoch [85/300], Step [2300/27733], Loss: 2.7550\n",
      "Epoch [85/300], Step [2400/27733], Loss: 2.4359\n",
      "Epoch [85/300], Step [2500/27733], Loss: 1.9751\n",
      "Epoch [85/300], Step [2600/27733], Loss: 3.4370\n",
      "Epoch [85/300], Step [2700/27733], Loss: 2.5587\n",
      "Epoch [85/300], Step [2800/27733], Loss: 2.3972\n",
      "Epoch [85/300], Step [2900/27733], Loss: 2.7144\n",
      "Epoch [85/300], Step [3000/27733], Loss: 2.8521\n",
      "Epoch [85/300], Step [3100/27733], Loss: 3.0104\n",
      "Epoch [85/300], Step [3200/27733], Loss: 2.1475\n",
      "Epoch [85/300], Step [3300/27733], Loss: 2.9146\n",
      "Epoch [85/300], Step [3400/27733], Loss: 2.4647\n",
      "Epoch [85/300], Step [3500/27733], Loss: 2.6142\n",
      "Epoch [85/300], Step [3600/27733], Loss: 2.3755\n",
      "Epoch [85/300], Step [3700/27733], Loss: 2.3064\n",
      "Epoch [85/300], Step [3800/27733], Loss: 2.4517\n",
      "Epoch [85/300], Step [3900/27733], Loss: 3.0878\n",
      "Epoch [85/300], Step [4000/27733], Loss: 2.4336\n",
      "Epoch [85/300], Step [4100/27733], Loss: 1.7730\n",
      "Epoch [85/300], Step [4200/27733], Loss: 2.6548\n",
      "Epoch [85/300], Step [4300/27733], Loss: 2.7209\n",
      "Epoch [85/300], Step [4400/27733], Loss: 2.1806\n",
      "Epoch [85/300], Step [4500/27733], Loss: 2.1481\n",
      "Epoch [85/300], Step [4600/27733], Loss: 2.4174\n",
      "Epoch [85/300], Step [4700/27733], Loss: 2.5349\n",
      "Epoch [85/300], Step [4800/27733], Loss: 3.2754\n",
      "Epoch [85/300], Step [4900/27733], Loss: 2.3396\n",
      "Epoch [85/300], Step [5000/27733], Loss: 2.2501\n",
      "Epoch [85/300], Step [5100/27733], Loss: 2.7431\n",
      "Epoch [85/300], Step [5200/27733], Loss: 2.8109\n",
      "Epoch [85/300], Step [5300/27733], Loss: 2.4164\n",
      "Epoch [85/300], Step [5400/27733], Loss: 2.2807\n",
      "Epoch [85/300], Step [5500/27733], Loss: 2.3895\n",
      "Epoch [85/300], Step [5600/27733], Loss: 2.5046\n",
      "Epoch [85/300], Step [5700/27733], Loss: 2.7770\n",
      "Epoch [85/300], Step [5800/27733], Loss: 3.6325\n",
      "Epoch [85/300], Step [5900/27733], Loss: 2.2984\n",
      "Epoch [85/300], Step [6000/27733], Loss: 2.5572\n",
      "Epoch [85/300], Step [6100/27733], Loss: 3.0919\n",
      "Epoch [85/300], Step [6200/27733], Loss: 2.3470\n",
      "Epoch [85/300], Step [6300/27733], Loss: 2.6598\n",
      "Epoch [85/300], Step [6400/27733], Loss: 2.9862\n",
      "Epoch [85/300], Step [6500/27733], Loss: 2.8299\n",
      "Epoch [85/300], Step [6600/27733], Loss: 2.7461\n",
      "Epoch [85/300], Step [6700/27733], Loss: 2.9144\n",
      "Epoch [85/300], Step [6800/27733], Loss: 2.3050\n",
      "Epoch [85/300], Step [6900/27733], Loss: 1.6148\n",
      "Epoch [85/300], Step [7000/27733], Loss: 3.6585\n",
      "Epoch [85/300], Step [7100/27733], Loss: 2.6962\n",
      "Epoch [85/300], Step [7200/27733], Loss: 2.8563\n",
      "Epoch [85/300], Step [7300/27733], Loss: 2.2230\n",
      "Epoch [85/300], Step [7400/27733], Loss: 3.5365\n",
      "Epoch [85/300], Step [7500/27733], Loss: 2.5969\n",
      "Epoch [85/300], Step [7600/27733], Loss: 2.6465\n",
      "Epoch [85/300], Step [7700/27733], Loss: 2.2488\n",
      "Epoch [85/300], Step [7800/27733], Loss: 2.1964\n",
      "Epoch [85/300], Step [7900/27733], Loss: 2.8782\n",
      "Epoch [85/300], Step [8000/27733], Loss: 2.7328\n",
      "Epoch [85/300], Step [8100/27733], Loss: 2.3852\n",
      "Epoch [85/300], Step [8200/27733], Loss: 3.0806\n",
      "Epoch [85/300], Step [8300/27733], Loss: 2.5753\n",
      "Epoch [85/300], Step [8400/27733], Loss: 3.0062\n",
      "Epoch [85/300], Step [8500/27733], Loss: 2.7097\n",
      "Epoch [85/300], Step [8600/27733], Loss: 3.3292\n",
      "Epoch [85/300], Step [8700/27733], Loss: 2.8114\n",
      "Epoch [85/300], Step [8800/27733], Loss: 2.3741\n",
      "Epoch [85/300], Step [8900/27733], Loss: 2.7213\n",
      "Epoch [85/300], Step [9000/27733], Loss: 2.3193\n",
      "Epoch [85/300], Step [9100/27733], Loss: 3.7668\n",
      "Epoch [85/300], Step [9200/27733], Loss: 2.2877\n",
      "Epoch [85/300], Step [9300/27733], Loss: 3.5654\n",
      "Epoch [85/300], Step [9400/27733], Loss: 2.3140\n",
      "Epoch [85/300], Step [9500/27733], Loss: 3.5257\n",
      "Epoch [85/300], Step [9600/27733], Loss: 2.7386\n",
      "Epoch [85/300], Step [9700/27733], Loss: 2.7139\n",
      "Epoch [85/300], Step [9800/27733], Loss: 2.4600\n",
      "Epoch [85/300], Step [9900/27733], Loss: 2.8926\n",
      "Epoch [85/300], Step [10000/27733], Loss: 2.9281\n",
      "Epoch [85/300], Step [10100/27733], Loss: 2.0072\n",
      "Epoch [85/300], Step [10200/27733], Loss: 2.3227\n",
      "Epoch [85/300], Step [10300/27733], Loss: 2.2703\n",
      "Epoch [85/300], Step [10400/27733], Loss: 2.8342\n",
      "Epoch [85/300], Step [10500/27733], Loss: 2.3983\n",
      "Epoch [85/300], Step [10600/27733], Loss: 2.4322\n",
      "Epoch [85/300], Step [10700/27733], Loss: 2.7176\n",
      "Epoch [85/300], Step [10800/27733], Loss: 2.5235\n",
      "Epoch [85/300], Step [10900/27733], Loss: 3.0246\n",
      "Epoch [85/300], Step [11000/27733], Loss: 2.9808\n",
      "Epoch [85/300], Step [11100/27733], Loss: 3.0316\n",
      "Epoch [85/300], Step [11200/27733], Loss: 3.1470\n",
      "Epoch [85/300], Step [11300/27733], Loss: 2.6521\n",
      "Epoch [85/300], Step [11400/27733], Loss: 2.8906\n",
      "Epoch [85/300], Step [11500/27733], Loss: 3.1058\n",
      "Epoch [85/300], Step [11600/27733], Loss: 3.0410\n",
      "Epoch [85/300], Step [11700/27733], Loss: 3.0674\n",
      "Epoch [85/300], Step [11800/27733], Loss: 2.1728\n",
      "Epoch [85/300], Step [11900/27733], Loss: 2.5713\n",
      "Epoch [85/300], Step [12000/27733], Loss: 2.0755\n",
      "Epoch [85/300], Step [12100/27733], Loss: 2.5210\n",
      "Epoch [85/300], Step [12200/27733], Loss: 2.4998\n",
      "Epoch [85/300], Step [12300/27733], Loss: 2.4397\n",
      "Epoch [85/300], Step [12400/27733], Loss: 2.6704\n",
      "Epoch [85/300], Step [12500/27733], Loss: 2.6004\n",
      "Epoch [85/300], Step [12600/27733], Loss: 2.8015\n",
      "Epoch [85/300], Step [12700/27733], Loss: 2.6270\n",
      "Epoch [85/300], Step [12800/27733], Loss: 2.5093\n",
      "Epoch [85/300], Step [12900/27733], Loss: 2.2300\n",
      "Epoch [85/300], Step [13000/27733], Loss: 2.6039\n",
      "Epoch [85/300], Step [13100/27733], Loss: 2.9053\n",
      "Epoch [85/300], Step [13200/27733], Loss: 2.6108\n",
      "Epoch [85/300], Step [13300/27733], Loss: 2.5346\n",
      "Epoch [85/300], Step [13400/27733], Loss: 2.7900\n",
      "Epoch [85/300], Step [13500/27733], Loss: 2.5432\n",
      "Epoch [85/300], Step [13600/27733], Loss: 2.6459\n",
      "Epoch [85/300], Step [13700/27733], Loss: 2.2327\n",
      "Epoch [85/300], Step [13800/27733], Loss: 3.1981\n",
      "Epoch [85/300], Step [13900/27733], Loss: 2.5190\n",
      "Epoch [85/300], Step [14000/27733], Loss: 2.9791\n",
      "Epoch [85/300], Step [14100/27733], Loss: 3.1257\n",
      "Epoch [85/300], Step [14200/27733], Loss: 2.7687\n",
      "Epoch [85/300], Step [14300/27733], Loss: 3.0108\n",
      "Epoch [85/300], Step [14400/27733], Loss: 2.5206\n",
      "Epoch [85/300], Step [14500/27733], Loss: 2.5168\n",
      "Epoch [85/300], Step [14600/27733], Loss: 2.8640\n",
      "Epoch [85/300], Step [14700/27733], Loss: 2.7710\n",
      "Epoch [85/300], Step [14800/27733], Loss: 2.7607\n",
      "Epoch [85/300], Step [14900/27733], Loss: 2.8908\n",
      "Epoch [85/300], Step [15000/27733], Loss: 2.5893\n",
      "Epoch [85/300], Step [15100/27733], Loss: 2.8655\n",
      "Epoch [85/300], Step [15200/27733], Loss: 2.8003\n",
      "Epoch [85/300], Step [15300/27733], Loss: 2.3362\n",
      "Epoch [85/300], Step [15400/27733], Loss: 3.8451\n",
      "Epoch [85/300], Step [15500/27733], Loss: 1.8240\n",
      "Epoch [85/300], Step [15600/27733], Loss: 2.6328\n",
      "Epoch [85/300], Step [15700/27733], Loss: 2.4698\n",
      "Epoch [85/300], Step [15800/27733], Loss: 2.6623\n",
      "Epoch [85/300], Step [15900/27733], Loss: 2.7818\n",
      "Epoch [85/300], Step [16000/27733], Loss: 2.6730\n",
      "Epoch [85/300], Step [16100/27733], Loss: 2.7026\n",
      "Epoch [85/300], Step [16200/27733], Loss: 2.8115\n",
      "Epoch [85/300], Step [16300/27733], Loss: 2.4604\n",
      "Epoch [85/300], Step [16400/27733], Loss: 2.2258\n",
      "Epoch [85/300], Step [16500/27733], Loss: 3.0765\n",
      "Epoch [85/300], Step [16600/27733], Loss: 2.4501\n",
      "Epoch [85/300], Step [16700/27733], Loss: 2.8281\n",
      "Epoch [85/300], Step [16800/27733], Loss: 2.2251\n",
      "Epoch [85/300], Step [16900/27733], Loss: 3.2998\n",
      "Epoch [85/300], Step [17000/27733], Loss: 3.0632\n",
      "Epoch [85/300], Step [17100/27733], Loss: 2.0166\n",
      "Epoch [85/300], Step [17200/27733], Loss: 2.8321\n",
      "Epoch [85/300], Step [17300/27733], Loss: 3.2193\n",
      "Epoch [85/300], Step [17400/27733], Loss: 2.2361\n",
      "Epoch [85/300], Step [17500/27733], Loss: 3.0402\n",
      "Epoch [85/300], Step [17600/27733], Loss: 2.5755\n",
      "Epoch [85/300], Step [17700/27733], Loss: 2.6600\n",
      "Epoch [85/300], Step [17800/27733], Loss: 3.0459\n",
      "Epoch [85/300], Step [17900/27733], Loss: 2.7952\n",
      "Epoch [85/300], Step [18000/27733], Loss: 3.2486\n",
      "Epoch [85/300], Step [18100/27733], Loss: 2.1671\n",
      "Epoch [85/300], Step [18200/27733], Loss: 2.9609\n",
      "Epoch [85/300], Step [18300/27733], Loss: 2.5136\n",
      "Epoch [85/300], Step [18400/27733], Loss: 3.1279\n",
      "Epoch [85/300], Step [18500/27733], Loss: 2.6274\n",
      "Epoch [85/300], Step [18600/27733], Loss: 2.0607\n",
      "Epoch [85/300], Step [18700/27733], Loss: 2.6621\n",
      "Epoch [85/300], Step [18800/27733], Loss: 2.3563\n",
      "Epoch [85/300], Step [18900/27733], Loss: 2.5293\n",
      "Epoch [85/300], Step [19000/27733], Loss: 2.9221\n",
      "Epoch [85/300], Step [19100/27733], Loss: 2.5499\n",
      "Epoch [85/300], Step [19200/27733], Loss: 2.8345\n",
      "Epoch [85/300], Step [19300/27733], Loss: 2.7485\n",
      "Epoch [85/300], Step [19400/27733], Loss: 2.6616\n",
      "Epoch [85/300], Step [19500/27733], Loss: 2.6070\n",
      "Epoch [85/300], Step [19600/27733], Loss: 2.2997\n",
      "Epoch [85/300], Step [19700/27733], Loss: 3.6005\n",
      "Epoch [85/300], Step [19800/27733], Loss: 3.4025\n",
      "Epoch [85/300], Step [19900/27733], Loss: 2.9278\n",
      "Epoch [85/300], Step [20000/27733], Loss: 2.4064\n",
      "Epoch [85/300], Step [20100/27733], Loss: 2.9997\n",
      "Epoch [85/300], Step [20200/27733], Loss: 2.9472\n",
      "Epoch [85/300], Step [20300/27733], Loss: 3.4509\n",
      "Epoch [85/300], Step [20400/27733], Loss: 3.4123\n",
      "Epoch [85/300], Step [20500/27733], Loss: 2.5783\n",
      "Epoch [85/300], Step [20600/27733], Loss: 2.9924\n",
      "Epoch [85/300], Step [20700/27733], Loss: 2.5442\n",
      "Epoch [85/300], Step [20800/27733], Loss: 2.4135\n",
      "Epoch [85/300], Step [20900/27733], Loss: 3.4321\n",
      "Epoch [85/300], Step [21000/27733], Loss: 2.7300\n",
      "Epoch [85/300], Step [21100/27733], Loss: 2.1630\n",
      "Epoch [85/300], Step [21200/27733], Loss: 2.1955\n",
      "Epoch [85/300], Step [21300/27733], Loss: 3.6348\n",
      "Epoch [85/300], Step [21400/27733], Loss: 2.3312\n",
      "Epoch [85/300], Step [21500/27733], Loss: 3.3680\n",
      "Epoch [85/300], Step [21600/27733], Loss: 2.9176\n",
      "Epoch [85/300], Step [21700/27733], Loss: 2.7929\n",
      "Epoch [85/300], Step [21800/27733], Loss: 2.9427\n",
      "Epoch [85/300], Step [21900/27733], Loss: 2.2007\n",
      "Epoch [85/300], Step [22000/27733], Loss: 3.3832\n",
      "Epoch [85/300], Step [22100/27733], Loss: 2.6065\n",
      "Epoch [85/300], Step [22200/27733], Loss: 2.4750\n",
      "Epoch [85/300], Step [22300/27733], Loss: 2.7848\n",
      "Epoch [85/300], Step [22400/27733], Loss: 3.1013\n",
      "Epoch [85/300], Step [22500/27733], Loss: 2.5461\n",
      "Epoch [85/300], Step [22600/27733], Loss: 2.7180\n",
      "Epoch [85/300], Step [22700/27733], Loss: 1.8085\n",
      "Epoch [85/300], Step [22800/27733], Loss: 4.5122\n",
      "Epoch [85/300], Step [22900/27733], Loss: 3.6010\n",
      "Epoch [85/300], Step [23000/27733], Loss: 3.7803\n",
      "Epoch [85/300], Step [23100/27733], Loss: 3.5142\n",
      "Epoch [85/300], Step [23200/27733], Loss: 2.3488\n",
      "Epoch [85/300], Step [23300/27733], Loss: 2.9175\n",
      "Epoch [85/300], Step [23400/27733], Loss: 2.7500\n",
      "Epoch [85/300], Step [23500/27733], Loss: 2.7642\n",
      "Epoch [85/300], Step [23600/27733], Loss: 3.3215\n",
      "Epoch [85/300], Step [23700/27733], Loss: 3.0849\n",
      "Epoch [85/300], Step [23800/27733], Loss: 3.1913\n",
      "Epoch [85/300], Step [23900/27733], Loss: 3.6863\n",
      "Epoch [85/300], Step [24000/27733], Loss: 2.2654\n",
      "Epoch [85/300], Step [24100/27733], Loss: 2.4525\n",
      "Epoch [85/300], Step [24200/27733], Loss: 2.6822\n",
      "Epoch [85/300], Step [24300/27733], Loss: 2.8433\n",
      "Epoch [85/300], Step [24400/27733], Loss: 2.9107\n",
      "Epoch [85/300], Step [24500/27733], Loss: 2.9653\n",
      "Epoch [85/300], Step [24600/27733], Loss: 3.3150\n",
      "Epoch [85/300], Step [24700/27733], Loss: 2.3516\n",
      "Epoch [85/300], Step [24800/27733], Loss: 2.4003\n",
      "Epoch [85/300], Step [24900/27733], Loss: 2.2927\n",
      "Epoch [85/300], Step [25000/27733], Loss: 2.8398\n",
      "Epoch [85/300], Step [25100/27733], Loss: 2.7574\n",
      "Epoch [85/300], Step [25200/27733], Loss: 2.6236\n",
      "Epoch [85/300], Step [25300/27733], Loss: 3.4825\n",
      "Epoch [85/300], Step [25400/27733], Loss: 4.0460\n",
      "Epoch [85/300], Step [25500/27733], Loss: 3.7102\n",
      "Epoch [85/300], Step [25600/27733], Loss: 3.7724\n",
      "Epoch [85/300], Step [25700/27733], Loss: 3.5448\n",
      "Epoch [85/300], Step [25800/27733], Loss: 2.4154\n",
      "Epoch [85/300], Step [25900/27733], Loss: 3.3247\n",
      "Epoch [85/300], Step [26000/27733], Loss: 3.4197\n",
      "Epoch [85/300], Step [26100/27733], Loss: 2.6182\n",
      "Epoch [85/300], Step [26200/27733], Loss: 2.5886\n",
      "Epoch [85/300], Step [26300/27733], Loss: 2.6314\n",
      "Epoch [85/300], Step [26400/27733], Loss: 2.9140\n",
      "Epoch [85/300], Step [26500/27733], Loss: 3.1340\n",
      "Epoch [85/300], Step [26600/27733], Loss: 3.2119\n",
      "Epoch [85/300], Step [26700/27733], Loss: 2.7143\n",
      "Epoch [85/300], Step [26800/27733], Loss: 3.2012\n",
      "Epoch [85/300], Step [26900/27733], Loss: 2.5789\n",
      "Epoch [85/300], Step [27000/27733], Loss: 2.6840\n",
      "Epoch [85/300], Step [27100/27733], Loss: 2.4141\n",
      "Epoch [85/300], Step [27200/27733], Loss: 2.3861\n",
      "Epoch [85/300], Step [27300/27733], Loss: 2.6983\n",
      "Epoch [85/300], Step [27400/27733], Loss: 2.8039\n",
      "Epoch [85/300], Step [27500/27733], Loss: 4.7238\n",
      "Epoch [85/300], Step [27600/27733], Loss: 2.6432\n",
      "Epoch [85/300], Step [27700/27733], Loss: 2.8619\n",
      "Epoch [86/300], Step [100/27733], Loss: 2.2804\n",
      "Epoch [86/300], Step [200/27733], Loss: 2.4888\n",
      "Epoch [86/300], Step [300/27733], Loss: 2.5467\n",
      "Epoch [86/300], Step [400/27733], Loss: 2.3368\n",
      "Epoch [86/300], Step [500/27733], Loss: 2.4412\n",
      "Epoch [86/300], Step [600/27733], Loss: 2.8260\n",
      "Epoch [86/300], Step [700/27733], Loss: 1.7425\n",
      "Epoch [86/300], Step [800/27733], Loss: 1.9166\n",
      "Epoch [86/300], Step [900/27733], Loss: 1.7548\n",
      "Epoch [86/300], Step [1000/27733], Loss: 2.6264\n",
      "Epoch [86/300], Step [1100/27733], Loss: 2.5840\n",
      "Epoch [86/300], Step [1200/27733], Loss: 2.6234\n",
      "Epoch [86/300], Step [1300/27733], Loss: 2.8458\n",
      "Epoch [86/300], Step [1400/27733], Loss: 2.6089\n",
      "Epoch [86/300], Step [1500/27733], Loss: 2.1682\n",
      "Epoch [86/300], Step [1600/27733], Loss: 2.3657\n",
      "Epoch [86/300], Step [1700/27733], Loss: 1.9536\n",
      "Epoch [86/300], Step [1800/27733], Loss: 2.7337\n",
      "Epoch [86/300], Step [1900/27733], Loss: 2.9468\n",
      "Epoch [86/300], Step [2000/27733], Loss: 2.2994\n",
      "Epoch [86/300], Step [2100/27733], Loss: 2.8242\n",
      "Epoch [86/300], Step [2200/27733], Loss: 2.5968\n",
      "Epoch [86/300], Step [2300/27733], Loss: 2.3308\n",
      "Epoch [86/300], Step [2400/27733], Loss: 2.4023\n",
      "Epoch [86/300], Step [2500/27733], Loss: 2.5914\n",
      "Epoch [86/300], Step [2600/27733], Loss: 2.2708\n",
      "Epoch [86/300], Step [2700/27733], Loss: 2.2891\n",
      "Epoch [86/300], Step [2800/27733], Loss: 2.4379\n",
      "Epoch [86/300], Step [2900/27733], Loss: 1.7370\n",
      "Epoch [86/300], Step [3000/27733], Loss: 2.6800\n",
      "Epoch [86/300], Step [3100/27733], Loss: 2.9478\n",
      "Epoch [86/300], Step [3200/27733], Loss: 2.9156\n",
      "Epoch [86/300], Step [3300/27733], Loss: 2.4500\n",
      "Epoch [86/300], Step [3400/27733], Loss: 3.2494\n",
      "Epoch [86/300], Step [3500/27733], Loss: 3.0788\n",
      "Epoch [86/300], Step [3600/27733], Loss: 2.7224\n",
      "Epoch [86/300], Step [3700/27733], Loss: 2.7575\n",
      "Epoch [86/300], Step [3800/27733], Loss: 1.9790\n",
      "Epoch [86/300], Step [3900/27733], Loss: 3.1127\n",
      "Epoch [86/300], Step [4000/27733], Loss: 1.9550\n",
      "Epoch [86/300], Step [4100/27733], Loss: 2.5154\n",
      "Epoch [86/300], Step [4200/27733], Loss: 2.2863\n",
      "Epoch [86/300], Step [4300/27733], Loss: 2.3682\n",
      "Epoch [86/300], Step [4400/27733], Loss: 2.9968\n",
      "Epoch [86/300], Step [4500/27733], Loss: 1.4392\n",
      "Epoch [86/300], Step [4600/27733], Loss: 2.1662\n",
      "Epoch [86/300], Step [4700/27733], Loss: 3.4432\n",
      "Epoch [86/300], Step [4800/27733], Loss: 2.5413\n",
      "Epoch [86/300], Step [4900/27733], Loss: 2.7479\n",
      "Epoch [86/300], Step [5000/27733], Loss: 2.5149\n",
      "Epoch [86/300], Step [5100/27733], Loss: 2.2825\n",
      "Epoch [86/300], Step [5200/27733], Loss: 2.5216\n",
      "Epoch [86/300], Step [5300/27733], Loss: 2.4758\n",
      "Epoch [86/300], Step [5400/27733], Loss: 2.9879\n",
      "Epoch [86/300], Step [5500/27733], Loss: 2.5491\n",
      "Epoch [86/300], Step [5600/27733], Loss: 2.3703\n",
      "Epoch [86/300], Step [5700/27733], Loss: 3.1756\n",
      "Epoch [86/300], Step [5800/27733], Loss: 3.4538\n",
      "Epoch [86/300], Step [5900/27733], Loss: 2.2260\n",
      "Epoch [86/300], Step [6000/27733], Loss: 2.3257\n",
      "Epoch [86/300], Step [6100/27733], Loss: 3.1593\n",
      "Epoch [86/300], Step [6200/27733], Loss: 3.0919\n",
      "Epoch [86/300], Step [6300/27733], Loss: 3.2283\n",
      "Epoch [86/300], Step [6400/27733], Loss: 2.5288\n",
      "Epoch [86/300], Step [6500/27733], Loss: 2.4971\n",
      "Epoch [86/300], Step [6600/27733], Loss: 2.3895\n",
      "Epoch [86/300], Step [6700/27733], Loss: 2.3253\n",
      "Epoch [86/300], Step [6800/27733], Loss: 2.8730\n",
      "Epoch [86/300], Step [6900/27733], Loss: 2.9695\n",
      "Epoch [86/300], Step [7000/27733], Loss: 2.3422\n",
      "Epoch [86/300], Step [7100/27733], Loss: 2.5018\n",
      "Epoch [86/300], Step [7200/27733], Loss: 2.9446\n",
      "Epoch [86/300], Step [7300/27733], Loss: 2.8113\n",
      "Epoch [86/300], Step [7400/27733], Loss: 2.3076\n",
      "Epoch [86/300], Step [7500/27733], Loss: 2.6360\n",
      "Epoch [86/300], Step [7600/27733], Loss: 2.6803\n",
      "Epoch [86/300], Step [7700/27733], Loss: 2.2119\n",
      "Epoch [86/300], Step [7800/27733], Loss: 2.4777\n",
      "Epoch [86/300], Step [7900/27733], Loss: 3.6658\n",
      "Epoch [86/300], Step [8000/27733], Loss: 1.9206\n",
      "Epoch [86/300], Step [8100/27733], Loss: 2.6450\n",
      "Epoch [86/300], Step [8200/27733], Loss: 2.2767\n",
      "Epoch [86/300], Step [8300/27733], Loss: 2.1435\n",
      "Epoch [86/300], Step [8400/27733], Loss: 2.5276\n",
      "Epoch [86/300], Step [8500/27733], Loss: 2.1064\n",
      "Epoch [86/300], Step [8600/27733], Loss: 2.9571\n",
      "Epoch [86/300], Step [8700/27733], Loss: 3.0696\n",
      "Epoch [86/300], Step [8800/27733], Loss: 3.0688\n",
      "Epoch [86/300], Step [8900/27733], Loss: 2.4981\n",
      "Epoch [86/300], Step [9000/27733], Loss: 2.0911\n",
      "Epoch [86/300], Step [9100/27733], Loss: 3.3353\n",
      "Epoch [86/300], Step [9200/27733], Loss: 1.9582\n",
      "Epoch [86/300], Step [9300/27733], Loss: 3.1591\n",
      "Epoch [86/300], Step [9400/27733], Loss: 2.4864\n",
      "Epoch [86/300], Step [9500/27733], Loss: 2.3879\n",
      "Epoch [86/300], Step [9600/27733], Loss: 2.3671\n",
      "Epoch [86/300], Step [9700/27733], Loss: 2.3183\n",
      "Epoch [86/300], Step [9800/27733], Loss: 2.7759\n",
      "Epoch [86/300], Step [9900/27733], Loss: 2.5636\n",
      "Epoch [86/300], Step [10000/27733], Loss: 2.2708\n",
      "Epoch [86/300], Step [10100/27733], Loss: 2.8936\n",
      "Epoch [86/300], Step [10200/27733], Loss: 2.1567\n",
      "Epoch [86/300], Step [10300/27733], Loss: 2.6914\n",
      "Epoch [86/300], Step [10400/27733], Loss: 2.8663\n",
      "Epoch [86/300], Step [10500/27733], Loss: 2.1245\n",
      "Epoch [86/300], Step [10600/27733], Loss: 2.5652\n",
      "Epoch [86/300], Step [10700/27733], Loss: 2.6553\n",
      "Epoch [86/300], Step [10800/27733], Loss: 2.2932\n",
      "Epoch [86/300], Step [10900/27733], Loss: 3.1036\n",
      "Epoch [86/300], Step [11000/27733], Loss: 3.0282\n",
      "Epoch [86/300], Step [11100/27733], Loss: 2.9736\n",
      "Epoch [86/300], Step [11200/27733], Loss: 2.5893\n",
      "Epoch [86/300], Step [11300/27733], Loss: 2.8439\n",
      "Epoch [86/300], Step [11400/27733], Loss: 3.3428\n",
      "Epoch [86/300], Step [11500/27733], Loss: 2.7090\n",
      "Epoch [86/300], Step [11600/27733], Loss: 2.9533\n",
      "Epoch [86/300], Step [11700/27733], Loss: 2.5686\n",
      "Epoch [86/300], Step [11800/27733], Loss: 3.0151\n",
      "Epoch [86/300], Step [11900/27733], Loss: 3.1054\n",
      "Epoch [86/300], Step [12000/27733], Loss: 2.6896\n",
      "Epoch [86/300], Step [12100/27733], Loss: 2.6484\n",
      "Epoch [86/300], Step [12200/27733], Loss: 2.4985\n",
      "Epoch [86/300], Step [12300/27733], Loss: 2.3519\n",
      "Epoch [86/300], Step [12400/27733], Loss: 2.6921\n",
      "Epoch [86/300], Step [12500/27733], Loss: 1.9728\n",
      "Epoch [86/300], Step [12600/27733], Loss: 3.0562\n",
      "Epoch [86/300], Step [12700/27733], Loss: 2.4233\n",
      "Epoch [86/300], Step [12800/27733], Loss: 2.3892\n",
      "Epoch [86/300], Step [12900/27733], Loss: 2.6132\n",
      "Epoch [86/300], Step [13000/27733], Loss: 2.5441\n",
      "Epoch [86/300], Step [13100/27733], Loss: 2.3573\n",
      "Epoch [86/300], Step [13200/27733], Loss: 2.4535\n",
      "Epoch [86/300], Step [13300/27733], Loss: 3.1964\n",
      "Epoch [86/300], Step [13400/27733], Loss: 2.6249\n",
      "Epoch [86/300], Step [13500/27733], Loss: 3.8092\n",
      "Epoch [86/300], Step [13600/27733], Loss: 2.4499\n",
      "Epoch [86/300], Step [13700/27733], Loss: 3.0973\n",
      "Epoch [86/300], Step [13800/27733], Loss: 2.9424\n",
      "Epoch [86/300], Step [13900/27733], Loss: 2.8114\n",
      "Epoch [86/300], Step [14000/27733], Loss: 2.2956\n",
      "Epoch [86/300], Step [14100/27733], Loss: 2.3389\n",
      "Epoch [86/300], Step [14200/27733], Loss: 3.4144\n",
      "Epoch [86/300], Step [14300/27733], Loss: 3.1731\n",
      "Epoch [86/300], Step [14400/27733], Loss: 2.2575\n",
      "Epoch [86/300], Step [14500/27733], Loss: 2.9122\n",
      "Epoch [86/300], Step [14600/27733], Loss: 2.1848\n",
      "Epoch [86/300], Step [14700/27733], Loss: 2.1889\n",
      "Epoch [86/300], Step [14800/27733], Loss: 3.5891\n",
      "Epoch [86/300], Step [14900/27733], Loss: 2.1863\n",
      "Epoch [86/300], Step [15000/27733], Loss: 3.0512\n",
      "Epoch [86/300], Step [15100/27733], Loss: 2.5753\n",
      "Epoch [86/300], Step [15200/27733], Loss: 3.7073\n",
      "Epoch [86/300], Step [15300/27733], Loss: 3.0221\n",
      "Epoch [86/300], Step [15400/27733], Loss: 1.7683\n",
      "Epoch [86/300], Step [15500/27733], Loss: 3.1008\n",
      "Epoch [86/300], Step [15600/27733], Loss: 2.7555\n",
      "Epoch [86/300], Step [15700/27733], Loss: 2.6472\n",
      "Epoch [86/300], Step [15800/27733], Loss: 2.6983\n",
      "Epoch [86/300], Step [15900/27733], Loss: 2.8155\n",
      "Epoch [86/300], Step [16000/27733], Loss: 2.6958\n",
      "Epoch [86/300], Step [16100/27733], Loss: 2.6728\n",
      "Epoch [86/300], Step [16200/27733], Loss: 3.1557\n",
      "Epoch [86/300], Step [16300/27733], Loss: 2.8804\n",
      "Epoch [86/300], Step [16400/27733], Loss: 2.9111\n",
      "Epoch [86/300], Step [16500/27733], Loss: 2.4933\n",
      "Epoch [86/300], Step [16600/27733], Loss: 3.2085\n",
      "Epoch [86/300], Step [16700/27733], Loss: 2.6604\n",
      "Epoch [86/300], Step [16800/27733], Loss: 2.4400\n",
      "Epoch [86/300], Step [16900/27733], Loss: 3.0635\n",
      "Epoch [86/300], Step [17000/27733], Loss: 2.1616\n",
      "Epoch [86/300], Step [17100/27733], Loss: 3.3313\n",
      "Epoch [86/300], Step [17200/27733], Loss: 3.1426\n",
      "Epoch [86/300], Step [17300/27733], Loss: 2.6345\n",
      "Epoch [86/300], Step [17400/27733], Loss: 2.4541\n",
      "Epoch [86/300], Step [17500/27733], Loss: 2.7354\n",
      "Epoch [86/300], Step [17600/27733], Loss: 3.6997\n",
      "Epoch [86/300], Step [17700/27733], Loss: 3.4683\n",
      "Epoch [86/300], Step [17800/27733], Loss: 2.8332\n",
      "Epoch [86/300], Step [17900/27733], Loss: 2.2395\n",
      "Epoch [86/300], Step [18000/27733], Loss: 2.8780\n",
      "Epoch [86/300], Step [18100/27733], Loss: 1.9823\n",
      "Epoch [86/300], Step [18200/27733], Loss: 2.2119\n",
      "Epoch [86/300], Step [18300/27733], Loss: 2.7334\n",
      "Epoch [86/300], Step [18400/27733], Loss: 2.6459\n",
      "Epoch [86/300], Step [18500/27733], Loss: 2.6253\n",
      "Epoch [86/300], Step [18600/27733], Loss: 2.7201\n",
      "Epoch [86/300], Step [18700/27733], Loss: 2.7171\n",
      "Epoch [86/300], Step [18800/27733], Loss: 2.9715\n",
      "Epoch [86/300], Step [18900/27733], Loss: 2.6131\n",
      "Epoch [86/300], Step [19000/27733], Loss: 2.2889\n",
      "Epoch [86/300], Step [19100/27733], Loss: 2.8203\n",
      "Epoch [86/300], Step [19200/27733], Loss: 2.9103\n",
      "Epoch [86/300], Step [19300/27733], Loss: 2.6223\n",
      "Epoch [86/300], Step [19400/27733], Loss: 2.6371\n",
      "Epoch [86/300], Step [19500/27733], Loss: 2.9642\n",
      "Epoch [86/300], Step [19600/27733], Loss: 2.3455\n",
      "Epoch [86/300], Step [19700/27733], Loss: 2.3624\n",
      "Epoch [86/300], Step [19800/27733], Loss: 3.2329\n",
      "Epoch [86/300], Step [19900/27733], Loss: 3.6522\n",
      "Epoch [86/300], Step [20000/27733], Loss: 2.1440\n",
      "Epoch [86/300], Step [20100/27733], Loss: 3.2204\n",
      "Epoch [86/300], Step [20200/27733], Loss: 3.8343\n",
      "Epoch [86/300], Step [20300/27733], Loss: 3.5728\n",
      "Epoch [86/300], Step [20400/27733], Loss: 2.1664\n",
      "Epoch [86/300], Step [20500/27733], Loss: 2.7037\n",
      "Epoch [86/300], Step [20600/27733], Loss: 2.2498\n",
      "Epoch [86/300], Step [20700/27733], Loss: 2.2448\n",
      "Epoch [86/300], Step [20800/27733], Loss: 2.9285\n",
      "Epoch [86/300], Step [20900/27733], Loss: 2.7896\n",
      "Epoch [86/300], Step [21000/27733], Loss: 2.8863\n",
      "Epoch [86/300], Step [21100/27733], Loss: 2.0370\n",
      "Epoch [86/300], Step [21200/27733], Loss: 1.9889\n",
      "Epoch [86/300], Step [21300/27733], Loss: 2.9112\n",
      "Epoch [86/300], Step [21400/27733], Loss: 2.8811\n",
      "Epoch [86/300], Step [21500/27733], Loss: 3.9204\n",
      "Epoch [86/300], Step [21600/27733], Loss: 3.4902\n",
      "Epoch [86/300], Step [21700/27733], Loss: 2.7720\n",
      "Epoch [86/300], Step [21800/27733], Loss: 3.1669\n",
      "Epoch [86/300], Step [21900/27733], Loss: 3.5158\n",
      "Epoch [86/300], Step [22000/27733], Loss: 2.6030\n",
      "Epoch [86/300], Step [22100/27733], Loss: 3.4743\n",
      "Epoch [86/300], Step [22200/27733], Loss: 4.2064\n",
      "Epoch [86/300], Step [22300/27733], Loss: 2.6322\n",
      "Epoch [86/300], Step [22400/27733], Loss: 2.4127\n",
      "Epoch [86/300], Step [22500/27733], Loss: 2.9851\n",
      "Epoch [86/300], Step [22600/27733], Loss: 3.0928\n",
      "Epoch [86/300], Step [22700/27733], Loss: 3.0434\n",
      "Epoch [86/300], Step [22800/27733], Loss: 3.5835\n",
      "Epoch [86/300], Step [22900/27733], Loss: 3.2265\n",
      "Epoch [86/300], Step [23000/27733], Loss: 3.6353\n",
      "Epoch [86/300], Step [23100/27733], Loss: 2.4604\n",
      "Epoch [86/300], Step [23200/27733], Loss: 3.5972\n",
      "Epoch [86/300], Step [23300/27733], Loss: 3.9729\n",
      "Epoch [86/300], Step [23400/27733], Loss: 2.7550\n",
      "Epoch [86/300], Step [23500/27733], Loss: 2.9748\n",
      "Epoch [86/300], Step [23600/27733], Loss: 2.5683\n",
      "Epoch [86/300], Step [23700/27733], Loss: 2.9271\n",
      "Epoch [86/300], Step [23800/27733], Loss: 3.4964\n",
      "Epoch [86/300], Step [23900/27733], Loss: 2.5805\n",
      "Epoch [86/300], Step [24000/27733], Loss: 3.1861\n",
      "Epoch [86/300], Step [24100/27733], Loss: 2.4599\n",
      "Epoch [86/300], Step [24200/27733], Loss: 3.0520\n",
      "Epoch [86/300], Step [24300/27733], Loss: 2.7590\n",
      "Epoch [86/300], Step [24400/27733], Loss: 2.8747\n",
      "Epoch [86/300], Step [24500/27733], Loss: 4.0866\n",
      "Epoch [86/300], Step [24600/27733], Loss: 3.1464\n",
      "Epoch [86/300], Step [24700/27733], Loss: 2.5504\n",
      "Epoch [86/300], Step [24800/27733], Loss: 2.5764\n",
      "Epoch [86/300], Step [24900/27733], Loss: 2.8059\n",
      "Epoch [86/300], Step [25000/27733], Loss: 3.1562\n",
      "Epoch [86/300], Step [25100/27733], Loss: 2.8934\n",
      "Epoch [86/300], Step [25200/27733], Loss: 3.5903\n",
      "Epoch [86/300], Step [25300/27733], Loss: 3.1090\n",
      "Epoch [86/300], Step [25400/27733], Loss: 2.9496\n",
      "Epoch [86/300], Step [25500/27733], Loss: 3.0318\n",
      "Epoch [86/300], Step [25600/27733], Loss: 2.6139\n",
      "Epoch [86/300], Step [25700/27733], Loss: 2.7495\n",
      "Epoch [86/300], Step [25800/27733], Loss: 4.0278\n",
      "Epoch [86/300], Step [25900/27733], Loss: 3.1409\n",
      "Epoch [86/300], Step [26000/27733], Loss: 2.8354\n",
      "Epoch [86/300], Step [26100/27733], Loss: 2.9439\n",
      "Epoch [86/300], Step [26200/27733], Loss: 3.5542\n",
      "Epoch [86/300], Step [26300/27733], Loss: 3.7512\n",
      "Epoch [86/300], Step [26400/27733], Loss: 2.7797\n",
      "Epoch [86/300], Step [26500/27733], Loss: 2.9622\n",
      "Epoch [86/300], Step [26600/27733], Loss: 2.2243\n",
      "Epoch [86/300], Step [26700/27733], Loss: 1.9429\n",
      "Epoch [86/300], Step [26800/27733], Loss: 2.9349\n",
      "Epoch [86/300], Step [26900/27733], Loss: 4.1013\n",
      "Epoch [86/300], Step [27000/27733], Loss: 2.7038\n",
      "Epoch [86/300], Step [27100/27733], Loss: 2.6075\n",
      "Epoch [86/300], Step [27200/27733], Loss: 3.0474\n",
      "Epoch [86/300], Step [27300/27733], Loss: 2.5300\n",
      "Epoch [86/300], Step [27400/27733], Loss: 3.3404\n",
      "Epoch [86/300], Step [27500/27733], Loss: 2.1539\n",
      "Epoch [86/300], Step [27600/27733], Loss: 3.2077\n",
      "Epoch [86/300], Step [27700/27733], Loss: 1.9329\n",
      "Epoch [87/300], Step [100/27733], Loss: 2.1693\n",
      "Epoch [87/300], Step [200/27733], Loss: 2.5765\n",
      "Epoch [87/300], Step [300/27733], Loss: 2.7179\n",
      "Epoch [87/300], Step [400/27733], Loss: 1.7672\n",
      "Epoch [87/300], Step [500/27733], Loss: 2.3802\n",
      "Epoch [87/300], Step [600/27733], Loss: 1.5897\n",
      "Epoch [87/300], Step [700/27733], Loss: 2.3416\n",
      "Epoch [87/300], Step [800/27733], Loss: 2.0599\n",
      "Epoch [87/300], Step [900/27733], Loss: 1.8622\n",
      "Epoch [87/300], Step [1000/27733], Loss: 1.8794\n",
      "Epoch [87/300], Step [1100/27733], Loss: 2.1298\n",
      "Epoch [87/300], Step [1200/27733], Loss: 2.7354\n",
      "Epoch [87/300], Step [1300/27733], Loss: 3.0929\n",
      "Epoch [87/300], Step [1400/27733], Loss: 2.6129\n",
      "Epoch [87/300], Step [1500/27733], Loss: 1.7253\n",
      "Epoch [87/300], Step [1600/27733], Loss: 2.5610\n",
      "Epoch [87/300], Step [1700/27733], Loss: 2.1855\n",
      "Epoch [87/300], Step [1800/27733], Loss: 1.9011\n",
      "Epoch [87/300], Step [1900/27733], Loss: 2.4320\n",
      "Epoch [87/300], Step [2000/27733], Loss: 2.7893\n",
      "Epoch [87/300], Step [2100/27733], Loss: 2.6910\n",
      "Epoch [87/300], Step [2200/27733], Loss: 2.4249\n",
      "Epoch [87/300], Step [2300/27733], Loss: 2.3998\n",
      "Epoch [87/300], Step [2400/27733], Loss: 2.0456\n",
      "Epoch [87/300], Step [2500/27733], Loss: 1.9985\n",
      "Epoch [87/300], Step [2600/27733], Loss: 2.7131\n",
      "Epoch [87/300], Step [2700/27733], Loss: 3.1853\n",
      "Epoch [87/300], Step [2800/27733], Loss: 1.9936\n",
      "Epoch [87/300], Step [2900/27733], Loss: 3.0054\n",
      "Epoch [87/300], Step [3000/27733], Loss: 2.7996\n",
      "Epoch [87/300], Step [3100/27733], Loss: 2.1682\n",
      "Epoch [87/300], Step [3200/27733], Loss: 3.4695\n",
      "Epoch [87/300], Step [3300/27733], Loss: 3.2489\n",
      "Epoch [87/300], Step [3400/27733], Loss: 2.5761\n",
      "Epoch [87/300], Step [3500/27733], Loss: 2.6673\n",
      "Epoch [87/300], Step [3600/27733], Loss: 2.3791\n",
      "Epoch [87/300], Step [3700/27733], Loss: 2.1287\n",
      "Epoch [87/300], Step [3800/27733], Loss: 1.6189\n",
      "Epoch [87/300], Step [3900/27733], Loss: 2.9173\n",
      "Epoch [87/300], Step [4000/27733], Loss: 2.9544\n",
      "Epoch [87/300], Step [4100/27733], Loss: 2.4094\n",
      "Epoch [87/300], Step [4200/27733], Loss: 2.6746\n",
      "Epoch [87/300], Step [4300/27733], Loss: 2.7120\n",
      "Epoch [87/300], Step [4400/27733], Loss: 2.6558\n",
      "Epoch [87/300], Step [4500/27733], Loss: 3.0465\n",
      "Epoch [87/300], Step [4600/27733], Loss: 3.0309\n",
      "Epoch [87/300], Step [4700/27733], Loss: 2.1848\n",
      "Epoch [87/300], Step [4800/27733], Loss: 2.8063\n",
      "Epoch [87/300], Step [4900/27733], Loss: 2.3384\n",
      "Epoch [87/300], Step [5000/27733], Loss: 2.7358\n",
      "Epoch [87/300], Step [5100/27733], Loss: 3.1902\n",
      "Epoch [87/300], Step [5200/27733], Loss: 2.5444\n",
      "Epoch [87/300], Step [5300/27733], Loss: 2.2756\n",
      "Epoch [87/300], Step [5400/27733], Loss: 2.4668\n",
      "Epoch [87/300], Step [5500/27733], Loss: 3.0503\n",
      "Epoch [87/300], Step [5600/27733], Loss: 2.6810\n",
      "Epoch [87/300], Step [5700/27733], Loss: 3.8076\n",
      "Epoch [87/300], Step [5800/27733], Loss: 2.6406\n",
      "Epoch [87/300], Step [5900/27733], Loss: 1.9037\n",
      "Epoch [87/300], Step [6000/27733], Loss: 2.3780\n",
      "Epoch [87/300], Step [6100/27733], Loss: 3.2193\n",
      "Epoch [87/300], Step [6200/27733], Loss: 1.7697\n",
      "Epoch [87/300], Step [6300/27733], Loss: 2.4689\n",
      "Epoch [87/300], Step [6400/27733], Loss: 2.5775\n",
      "Epoch [87/300], Step [6500/27733], Loss: 2.6718\n",
      "Epoch [87/300], Step [6600/27733], Loss: 2.5728\n",
      "Epoch [87/300], Step [6700/27733], Loss: 2.6885\n",
      "Epoch [87/300], Step [6800/27733], Loss: 2.7781\n",
      "Epoch [87/300], Step [6900/27733], Loss: 2.4178\n",
      "Epoch [87/300], Step [7000/27733], Loss: 3.4558\n",
      "Epoch [87/300], Step [7100/27733], Loss: 2.7060\n",
      "Epoch [87/300], Step [7200/27733], Loss: 2.5386\n",
      "Epoch [87/300], Step [7300/27733], Loss: 3.5086\n",
      "Epoch [87/300], Step [7400/27733], Loss: 2.3005\n",
      "Epoch [87/300], Step [7500/27733], Loss: 2.0399\n",
      "Epoch [87/300], Step [7600/27733], Loss: 2.5162\n",
      "Epoch [87/300], Step [7700/27733], Loss: 2.6571\n",
      "Epoch [87/300], Step [7800/27733], Loss: 2.7381\n",
      "Epoch [87/300], Step [7900/27733], Loss: 2.5675\n",
      "Epoch [87/300], Step [8000/27733], Loss: 1.8616\n",
      "Epoch [87/300], Step [8100/27733], Loss: 2.3839\n",
      "Epoch [87/300], Step [8200/27733], Loss: 2.7163\n",
      "Epoch [87/300], Step [8300/27733], Loss: 3.0665\n",
      "Epoch [87/300], Step [8400/27733], Loss: 3.2547\n",
      "Epoch [87/300], Step [8500/27733], Loss: 2.7359\n",
      "Epoch [87/300], Step [8600/27733], Loss: 2.0342\n",
      "Epoch [87/300], Step [8700/27733], Loss: 2.9173\n",
      "Epoch [87/300], Step [8800/27733], Loss: 2.1544\n",
      "Epoch [87/300], Step [8900/27733], Loss: 3.1191\n",
      "Epoch [87/300], Step [9000/27733], Loss: 2.4329\n",
      "Epoch [87/300], Step [9100/27733], Loss: 2.4791\n",
      "Epoch [87/300], Step [9200/27733], Loss: 2.6796\n",
      "Epoch [87/300], Step [9300/27733], Loss: 2.5135\n",
      "Epoch [87/300], Step [9400/27733], Loss: 2.4100\n",
      "Epoch [87/300], Step [9500/27733], Loss: 2.6109\n",
      "Epoch [87/300], Step [9600/27733], Loss: 2.6852\n",
      "Epoch [87/300], Step [9700/27733], Loss: 2.6944\n",
      "Epoch [87/300], Step [9800/27733], Loss: 3.7363\n",
      "Epoch [87/300], Step [9900/27733], Loss: 2.6060\n",
      "Epoch [87/300], Step [10000/27733], Loss: 2.9018\n",
      "Epoch [87/300], Step [10100/27733], Loss: 2.1737\n",
      "Epoch [87/300], Step [10200/27733], Loss: 2.4601\n",
      "Epoch [87/300], Step [10300/27733], Loss: 2.0716\n",
      "Epoch [87/300], Step [10400/27733], Loss: 2.6669\n",
      "Epoch [87/300], Step [10500/27733], Loss: 3.4088\n",
      "Epoch [87/300], Step [10600/27733], Loss: 3.2888\n",
      "Epoch [87/300], Step [10700/27733], Loss: 3.0681\n",
      "Epoch [87/300], Step [10800/27733], Loss: 1.8457\n",
      "Epoch [87/300], Step [10900/27733], Loss: 2.4117\n",
      "Epoch [87/300], Step [11000/27733], Loss: 2.5181\n",
      "Epoch [87/300], Step [11100/27733], Loss: 3.6814\n",
      "Epoch [87/300], Step [11200/27733], Loss: 2.7880\n",
      "Epoch [87/300], Step [11300/27733], Loss: 2.8771\n",
      "Epoch [87/300], Step [11400/27733], Loss: 2.2851\n",
      "Epoch [87/300], Step [11500/27733], Loss: 3.3276\n",
      "Epoch [87/300], Step [11600/27733], Loss: 2.8808\n",
      "Epoch [87/300], Step [11700/27733], Loss: 3.5667\n",
      "Epoch [87/300], Step [11800/27733], Loss: 2.8000\n",
      "Epoch [87/300], Step [11900/27733], Loss: 2.9262\n",
      "Epoch [87/300], Step [12000/27733], Loss: 2.2795\n",
      "Epoch [87/300], Step [12100/27733], Loss: 3.3008\n",
      "Epoch [87/300], Step [12200/27733], Loss: 2.8156\n",
      "Epoch [87/300], Step [12300/27733], Loss: 2.7102\n",
      "Epoch [87/300], Step [12400/27733], Loss: 3.4775\n",
      "Epoch [87/300], Step [12500/27733], Loss: 2.9052\n",
      "Epoch [87/300], Step [12600/27733], Loss: 3.2270\n",
      "Epoch [87/300], Step [12700/27733], Loss: 2.1788\n",
      "Epoch [87/300], Step [12800/27733], Loss: 2.2585\n",
      "Epoch [87/300], Step [12900/27733], Loss: 3.3429\n",
      "Epoch [87/300], Step [13000/27733], Loss: 2.1088\n",
      "Epoch [87/300], Step [13100/27733], Loss: 2.6992\n",
      "Epoch [87/300], Step [13200/27733], Loss: 2.9039\n",
      "Epoch [87/300], Step [13300/27733], Loss: 2.9399\n",
      "Epoch [87/300], Step [13400/27733], Loss: 2.8938\n",
      "Epoch [87/300], Step [13500/27733], Loss: 2.8812\n",
      "Epoch [87/300], Step [13600/27733], Loss: 2.9774\n",
      "Epoch [87/300], Step [13700/27733], Loss: 2.8636\n",
      "Epoch [87/300], Step [13800/27733], Loss: 2.6681\n",
      "Epoch [87/300], Step [13900/27733], Loss: 2.1604\n",
      "Epoch [87/300], Step [14000/27733], Loss: 2.3729\n",
      "Epoch [87/300], Step [14100/27733], Loss: 3.0524\n",
      "Epoch [87/300], Step [14200/27733], Loss: 2.3964\n",
      "Epoch [87/300], Step [14300/27733], Loss: 2.5317\n",
      "Epoch [87/300], Step [14400/27733], Loss: 1.7330\n",
      "Epoch [87/300], Step [14500/27733], Loss: 2.7459\n",
      "Epoch [87/300], Step [14600/27733], Loss: 2.6445\n",
      "Epoch [87/300], Step [14700/27733], Loss: 2.6771\n",
      "Epoch [87/300], Step [14800/27733], Loss: 2.8597\n",
      "Epoch [87/300], Step [14900/27733], Loss: 2.7967\n",
      "Epoch [87/300], Step [15000/27733], Loss: 2.7843\n",
      "Epoch [87/300], Step [15100/27733], Loss: 3.2511\n",
      "Epoch [87/300], Step [15200/27733], Loss: 2.5790\n",
      "Epoch [87/300], Step [15300/27733], Loss: 2.3630\n",
      "Epoch [87/300], Step [15400/27733], Loss: 3.0111\n",
      "Epoch [87/300], Step [15500/27733], Loss: 2.4258\n",
      "Epoch [87/300], Step [15600/27733], Loss: 2.4480\n",
      "Epoch [87/300], Step [15700/27733], Loss: 2.9978\n",
      "Epoch [87/300], Step [15800/27733], Loss: 2.3604\n",
      "Epoch [87/300], Step [15900/27733], Loss: 3.6398\n",
      "Epoch [87/300], Step [16000/27733], Loss: 2.1767\n",
      "Epoch [87/300], Step [16100/27733], Loss: 2.4965\n",
      "Epoch [87/300], Step [16200/27733], Loss: 2.2180\n",
      "Epoch [87/300], Step [16300/27733], Loss: 3.0242\n",
      "Epoch [87/300], Step [16400/27733], Loss: 2.1312\n",
      "Epoch [87/300], Step [16500/27733], Loss: 3.1999\n",
      "Epoch [87/300], Step [16600/27733], Loss: 2.3553\n",
      "Epoch [87/300], Step [16700/27733], Loss: 2.4010\n",
      "Epoch [87/300], Step [16800/27733], Loss: 3.2401\n",
      "Epoch [87/300], Step [16900/27733], Loss: 2.9442\n",
      "Epoch [87/300], Step [17000/27733], Loss: 2.7373\n",
      "Epoch [87/300], Step [17100/27733], Loss: 3.1071\n",
      "Epoch [87/300], Step [17200/27733], Loss: 2.5581\n",
      "Epoch [87/300], Step [17300/27733], Loss: 2.5553\n",
      "Epoch [87/300], Step [17400/27733], Loss: 2.8708\n",
      "Epoch [87/300], Step [17500/27733], Loss: 2.9140\n",
      "Epoch [87/300], Step [17600/27733], Loss: 2.9655\n",
      "Epoch [87/300], Step [17700/27733], Loss: 3.2924\n",
      "Epoch [87/300], Step [17800/27733], Loss: 2.5796\n",
      "Epoch [87/300], Step [17900/27733], Loss: 3.0606\n",
      "Epoch [87/300], Step [18000/27733], Loss: 3.2594\n",
      "Epoch [87/300], Step [18100/27733], Loss: 3.6361\n",
      "Epoch [87/300], Step [18200/27733], Loss: 2.4765\n",
      "Epoch [87/300], Step [18300/27733], Loss: 2.8194\n",
      "Epoch [87/300], Step [18400/27733], Loss: 2.7735\n",
      "Epoch [87/300], Step [18500/27733], Loss: 2.5776\n",
      "Epoch [87/300], Step [18600/27733], Loss: 3.0677\n",
      "Epoch [87/300], Step [18700/27733], Loss: 1.8223\n",
      "Epoch [87/300], Step [18800/27733], Loss: 3.1640\n",
      "Epoch [87/300], Step [18900/27733], Loss: 3.3368\n",
      "Epoch [87/300], Step [19000/27733], Loss: 3.3746\n",
      "Epoch [87/300], Step [19100/27733], Loss: 2.9619\n",
      "Epoch [87/300], Step [19200/27733], Loss: 3.1842\n",
      "Epoch [87/300], Step [19300/27733], Loss: 2.5549\n",
      "Epoch [87/300], Step [19400/27733], Loss: 2.4248\n",
      "Epoch [87/300], Step [19500/27733], Loss: 3.0698\n",
      "Epoch [87/300], Step [19600/27733], Loss: 2.2711\n",
      "Epoch [87/300], Step [19700/27733], Loss: 3.0318\n",
      "Epoch [87/300], Step [19800/27733], Loss: 3.3628\n",
      "Epoch [87/300], Step [19900/27733], Loss: 2.5424\n",
      "Epoch [87/300], Step [20000/27733], Loss: 3.0287\n",
      "Epoch [87/300], Step [20100/27733], Loss: 3.0401\n",
      "Epoch [87/300], Step [20200/27733], Loss: 2.9715\n",
      "Epoch [87/300], Step [20300/27733], Loss: 3.1522\n",
      "Epoch [87/300], Step [20400/27733], Loss: 3.4244\n",
      "Epoch [87/300], Step [20500/27733], Loss: 2.2924\n",
      "Epoch [87/300], Step [20600/27733], Loss: 2.6121\n",
      "Epoch [87/300], Step [20700/27733], Loss: 3.6983\n",
      "Epoch [87/300], Step [20800/27733], Loss: 2.3149\n",
      "Epoch [87/300], Step [20900/27733], Loss: 3.3669\n",
      "Epoch [87/300], Step [21000/27733], Loss: 3.0244\n",
      "Epoch [87/300], Step [21100/27733], Loss: 2.0681\n",
      "Epoch [87/300], Step [21200/27733], Loss: 3.2274\n",
      "Epoch [87/300], Step [21300/27733], Loss: 2.6381\n",
      "Epoch [87/300], Step [21400/27733], Loss: 3.6681\n",
      "Epoch [87/300], Step [21500/27733], Loss: 1.8693\n",
      "Epoch [87/300], Step [21600/27733], Loss: 3.0769\n",
      "Epoch [87/300], Step [21700/27733], Loss: 2.5861\n",
      "Epoch [87/300], Step [21800/27733], Loss: 2.2012\n",
      "Epoch [87/300], Step [21900/27733], Loss: 2.9212\n",
      "Epoch [87/300], Step [22000/27733], Loss: 2.7792\n",
      "Epoch [87/300], Step [22100/27733], Loss: 2.8030\n",
      "Epoch [87/300], Step [22200/27733], Loss: 2.7053\n",
      "Epoch [87/300], Step [22300/27733], Loss: 3.1994\n",
      "Epoch [87/300], Step [22400/27733], Loss: 2.7564\n",
      "Epoch [87/300], Step [22500/27733], Loss: 2.6561\n",
      "Epoch [87/300], Step [22600/27733], Loss: 2.2457\n",
      "Epoch [87/300], Step [22700/27733], Loss: 3.1943\n",
      "Epoch [87/300], Step [22800/27733], Loss: 2.9337\n",
      "Epoch [87/300], Step [22900/27733], Loss: 2.8540\n",
      "Epoch [87/300], Step [23000/27733], Loss: 2.0353\n",
      "Epoch [87/300], Step [23100/27733], Loss: 2.3097\n",
      "Epoch [87/300], Step [23200/27733], Loss: 3.5906\n",
      "Epoch [87/300], Step [23300/27733], Loss: 3.8731\n",
      "Epoch [87/300], Step [23400/27733], Loss: 2.6392\n",
      "Epoch [87/300], Step [23500/27733], Loss: 3.3431\n",
      "Epoch [87/300], Step [23600/27733], Loss: 1.6867\n",
      "Epoch [87/300], Step [23700/27733], Loss: 3.6951\n",
      "Epoch [87/300], Step [23800/27733], Loss: 2.8304\n",
      "Epoch [87/300], Step [23900/27733], Loss: 2.9622\n",
      "Epoch [87/300], Step [24000/27733], Loss: 2.9387\n",
      "Epoch [87/300], Step [24100/27733], Loss: 3.6288\n",
      "Epoch [87/300], Step [24200/27733], Loss: 3.7198\n",
      "Epoch [87/300], Step [24300/27733], Loss: 2.8592\n",
      "Epoch [87/300], Step [24400/27733], Loss: 3.9095\n",
      "Epoch [87/300], Step [24500/27733], Loss: 2.2909\n",
      "Epoch [87/300], Step [24600/27733], Loss: 2.4972\n",
      "Epoch [87/300], Step [24700/27733], Loss: 2.6421\n",
      "Epoch [87/300], Step [24800/27733], Loss: 2.6302\n",
      "Epoch [87/300], Step [24900/27733], Loss: 3.3479\n",
      "Epoch [87/300], Step [25000/27733], Loss: 2.9396\n",
      "Epoch [87/300], Step [25100/27733], Loss: 2.5406\n",
      "Epoch [87/300], Step [25200/27733], Loss: 2.3089\n",
      "Epoch [87/300], Step [25300/27733], Loss: 3.7894\n",
      "Epoch [87/300], Step [25400/27733], Loss: 2.9659\n",
      "Epoch [87/300], Step [25500/27733], Loss: 2.5995\n",
      "Epoch [87/300], Step [25600/27733], Loss: 4.3584\n",
      "Epoch [87/300], Step [25700/27733], Loss: 2.5311\n",
      "Epoch [87/300], Step [25800/27733], Loss: 2.8915\n",
      "Epoch [87/300], Step [25900/27733], Loss: 2.6907\n",
      "Epoch [87/300], Step [26000/27733], Loss: 3.4899\n",
      "Epoch [87/300], Step [26100/27733], Loss: 2.9596\n",
      "Epoch [87/300], Step [26200/27733], Loss: 3.1130\n",
      "Epoch [87/300], Step [26300/27733], Loss: 3.9534\n",
      "Epoch [87/300], Step [26400/27733], Loss: 2.6880\n",
      "Epoch [87/300], Step [26500/27733], Loss: 2.7129\n",
      "Epoch [87/300], Step [26600/27733], Loss: 3.6657\n",
      "Epoch [87/300], Step [26700/27733], Loss: 2.9240\n",
      "Epoch [87/300], Step [26800/27733], Loss: 2.5586\n",
      "Epoch [87/300], Step [26900/27733], Loss: 2.7872\n",
      "Epoch [87/300], Step [27000/27733], Loss: 2.3885\n",
      "Epoch [87/300], Step [27100/27733], Loss: 3.0487\n",
      "Epoch [87/300], Step [27200/27733], Loss: 4.3059\n",
      "Epoch [87/300], Step [27300/27733], Loss: 3.3493\n",
      "Epoch [87/300], Step [27400/27733], Loss: 2.8397\n",
      "Epoch [87/300], Step [27500/27733], Loss: 2.7421\n",
      "Epoch [87/300], Step [27600/27733], Loss: 3.1024\n",
      "Epoch [87/300], Step [27700/27733], Loss: 3.3657\n",
      "Epoch [88/300], Step [100/27733], Loss: 2.2485\n",
      "Epoch [88/300], Step [200/27733], Loss: 1.6624\n",
      "Epoch [88/300], Step [300/27733], Loss: 2.6194\n",
      "Epoch [88/300], Step [400/27733], Loss: 1.7968\n",
      "Epoch [88/300], Step [500/27733], Loss: 3.0459\n",
      "Epoch [88/300], Step [600/27733], Loss: 2.4389\n",
      "Epoch [88/300], Step [700/27733], Loss: 2.3776\n",
      "Epoch [88/300], Step [800/27733], Loss: 3.3180\n",
      "Epoch [88/300], Step [900/27733], Loss: 2.3816\n",
      "Epoch [88/300], Step [1000/27733], Loss: 2.4972\n",
      "Epoch [88/300], Step [1100/27733], Loss: 2.7755\n",
      "Epoch [88/300], Step [1200/27733], Loss: 2.7031\n",
      "Epoch [88/300], Step [1300/27733], Loss: 3.2162\n",
      "Epoch [88/300], Step [1400/27733], Loss: 1.8639\n",
      "Epoch [88/300], Step [1500/27733], Loss: 2.9792\n",
      "Epoch [88/300], Step [1600/27733], Loss: 2.2339\n",
      "Epoch [88/300], Step [1700/27733], Loss: 2.5014\n",
      "Epoch [88/300], Step [1800/27733], Loss: 2.7485\n",
      "Epoch [88/300], Step [1900/27733], Loss: 2.0108\n",
      "Epoch [88/300], Step [2000/27733], Loss: 2.3978\n",
      "Epoch [88/300], Step [2100/27733], Loss: 2.5105\n",
      "Epoch [88/300], Step [2200/27733], Loss: 2.8801\n",
      "Epoch [88/300], Step [2300/27733], Loss: 2.5259\n",
      "Epoch [88/300], Step [2400/27733], Loss: 2.1666\n",
      "Epoch [88/300], Step [2500/27733], Loss: 2.2132\n",
      "Epoch [88/300], Step [2600/27733], Loss: 1.9235\n",
      "Epoch [88/300], Step [2700/27733], Loss: 2.0451\n",
      "Epoch [88/300], Step [2800/27733], Loss: 2.1435\n",
      "Epoch [88/300], Step [2900/27733], Loss: 2.6554\n",
      "Epoch [88/300], Step [3000/27733], Loss: 1.2721\n",
      "Epoch [88/300], Step [3100/27733], Loss: 2.2727\n",
      "Epoch [88/300], Step [3200/27733], Loss: 2.1648\n",
      "Epoch [88/300], Step [3300/27733], Loss: 2.2400\n",
      "Epoch [88/300], Step [3400/27733], Loss: 2.3043\n",
      "Epoch [88/300], Step [3500/27733], Loss: 2.5892\n",
      "Epoch [88/300], Step [3600/27733], Loss: 2.8994\n",
      "Epoch [88/300], Step [3700/27733], Loss: 2.3272\n",
      "Epoch [88/300], Step [3800/27733], Loss: 2.7818\n",
      "Epoch [88/300], Step [3900/27733], Loss: 2.9260\n",
      "Epoch [88/300], Step [4000/27733], Loss: 2.9173\n",
      "Epoch [88/300], Step [4100/27733], Loss: 3.4715\n",
      "Epoch [88/300], Step [4200/27733], Loss: 2.3146\n",
      "Epoch [88/300], Step [4300/27733], Loss: 3.0155\n",
      "Epoch [88/300], Step [4400/27733], Loss: 2.9950\n",
      "Epoch [88/300], Step [4500/27733], Loss: 1.8055\n",
      "Epoch [88/300], Step [4600/27733], Loss: 2.5477\n",
      "Epoch [88/300], Step [4700/27733], Loss: 2.5834\n",
      "Epoch [88/300], Step [4800/27733], Loss: 2.8358\n",
      "Epoch [88/300], Step [4900/27733], Loss: 2.0960\n",
      "Epoch [88/300], Step [5000/27733], Loss: 2.8071\n",
      "Epoch [88/300], Step [5100/27733], Loss: 2.4618\n",
      "Epoch [88/300], Step [5200/27733], Loss: 2.2839\n",
      "Epoch [88/300], Step [5300/27733], Loss: 3.0725\n",
      "Epoch [88/300], Step [5400/27733], Loss: 2.6744\n",
      "Epoch [88/300], Step [5500/27733], Loss: 3.4444\n",
      "Epoch [88/300], Step [5600/27733], Loss: 2.1995\n",
      "Epoch [88/300], Step [5700/27733], Loss: 2.4369\n",
      "Epoch [88/300], Step [5800/27733], Loss: 2.1500\n",
      "Epoch [88/300], Step [5900/27733], Loss: 3.1386\n",
      "Epoch [88/300], Step [6000/27733], Loss: 2.6306\n",
      "Epoch [88/300], Step [6100/27733], Loss: 2.7143\n",
      "Epoch [88/300], Step [6200/27733], Loss: 3.0468\n",
      "Epoch [88/300], Step [6300/27733], Loss: 2.5006\n",
      "Epoch [88/300], Step [6400/27733], Loss: 2.5189\n",
      "Epoch [88/300], Step [6500/27733], Loss: 2.6367\n",
      "Epoch [88/300], Step [6600/27733], Loss: 2.8638\n",
      "Epoch [88/300], Step [6700/27733], Loss: 2.7273\n",
      "Epoch [88/300], Step [6800/27733], Loss: 2.2652\n",
      "Epoch [88/300], Step [6900/27733], Loss: 2.3202\n",
      "Epoch [88/300], Step [7000/27733], Loss: 2.6173\n",
      "Epoch [88/300], Step [7100/27733], Loss: 3.0756\n",
      "Epoch [88/300], Step [7200/27733], Loss: 2.1218\n",
      "Epoch [88/300], Step [7300/27733], Loss: 3.0334\n",
      "Epoch [88/300], Step [7400/27733], Loss: 2.7040\n",
      "Epoch [88/300], Step [7500/27733], Loss: 2.2172\n",
      "Epoch [88/300], Step [7600/27733], Loss: 2.3890\n",
      "Epoch [88/300], Step [7700/27733], Loss: 2.3467\n",
      "Epoch [88/300], Step [7800/27733], Loss: 2.9477\n",
      "Epoch [88/300], Step [7900/27733], Loss: 3.4425\n",
      "Epoch [88/300], Step [8000/27733], Loss: 1.8843\n",
      "Epoch [88/300], Step [8100/27733], Loss: 2.8831\n",
      "Epoch [88/300], Step [8200/27733], Loss: 3.3773\n",
      "Epoch [88/300], Step [8300/27733], Loss: 2.9405\n",
      "Epoch [88/300], Step [8400/27733], Loss: 2.5775\n",
      "Epoch [88/300], Step [8500/27733], Loss: 2.8472\n",
      "Epoch [88/300], Step [8600/27733], Loss: 2.7810\n",
      "Epoch [88/300], Step [8700/27733], Loss: 2.4749\n",
      "Epoch [88/300], Step [8800/27733], Loss: 2.4741\n",
      "Epoch [88/300], Step [8900/27733], Loss: 3.0382\n",
      "Epoch [88/300], Step [9000/27733], Loss: 1.9389\n",
      "Epoch [88/300], Step [9100/27733], Loss: 2.7275\n",
      "Epoch [88/300], Step [9200/27733], Loss: 2.0844\n",
      "Epoch [88/300], Step [9300/27733], Loss: 2.6382\n",
      "Epoch [88/300], Step [9400/27733], Loss: 2.4001\n",
      "Epoch [88/300], Step [9500/27733], Loss: 2.9903\n",
      "Epoch [88/300], Step [9600/27733], Loss: 2.6471\n",
      "Epoch [88/300], Step [9700/27733], Loss: 2.4346\n",
      "Epoch [88/300], Step [9800/27733], Loss: 3.0778\n",
      "Epoch [88/300], Step [9900/27733], Loss: 2.8873\n",
      "Epoch [88/300], Step [10000/27733], Loss: 2.9814\n",
      "Epoch [88/300], Step [10100/27733], Loss: 2.3705\n",
      "Epoch [88/300], Step [10200/27733], Loss: 2.4484\n",
      "Epoch [88/300], Step [10300/27733], Loss: 3.3659\n",
      "Epoch [88/300], Step [10400/27733], Loss: 2.4614\n",
      "Epoch [88/300], Step [10500/27733], Loss: 2.3902\n",
      "Epoch [88/300], Step [10600/27733], Loss: 2.5546\n",
      "Epoch [88/300], Step [10700/27733], Loss: 2.5186\n",
      "Epoch [88/300], Step [10800/27733], Loss: 2.6829\n",
      "Epoch [88/300], Step [10900/27733], Loss: 1.8196\n",
      "Epoch [88/300], Step [11000/27733], Loss: 1.9793\n",
      "Epoch [88/300], Step [11100/27733], Loss: 2.5360\n",
      "Epoch [88/300], Step [11200/27733], Loss: 2.4868\n",
      "Epoch [88/300], Step [11300/27733], Loss: 2.4482\n",
      "Epoch [88/300], Step [11400/27733], Loss: 2.4245\n",
      "Epoch [88/300], Step [11500/27733], Loss: 2.7002\n",
      "Epoch [88/300], Step [11600/27733], Loss: 2.9308\n",
      "Epoch [88/300], Step [11700/27733], Loss: 3.4750\n",
      "Epoch [88/300], Step [11800/27733], Loss: 2.1270\n",
      "Epoch [88/300], Step [11900/27733], Loss: 2.1662\n",
      "Epoch [88/300], Step [12000/27733], Loss: 3.1499\n",
      "Epoch [88/300], Step [12100/27733], Loss: 2.4948\n",
      "Epoch [88/300], Step [12200/27733], Loss: 2.6560\n",
      "Epoch [88/300], Step [12300/27733], Loss: 2.8491\n",
      "Epoch [88/300], Step [12400/27733], Loss: 2.8723\n",
      "Epoch [88/300], Step [12500/27733], Loss: 1.8104\n",
      "Epoch [88/300], Step [12600/27733], Loss: 3.4334\n",
      "Epoch [88/300], Step [12700/27733], Loss: 2.4363\n",
      "Epoch [88/300], Step [12800/27733], Loss: 2.6447\n",
      "Epoch [88/300], Step [12900/27733], Loss: 3.1284\n",
      "Epoch [88/300], Step [13000/27733], Loss: 2.3049\n",
      "Epoch [88/300], Step [13100/27733], Loss: 2.9442\n",
      "Epoch [88/300], Step [13200/27733], Loss: 2.2313\n",
      "Epoch [88/300], Step [13300/27733], Loss: 2.6680\n",
      "Epoch [88/300], Step [13400/27733], Loss: 2.8233\n",
      "Epoch [88/300], Step [13500/27733], Loss: 2.9741\n",
      "Epoch [88/300], Step [13600/27733], Loss: 2.7429\n",
      "Epoch [88/300], Step [13700/27733], Loss: 2.0396\n",
      "Epoch [88/300], Step [13800/27733], Loss: 2.2270\n",
      "Epoch [88/300], Step [13900/27733], Loss: 3.4254\n",
      "Epoch [88/300], Step [14000/27733], Loss: 2.2368\n",
      "Epoch [88/300], Step [14100/27733], Loss: 3.1908\n",
      "Epoch [88/300], Step [14200/27733], Loss: 3.3623\n",
      "Epoch [88/300], Step [14300/27733], Loss: 2.6478\n",
      "Epoch [88/300], Step [14400/27733], Loss: 2.5818\n",
      "Epoch [88/300], Step [14500/27733], Loss: 4.3353\n",
      "Epoch [88/300], Step [14600/27733], Loss: 3.1704\n",
      "Epoch [88/300], Step [14700/27733], Loss: 3.0930\n",
      "Epoch [88/300], Step [14800/27733], Loss: 2.3277\n",
      "Epoch [88/300], Step [14900/27733], Loss: 2.7316\n",
      "Epoch [88/300], Step [15000/27733], Loss: 2.6762\n",
      "Epoch [88/300], Step [15100/27733], Loss: 1.9765\n",
      "Epoch [88/300], Step [15200/27733], Loss: 3.6038\n",
      "Epoch [88/300], Step [15300/27733], Loss: 2.0847\n",
      "Epoch [88/300], Step [15400/27733], Loss: 3.1060\n",
      "Epoch [88/300], Step [15500/27733], Loss: 3.5112\n",
      "Epoch [88/300], Step [15600/27733], Loss: 2.8764\n",
      "Epoch [88/300], Step [15700/27733], Loss: 3.2775\n",
      "Epoch [88/300], Step [15800/27733], Loss: 2.4976\n",
      "Epoch [88/300], Step [15900/27733], Loss: 2.2869\n",
      "Epoch [88/300], Step [16000/27733], Loss: 2.3410\n",
      "Epoch [88/300], Step [16100/27733], Loss: 2.8768\n",
      "Epoch [88/300], Step [16200/27733], Loss: 2.8002\n",
      "Epoch [88/300], Step [16300/27733], Loss: 2.2656\n",
      "Epoch [88/300], Step [16400/27733], Loss: 2.8547\n",
      "Epoch [88/300], Step [16500/27733], Loss: 3.0196\n",
      "Epoch [88/300], Step [16600/27733], Loss: 3.2116\n",
      "Epoch [88/300], Step [16700/27733], Loss: 2.3157\n",
      "Epoch [88/300], Step [16800/27733], Loss: 3.8767\n",
      "Epoch [88/300], Step [16900/27733], Loss: 3.2011\n",
      "Epoch [88/300], Step [17000/27733], Loss: 2.8940\n",
      "Epoch [88/300], Step [17100/27733], Loss: 2.5755\n",
      "Epoch [88/300], Step [17200/27733], Loss: 3.4037\n",
      "Epoch [88/300], Step [17300/27733], Loss: 2.9197\n",
      "Epoch [88/300], Step [17400/27733], Loss: 3.4997\n",
      "Epoch [88/300], Step [17500/27733], Loss: 2.3538\n",
      "Epoch [88/300], Step [17600/27733], Loss: 2.6214\n",
      "Epoch [88/300], Step [17700/27733], Loss: 2.7890\n",
      "Epoch [88/300], Step [17800/27733], Loss: 3.1316\n",
      "Epoch [88/300], Step [17900/27733], Loss: 3.0931\n",
      "Epoch [88/300], Step [18000/27733], Loss: 2.5999\n",
      "Epoch [88/300], Step [18100/27733], Loss: 3.1538\n",
      "Epoch [88/300], Step [18200/27733], Loss: 1.7972\n",
      "Epoch [88/300], Step [18300/27733], Loss: 3.1137\n",
      "Epoch [88/300], Step [18400/27733], Loss: 2.5100\n",
      "Epoch [88/300], Step [18500/27733], Loss: 3.4994\n",
      "Epoch [88/300], Step [18600/27733], Loss: 2.4600\n",
      "Epoch [88/300], Step [18700/27733], Loss: 3.1729\n",
      "Epoch [88/300], Step [18800/27733], Loss: 2.8286\n",
      "Epoch [88/300], Step [18900/27733], Loss: 2.7793\n",
      "Epoch [88/300], Step [19000/27733], Loss: 1.7632\n",
      "Epoch [88/300], Step [19100/27733], Loss: 3.0869\n",
      "Epoch [88/300], Step [19200/27733], Loss: 2.4153\n",
      "Epoch [88/300], Step [19300/27733], Loss: 3.5789\n",
      "Epoch [88/300], Step [19400/27733], Loss: 2.8724\n",
      "Epoch [88/300], Step [19500/27733], Loss: 2.5353\n",
      "Epoch [88/300], Step [19600/27733], Loss: 2.2138\n",
      "Epoch [88/300], Step [19700/27733], Loss: 3.2949\n",
      "Epoch [88/300], Step [19800/27733], Loss: 3.1271\n",
      "Epoch [88/300], Step [19900/27733], Loss: 2.8316\n",
      "Epoch [88/300], Step [20000/27733], Loss: 3.1005\n",
      "Epoch [88/300], Step [20100/27733], Loss: 2.9670\n",
      "Epoch [88/300], Step [20200/27733], Loss: 2.6350\n",
      "Epoch [88/300], Step [20300/27733], Loss: 2.4981\n",
      "Epoch [88/300], Step [20400/27733], Loss: 2.5846\n",
      "Epoch [88/300], Step [20500/27733], Loss: 2.7768\n",
      "Epoch [88/300], Step [20600/27733], Loss: 3.5239\n",
      "Epoch [88/300], Step [20700/27733], Loss: 3.1579\n",
      "Epoch [88/300], Step [20800/27733], Loss: 2.4004\n",
      "Epoch [88/300], Step [20900/27733], Loss: 2.8534\n",
      "Epoch [88/300], Step [21000/27733], Loss: 2.7920\n",
      "Epoch [88/300], Step [21100/27733], Loss: 2.6344\n",
      "Epoch [88/300], Step [21200/27733], Loss: 2.7490\n",
      "Epoch [88/300], Step [21300/27733], Loss: 2.9593\n",
      "Epoch [88/300], Step [21400/27733], Loss: 2.8053\n",
      "Epoch [88/300], Step [21500/27733], Loss: 2.8496\n",
      "Epoch [88/300], Step [21600/27733], Loss: 3.2390\n",
      "Epoch [88/300], Step [21700/27733], Loss: 2.9161\n",
      "Epoch [88/300], Step [21800/27733], Loss: 2.3934\n",
      "Epoch [88/300], Step [21900/27733], Loss: 2.8676\n",
      "Epoch [88/300], Step [22000/27733], Loss: 2.5058\n",
      "Epoch [88/300], Step [22100/27733], Loss: 3.1095\n",
      "Epoch [88/300], Step [22200/27733], Loss: 2.9669\n",
      "Epoch [88/300], Step [22300/27733], Loss: 3.0929\n",
      "Epoch [88/300], Step [22400/27733], Loss: 2.7765\n",
      "Epoch [88/300], Step [22500/27733], Loss: 2.8900\n",
      "Epoch [88/300], Step [22600/27733], Loss: 2.4086\n",
      "Epoch [88/300], Step [22700/27733], Loss: 2.7516\n",
      "Epoch [88/300], Step [22800/27733], Loss: 3.0470\n",
      "Epoch [88/300], Step [22900/27733], Loss: 2.9635\n",
      "Epoch [88/300], Step [23000/27733], Loss: 2.5212\n",
      "Epoch [88/300], Step [23100/27733], Loss: 1.8943\n",
      "Epoch [88/300], Step [23200/27733], Loss: 2.5736\n",
      "Epoch [88/300], Step [23300/27733], Loss: 2.4254\n",
      "Epoch [88/300], Step [23400/27733], Loss: 2.8319\n",
      "Epoch [88/300], Step [23500/27733], Loss: 2.8874\n",
      "Epoch [88/300], Step [23600/27733], Loss: 2.6644\n",
      "Epoch [88/300], Step [23700/27733], Loss: 3.6903\n",
      "Epoch [88/300], Step [23800/27733], Loss: 2.4079\n",
      "Epoch [88/300], Step [23900/27733], Loss: 3.2480\n",
      "Epoch [88/300], Step [24000/27733], Loss: 2.9683\n",
      "Epoch [88/300], Step [24100/27733], Loss: 2.9666\n",
      "Epoch [88/300], Step [24200/27733], Loss: 2.3158\n",
      "Epoch [88/300], Step [24300/27733], Loss: 2.7203\n",
      "Epoch [88/300], Step [24400/27733], Loss: 3.1617\n",
      "Epoch [88/300], Step [24500/27733], Loss: 4.0302\n",
      "Epoch [88/300], Step [24600/27733], Loss: 2.0145\n",
      "Epoch [88/300], Step [24700/27733], Loss: 3.5670\n",
      "Epoch [88/300], Step [24800/27733], Loss: 3.4415\n",
      "Epoch [88/300], Step [24900/27733], Loss: 2.8771\n",
      "Epoch [88/300], Step [25000/27733], Loss: 3.1083\n",
      "Epoch [88/300], Step [25100/27733], Loss: 3.4906\n",
      "Epoch [88/300], Step [25200/27733], Loss: 2.9149\n",
      "Epoch [88/300], Step [25300/27733], Loss: 3.2700\n",
      "Epoch [88/300], Step [25400/27733], Loss: 3.1360\n",
      "Epoch [88/300], Step [25500/27733], Loss: 2.8217\n",
      "Epoch [88/300], Step [25600/27733], Loss: 2.8531\n",
      "Epoch [88/300], Step [25700/27733], Loss: 2.5458\n",
      "Epoch [88/300], Step [25800/27733], Loss: 3.8732\n",
      "Epoch [88/300], Step [25900/27733], Loss: 2.8191\n",
      "Epoch [88/300], Step [26000/27733], Loss: 3.3714\n",
      "Epoch [88/300], Step [26100/27733], Loss: 2.6550\n",
      "Epoch [88/300], Step [26200/27733], Loss: 3.5551\n",
      "Epoch [88/300], Step [26300/27733], Loss: 4.4925\n",
      "Epoch [88/300], Step [26400/27733], Loss: 2.8091\n",
      "Epoch [88/300], Step [26500/27733], Loss: 2.6000\n",
      "Epoch [88/300], Step [26600/27733], Loss: 3.7164\n",
      "Epoch [88/300], Step [26700/27733], Loss: 2.8705\n",
      "Epoch [88/300], Step [26800/27733], Loss: 2.8088\n",
      "Epoch [88/300], Step [26900/27733], Loss: 3.3704\n",
      "Epoch [88/300], Step [27000/27733], Loss: 3.2509\n",
      "Epoch [88/300], Step [27100/27733], Loss: 3.0672\n",
      "Epoch [88/300], Step [27200/27733], Loss: 3.9550\n",
      "Epoch [88/300], Step [27300/27733], Loss: 2.9363\n",
      "Epoch [88/300], Step [27400/27733], Loss: 3.4374\n",
      "Epoch [88/300], Step [27500/27733], Loss: 2.4836\n",
      "Epoch [88/300], Step [27600/27733], Loss: 2.9929\n",
      "Epoch [88/300], Step [27700/27733], Loss: 2.8417\n",
      "Epoch [89/300], Step [100/27733], Loss: 2.1973\n",
      "Epoch [89/300], Step [200/27733], Loss: 2.0447\n",
      "Epoch [89/300], Step [300/27733], Loss: 2.4737\n",
      "Epoch [89/300], Step [400/27733], Loss: 2.4384\n",
      "Epoch [89/300], Step [500/27733], Loss: 1.7693\n",
      "Epoch [89/300], Step [600/27733], Loss: 2.4517\n",
      "Epoch [89/300], Step [700/27733], Loss: 2.5231\n",
      "Epoch [89/300], Step [800/27733], Loss: 2.1553\n",
      "Epoch [89/300], Step [900/27733], Loss: 2.2472\n",
      "Epoch [89/300], Step [1000/27733], Loss: 2.9606\n",
      "Epoch [89/300], Step [1100/27733], Loss: 2.4191\n",
      "Epoch [89/300], Step [1200/27733], Loss: 2.8207\n",
      "Epoch [89/300], Step [1300/27733], Loss: 2.4843\n",
      "Epoch [89/300], Step [1400/27733], Loss: 2.1194\n",
      "Epoch [89/300], Step [1500/27733], Loss: 1.9677\n",
      "Epoch [89/300], Step [1600/27733], Loss: 2.3502\n",
      "Epoch [89/300], Step [1700/27733], Loss: 2.7561\n",
      "Epoch [89/300], Step [1800/27733], Loss: 2.4003\n",
      "Epoch [89/300], Step [1900/27733], Loss: 2.0257\n",
      "Epoch [89/300], Step [2000/27733], Loss: 1.9436\n",
      "Epoch [89/300], Step [2100/27733], Loss: 2.4083\n",
      "Epoch [89/300], Step [2200/27733], Loss: 2.6253\n",
      "Epoch [89/300], Step [2300/27733], Loss: 2.5164\n",
      "Epoch [89/300], Step [2400/27733], Loss: 1.7529\n",
      "Epoch [89/300], Step [2500/27733], Loss: 2.9059\n",
      "Epoch [89/300], Step [2600/27733], Loss: 2.3434\n",
      "Epoch [89/300], Step [2700/27733], Loss: 3.4459\n",
      "Epoch [89/300], Step [2800/27733], Loss: 2.6918\n",
      "Epoch [89/300], Step [2900/27733], Loss: 2.6224\n",
      "Epoch [89/300], Step [3000/27733], Loss: 2.5115\n",
      "Epoch [89/300], Step [3100/27733], Loss: 2.4302\n",
      "Epoch [89/300], Step [3200/27733], Loss: 2.8894\n",
      "Epoch [89/300], Step [3300/27733], Loss: 1.9412\n",
      "Epoch [89/300], Step [3400/27733], Loss: 2.4863\n",
      "Epoch [89/300], Step [3500/27733], Loss: 2.3216\n",
      "Epoch [89/300], Step [3600/27733], Loss: 2.4294\n",
      "Epoch [89/300], Step [3700/27733], Loss: 2.4387\n",
      "Epoch [89/300], Step [3800/27733], Loss: 2.3303\n",
      "Epoch [89/300], Step [3900/27733], Loss: 2.6816\n",
      "Epoch [89/300], Step [4000/27733], Loss: 2.4372\n",
      "Epoch [89/300], Step [4100/27733], Loss: 2.1772\n",
      "Epoch [89/300], Step [4200/27733], Loss: 2.9564\n",
      "Epoch [89/300], Step [4300/27733], Loss: 2.7492\n",
      "Epoch [89/300], Step [4400/27733], Loss: 2.4658\n",
      "Epoch [89/300], Step [4500/27733], Loss: 2.3494\n",
      "Epoch [89/300], Step [4600/27733], Loss: 3.0880\n",
      "Epoch [89/300], Step [4700/27733], Loss: 2.2524\n",
      "Epoch [89/300], Step [4800/27733], Loss: 3.0448\n",
      "Epoch [89/300], Step [4900/27733], Loss: 3.2770\n",
      "Epoch [89/300], Step [5000/27733], Loss: 2.4852\n",
      "Epoch [89/300], Step [5100/27733], Loss: 2.3719\n",
      "Epoch [89/300], Step [5200/27733], Loss: 2.8486\n",
      "Epoch [89/300], Step [5300/27733], Loss: 3.0600\n",
      "Epoch [89/300], Step [5400/27733], Loss: 2.7349\n",
      "Epoch [89/300], Step [5500/27733], Loss: 2.2381\n",
      "Epoch [89/300], Step [5600/27733], Loss: 2.1738\n",
      "Epoch [89/300], Step [5700/27733], Loss: 1.9462\n",
      "Epoch [89/300], Step [5800/27733], Loss: 2.9159\n",
      "Epoch [89/300], Step [5900/27733], Loss: 2.1305\n",
      "Epoch [89/300], Step [6000/27733], Loss: 2.3882\n",
      "Epoch [89/300], Step [6100/27733], Loss: 2.0289\n",
      "Epoch [89/300], Step [6200/27733], Loss: 2.5464\n",
      "Epoch [89/300], Step [6300/27733], Loss: 1.9550\n",
      "Epoch [89/300], Step [6400/27733], Loss: 2.2626\n",
      "Epoch [89/300], Step [6500/27733], Loss: 2.2822\n",
      "Epoch [89/300], Step [6600/27733], Loss: 3.1027\n",
      "Epoch [89/300], Step [6700/27733], Loss: 2.0489\n",
      "Epoch [89/300], Step [6800/27733], Loss: 2.3778\n",
      "Epoch [89/300], Step [6900/27733], Loss: 2.9663\n",
      "Epoch [89/300], Step [7000/27733], Loss: 2.3664\n",
      "Epoch [89/300], Step [7100/27733], Loss: 2.8468\n",
      "Epoch [89/300], Step [7200/27733], Loss: 2.1627\n",
      "Epoch [89/300], Step [7300/27733], Loss: 1.8866\n",
      "Epoch [89/300], Step [7400/27733], Loss: 3.2090\n",
      "Epoch [89/300], Step [7500/27733], Loss: 3.1153\n",
      "Epoch [89/300], Step [7600/27733], Loss: 2.4039\n",
      "Epoch [89/300], Step [7700/27733], Loss: 2.6751\n",
      "Epoch [89/300], Step [7800/27733], Loss: 3.1830\n",
      "Epoch [89/300], Step [7900/27733], Loss: 2.5914\n",
      "Epoch [89/300], Step [8000/27733], Loss: 2.2780\n",
      "Epoch [89/300], Step [8100/27733], Loss: 2.7480\n",
      "Epoch [89/300], Step [8200/27733], Loss: 2.4598\n",
      "Epoch [89/300], Step [8300/27733], Loss: 3.0221\n",
      "Epoch [89/300], Step [8400/27733], Loss: 2.4314\n",
      "Epoch [89/300], Step [8500/27733], Loss: 2.2658\n",
      "Epoch [89/300], Step [8600/27733], Loss: 2.6064\n",
      "Epoch [89/300], Step [8700/27733], Loss: 2.3009\n",
      "Epoch [89/300], Step [8800/27733], Loss: 3.2084\n",
      "Epoch [89/300], Step [8900/27733], Loss: 3.0585\n",
      "Epoch [89/300], Step [9000/27733], Loss: 2.2562\n",
      "Epoch [89/300], Step [9100/27733], Loss: 3.4556\n",
      "Epoch [89/300], Step [9200/27733], Loss: 2.2003\n",
      "Epoch [89/300], Step [9300/27733], Loss: 3.3382\n",
      "Epoch [89/300], Step [9400/27733], Loss: 2.2930\n",
      "Epoch [89/300], Step [9500/27733], Loss: 2.5158\n",
      "Epoch [89/300], Step [9600/27733], Loss: 2.4937\n",
      "Epoch [89/300], Step [9700/27733], Loss: 2.5780\n",
      "Epoch [89/300], Step [9800/27733], Loss: 2.7946\n",
      "Epoch [89/300], Step [9900/27733], Loss: 2.9242\n",
      "Epoch [89/300], Step [10000/27733], Loss: 2.1791\n",
      "Epoch [89/300], Step [10100/27733], Loss: 2.3415\n",
      "Epoch [89/300], Step [10200/27733], Loss: 2.4293\n",
      "Epoch [89/300], Step [10300/27733], Loss: 2.6368\n",
      "Epoch [89/300], Step [10400/27733], Loss: 2.8338\n",
      "Epoch [89/300], Step [10500/27733], Loss: 2.4137\n",
      "Epoch [89/300], Step [10600/27733], Loss: 3.4933\n",
      "Epoch [89/300], Step [10700/27733], Loss: 2.1986\n",
      "Epoch [89/300], Step [10800/27733], Loss: 2.4067\n",
      "Epoch [89/300], Step [10900/27733], Loss: 2.4852\n",
      "Epoch [89/300], Step [11000/27733], Loss: 2.4839\n",
      "Epoch [89/300], Step [11100/27733], Loss: 3.0114\n",
      "Epoch [89/300], Step [11200/27733], Loss: 2.6664\n",
      "Epoch [89/300], Step [11300/27733], Loss: 2.5706\n",
      "Epoch [89/300], Step [11400/27733], Loss: 3.2792\n",
      "Epoch [89/300], Step [11500/27733], Loss: 2.4612\n",
      "Epoch [89/300], Step [11600/27733], Loss: 3.1321\n",
      "Epoch [89/300], Step [11700/27733], Loss: 2.8114\n",
      "Epoch [89/300], Step [11800/27733], Loss: 3.1225\n",
      "Epoch [89/300], Step [11900/27733], Loss: 2.7622\n",
      "Epoch [89/300], Step [12000/27733], Loss: 3.3593\n",
      "Epoch [89/300], Step [12100/27733], Loss: 2.5595\n",
      "Epoch [89/300], Step [12200/27733], Loss: 2.1529\n",
      "Epoch [89/300], Step [12300/27733], Loss: 3.0648\n",
      "Epoch [89/300], Step [12400/27733], Loss: 2.6503\n",
      "Epoch [89/300], Step [12500/27733], Loss: 3.0919\n",
      "Epoch [89/300], Step [12600/27733], Loss: 3.1210\n",
      "Epoch [89/300], Step [12700/27733], Loss: 2.4516\n",
      "Epoch [89/300], Step [12800/27733], Loss: 2.8495\n",
      "Epoch [89/300], Step [12900/27733], Loss: 2.0991\n",
      "Epoch [89/300], Step [13000/27733], Loss: 3.4770\n",
      "Epoch [89/300], Step [13100/27733], Loss: 2.9055\n",
      "Epoch [89/300], Step [13200/27733], Loss: 2.6799\n",
      "Epoch [89/300], Step [13300/27733], Loss: 2.6164\n",
      "Epoch [89/300], Step [13400/27733], Loss: 2.6313\n",
      "Epoch [89/300], Step [13500/27733], Loss: 2.9407\n",
      "Epoch [89/300], Step [13600/27733], Loss: 1.9379\n",
      "Epoch [89/300], Step [13700/27733], Loss: 3.1519\n",
      "Epoch [89/300], Step [13800/27733], Loss: 2.2153\n",
      "Epoch [89/300], Step [13900/27733], Loss: 2.9324\n",
      "Epoch [89/300], Step [14000/27733], Loss: 3.1273\n",
      "Epoch [89/300], Step [14100/27733], Loss: 2.9208\n",
      "Epoch [89/300], Step [14200/27733], Loss: 3.0136\n",
      "Epoch [89/300], Step [14300/27733], Loss: 2.2004\n",
      "Epoch [89/300], Step [14400/27733], Loss: 2.9637\n",
      "Epoch [89/300], Step [14500/27733], Loss: 3.1495\n",
      "Epoch [89/300], Step [14600/27733], Loss: 2.8561\n",
      "Epoch [89/300], Step [14700/27733], Loss: 2.3713\n",
      "Epoch [89/300], Step [14800/27733], Loss: 2.3307\n",
      "Epoch [89/300], Step [14900/27733], Loss: 3.1186\n",
      "Epoch [89/300], Step [15000/27733], Loss: 3.3413\n",
      "Epoch [89/300], Step [15100/27733], Loss: 3.1229\n",
      "Epoch [89/300], Step [15200/27733], Loss: 2.7016\n",
      "Epoch [89/300], Step [15300/27733], Loss: 3.0174\n",
      "Epoch [89/300], Step [15400/27733], Loss: 1.7438\n",
      "Epoch [89/300], Step [15500/27733], Loss: 2.5501\n",
      "Epoch [89/300], Step [15600/27733], Loss: 3.1427\n",
      "Epoch [89/300], Step [15700/27733], Loss: 2.6154\n",
      "Epoch [89/300], Step [15800/27733], Loss: 2.7484\n",
      "Epoch [89/300], Step [15900/27733], Loss: 2.3773\n",
      "Epoch [89/300], Step [16000/27733], Loss: 2.7581\n",
      "Epoch [89/300], Step [16100/27733], Loss: 2.4049\n",
      "Epoch [89/300], Step [16200/27733], Loss: 2.2095\n",
      "Epoch [89/300], Step [16300/27733], Loss: 3.9730\n",
      "Epoch [89/300], Step [16400/27733], Loss: 1.7748\n",
      "Epoch [89/300], Step [16500/27733], Loss: 2.9122\n",
      "Epoch [89/300], Step [16600/27733], Loss: 2.5237\n",
      "Epoch [89/300], Step [16700/27733], Loss: 2.3124\n",
      "Epoch [89/300], Step [16800/27733], Loss: 2.4356\n",
      "Epoch [89/300], Step [16900/27733], Loss: 2.8252\n",
      "Epoch [89/300], Step [17000/27733], Loss: 3.0536\n",
      "Epoch [89/300], Step [17100/27733], Loss: 2.4119\n",
      "Epoch [89/300], Step [17200/27733], Loss: 3.5232\n",
      "Epoch [89/300], Step [17300/27733], Loss: 2.8949\n",
      "Epoch [89/300], Step [17400/27733], Loss: 1.9766\n",
      "Epoch [89/300], Step [17500/27733], Loss: 3.2377\n",
      "Epoch [89/300], Step [17600/27733], Loss: 2.9498\n",
      "Epoch [89/300], Step [17700/27733], Loss: 2.6237\n",
      "Epoch [89/300], Step [17800/27733], Loss: 3.0959\n",
      "Epoch [89/300], Step [17900/27733], Loss: 3.2340\n",
      "Epoch [89/300], Step [18000/27733], Loss: 2.7940\n",
      "Epoch [89/300], Step [18100/27733], Loss: 2.8292\n",
      "Epoch [89/300], Step [18200/27733], Loss: 2.9537\n",
      "Epoch [89/300], Step [18300/27733], Loss: 3.4511\n",
      "Epoch [89/300], Step [18400/27733], Loss: 2.8338\n",
      "Epoch [89/300], Step [18500/27733], Loss: 2.6069\n",
      "Epoch [89/300], Step [18600/27733], Loss: 2.9476\n",
      "Epoch [89/300], Step [18700/27733], Loss: 2.2281\n",
      "Epoch [89/300], Step [18800/27733], Loss: 2.6302\n",
      "Epoch [89/300], Step [18900/27733], Loss: 2.8546\n",
      "Epoch [89/300], Step [19000/27733], Loss: 2.9387\n",
      "Epoch [89/300], Step [19100/27733], Loss: 2.7155\n",
      "Epoch [89/300], Step [19200/27733], Loss: 2.5773\n",
      "Epoch [89/300], Step [19300/27733], Loss: 2.8674\n",
      "Epoch [89/300], Step [19400/27733], Loss: 2.3161\n",
      "Epoch [89/300], Step [19500/27733], Loss: 3.6120\n",
      "Epoch [89/300], Step [19600/27733], Loss: 2.0524\n",
      "Epoch [89/300], Step [19700/27733], Loss: 3.1031\n",
      "Epoch [89/300], Step [19800/27733], Loss: 3.0791\n",
      "Epoch [89/300], Step [19900/27733], Loss: 2.4291\n",
      "Epoch [89/300], Step [20000/27733], Loss: 3.3609\n",
      "Epoch [89/300], Step [20100/27733], Loss: 3.4080\n",
      "Epoch [89/300], Step [20200/27733], Loss: 2.8529\n",
      "Epoch [89/300], Step [20300/27733], Loss: 2.7619\n",
      "Epoch [89/300], Step [20400/27733], Loss: 3.5749\n",
      "Epoch [89/300], Step [20500/27733], Loss: 3.7080\n",
      "Epoch [89/300], Step [20600/27733], Loss: 3.5622\n",
      "Epoch [89/300], Step [20700/27733], Loss: 3.3744\n",
      "Epoch [89/300], Step [20800/27733], Loss: 3.2105\n",
      "Epoch [89/300], Step [20900/27733], Loss: 2.3037\n",
      "Epoch [89/300], Step [21000/27733], Loss: 3.0589\n",
      "Epoch [89/300], Step [21100/27733], Loss: 3.1820\n",
      "Epoch [89/300], Step [21200/27733], Loss: 2.9233\n",
      "Epoch [89/300], Step [21300/27733], Loss: 2.5316\n",
      "Epoch [89/300], Step [21400/27733], Loss: 3.0677\n",
      "Epoch [89/300], Step [21500/27733], Loss: 2.9200\n",
      "Epoch [89/300], Step [21600/27733], Loss: 3.4057\n",
      "Epoch [89/300], Step [21700/27733], Loss: 3.0687\n",
      "Epoch [89/300], Step [21800/27733], Loss: 2.8665\n",
      "Epoch [89/300], Step [21900/27733], Loss: 2.9907\n",
      "Epoch [89/300], Step [22000/27733], Loss: 3.0287\n",
      "Epoch [89/300], Step [22100/27733], Loss: 3.1663\n",
      "Epoch [89/300], Step [22200/27733], Loss: 2.7105\n",
      "Epoch [89/300], Step [22300/27733], Loss: 2.8665\n",
      "Epoch [89/300], Step [22400/27733], Loss: 2.8916\n",
      "Epoch [89/300], Step [22500/27733], Loss: 1.8882\n",
      "Epoch [89/300], Step [22600/27733], Loss: 3.0156\n",
      "Epoch [89/300], Step [22700/27733], Loss: 2.2067\n",
      "Epoch [89/300], Step [22800/27733], Loss: 3.1632\n",
      "Epoch [89/300], Step [22900/27733], Loss: 2.2342\n",
      "Epoch [89/300], Step [23000/27733], Loss: 3.1567\n",
      "Epoch [89/300], Step [23100/27733], Loss: 1.9903\n",
      "Epoch [89/300], Step [23200/27733], Loss: 3.2784\n",
      "Epoch [89/300], Step [23300/27733], Loss: 2.8586\n",
      "Epoch [89/300], Step [23400/27733], Loss: 4.1811\n",
      "Epoch [89/300], Step [23500/27733], Loss: 2.6685\n",
      "Epoch [89/300], Step [23600/27733], Loss: 3.3309\n",
      "Epoch [89/300], Step [23700/27733], Loss: 4.0887\n",
      "Epoch [89/300], Step [23800/27733], Loss: 3.3600\n",
      "Epoch [89/300], Step [23900/27733], Loss: 2.5829\n",
      "Epoch [89/300], Step [24000/27733], Loss: 2.7570\n",
      "Epoch [89/300], Step [24100/27733], Loss: 2.7146\n",
      "Epoch [89/300], Step [24200/27733], Loss: 2.8929\n",
      "Epoch [89/300], Step [24300/27733], Loss: 3.3268\n",
      "Epoch [89/300], Step [24400/27733], Loss: 2.1409\n",
      "Epoch [89/300], Step [24500/27733], Loss: 2.5288\n",
      "Epoch [89/300], Step [24600/27733], Loss: 1.8226\n",
      "Epoch [89/300], Step [24700/27733], Loss: 2.4885\n",
      "Epoch [89/300], Step [24800/27733], Loss: 2.1659\n",
      "Epoch [89/300], Step [24900/27733], Loss: 3.7988\n",
      "Epoch [89/300], Step [25000/27733], Loss: 3.0359\n",
      "Epoch [89/300], Step [25100/27733], Loss: 2.9458\n",
      "Epoch [89/300], Step [25200/27733], Loss: 3.3868\n",
      "Epoch [89/300], Step [25300/27733], Loss: 3.3709\n",
      "Epoch [89/300], Step [25400/27733], Loss: 2.9565\n",
      "Epoch [89/300], Step [25500/27733], Loss: 3.2965\n",
      "Epoch [89/300], Step [25600/27733], Loss: 2.8532\n",
      "Epoch [89/300], Step [25700/27733], Loss: 2.8000\n",
      "Epoch [89/300], Step [25800/27733], Loss: 2.8238\n",
      "Epoch [89/300], Step [25900/27733], Loss: 2.6011\n",
      "Epoch [89/300], Step [26000/27733], Loss: 2.2947\n",
      "Epoch [89/300], Step [26100/27733], Loss: 2.7673\n",
      "Epoch [89/300], Step [26200/27733], Loss: 3.2053\n",
      "Epoch [89/300], Step [26300/27733], Loss: 3.8258\n",
      "Epoch [89/300], Step [26400/27733], Loss: 2.7330\n",
      "Epoch [89/300], Step [26500/27733], Loss: 2.3774\n",
      "Epoch [89/300], Step [26600/27733], Loss: 3.4170\n",
      "Epoch [89/300], Step [26700/27733], Loss: 2.4766\n",
      "Epoch [89/300], Step [26800/27733], Loss: 2.4699\n",
      "Epoch [89/300], Step [26900/27733], Loss: 2.6068\n",
      "Epoch [89/300], Step [27000/27733], Loss: 3.7751\n",
      "Epoch [89/300], Step [27100/27733], Loss: 2.5844\n",
      "Epoch [89/300], Step [27200/27733], Loss: 3.1961\n",
      "Epoch [89/300], Step [27300/27733], Loss: 3.8405\n",
      "Epoch [89/300], Step [27400/27733], Loss: 2.7458\n",
      "Epoch [89/300], Step [27500/27733], Loss: 2.8157\n",
      "Epoch [89/300], Step [27600/27733], Loss: 2.7332\n",
      "Epoch [89/300], Step [27700/27733], Loss: 3.8055\n",
      "Epoch [90/300], Step [100/27733], Loss: 2.6242\n",
      "Epoch [90/300], Step [200/27733], Loss: 2.4000\n",
      "Epoch [90/300], Step [300/27733], Loss: 3.0999\n",
      "Epoch [90/300], Step [400/27733], Loss: 2.5585\n",
      "Epoch [90/300], Step [500/27733], Loss: 2.4947\n",
      "Epoch [90/300], Step [600/27733], Loss: 2.4183\n",
      "Epoch [90/300], Step [700/27733], Loss: 2.1251\n",
      "Epoch [90/300], Step [800/27733], Loss: 2.3980\n",
      "Epoch [90/300], Step [900/27733], Loss: 3.0313\n",
      "Epoch [90/300], Step [1000/27733], Loss: 1.9137\n",
      "Epoch [90/300], Step [1100/27733], Loss: 3.0104\n",
      "Epoch [90/300], Step [1200/27733], Loss: 2.5522\n",
      "Epoch [90/300], Step [1300/27733], Loss: 1.8770\n",
      "Epoch [90/300], Step [1400/27733], Loss: 2.5241\n",
      "Epoch [90/300], Step [1500/27733], Loss: 2.5155\n",
      "Epoch [90/300], Step [1600/27733], Loss: 1.9720\n",
      "Epoch [90/300], Step [1700/27733], Loss: 3.0035\n",
      "Epoch [90/300], Step [1800/27733], Loss: 1.9021\n",
      "Epoch [90/300], Step [1900/27733], Loss: 2.9324\n",
      "Epoch [90/300], Step [2000/27733], Loss: 3.4286\n",
      "Epoch [90/300], Step [2100/27733], Loss: 2.3461\n",
      "Epoch [90/300], Step [2200/27733], Loss: 2.5382\n",
      "Epoch [90/300], Step [2300/27733], Loss: 2.5562\n",
      "Epoch [90/300], Step [2400/27733], Loss: 2.5681\n",
      "Epoch [90/300], Step [2500/27733], Loss: 2.7798\n",
      "Epoch [90/300], Step [2600/27733], Loss: 1.9436\n",
      "Epoch [90/300], Step [2700/27733], Loss: 2.3772\n",
      "Epoch [90/300], Step [2800/27733], Loss: 1.5981\n",
      "Epoch [90/300], Step [2900/27733], Loss: 2.3206\n",
      "Epoch [90/300], Step [3000/27733], Loss: 2.7954\n",
      "Epoch [90/300], Step [3100/27733], Loss: 2.2271\n",
      "Epoch [90/300], Step [3200/27733], Loss: 2.4036\n",
      "Epoch [90/300], Step [3300/27733], Loss: 2.0263\n",
      "Epoch [90/300], Step [3400/27733], Loss: 2.5581\n",
      "Epoch [90/300], Step [3500/27733], Loss: 2.7559\n",
      "Epoch [90/300], Step [3600/27733], Loss: 2.3471\n",
      "Epoch [90/300], Step [3700/27733], Loss: 2.4580\n",
      "Epoch [90/300], Step [3800/27733], Loss: 2.3982\n",
      "Epoch [90/300], Step [3900/27733], Loss: 2.2540\n",
      "Epoch [90/300], Step [4000/27733], Loss: 2.6743\n",
      "Epoch [90/300], Step [4100/27733], Loss: 2.7410\n",
      "Epoch [90/300], Step [4200/27733], Loss: 2.9214\n",
      "Epoch [90/300], Step [4300/27733], Loss: 2.3384\n",
      "Epoch [90/300], Step [4400/27733], Loss: 3.0947\n",
      "Epoch [90/300], Step [4500/27733], Loss: 3.3096\n",
      "Epoch [90/300], Step [4600/27733], Loss: 2.2234\n",
      "Epoch [90/300], Step [4700/27733], Loss: 3.8456\n",
      "Epoch [90/300], Step [4800/27733], Loss: 2.5164\n",
      "Epoch [90/300], Step [4900/27733], Loss: 2.5949\n",
      "Epoch [90/300], Step [5000/27733], Loss: 2.0257\n",
      "Epoch [90/300], Step [5100/27733], Loss: 2.0017\n",
      "Epoch [90/300], Step [5200/27733], Loss: 2.1720\n",
      "Epoch [90/300], Step [5300/27733], Loss: 2.4620\n",
      "Epoch [90/300], Step [5400/27733], Loss: 2.3897\n",
      "Epoch [90/300], Step [5500/27733], Loss: 2.5692\n",
      "Epoch [90/300], Step [5600/27733], Loss: 1.7086\n",
      "Epoch [90/300], Step [5700/27733], Loss: 2.4875\n",
      "Epoch [90/300], Step [5800/27733], Loss: 1.9538\n",
      "Epoch [90/300], Step [5900/27733], Loss: 2.4330\n",
      "Epoch [90/300], Step [6000/27733], Loss: 2.6034\n",
      "Epoch [90/300], Step [6100/27733], Loss: 2.4649\n",
      "Epoch [90/300], Step [6200/27733], Loss: 3.1582\n",
      "Epoch [90/300], Step [6300/27733], Loss: 2.4686\n",
      "Epoch [90/300], Step [6400/27733], Loss: 3.4303\n",
      "Epoch [90/300], Step [6500/27733], Loss: 2.5822\n",
      "Epoch [90/300], Step [6600/27733], Loss: 2.8099\n",
      "Epoch [90/300], Step [6700/27733], Loss: 3.4261\n",
      "Epoch [90/300], Step [6800/27733], Loss: 2.7505\n",
      "Epoch [90/300], Step [6900/27733], Loss: 3.3241\n",
      "Epoch [90/300], Step [7000/27733], Loss: 2.2077\n",
      "Epoch [90/300], Step [7100/27733], Loss: 2.1522\n",
      "Epoch [90/300], Step [7200/27733], Loss: 2.6700\n",
      "Epoch [90/300], Step [7300/27733], Loss: 3.0087\n",
      "Epoch [90/300], Step [7400/27733], Loss: 2.0512\n",
      "Epoch [90/300], Step [7500/27733], Loss: 2.0026\n",
      "Epoch [90/300], Step [7600/27733], Loss: 2.0694\n",
      "Epoch [90/300], Step [7700/27733], Loss: 2.4479\n",
      "Epoch [90/300], Step [7800/27733], Loss: 2.3742\n",
      "Epoch [90/300], Step [7900/27733], Loss: 2.2551\n",
      "Epoch [90/300], Step [8000/27733], Loss: 2.4418\n",
      "Epoch [90/300], Step [8100/27733], Loss: 2.3502\n",
      "Epoch [90/300], Step [8200/27733], Loss: 2.3157\n",
      "Epoch [90/300], Step [8300/27733], Loss: 3.0113\n",
      "Epoch [90/300], Step [8400/27733], Loss: 2.3632\n",
      "Epoch [90/300], Step [8500/27733], Loss: 2.9257\n",
      "Epoch [90/300], Step [8600/27733], Loss: 2.2835\n",
      "Epoch [90/300], Step [8700/27733], Loss: 2.1879\n",
      "Epoch [90/300], Step [8800/27733], Loss: 3.3338\n",
      "Epoch [90/300], Step [8900/27733], Loss: 3.0176\n",
      "Epoch [90/300], Step [9000/27733], Loss: 2.0633\n",
      "Epoch [90/300], Step [9100/27733], Loss: 2.2650\n",
      "Epoch [90/300], Step [9200/27733], Loss: 2.5453\n",
      "Epoch [90/300], Step [9300/27733], Loss: 2.0010\n",
      "Epoch [90/300], Step [9400/27733], Loss: 2.6142\n",
      "Epoch [90/300], Step [9500/27733], Loss: 3.2094\n",
      "Epoch [90/300], Step [9600/27733], Loss: 3.0245\n",
      "Epoch [90/300], Step [9700/27733], Loss: 2.7567\n",
      "Epoch [90/300], Step [9800/27733], Loss: 2.7820\n",
      "Epoch [90/300], Step [9900/27733], Loss: 2.1173\n",
      "Epoch [90/300], Step [10000/27733], Loss: 3.5141\n",
      "Epoch [90/300], Step [10100/27733], Loss: 1.8879\n",
      "Epoch [90/300], Step [10200/27733], Loss: 2.6095\n",
      "Epoch [90/300], Step [10300/27733], Loss: 3.1298\n",
      "Epoch [90/300], Step [10400/27733], Loss: 2.6263\n",
      "Epoch [90/300], Step [10500/27733], Loss: 3.1686\n",
      "Epoch [90/300], Step [10600/27733], Loss: 2.5643\n",
      "Epoch [90/300], Step [10700/27733], Loss: 2.8923\n",
      "Epoch [90/300], Step [10800/27733], Loss: 2.4103\n",
      "Epoch [90/300], Step [10900/27733], Loss: 2.3011\n",
      "Epoch [90/300], Step [11000/27733], Loss: 2.5109\n",
      "Epoch [90/300], Step [11100/27733], Loss: 2.8580\n",
      "Epoch [90/300], Step [11200/27733], Loss: 2.5914\n",
      "Epoch [90/300], Step [11300/27733], Loss: 2.3474\n",
      "Epoch [90/300], Step [11400/27733], Loss: 2.9906\n",
      "Epoch [90/300], Step [11500/27733], Loss: 3.6655\n",
      "Epoch [90/300], Step [11600/27733], Loss: 2.1511\n",
      "Epoch [90/300], Step [11700/27733], Loss: 2.2361\n",
      "Epoch [90/300], Step [11800/27733], Loss: 2.5276\n",
      "Epoch [90/300], Step [11900/27733], Loss: 3.4683\n",
      "Epoch [90/300], Step [12000/27733], Loss: 3.7420\n",
      "Epoch [90/300], Step [12100/27733], Loss: 2.4409\n",
      "Epoch [90/300], Step [12200/27733], Loss: 2.5386\n",
      "Epoch [90/300], Step [12300/27733], Loss: 2.7957\n",
      "Epoch [90/300], Step [12400/27733], Loss: 4.0157\n",
      "Epoch [90/300], Step [12500/27733], Loss: 2.8332\n",
      "Epoch [90/300], Step [12600/27733], Loss: 2.6169\n",
      "Epoch [90/300], Step [12700/27733], Loss: 3.3534\n",
      "Epoch [90/300], Step [12800/27733], Loss: 2.0661\n",
      "Epoch [90/300], Step [12900/27733], Loss: 2.2434\n",
      "Epoch [90/300], Step [13000/27733], Loss: 3.5620\n",
      "Epoch [90/300], Step [13100/27733], Loss: 2.6210\n",
      "Epoch [90/300], Step [13200/27733], Loss: 3.0088\n",
      "Epoch [90/300], Step [13300/27733], Loss: 2.2483\n",
      "Epoch [90/300], Step [13400/27733], Loss: 3.1377\n",
      "Epoch [90/300], Step [13500/27733], Loss: 2.4520\n",
      "Epoch [90/300], Step [13600/27733], Loss: 2.5213\n",
      "Epoch [90/300], Step [13700/27733], Loss: 2.3376\n",
      "Epoch [90/300], Step [13800/27733], Loss: 3.0149\n",
      "Epoch [90/300], Step [13900/27733], Loss: 2.7327\n",
      "Epoch [90/300], Step [14000/27733], Loss: 3.1079\n",
      "Epoch [90/300], Step [14100/27733], Loss: 3.0845\n",
      "Epoch [90/300], Step [14200/27733], Loss: 2.6376\n",
      "Epoch [90/300], Step [14300/27733], Loss: 1.7735\n",
      "Epoch [90/300], Step [14400/27733], Loss: 2.9244\n",
      "Epoch [90/300], Step [14500/27733], Loss: 2.5722\n",
      "Epoch [90/300], Step [14600/27733], Loss: 2.3377\n",
      "Epoch [90/300], Step [14700/27733], Loss: 2.4180\n",
      "Epoch [90/300], Step [14800/27733], Loss: 2.6252\n",
      "Epoch [90/300], Step [14900/27733], Loss: 2.9783\n",
      "Epoch [90/300], Step [15000/27733], Loss: 3.1598\n",
      "Epoch [90/300], Step [15100/27733], Loss: 2.3116\n",
      "Epoch [90/300], Step [15200/27733], Loss: 2.7814\n",
      "Epoch [90/300], Step [15300/27733], Loss: 2.0140\n",
      "Epoch [90/300], Step [15400/27733], Loss: 2.9660\n",
      "Epoch [90/300], Step [15500/27733], Loss: 2.2988\n",
      "Epoch [90/300], Step [15600/27733], Loss: 3.2076\n",
      "Epoch [90/300], Step [15700/27733], Loss: 2.4106\n",
      "Epoch [90/300], Step [15800/27733], Loss: 2.4586\n",
      "Epoch [90/300], Step [15900/27733], Loss: 2.0098\n",
      "Epoch [90/300], Step [16000/27733], Loss: 2.8710\n",
      "Epoch [90/300], Step [16100/27733], Loss: 2.2528\n",
      "Epoch [90/300], Step [16200/27733], Loss: 3.0369\n",
      "Epoch [90/300], Step [16300/27733], Loss: 3.4758\n",
      "Epoch [90/300], Step [16400/27733], Loss: 3.2772\n",
      "Epoch [90/300], Step [16500/27733], Loss: 2.7962\n",
      "Epoch [90/300], Step [16600/27733], Loss: 2.3923\n",
      "Epoch [90/300], Step [16700/27733], Loss: 3.2761\n",
      "Epoch [90/300], Step [16800/27733], Loss: 3.0989\n",
      "Epoch [90/300], Step [16900/27733], Loss: 1.8685\n",
      "Epoch [90/300], Step [17000/27733], Loss: 4.3707\n",
      "Epoch [90/300], Step [17100/27733], Loss: 2.7264\n",
      "Epoch [90/300], Step [17200/27733], Loss: 2.5951\n",
      "Epoch [90/300], Step [17300/27733], Loss: 3.0146\n",
      "Epoch [90/300], Step [17400/27733], Loss: 2.0324\n",
      "Epoch [90/300], Step [17500/27733], Loss: 2.8342\n",
      "Epoch [90/300], Step [17600/27733], Loss: 1.8069\n",
      "Epoch [90/300], Step [17700/27733], Loss: 2.5643\n",
      "Epoch [90/300], Step [17800/27733], Loss: 2.8885\n",
      "Epoch [90/300], Step [17900/27733], Loss: 3.8900\n",
      "Epoch [90/300], Step [18000/27733], Loss: 2.6030\n",
      "Epoch [90/300], Step [18100/27733], Loss: 2.2923\n",
      "Epoch [90/300], Step [18200/27733], Loss: 2.8738\n",
      "Epoch [90/300], Step [18300/27733], Loss: 3.3719\n",
      "Epoch [90/300], Step [18400/27733], Loss: 2.6454\n",
      "Epoch [90/300], Step [18500/27733], Loss: 2.7365\n",
      "Epoch [90/300], Step [18600/27733], Loss: 1.6240\n",
      "Epoch [90/300], Step [18700/27733], Loss: 2.7048\n",
      "Epoch [90/300], Step [18800/27733], Loss: 2.2263\n",
      "Epoch [90/300], Step [18900/27733], Loss: 2.7180\n",
      "Epoch [90/300], Step [19000/27733], Loss: 3.0145\n",
      "Epoch [90/300], Step [19100/27733], Loss: 2.8595\n",
      "Epoch [90/300], Step [19200/27733], Loss: 2.5786\n",
      "Epoch [90/300], Step [19300/27733], Loss: 2.5366\n",
      "Epoch [90/300], Step [19400/27733], Loss: 2.8543\n",
      "Epoch [90/300], Step [19500/27733], Loss: 2.9633\n",
      "Epoch [90/300], Step [19600/27733], Loss: 2.7535\n",
      "Epoch [90/300], Step [19700/27733], Loss: 2.7275\n",
      "Epoch [90/300], Step [19800/27733], Loss: 2.9057\n",
      "Epoch [90/300], Step [19900/27733], Loss: 2.9542\n",
      "Epoch [90/300], Step [20000/27733], Loss: 3.0556\n",
      "Epoch [90/300], Step [20100/27733], Loss: 2.4234\n",
      "Epoch [90/300], Step [20200/27733], Loss: 3.4596\n",
      "Epoch [90/300], Step [20300/27733], Loss: 2.4922\n",
      "Epoch [90/300], Step [20400/27733], Loss: 1.6525\n",
      "Epoch [90/300], Step [20500/27733], Loss: 3.4905\n",
      "Epoch [90/300], Step [20600/27733], Loss: 2.6403\n",
      "Epoch [90/300], Step [20700/27733], Loss: 2.8976\n",
      "Epoch [90/300], Step [20800/27733], Loss: 3.3917\n",
      "Epoch [90/300], Step [20900/27733], Loss: 3.2030\n",
      "Epoch [90/300], Step [21000/27733], Loss: 1.9557\n",
      "Epoch [90/300], Step [21100/27733], Loss: 2.2307\n",
      "Epoch [90/300], Step [21200/27733], Loss: 3.1281\n",
      "Epoch [90/300], Step [21300/27733], Loss: 2.1514\n",
      "Epoch [90/300], Step [21400/27733], Loss: 3.0331\n",
      "Epoch [90/300], Step [21500/27733], Loss: 3.6256\n",
      "Epoch [90/300], Step [21600/27733], Loss: 4.2638\n",
      "Epoch [90/300], Step [21700/27733], Loss: 2.9151\n",
      "Epoch [90/300], Step [21800/27733], Loss: 3.0956\n",
      "Epoch [90/300], Step [21900/27733], Loss: 2.3675\n",
      "Epoch [90/300], Step [22000/27733], Loss: 2.4343\n",
      "Epoch [90/300], Step [22100/27733], Loss: 2.4661\n",
      "Epoch [90/300], Step [22200/27733], Loss: 2.4780\n",
      "Epoch [90/300], Step [22300/27733], Loss: 2.6597\n",
      "Epoch [90/300], Step [22400/27733], Loss: 2.4526\n",
      "Epoch [90/300], Step [22500/27733], Loss: 2.7181\n",
      "Epoch [90/300], Step [22600/27733], Loss: 3.2352\n",
      "Epoch [90/300], Step [22700/27733], Loss: 3.0987\n",
      "Epoch [90/300], Step [22800/27733], Loss: 3.3778\n",
      "Epoch [90/300], Step [22900/27733], Loss: 2.5323\n",
      "Epoch [90/300], Step [23000/27733], Loss: 3.2561\n",
      "Epoch [90/300], Step [23100/27733], Loss: 2.9050\n",
      "Epoch [90/300], Step [23200/27733], Loss: 2.4393\n",
      "Epoch [90/300], Step [23300/27733], Loss: 2.5885\n",
      "Epoch [90/300], Step [23400/27733], Loss: 3.0566\n",
      "Epoch [90/300], Step [23500/27733], Loss: 3.1156\n",
      "Epoch [90/300], Step [23600/27733], Loss: 2.6201\n",
      "Epoch [90/300], Step [23700/27733], Loss: 2.4955\n",
      "Epoch [90/300], Step [23800/27733], Loss: 2.7823\n",
      "Epoch [90/300], Step [23900/27733], Loss: 2.7262\n",
      "Epoch [90/300], Step [24000/27733], Loss: 2.3849\n",
      "Epoch [90/300], Step [24100/27733], Loss: 2.9011\n",
      "Epoch [90/300], Step [24200/27733], Loss: 2.9084\n",
      "Epoch [90/300], Step [24300/27733], Loss: 3.5611\n",
      "Epoch [90/300], Step [24400/27733], Loss: 2.2636\n",
      "Epoch [90/300], Step [24500/27733], Loss: 2.5642\n",
      "Epoch [90/300], Step [24600/27733], Loss: 2.9649\n",
      "Epoch [90/300], Step [24700/27733], Loss: 2.5353\n",
      "Epoch [90/300], Step [24800/27733], Loss: 3.0543\n",
      "Epoch [90/300], Step [24900/27733], Loss: 2.5124\n",
      "Epoch [90/300], Step [25000/27733], Loss: 3.9507\n",
      "Epoch [90/300], Step [25100/27733], Loss: 2.9586\n",
      "Epoch [90/300], Step [25200/27733], Loss: 3.0193\n",
      "Epoch [90/300], Step [25300/27733], Loss: 2.9631\n",
      "Epoch [90/300], Step [25400/27733], Loss: 3.3112\n",
      "Epoch [90/300], Step [25500/27733], Loss: 2.6659\n",
      "Epoch [90/300], Step [25600/27733], Loss: 2.6979\n",
      "Epoch [90/300], Step [25700/27733], Loss: 2.3863\n",
      "Epoch [90/300], Step [25800/27733], Loss: 3.7566\n",
      "Epoch [90/300], Step [25900/27733], Loss: 2.5557\n",
      "Epoch [90/300], Step [26000/27733], Loss: 3.0154\n",
      "Epoch [90/300], Step [26100/27733], Loss: 3.2019\n",
      "Epoch [90/300], Step [26200/27733], Loss: 3.1726\n",
      "Epoch [90/300], Step [26300/27733], Loss: 2.4395\n",
      "Epoch [90/300], Step [26400/27733], Loss: 2.2506\n",
      "Epoch [90/300], Step [26500/27733], Loss: 2.3836\n",
      "Epoch [90/300], Step [26600/27733], Loss: 4.2909\n",
      "Epoch [90/300], Step [26700/27733], Loss: 2.3224\n",
      "Epoch [90/300], Step [26800/27733], Loss: 3.1999\n",
      "Epoch [90/300], Step [26900/27733], Loss: 2.7741\n",
      "Epoch [90/300], Step [27000/27733], Loss: 3.3806\n",
      "Epoch [90/300], Step [27100/27733], Loss: 4.1627\n",
      "Epoch [90/300], Step [27200/27733], Loss: 3.2373\n",
      "Epoch [90/300], Step [27300/27733], Loss: 3.5977\n",
      "Epoch [90/300], Step [27400/27733], Loss: 2.8828\n",
      "Epoch [90/300], Step [27500/27733], Loss: 3.3394\n",
      "Epoch [90/300], Step [27600/27733], Loss: 3.4067\n",
      "Epoch [90/300], Step [27700/27733], Loss: 2.9918\n",
      "Epoch [91/300], Step [100/27733], Loss: 1.9159\n",
      "Epoch [91/300], Step [200/27733], Loss: 3.1455\n",
      "Epoch [91/300], Step [300/27733], Loss: 2.1653\n",
      "Epoch [91/300], Step [400/27733], Loss: 2.9793\n",
      "Epoch [91/300], Step [500/27733], Loss: 3.0548\n",
      "Epoch [91/300], Step [600/27733], Loss: 2.2960\n",
      "Epoch [91/300], Step [700/27733], Loss: 3.0638\n",
      "Epoch [91/300], Step [800/27733], Loss: 2.5032\n",
      "Epoch [91/300], Step [900/27733], Loss: 2.7240\n",
      "Epoch [91/300], Step [1000/27733], Loss: 2.3288\n",
      "Epoch [91/300], Step [1100/27733], Loss: 2.2213\n",
      "Epoch [91/300], Step [1200/27733], Loss: 2.6693\n",
      "Epoch [91/300], Step [1300/27733], Loss: 2.4358\n",
      "Epoch [91/300], Step [1400/27733], Loss: 2.9989\n",
      "Epoch [91/300], Step [1500/27733], Loss: 2.4601\n",
      "Epoch [91/300], Step [1600/27733], Loss: 1.4286\n",
      "Epoch [91/300], Step [1700/27733], Loss: 2.6520\n",
      "Epoch [91/300], Step [1800/27733], Loss: 2.5605\n",
      "Epoch [91/300], Step [1900/27733], Loss: 2.5752\n",
      "Epoch [91/300], Step [2000/27733], Loss: 2.7064\n",
      "Epoch [91/300], Step [2100/27733], Loss: 3.1458\n",
      "Epoch [91/300], Step [2200/27733], Loss: 1.7852\n",
      "Epoch [91/300], Step [2300/27733], Loss: 2.1929\n",
      "Epoch [91/300], Step [2400/27733], Loss: 2.3346\n",
      "Epoch [91/300], Step [2500/27733], Loss: 2.4269\n",
      "Epoch [91/300], Step [2600/27733], Loss: 2.3464\n",
      "Epoch [91/300], Step [2700/27733], Loss: 2.0039\n",
      "Epoch [91/300], Step [2800/27733], Loss: 2.3582\n",
      "Epoch [91/300], Step [2900/27733], Loss: 1.8415\n",
      "Epoch [91/300], Step [3000/27733], Loss: 2.5086\n",
      "Epoch [91/300], Step [3100/27733], Loss: 2.9171\n",
      "Epoch [91/300], Step [3200/27733], Loss: 2.3985\n",
      "Epoch [91/300], Step [3300/27733], Loss: 2.4520\n",
      "Epoch [91/300], Step [3400/27733], Loss: 2.9135\n",
      "Epoch [91/300], Step [3500/27733], Loss: 2.8100\n",
      "Epoch [91/300], Step [3600/27733], Loss: 2.7398\n",
      "Epoch [91/300], Step [3700/27733], Loss: 3.3073\n",
      "Epoch [91/300], Step [3800/27733], Loss: 2.7778\n",
      "Epoch [91/300], Step [3900/27733], Loss: 1.9733\n",
      "Epoch [91/300], Step [4000/27733], Loss: 2.6151\n",
      "Epoch [91/300], Step [4100/27733], Loss: 3.4420\n",
      "Epoch [91/300], Step [4200/27733], Loss: 2.3036\n",
      "Epoch [91/300], Step [4300/27733], Loss: 2.8863\n",
      "Epoch [91/300], Step [4400/27733], Loss: 2.5046\n",
      "Epoch [91/300], Step [4500/27733], Loss: 2.7627\n",
      "Epoch [91/300], Step [4600/27733], Loss: 3.2375\n",
      "Epoch [91/300], Step [4700/27733], Loss: 3.0867\n",
      "Epoch [91/300], Step [4800/27733], Loss: 2.9056\n",
      "Epoch [91/300], Step [4900/27733], Loss: 3.0125\n",
      "Epoch [91/300], Step [5000/27733], Loss: 1.5561\n",
      "Epoch [91/300], Step [5100/27733], Loss: 3.1069\n",
      "Epoch [91/300], Step [5200/27733], Loss: 2.1363\n",
      "Epoch [91/300], Step [5300/27733], Loss: 2.4644\n",
      "Epoch [91/300], Step [5400/27733], Loss: 2.8698\n",
      "Epoch [91/300], Step [5500/27733], Loss: 2.3335\n",
      "Epoch [91/300], Step [5600/27733], Loss: 2.7451\n",
      "Epoch [91/300], Step [5700/27733], Loss: 2.0804\n",
      "Epoch [91/300], Step [5800/27733], Loss: 2.7055\n",
      "Epoch [91/300], Step [5900/27733], Loss: 3.0997\n",
      "Epoch [91/300], Step [6000/27733], Loss: 1.7495\n",
      "Epoch [91/300], Step [6100/27733], Loss: 2.6224\n",
      "Epoch [91/300], Step [6200/27733], Loss: 2.4851\n",
      "Epoch [91/300], Step [6300/27733], Loss: 2.6080\n",
      "Epoch [91/300], Step [6400/27733], Loss: 1.6725\n",
      "Epoch [91/300], Step [6500/27733], Loss: 2.4021\n",
      "Epoch [91/300], Step [6600/27733], Loss: 2.3938\n",
      "Epoch [91/300], Step [6700/27733], Loss: 2.4608\n",
      "Epoch [91/300], Step [6800/27733], Loss: 3.0531\n",
      "Epoch [91/300], Step [6900/27733], Loss: 2.8674\n",
      "Epoch [91/300], Step [7000/27733], Loss: 2.5431\n",
      "Epoch [91/300], Step [7100/27733], Loss: 2.1897\n",
      "Epoch [91/300], Step [7200/27733], Loss: 2.3266\n",
      "Epoch [91/300], Step [7300/27733], Loss: 3.2689\n",
      "Epoch [91/300], Step [7400/27733], Loss: 2.8480\n",
      "Epoch [91/300], Step [7500/27733], Loss: 3.4321\n",
      "Epoch [91/300], Step [7600/27733], Loss: 2.1184\n",
      "Epoch [91/300], Step [7700/27733], Loss: 3.2546\n",
      "Epoch [91/300], Step [7800/27733], Loss: 4.0309\n",
      "Epoch [91/300], Step [7900/27733], Loss: 2.7144\n",
      "Epoch [91/300], Step [8000/27733], Loss: 3.0130\n",
      "Epoch [91/300], Step [8100/27733], Loss: 3.7171\n",
      "Epoch [91/300], Step [8200/27733], Loss: 3.1553\n",
      "Epoch [91/300], Step [8300/27733], Loss: 2.5834\n",
      "Epoch [91/300], Step [8400/27733], Loss: 2.4474\n",
      "Epoch [91/300], Step [8500/27733], Loss: 2.5107\n",
      "Epoch [91/300], Step [8600/27733], Loss: 2.6673\n",
      "Epoch [91/300], Step [8700/27733], Loss: 3.0388\n",
      "Epoch [91/300], Step [8800/27733], Loss: 2.6293\n",
      "Epoch [91/300], Step [8900/27733], Loss: 2.9954\n",
      "Epoch [91/300], Step [9000/27733], Loss: 2.6903\n",
      "Epoch [91/300], Step [9100/27733], Loss: 2.5807\n",
      "Epoch [91/300], Step [9200/27733], Loss: 2.0687\n",
      "Epoch [91/300], Step [9300/27733], Loss: 2.9062\n",
      "Epoch [91/300], Step [9400/27733], Loss: 1.6918\n",
      "Epoch [91/300], Step [9500/27733], Loss: 2.3365\n",
      "Epoch [91/300], Step [9600/27733], Loss: 3.0029\n",
      "Epoch [91/300], Step [9700/27733], Loss: 2.6276\n",
      "Epoch [91/300], Step [9800/27733], Loss: 2.7436\n",
      "Epoch [91/300], Step [9900/27733], Loss: 2.6089\n",
      "Epoch [91/300], Step [10000/27733], Loss: 2.5152\n",
      "Epoch [91/300], Step [10100/27733], Loss: 2.1121\n",
      "Epoch [91/300], Step [10200/27733], Loss: 2.7799\n",
      "Epoch [91/300], Step [10300/27733], Loss: 2.4353\n",
      "Epoch [91/300], Step [10400/27733], Loss: 2.1086\n",
      "Epoch [91/300], Step [10500/27733], Loss: 2.3638\n",
      "Epoch [91/300], Step [10600/27733], Loss: 2.6272\n",
      "Epoch [91/300], Step [10700/27733], Loss: 2.8021\n",
      "Epoch [91/300], Step [10800/27733], Loss: 3.8410\n",
      "Epoch [91/300], Step [10900/27733], Loss: 2.4127\n",
      "Epoch [91/300], Step [11000/27733], Loss: 2.1987\n",
      "Epoch [91/300], Step [11100/27733], Loss: 3.1578\n",
      "Epoch [91/300], Step [11200/27733], Loss: 2.3359\n",
      "Epoch [91/300], Step [11300/27733], Loss: 2.8369\n",
      "Epoch [91/300], Step [11400/27733], Loss: 3.1250\n",
      "Epoch [91/300], Step [11500/27733], Loss: 2.9686\n",
      "Epoch [91/300], Step [11600/27733], Loss: 2.6884\n",
      "Epoch [91/300], Step [11700/27733], Loss: 1.9000\n",
      "Epoch [91/300], Step [11800/27733], Loss: 2.7671\n",
      "Epoch [91/300], Step [11900/27733], Loss: 3.4188\n",
      "Epoch [91/300], Step [12000/27733], Loss: 2.8395\n",
      "Epoch [91/300], Step [12100/27733], Loss: 2.8471\n",
      "Epoch [91/300], Step [12200/27733], Loss: 2.4001\n",
      "Epoch [91/300], Step [12300/27733], Loss: 3.2005\n",
      "Epoch [91/300], Step [12400/27733], Loss: 3.5579\n",
      "Epoch [91/300], Step [12500/27733], Loss: 2.5185\n",
      "Epoch [91/300], Step [12600/27733], Loss: 2.6231\n",
      "Epoch [91/300], Step [12700/27733], Loss: 1.8049\n",
      "Epoch [91/300], Step [12800/27733], Loss: 2.4384\n",
      "Epoch [91/300], Step [12900/27733], Loss: 3.2885\n",
      "Epoch [91/300], Step [13000/27733], Loss: 3.9295\n",
      "Epoch [91/300], Step [13100/27733], Loss: 3.2941\n",
      "Epoch [91/300], Step [13200/27733], Loss: 2.9207\n",
      "Epoch [91/300], Step [13300/27733], Loss: 3.6875\n",
      "Epoch [91/300], Step [13400/27733], Loss: 2.4326\n",
      "Epoch [91/300], Step [13500/27733], Loss: 2.0608\n",
      "Epoch [91/300], Step [13600/27733], Loss: 2.8553\n",
      "Epoch [91/300], Step [13700/27733], Loss: 3.2559\n",
      "Epoch [91/300], Step [13800/27733], Loss: 2.3192\n",
      "Epoch [91/300], Step [13900/27733], Loss: 2.8375\n",
      "Epoch [91/300], Step [14000/27733], Loss: 2.5704\n",
      "Epoch [91/300], Step [14100/27733], Loss: 2.3388\n",
      "Epoch [91/300], Step [14200/27733], Loss: 2.9102\n",
      "Epoch [91/300], Step [14300/27733], Loss: 2.9984\n",
      "Epoch [91/300], Step [14400/27733], Loss: 2.5082\n",
      "Epoch [91/300], Step [14500/27733], Loss: 2.3527\n",
      "Epoch [91/300], Step [14600/27733], Loss: 3.2298\n",
      "Epoch [91/300], Step [14700/27733], Loss: 2.9349\n",
      "Epoch [91/300], Step [14800/27733], Loss: 2.4495\n",
      "Epoch [91/300], Step [14900/27733], Loss: 2.2949\n",
      "Epoch [91/300], Step [15000/27733], Loss: 1.9365\n",
      "Epoch [91/300], Step [15100/27733], Loss: 2.7790\n",
      "Epoch [91/300], Step [15200/27733], Loss: 4.2211\n",
      "Epoch [91/300], Step [15300/27733], Loss: 2.3732\n",
      "Epoch [91/300], Step [15400/27733], Loss: 2.3813\n",
      "Epoch [91/300], Step [15500/27733], Loss: 3.2112\n",
      "Epoch [91/300], Step [15600/27733], Loss: 1.9974\n",
      "Epoch [91/300], Step [15700/27733], Loss: 2.6529\n",
      "Epoch [91/300], Step [15800/27733], Loss: 2.1425\n",
      "Epoch [91/300], Step [15900/27733], Loss: 2.4204\n",
      "Epoch [91/300], Step [16000/27733], Loss: 4.4629\n",
      "Epoch [91/300], Step [16100/27733], Loss: 3.4121\n",
      "Epoch [91/300], Step [16200/27733], Loss: 3.2485\n",
      "Epoch [91/300], Step [16300/27733], Loss: 2.6233\n",
      "Epoch [91/300], Step [16400/27733], Loss: 3.0683\n",
      "Epoch [91/300], Step [16500/27733], Loss: 2.8063\n",
      "Epoch [91/300], Step [16600/27733], Loss: 2.8644\n",
      "Epoch [91/300], Step [16700/27733], Loss: 3.1338\n",
      "Epoch [91/300], Step [16800/27733], Loss: 2.7926\n",
      "Epoch [91/300], Step [16900/27733], Loss: 3.1999\n",
      "Epoch [91/300], Step [17000/27733], Loss: 3.0068\n",
      "Epoch [91/300], Step [17100/27733], Loss: 2.9341\n",
      "Epoch [91/300], Step [17200/27733], Loss: 2.7270\n",
      "Epoch [91/300], Step [17300/27733], Loss: 2.8488\n",
      "Epoch [91/300], Step [17400/27733], Loss: 2.4014\n",
      "Epoch [91/300], Step [17500/27733], Loss: 3.0292\n",
      "Epoch [91/300], Step [17600/27733], Loss: 3.2096\n",
      "Epoch [91/300], Step [17700/27733], Loss: 2.5124\n",
      "Epoch [91/300], Step [17800/27733], Loss: 2.7654\n",
      "Epoch [91/300], Step [17900/27733], Loss: 2.7973\n",
      "Epoch [91/300], Step [18000/27733], Loss: 2.3893\n",
      "Epoch [91/300], Step [18100/27733], Loss: 2.6262\n",
      "Epoch [91/300], Step [18200/27733], Loss: 3.2654\n",
      "Epoch [91/300], Step [18300/27733], Loss: 2.4248\n",
      "Epoch [91/300], Step [18400/27733], Loss: 2.5684\n",
      "Epoch [91/300], Step [18500/27733], Loss: 2.2098\n",
      "Epoch [91/300], Step [18600/27733], Loss: 2.4541\n",
      "Epoch [91/300], Step [18700/27733], Loss: 3.5985\n",
      "Epoch [91/300], Step [18800/27733], Loss: 2.7322\n",
      "Epoch [91/300], Step [18900/27733], Loss: 2.7387\n",
      "Epoch [91/300], Step [19000/27733], Loss: 2.5294\n",
      "Epoch [91/300], Step [19100/27733], Loss: 3.6553\n",
      "Epoch [91/300], Step [19200/27733], Loss: 1.9329\n",
      "Epoch [91/300], Step [19300/27733], Loss: 2.1170\n",
      "Epoch [91/300], Step [19400/27733], Loss: 3.5005\n",
      "Epoch [91/300], Step [19500/27733], Loss: 2.8043\n",
      "Epoch [91/300], Step [19600/27733], Loss: 2.3101\n",
      "Epoch [91/300], Step [19700/27733], Loss: 3.1329\n",
      "Epoch [91/300], Step [19800/27733], Loss: 2.7760\n",
      "Epoch [91/300], Step [19900/27733], Loss: 2.6700\n",
      "Epoch [91/300], Step [20000/27733], Loss: 2.6084\n",
      "Epoch [91/300], Step [20100/27733], Loss: 3.3471\n",
      "Epoch [91/300], Step [20200/27733], Loss: 2.9244\n",
      "Epoch [91/300], Step [20300/27733], Loss: 3.1032\n",
      "Epoch [91/300], Step [20400/27733], Loss: 3.3371\n",
      "Epoch [91/300], Step [20500/27733], Loss: 2.1448\n",
      "Epoch [91/300], Step [20600/27733], Loss: 3.4971\n",
      "Epoch [91/300], Step [20700/27733], Loss: 2.8668\n",
      "Epoch [91/300], Step [20800/27733], Loss: 2.7113\n",
      "Epoch [91/300], Step [20900/27733], Loss: 3.0333\n",
      "Epoch [91/300], Step [21000/27733], Loss: 2.4997\n",
      "Epoch [91/300], Step [21100/27733], Loss: 4.3210\n",
      "Epoch [91/300], Step [21200/27733], Loss: 3.1019\n",
      "Epoch [91/300], Step [21300/27733], Loss: 2.4975\n",
      "Epoch [91/300], Step [21400/27733], Loss: 3.0164\n",
      "Epoch [91/300], Step [21500/27733], Loss: 3.3591\n",
      "Epoch [91/300], Step [21600/27733], Loss: 3.8072\n",
      "Epoch [91/300], Step [21700/27733], Loss: 2.3343\n",
      "Epoch [91/300], Step [21800/27733], Loss: 2.5888\n",
      "Epoch [91/300], Step [21900/27733], Loss: 2.9469\n",
      "Epoch [91/300], Step [22000/27733], Loss: 2.5861\n",
      "Epoch [91/300], Step [22100/27733], Loss: 2.6373\n",
      "Epoch [91/300], Step [22200/27733], Loss: 2.6041\n",
      "Epoch [91/300], Step [22300/27733], Loss: 2.6672\n",
      "Epoch [91/300], Step [22400/27733], Loss: 2.9035\n",
      "Epoch [91/300], Step [22500/27733], Loss: 2.9622\n",
      "Epoch [91/300], Step [22600/27733], Loss: 3.5751\n",
      "Epoch [91/300], Step [22700/27733], Loss: 2.0543\n",
      "Epoch [91/300], Step [22800/27733], Loss: 2.8473\n",
      "Epoch [91/300], Step [22900/27733], Loss: 2.9167\n",
      "Epoch [91/300], Step [23000/27733], Loss: 3.0971\n",
      "Epoch [91/300], Step [23100/27733], Loss: 3.2106\n",
      "Epoch [91/300], Step [23200/27733], Loss: 3.2936\n",
      "Epoch [91/300], Step [23300/27733], Loss: 2.8009\n",
      "Epoch [91/300], Step [23400/27733], Loss: 2.5764\n",
      "Epoch [91/300], Step [23500/27733], Loss: 2.6813\n",
      "Epoch [91/300], Step [23600/27733], Loss: 3.0397\n",
      "Epoch [91/300], Step [23700/27733], Loss: 2.1497\n",
      "Epoch [91/300], Step [23800/27733], Loss: 2.4684\n",
      "Epoch [91/300], Step [23900/27733], Loss: 3.4223\n",
      "Epoch [91/300], Step [24000/27733], Loss: 3.9943\n",
      "Epoch [91/300], Step [24100/27733], Loss: 3.3100\n",
      "Epoch [91/300], Step [24200/27733], Loss: 2.5984\n",
      "Epoch [91/300], Step [24300/27733], Loss: 3.6153\n",
      "Epoch [91/300], Step [24400/27733], Loss: 2.3222\n",
      "Epoch [91/300], Step [24500/27733], Loss: 3.4981\n",
      "Epoch [91/300], Step [24600/27733], Loss: 3.3307\n",
      "Epoch [91/300], Step [24700/27733], Loss: 2.6512\n",
      "Epoch [91/300], Step [24800/27733], Loss: 4.1979\n",
      "Epoch [91/300], Step [24900/27733], Loss: 2.7362\n",
      "Epoch [91/300], Step [25000/27733], Loss: 2.4099\n",
      "Epoch [91/300], Step [25100/27733], Loss: 2.8523\n",
      "Epoch [91/300], Step [25200/27733], Loss: 2.8512\n",
      "Epoch [91/300], Step [25300/27733], Loss: 2.7170\n",
      "Epoch [91/300], Step [25400/27733], Loss: 2.8351\n",
      "Epoch [91/300], Step [25500/27733], Loss: 3.6350\n",
      "Epoch [91/300], Step [25600/27733], Loss: 1.9328\n",
      "Epoch [91/300], Step [25700/27733], Loss: 2.4781\n",
      "Epoch [91/300], Step [25800/27733], Loss: 2.8345\n",
      "Epoch [91/300], Step [25900/27733], Loss: 2.3223\n",
      "Epoch [91/300], Step [26000/27733], Loss: 2.8736\n",
      "Epoch [91/300], Step [26100/27733], Loss: 3.0271\n",
      "Epoch [91/300], Step [26200/27733], Loss: 2.9007\n",
      "Epoch [91/300], Step [26300/27733], Loss: 4.2772\n",
      "Epoch [91/300], Step [26400/27733], Loss: 3.1948\n",
      "Epoch [91/300], Step [26500/27733], Loss: 2.6573\n",
      "Epoch [91/300], Step [26600/27733], Loss: 2.9779\n",
      "Epoch [91/300], Step [26700/27733], Loss: 3.3075\n",
      "Epoch [91/300], Step [26800/27733], Loss: 3.5567\n",
      "Epoch [91/300], Step [26900/27733], Loss: 2.9732\n",
      "Epoch [91/300], Step [27000/27733], Loss: 2.9525\n",
      "Epoch [91/300], Step [27100/27733], Loss: 2.9782\n",
      "Epoch [91/300], Step [27200/27733], Loss: 3.0609\n",
      "Epoch [91/300], Step [27300/27733], Loss: 2.4890\n",
      "Epoch [91/300], Step [27400/27733], Loss: 1.7038\n",
      "Epoch [91/300], Step [27500/27733], Loss: 2.8224\n",
      "Epoch [91/300], Step [27600/27733], Loss: 2.9358\n",
      "Epoch [91/300], Step [27700/27733], Loss: 3.1233\n",
      "Epoch [92/300], Step [100/27733], Loss: 2.3859\n",
      "Epoch [92/300], Step [200/27733], Loss: 2.5675\n",
      "Epoch [92/300], Step [300/27733], Loss: 3.1797\n",
      "Epoch [92/300], Step [400/27733], Loss: 2.2913\n",
      "Epoch [92/300], Step [500/27733], Loss: 2.0365\n",
      "Epoch [92/300], Step [600/27733], Loss: 3.1422\n",
      "Epoch [92/300], Step [700/27733], Loss: 2.0302\n",
      "Epoch [92/300], Step [800/27733], Loss: 2.4747\n",
      "Epoch [92/300], Step [900/27733], Loss: 2.1209\n",
      "Epoch [92/300], Step [1000/27733], Loss: 3.7801\n",
      "Epoch [92/300], Step [1100/27733], Loss: 2.7068\n",
      "Epoch [92/300], Step [1200/27733], Loss: 2.9282\n",
      "Epoch [92/300], Step [1300/27733], Loss: 2.3256\n",
      "Epoch [92/300], Step [1400/27733], Loss: 3.1010\n",
      "Epoch [92/300], Step [1500/27733], Loss: 2.4562\n",
      "Epoch [92/300], Step [1600/27733], Loss: 2.2072\n",
      "Epoch [92/300], Step [1700/27733], Loss: 2.0082\n",
      "Epoch [92/300], Step [1800/27733], Loss: 2.7814\n",
      "Epoch [92/300], Step [1900/27733], Loss: 2.0392\n",
      "Epoch [92/300], Step [2000/27733], Loss: 1.8116\n",
      "Epoch [92/300], Step [2100/27733], Loss: 1.9833\n",
      "Epoch [92/300], Step [2200/27733], Loss: 1.8034\n",
      "Epoch [92/300], Step [2300/27733], Loss: 1.9843\n",
      "Epoch [92/300], Step [2400/27733], Loss: 2.6629\n",
      "Epoch [92/300], Step [2500/27733], Loss: 2.5338\n",
      "Epoch [92/300], Step [2600/27733], Loss: 2.4374\n",
      "Epoch [92/300], Step [2700/27733], Loss: 2.5232\n",
      "Epoch [92/300], Step [2800/27733], Loss: 2.7486\n",
      "Epoch [92/300], Step [2900/27733], Loss: 2.7103\n",
      "Epoch [92/300], Step [3000/27733], Loss: 2.2919\n",
      "Epoch [92/300], Step [3100/27733], Loss: 2.7689\n",
      "Epoch [92/300], Step [3200/27733], Loss: 2.7922\n",
      "Epoch [92/300], Step [3300/27733], Loss: 2.5598\n",
      "Epoch [92/300], Step [3400/27733], Loss: 2.0224\n",
      "Epoch [92/300], Step [3500/27733], Loss: 2.3594\n",
      "Epoch [92/300], Step [3600/27733], Loss: 2.0787\n",
      "Epoch [92/300], Step [3700/27733], Loss: 3.2012\n",
      "Epoch [92/300], Step [3800/27733], Loss: 3.7960\n",
      "Epoch [92/300], Step [3900/27733], Loss: 2.2899\n",
      "Epoch [92/300], Step [4000/27733], Loss: 1.7308\n",
      "Epoch [92/300], Step [4100/27733], Loss: 2.5450\n",
      "Epoch [92/300], Step [4200/27733], Loss: 3.2349\n",
      "Epoch [92/300], Step [4300/27733], Loss: 2.3611\n",
      "Epoch [92/300], Step [4400/27733], Loss: 2.7976\n",
      "Epoch [92/300], Step [4500/27733], Loss: 3.5111\n",
      "Epoch [92/300], Step [4600/27733], Loss: 3.0496\n",
      "Epoch [92/300], Step [4700/27733], Loss: 2.9472\n",
      "Epoch [92/300], Step [4800/27733], Loss: 2.7834\n",
      "Epoch [92/300], Step [4900/27733], Loss: 2.3559\n",
      "Epoch [92/300], Step [5000/27733], Loss: 2.1548\n",
      "Epoch [92/300], Step [5100/27733], Loss: 2.3575\n",
      "Epoch [92/300], Step [5200/27733], Loss: 2.7388\n",
      "Epoch [92/300], Step [5300/27733], Loss: 3.4728\n",
      "Epoch [92/300], Step [5400/27733], Loss: 2.0974\n",
      "Epoch [92/300], Step [5500/27733], Loss: 2.2978\n",
      "Epoch [92/300], Step [5600/27733], Loss: 3.3675\n",
      "Epoch [92/300], Step [5700/27733], Loss: 2.3347\n",
      "Epoch [92/300], Step [5800/27733], Loss: 2.0266\n",
      "Epoch [92/300], Step [5900/27733], Loss: 2.6308\n",
      "Epoch [92/300], Step [6000/27733], Loss: 3.0429\n",
      "Epoch [92/300], Step [6100/27733], Loss: 2.6583\n",
      "Epoch [92/300], Step [6200/27733], Loss: 2.0977\n",
      "Epoch [92/300], Step [6300/27733], Loss: 2.8196\n",
      "Epoch [92/300], Step [6400/27733], Loss: 2.3814\n",
      "Epoch [92/300], Step [6500/27733], Loss: 2.0465\n",
      "Epoch [92/300], Step [6600/27733], Loss: 2.1725\n",
      "Epoch [92/300], Step [6700/27733], Loss: 2.8496\n",
      "Epoch [92/300], Step [6800/27733], Loss: 2.7578\n",
      "Epoch [92/300], Step [6900/27733], Loss: 2.8368\n",
      "Epoch [92/300], Step [7000/27733], Loss: 2.6485\n",
      "Epoch [92/300], Step [7100/27733], Loss: 2.7227\n",
      "Epoch [92/300], Step [7200/27733], Loss: 3.2934\n",
      "Epoch [92/300], Step [7300/27733], Loss: 2.6416\n",
      "Epoch [92/300], Step [7400/27733], Loss: 2.2627\n",
      "Epoch [92/300], Step [7500/27733], Loss: 2.2015\n",
      "Epoch [92/300], Step [7600/27733], Loss: 3.2529\n",
      "Epoch [92/300], Step [7700/27733], Loss: 2.6191\n",
      "Epoch [92/300], Step [7800/27733], Loss: 3.1637\n",
      "Epoch [92/300], Step [7900/27733], Loss: 1.8779\n",
      "Epoch [92/300], Step [8000/27733], Loss: 2.8778\n",
      "Epoch [92/300], Step [8100/27733], Loss: 2.4820\n",
      "Epoch [92/300], Step [8200/27733], Loss: 2.9576\n",
      "Epoch [92/300], Step [8300/27733], Loss: 2.7479\n",
      "Epoch [92/300], Step [8400/27733], Loss: 3.1419\n",
      "Epoch [92/300], Step [8500/27733], Loss: 2.3918\n",
      "Epoch [92/300], Step [8600/27733], Loss: 3.1322\n",
      "Epoch [92/300], Step [8700/27733], Loss: 2.7530\n",
      "Epoch [92/300], Step [8800/27733], Loss: 2.2819\n",
      "Epoch [92/300], Step [8900/27733], Loss: 3.1046\n",
      "Epoch [92/300], Step [9000/27733], Loss: 3.3982\n",
      "Epoch [92/300], Step [9100/27733], Loss: 2.2739\n",
      "Epoch [92/300], Step [9200/27733], Loss: 2.4023\n",
      "Epoch [92/300], Step [9300/27733], Loss: 3.6294\n",
      "Epoch [92/300], Step [9400/27733], Loss: 2.3632\n",
      "Epoch [92/300], Step [9500/27733], Loss: 2.6598\n",
      "Epoch [92/300], Step [9600/27733], Loss: 2.5476\n",
      "Epoch [92/300], Step [9700/27733], Loss: 3.3566\n",
      "Epoch [92/300], Step [9800/27733], Loss: 3.4475\n",
      "Epoch [92/300], Step [9900/27733], Loss: 2.0089\n",
      "Epoch [92/300], Step [10000/27733], Loss: 2.9185\n",
      "Epoch [92/300], Step [10100/27733], Loss: 2.6682\n",
      "Epoch [92/300], Step [10200/27733], Loss: 2.5222\n",
      "Epoch [92/300], Step [10300/27733], Loss: 2.4602\n",
      "Epoch [92/300], Step [10400/27733], Loss: 3.8462\n",
      "Epoch [92/300], Step [10500/27733], Loss: 2.6593\n",
      "Epoch [92/300], Step [10600/27733], Loss: 2.5857\n",
      "Epoch [92/300], Step [10700/27733], Loss: 3.0143\n",
      "Epoch [92/300], Step [10800/27733], Loss: 3.5816\n",
      "Epoch [92/300], Step [10900/27733], Loss: 2.3680\n",
      "Epoch [92/300], Step [11000/27733], Loss: 2.0794\n",
      "Epoch [92/300], Step [11100/27733], Loss: 2.8126\n",
      "Epoch [92/300], Step [11200/27733], Loss: 2.5778\n",
      "Epoch [92/300], Step [11300/27733], Loss: 2.2391\n",
      "Epoch [92/300], Step [11400/27733], Loss: 1.9517\n",
      "Epoch [92/300], Step [11500/27733], Loss: 2.7931\n",
      "Epoch [92/300], Step [11600/27733], Loss: 2.3299\n",
      "Epoch [92/300], Step [11700/27733], Loss: 3.2135\n",
      "Epoch [92/300], Step [11800/27733], Loss: 1.9519\n",
      "Epoch [92/300], Step [11900/27733], Loss: 2.0164\n",
      "Epoch [92/300], Step [12000/27733], Loss: 3.0025\n",
      "Epoch [92/300], Step [12100/27733], Loss: 2.9050\n",
      "Epoch [92/300], Step [12200/27733], Loss: 2.6466\n",
      "Epoch [92/300], Step [12300/27733], Loss: 2.2154\n",
      "Epoch [92/300], Step [12400/27733], Loss: 3.1682\n",
      "Epoch [92/300], Step [12500/27733], Loss: 3.2322\n",
      "Epoch [92/300], Step [12600/27733], Loss: 2.9344\n",
      "Epoch [92/300], Step [12700/27733], Loss: 2.1291\n",
      "Epoch [92/300], Step [12800/27733], Loss: 3.4298\n",
      "Epoch [92/300], Step [12900/27733], Loss: 2.4454\n",
      "Epoch [92/300], Step [13000/27733], Loss: 2.7267\n",
      "Epoch [92/300], Step [13100/27733], Loss: 2.3717\n",
      "Epoch [92/300], Step [13200/27733], Loss: 2.5292\n",
      "Epoch [92/300], Step [13300/27733], Loss: 4.4296\n",
      "Epoch [92/300], Step [13400/27733], Loss: 2.6030\n",
      "Epoch [92/300], Step [13500/27733], Loss: 3.3437\n",
      "Epoch [92/300], Step [13600/27733], Loss: 2.5456\n",
      "Epoch [92/300], Step [13700/27733], Loss: 2.6533\n",
      "Epoch [92/300], Step [13800/27733], Loss: 2.5832\n",
      "Epoch [92/300], Step [13900/27733], Loss: 3.1157\n",
      "Epoch [92/300], Step [14000/27733], Loss: 2.5937\n",
      "Epoch [92/300], Step [14100/27733], Loss: 3.2316\n",
      "Epoch [92/300], Step [14200/27733], Loss: 2.5538\n",
      "Epoch [92/300], Step [14300/27733], Loss: 2.2140\n",
      "Epoch [92/300], Step [14400/27733], Loss: 2.9803\n",
      "Epoch [92/300], Step [14500/27733], Loss: 2.6521\n",
      "Epoch [92/300], Step [14600/27733], Loss: 3.1429\n",
      "Epoch [92/300], Step [14700/27733], Loss: 2.6451\n",
      "Epoch [92/300], Step [14800/27733], Loss: 3.1946\n",
      "Epoch [92/300], Step [14900/27733], Loss: 2.6645\n",
      "Epoch [92/300], Step [15000/27733], Loss: 2.8084\n",
      "Epoch [92/300], Step [15100/27733], Loss: 2.7145\n",
      "Epoch [92/300], Step [15200/27733], Loss: 2.5318\n",
      "Epoch [92/300], Step [15300/27733], Loss: 3.7820\n",
      "Epoch [92/300], Step [15400/27733], Loss: 2.5874\n",
      "Epoch [92/300], Step [15500/27733], Loss: 2.8628\n",
      "Epoch [92/300], Step [15600/27733], Loss: 3.0191\n",
      "Epoch [92/300], Step [15700/27733], Loss: 2.8820\n",
      "Epoch [92/300], Step [15800/27733], Loss: 3.2300\n",
      "Epoch [92/300], Step [15900/27733], Loss: 2.5115\n",
      "Epoch [92/300], Step [16000/27733], Loss: 2.6682\n",
      "Epoch [92/300], Step [16100/27733], Loss: 3.2102\n",
      "Epoch [92/300], Step [16200/27733], Loss: 2.8521\n",
      "Epoch [92/300], Step [16300/27733], Loss: 3.0979\n",
      "Epoch [92/300], Step [16400/27733], Loss: 2.9063\n",
      "Epoch [92/300], Step [16500/27733], Loss: 2.6568\n",
      "Epoch [92/300], Step [16600/27733], Loss: 2.0226\n",
      "Epoch [92/300], Step [16700/27733], Loss: 1.9494\n",
      "Epoch [92/300], Step [16800/27733], Loss: 3.1591\n",
      "Epoch [92/300], Step [16900/27733], Loss: 2.5321\n",
      "Epoch [92/300], Step [17000/27733], Loss: 3.3603\n",
      "Epoch [92/300], Step [17100/27733], Loss: 2.5312\n",
      "Epoch [92/300], Step [17200/27733], Loss: 3.1281\n",
      "Epoch [92/300], Step [17300/27733], Loss: 3.4382\n",
      "Epoch [92/300], Step [17400/27733], Loss: 2.9522\n",
      "Epoch [92/300], Step [17500/27733], Loss: 2.3766\n",
      "Epoch [92/300], Step [17600/27733], Loss: 3.9684\n",
      "Epoch [92/300], Step [17700/27733], Loss: 3.1217\n",
      "Epoch [92/300], Step [17800/27733], Loss: 1.9622\n",
      "Epoch [92/300], Step [17900/27733], Loss: 2.7409\n",
      "Epoch [92/300], Step [18000/27733], Loss: 3.7506\n",
      "Epoch [92/300], Step [18100/27733], Loss: 2.1375\n",
      "Epoch [92/300], Step [18200/27733], Loss: 3.4671\n",
      "Epoch [92/300], Step [18300/27733], Loss: 2.5886\n",
      "Epoch [92/300], Step [18400/27733], Loss: 2.5178\n",
      "Epoch [92/300], Step [18500/27733], Loss: 2.2192\n",
      "Epoch [92/300], Step [18600/27733], Loss: 2.2524\n",
      "Epoch [92/300], Step [18700/27733], Loss: 2.6901\n",
      "Epoch [92/300], Step [18800/27733], Loss: 3.4751\n",
      "Epoch [92/300], Step [18900/27733], Loss: 3.4132\n",
      "Epoch [92/300], Step [19000/27733], Loss: 2.5989\n",
      "Epoch [92/300], Step [19100/27733], Loss: 3.0622\n",
      "Epoch [92/300], Step [19200/27733], Loss: 2.4484\n",
      "Epoch [92/300], Step [19300/27733], Loss: 3.2428\n",
      "Epoch [92/300], Step [19400/27733], Loss: 2.8233\n",
      "Epoch [92/300], Step [19500/27733], Loss: 3.0444\n",
      "Epoch [92/300], Step [19600/27733], Loss: 2.3775\n",
      "Epoch [92/300], Step [19700/27733], Loss: 2.5258\n",
      "Epoch [92/300], Step [19800/27733], Loss: 2.7393\n",
      "Epoch [92/300], Step [19900/27733], Loss: 3.0452\n",
      "Epoch [92/300], Step [20000/27733], Loss: 2.9330\n",
      "Epoch [92/300], Step [20100/27733], Loss: 2.6305\n",
      "Epoch [92/300], Step [20200/27733], Loss: 3.2184\n",
      "Epoch [92/300], Step [20300/27733], Loss: 3.5767\n",
      "Epoch [92/300], Step [20400/27733], Loss: 2.8144\n",
      "Epoch [92/300], Step [20500/27733], Loss: 2.4380\n",
      "Epoch [92/300], Step [20600/27733], Loss: 3.2463\n",
      "Epoch [92/300], Step [20700/27733], Loss: 2.3688\n",
      "Epoch [92/300], Step [20800/27733], Loss: 2.5550\n",
      "Epoch [92/300], Step [20900/27733], Loss: 2.5278\n",
      "Epoch [92/300], Step [21000/27733], Loss: 3.2509\n",
      "Epoch [92/300], Step [21100/27733], Loss: 2.5115\n",
      "Epoch [92/300], Step [21200/27733], Loss: 2.4012\n",
      "Epoch [92/300], Step [21300/27733], Loss: 2.8958\n",
      "Epoch [92/300], Step [21400/27733], Loss: 2.8732\n",
      "Epoch [92/300], Step [21500/27733], Loss: 3.2043\n",
      "Epoch [92/300], Step [21600/27733], Loss: 2.3821\n",
      "Epoch [92/300], Step [21700/27733], Loss: 2.8303\n",
      "Epoch [92/300], Step [21800/27733], Loss: 2.7794\n",
      "Epoch [92/300], Step [21900/27733], Loss: 2.6056\n",
      "Epoch [92/300], Step [22000/27733], Loss: 2.4869\n",
      "Epoch [92/300], Step [22100/27733], Loss: 2.8155\n",
      "Epoch [92/300], Step [22200/27733], Loss: 2.1900\n",
      "Epoch [92/300], Step [22300/27733], Loss: 2.8599\n",
      "Epoch [92/300], Step [22400/27733], Loss: 2.9055\n",
      "Epoch [92/300], Step [22500/27733], Loss: 2.6565\n",
      "Epoch [92/300], Step [22600/27733], Loss: 2.6582\n",
      "Epoch [92/300], Step [22700/27733], Loss: 2.8434\n",
      "Epoch [92/300], Step [22800/27733], Loss: 2.5423\n",
      "Epoch [92/300], Step [22900/27733], Loss: 3.1482\n",
      "Epoch [92/300], Step [23000/27733], Loss: 3.5840\n",
      "Epoch [92/300], Step [23100/27733], Loss: 3.7267\n",
      "Epoch [92/300], Step [23200/27733], Loss: 3.2191\n",
      "Epoch [92/300], Step [23300/27733], Loss: 2.7588\n",
      "Epoch [92/300], Step [23400/27733], Loss: 2.8080\n",
      "Epoch [92/300], Step [23500/27733], Loss: 1.8179\n",
      "Epoch [92/300], Step [23600/27733], Loss: 3.3980\n",
      "Epoch [92/300], Step [23700/27733], Loss: 2.7373\n",
      "Epoch [92/300], Step [23800/27733], Loss: 2.8572\n",
      "Epoch [92/300], Step [23900/27733], Loss: 3.5768\n",
      "Epoch [92/300], Step [24000/27733], Loss: 2.6243\n",
      "Epoch [92/300], Step [24100/27733], Loss: 2.6616\n",
      "Epoch [92/300], Step [24200/27733], Loss: 3.0679\n",
      "Epoch [92/300], Step [24300/27733], Loss: 2.4384\n",
      "Epoch [92/300], Step [24400/27733], Loss: 1.7255\n",
      "Epoch [92/300], Step [24500/27733], Loss: 3.2515\n",
      "Epoch [92/300], Step [24600/27733], Loss: 3.2284\n",
      "Epoch [92/300], Step [24700/27733], Loss: 2.8115\n",
      "Epoch [92/300], Step [24800/27733], Loss: 2.6590\n",
      "Epoch [92/300], Step [24900/27733], Loss: 2.7975\n",
      "Epoch [92/300], Step [25000/27733], Loss: 2.5490\n",
      "Epoch [92/300], Step [25100/27733], Loss: 3.3101\n",
      "Epoch [92/300], Step [25200/27733], Loss: 3.2243\n",
      "Epoch [92/300], Step [25300/27733], Loss: 3.1579\n",
      "Epoch [92/300], Step [25400/27733], Loss: 2.5230\n",
      "Epoch [92/300], Step [25500/27733], Loss: 3.1505\n",
      "Epoch [92/300], Step [25600/27733], Loss: 3.1510\n",
      "Epoch [92/300], Step [25700/27733], Loss: 2.5440\n",
      "Epoch [92/300], Step [25800/27733], Loss: 3.0111\n",
      "Epoch [92/300], Step [25900/27733], Loss: 2.7417\n",
      "Epoch [92/300], Step [26000/27733], Loss: 2.9522\n",
      "Epoch [92/300], Step [26100/27733], Loss: 3.3900\n",
      "Epoch [92/300], Step [26200/27733], Loss: 3.4125\n",
      "Epoch [92/300], Step [26300/27733], Loss: 2.6208\n",
      "Epoch [92/300], Step [26400/27733], Loss: 2.7277\n",
      "Epoch [92/300], Step [26500/27733], Loss: 3.7081\n",
      "Epoch [92/300], Step [26600/27733], Loss: 3.0602\n",
      "Epoch [92/300], Step [26700/27733], Loss: 3.0731\n",
      "Epoch [92/300], Step [26800/27733], Loss: 3.3161\n",
      "Epoch [92/300], Step [26900/27733], Loss: 3.3896\n",
      "Epoch [92/300], Step [27000/27733], Loss: 2.9884\n",
      "Epoch [92/300], Step [27100/27733], Loss: 2.4748\n",
      "Epoch [92/300], Step [27200/27733], Loss: 3.3373\n",
      "Epoch [92/300], Step [27300/27733], Loss: 2.6047\n",
      "Epoch [92/300], Step [27400/27733], Loss: 3.3820\n",
      "Epoch [92/300], Step [27500/27733], Loss: 2.7967\n",
      "Epoch [92/300], Step [27600/27733], Loss: 3.4780\n",
      "Epoch [92/300], Step [27700/27733], Loss: 3.7063\n",
      "Epoch [93/300], Step [100/27733], Loss: 2.0873\n",
      "Epoch [93/300], Step [200/27733], Loss: 2.6807\n",
      "Epoch [93/300], Step [300/27733], Loss: 1.9509\n",
      "Epoch [93/300], Step [400/27733], Loss: 2.3603\n",
      "Epoch [93/300], Step [500/27733], Loss: 2.1594\n",
      "Epoch [93/300], Step [600/27733], Loss: 1.7855\n",
      "Epoch [93/300], Step [700/27733], Loss: 2.5679\n",
      "Epoch [93/300], Step [800/27733], Loss: 2.0375\n",
      "Epoch [93/300], Step [900/27733], Loss: 2.6998\n",
      "Epoch [93/300], Step [1000/27733], Loss: 2.1741\n",
      "Epoch [93/300], Step [1100/27733], Loss: 1.6664\n",
      "Epoch [93/300], Step [1200/27733], Loss: 2.2873\n",
      "Epoch [93/300], Step [1300/27733], Loss: 2.3550\n",
      "Epoch [93/300], Step [1400/27733], Loss: 1.5806\n",
      "Epoch [93/300], Step [1500/27733], Loss: 2.0688\n",
      "Epoch [93/300], Step [1600/27733], Loss: 3.1914\n",
      "Epoch [93/300], Step [1700/27733], Loss: 2.1325\n",
      "Epoch [93/300], Step [1800/27733], Loss: 1.9570\n",
      "Epoch [93/300], Step [1900/27733], Loss: 2.7071\n",
      "Epoch [93/300], Step [2000/27733], Loss: 2.5691\n",
      "Epoch [93/300], Step [2100/27733], Loss: 1.9291\n",
      "Epoch [93/300], Step [2200/27733], Loss: 2.4983\n",
      "Epoch [93/300], Step [2300/27733], Loss: 2.2165\n",
      "Epoch [93/300], Step [2400/27733], Loss: 2.4334\n",
      "Epoch [93/300], Step [2500/27733], Loss: 3.2696\n",
      "Epoch [93/300], Step [2600/27733], Loss: 2.0527\n",
      "Epoch [93/300], Step [2700/27733], Loss: 3.4735\n",
      "Epoch [93/300], Step [2800/27733], Loss: 2.2814\n",
      "Epoch [93/300], Step [2900/27733], Loss: 2.9024\n",
      "Epoch [93/300], Step [3000/27733], Loss: 2.5862\n",
      "Epoch [93/300], Step [3100/27733], Loss: 3.0019\n",
      "Epoch [93/300], Step [3200/27733], Loss: 2.8327\n",
      "Epoch [93/300], Step [3300/27733], Loss: 2.4987\n",
      "Epoch [93/300], Step [3400/27733], Loss: 1.9845\n",
      "Epoch [93/300], Step [3500/27733], Loss: 2.4908\n",
      "Epoch [93/300], Step [3600/27733], Loss: 2.5763\n",
      "Epoch [93/300], Step [3700/27733], Loss: 2.3757\n",
      "Epoch [93/300], Step [3800/27733], Loss: 2.3296\n",
      "Epoch [93/300], Step [3900/27733], Loss: 2.8285\n",
      "Epoch [93/300], Step [4000/27733], Loss: 2.8302\n",
      "Epoch [93/300], Step [4100/27733], Loss: 3.1466\n",
      "Epoch [93/300], Step [4200/27733], Loss: 2.8948\n",
      "Epoch [93/300], Step [4300/27733], Loss: 2.5236\n",
      "Epoch [93/300], Step [4400/27733], Loss: 2.4940\n",
      "Epoch [93/300], Step [4500/27733], Loss: 2.7939\n",
      "Epoch [93/300], Step [4600/27733], Loss: 1.8237\n",
      "Epoch [93/300], Step [4700/27733], Loss: 2.0097\n",
      "Epoch [93/300], Step [4800/27733], Loss: 2.4894\n",
      "Epoch [93/300], Step [4900/27733], Loss: 2.6797\n",
      "Epoch [93/300], Step [5000/27733], Loss: 2.4532\n",
      "Epoch [93/300], Step [5100/27733], Loss: 2.8598\n",
      "Epoch [93/300], Step [5200/27733], Loss: 2.0924\n",
      "Epoch [93/300], Step [5300/27733], Loss: 2.8705\n",
      "Epoch [93/300], Step [5400/27733], Loss: 2.4112\n",
      "Epoch [93/300], Step [5500/27733], Loss: 2.7712\n",
      "Epoch [93/300], Step [5600/27733], Loss: 2.4456\n",
      "Epoch [93/300], Step [5700/27733], Loss: 2.9780\n",
      "Epoch [93/300], Step [5800/27733], Loss: 2.6742\n",
      "Epoch [93/300], Step [5900/27733], Loss: 2.9335\n",
      "Epoch [93/300], Step [6000/27733], Loss: 2.8414\n",
      "Epoch [93/300], Step [6100/27733], Loss: 2.4436\n",
      "Epoch [93/300], Step [6200/27733], Loss: 2.6325\n",
      "Epoch [93/300], Step [6300/27733], Loss: 2.6954\n",
      "Epoch [93/300], Step [6400/27733], Loss: 2.7410\n",
      "Epoch [93/300], Step [6500/27733], Loss: 2.6288\n",
      "Epoch [93/300], Step [6600/27733], Loss: 2.4492\n",
      "Epoch [93/300], Step [6700/27733], Loss: 2.5079\n",
      "Epoch [93/300], Step [6800/27733], Loss: 2.1820\n",
      "Epoch [93/300], Step [6900/27733], Loss: 2.2665\n",
      "Epoch [93/300], Step [7000/27733], Loss: 2.1526\n",
      "Epoch [93/300], Step [7100/27733], Loss: 2.7096\n",
      "Epoch [93/300], Step [7200/27733], Loss: 2.3306\n",
      "Epoch [93/300], Step [7300/27733], Loss: 1.5210\n",
      "Epoch [93/300], Step [7400/27733], Loss: 2.2647\n",
      "Epoch [93/300], Step [7500/27733], Loss: 2.6413\n",
      "Epoch [93/300], Step [7600/27733], Loss: 2.6605\n",
      "Epoch [93/300], Step [7700/27733], Loss: 3.0182\n",
      "Epoch [93/300], Step [7800/27733], Loss: 2.9076\n",
      "Epoch [93/300], Step [7900/27733], Loss: 2.7776\n",
      "Epoch [93/300], Step [8000/27733], Loss: 2.0015\n",
      "Epoch [93/300], Step [8100/27733], Loss: 2.7352\n",
      "Epoch [93/300], Step [8200/27733], Loss: 2.0605\n",
      "Epoch [93/300], Step [8300/27733], Loss: 2.5329\n",
      "Epoch [93/300], Step [8400/27733], Loss: 2.8152\n",
      "Epoch [93/300], Step [8500/27733], Loss: 2.7721\n",
      "Epoch [93/300], Step [8600/27733], Loss: 2.4791\n",
      "Epoch [93/300], Step [8700/27733], Loss: 2.8670\n",
      "Epoch [93/300], Step [8800/27733], Loss: 2.6800\n",
      "Epoch [93/300], Step [8900/27733], Loss: 2.6847\n",
      "Epoch [93/300], Step [9000/27733], Loss: 2.6566\n",
      "Epoch [93/300], Step [9100/27733], Loss: 2.4702\n",
      "Epoch [93/300], Step [9200/27733], Loss: 2.4357\n",
      "Epoch [93/300], Step [9300/27733], Loss: 2.6505\n",
      "Epoch [93/300], Step [9400/27733], Loss: 2.4183\n",
      "Epoch [93/300], Step [9500/27733], Loss: 2.2436\n",
      "Epoch [93/300], Step [9600/27733], Loss: 2.9396\n",
      "Epoch [93/300], Step [9700/27733], Loss: 2.5701\n",
      "Epoch [93/300], Step [9800/27733], Loss: 2.4236\n",
      "Epoch [93/300], Step [9900/27733], Loss: 2.5847\n",
      "Epoch [93/300], Step [10000/27733], Loss: 3.1730\n",
      "Epoch [93/300], Step [10100/27733], Loss: 2.7128\n",
      "Epoch [93/300], Step [10200/27733], Loss: 2.7340\n",
      "Epoch [93/300], Step [10300/27733], Loss: 3.3019\n",
      "Epoch [93/300], Step [10400/27733], Loss: 2.7225\n",
      "Epoch [93/300], Step [10500/27733], Loss: 3.1201\n",
      "Epoch [93/300], Step [10600/27733], Loss: 2.3352\n",
      "Epoch [93/300], Step [10700/27733], Loss: 2.5177\n",
      "Epoch [93/300], Step [10800/27733], Loss: 3.1457\n",
      "Epoch [93/300], Step [10900/27733], Loss: 2.5511\n",
      "Epoch [93/300], Step [11000/27733], Loss: 2.2029\n",
      "Epoch [93/300], Step [11100/27733], Loss: 2.5951\n",
      "Epoch [93/300], Step [11200/27733], Loss: 1.7163\n",
      "Epoch [93/300], Step [11300/27733], Loss: 2.0109\n",
      "Epoch [93/300], Step [11400/27733], Loss: 2.7675\n",
      "Epoch [93/300], Step [11500/27733], Loss: 1.9695\n",
      "Epoch [93/300], Step [11600/27733], Loss: 3.1755\n",
      "Epoch [93/300], Step [11700/27733], Loss: 2.5875\n",
      "Epoch [93/300], Step [11800/27733], Loss: 3.1260\n",
      "Epoch [93/300], Step [11900/27733], Loss: 2.2224\n",
      "Epoch [93/300], Step [12000/27733], Loss: 3.3043\n",
      "Epoch [93/300], Step [12100/27733], Loss: 3.0212\n",
      "Epoch [93/300], Step [12200/27733], Loss: 2.4540\n",
      "Epoch [93/300], Step [12300/27733], Loss: 2.7545\n",
      "Epoch [93/300], Step [12400/27733], Loss: 2.4506\n",
      "Epoch [93/300], Step [12500/27733], Loss: 2.6896\n",
      "Epoch [93/300], Step [12600/27733], Loss: 2.7336\n",
      "Epoch [93/300], Step [12700/27733], Loss: 2.8679\n",
      "Epoch [93/300], Step [12800/27733], Loss: 2.6480\n",
      "Epoch [93/300], Step [12900/27733], Loss: 2.2184\n",
      "Epoch [93/300], Step [13000/27733], Loss: 2.7938\n",
      "Epoch [93/300], Step [13100/27733], Loss: 3.1063\n",
      "Epoch [93/300], Step [13200/27733], Loss: 3.0168\n",
      "Epoch [93/300], Step [13300/27733], Loss: 3.0233\n",
      "Epoch [93/300], Step [13400/27733], Loss: 1.8598\n",
      "Epoch [93/300], Step [13500/27733], Loss: 2.9942\n",
      "Epoch [93/300], Step [13600/27733], Loss: 2.4891\n",
      "Epoch [93/300], Step [13700/27733], Loss: 2.6523\n",
      "Epoch [93/300], Step [13800/27733], Loss: 2.6673\n",
      "Epoch [93/300], Step [13900/27733], Loss: 3.4460\n",
      "Epoch [93/300], Step [14000/27733], Loss: 2.8198\n",
      "Epoch [93/300], Step [14100/27733], Loss: 2.6552\n",
      "Epoch [93/300], Step [14200/27733], Loss: 3.1659\n",
      "Epoch [93/300], Step [14300/27733], Loss: 2.2603\n",
      "Epoch [93/300], Step [14400/27733], Loss: 3.5336\n",
      "Epoch [93/300], Step [14500/27733], Loss: 2.9590\n",
      "Epoch [93/300], Step [14600/27733], Loss: 2.3693\n",
      "Epoch [93/300], Step [14700/27733], Loss: 3.1774\n",
      "Epoch [93/300], Step [14800/27733], Loss: 3.5627\n",
      "Epoch [93/300], Step [14900/27733], Loss: 3.7416\n",
      "Epoch [93/300], Step [15000/27733], Loss: 3.0923\n",
      "Epoch [93/300], Step [15100/27733], Loss: 2.4759\n",
      "Epoch [93/300], Step [15200/27733], Loss: 2.5750\n",
      "Epoch [93/300], Step [15300/27733], Loss: 2.6652\n",
      "Epoch [93/300], Step [15400/27733], Loss: 2.7582\n",
      "Epoch [93/300], Step [15500/27733], Loss: 3.3922\n",
      "Epoch [93/300], Step [15600/27733], Loss: 3.5894\n",
      "Epoch [93/300], Step [15700/27733], Loss: 3.5428\n",
      "Epoch [93/300], Step [15800/27733], Loss: 3.1408\n",
      "Epoch [93/300], Step [15900/27733], Loss: 3.2172\n",
      "Epoch [93/300], Step [16000/27733], Loss: 2.5604\n",
      "Epoch [93/300], Step [16100/27733], Loss: 2.0058\n",
      "Epoch [93/300], Step [16200/27733], Loss: 3.5075\n",
      "Epoch [93/300], Step [16300/27733], Loss: 2.4355\n",
      "Epoch [93/300], Step [16400/27733], Loss: 2.5586\n",
      "Epoch [93/300], Step [16500/27733], Loss: 3.5607\n",
      "Epoch [93/300], Step [16600/27733], Loss: 3.1239\n",
      "Epoch [93/300], Step [16700/27733], Loss: 2.8004\n",
      "Epoch [93/300], Step [16800/27733], Loss: 3.9375\n",
      "Epoch [93/300], Step [16900/27733], Loss: 3.1455\n",
      "Epoch [93/300], Step [17000/27733], Loss: 3.2338\n",
      "Epoch [93/300], Step [17100/27733], Loss: 2.7712\n",
      "Epoch [93/300], Step [17200/27733], Loss: 2.0188\n",
      "Epoch [93/300], Step [17300/27733], Loss: 2.5809\n",
      "Epoch [93/300], Step [17400/27733], Loss: 3.4355\n",
      "Epoch [93/300], Step [17500/27733], Loss: 2.2456\n",
      "Epoch [93/300], Step [17600/27733], Loss: 2.8885\n",
      "Epoch [93/300], Step [17700/27733], Loss: 2.9976\n",
      "Epoch [93/300], Step [17800/27733], Loss: 3.2483\n",
      "Epoch [93/300], Step [17900/27733], Loss: 2.5604\n",
      "Epoch [93/300], Step [18000/27733], Loss: 3.3139\n",
      "Epoch [93/300], Step [18100/27733], Loss: 2.1921\n",
      "Epoch [93/300], Step [18200/27733], Loss: 2.4320\n",
      "Epoch [93/300], Step [18300/27733], Loss: 2.7203\n",
      "Epoch [93/300], Step [18400/27733], Loss: 3.2309\n",
      "Epoch [93/300], Step [18500/27733], Loss: 3.0240\n",
      "Epoch [93/300], Step [18600/27733], Loss: 2.8949\n",
      "Epoch [93/300], Step [18700/27733], Loss: 2.3789\n",
      "Epoch [93/300], Step [18800/27733], Loss: 3.7612\n",
      "Epoch [93/300], Step [18900/27733], Loss: 2.6758\n",
      "Epoch [93/300], Step [19000/27733], Loss: 2.9325\n",
      "Epoch [93/300], Step [19100/27733], Loss: 2.3309\n",
      "Epoch [93/300], Step [19200/27733], Loss: 1.8473\n",
      "Epoch [93/300], Step [19300/27733], Loss: 3.3884\n",
      "Epoch [93/300], Step [19400/27733], Loss: 2.4951\n",
      "Epoch [93/300], Step [19500/27733], Loss: 2.6995\n",
      "Epoch [93/300], Step [19600/27733], Loss: 3.8252\n",
      "Epoch [93/300], Step [19700/27733], Loss: 3.2080\n",
      "Epoch [93/300], Step [19800/27733], Loss: 2.5707\n",
      "Epoch [93/300], Step [19900/27733], Loss: 3.6824\n",
      "Epoch [93/300], Step [20000/27733], Loss: 2.9071\n",
      "Epoch [93/300], Step [20100/27733], Loss: 2.1760\n",
      "Epoch [93/300], Step [20200/27733], Loss: 2.7385\n",
      "Epoch [93/300], Step [20300/27733], Loss: 2.7104\n",
      "Epoch [93/300], Step [20400/27733], Loss: 3.1136\n",
      "Epoch [93/300], Step [20500/27733], Loss: 2.5783\n",
      "Epoch [93/300], Step [20600/27733], Loss: 2.5089\n",
      "Epoch [93/300], Step [20700/27733], Loss: 2.8998\n",
      "Epoch [93/300], Step [20800/27733], Loss: 3.6253\n",
      "Epoch [93/300], Step [20900/27733], Loss: 2.7636\n",
      "Epoch [93/300], Step [21000/27733], Loss: 3.1603\n",
      "Epoch [93/300], Step [21100/27733], Loss: 2.7367\n",
      "Epoch [93/300], Step [21200/27733], Loss: 2.7902\n",
      "Epoch [93/300], Step [21300/27733], Loss: 2.7037\n",
      "Epoch [93/300], Step [21400/27733], Loss: 2.5353\n",
      "Epoch [93/300], Step [21500/27733], Loss: 2.9793\n",
      "Epoch [93/300], Step [21600/27733], Loss: 3.8120\n",
      "Epoch [93/300], Step [21700/27733], Loss: 3.6691\n",
      "Epoch [93/300], Step [21800/27733], Loss: 2.7945\n",
      "Epoch [93/300], Step [21900/27733], Loss: 2.3990\n",
      "Epoch [93/300], Step [22000/27733], Loss: 2.4445\n",
      "Epoch [93/300], Step [22100/27733], Loss: 2.9895\n",
      "Epoch [93/300], Step [22200/27733], Loss: 3.4817\n",
      "Epoch [93/300], Step [22300/27733], Loss: 3.2106\n",
      "Epoch [93/300], Step [22400/27733], Loss: 2.4144\n",
      "Epoch [93/300], Step [22500/27733], Loss: 2.4025\n",
      "Epoch [93/300], Step [22600/27733], Loss: 2.9080\n",
      "Epoch [93/300], Step [22700/27733], Loss: 2.7987\n",
      "Epoch [93/300], Step [22800/27733], Loss: 3.0429\n",
      "Epoch [93/300], Step [22900/27733], Loss: 2.7950\n",
      "Epoch [93/300], Step [23000/27733], Loss: 3.1600\n",
      "Epoch [93/300], Step [23100/27733], Loss: 2.7301\n",
      "Epoch [93/300], Step [23200/27733], Loss: 1.4907\n",
      "Epoch [93/300], Step [23300/27733], Loss: 3.7204\n",
      "Epoch [93/300], Step [23400/27733], Loss: 2.2190\n",
      "Epoch [93/300], Step [23500/27733], Loss: 2.8270\n",
      "Epoch [93/300], Step [23600/27733], Loss: 3.5823\n",
      "Epoch [93/300], Step [23700/27733], Loss: 2.4616\n",
      "Epoch [93/300], Step [23800/27733], Loss: 3.3398\n",
      "Epoch [93/300], Step [23900/27733], Loss: 3.5704\n",
      "Epoch [93/300], Step [24000/27733], Loss: 3.0295\n",
      "Epoch [93/300], Step [24100/27733], Loss: 2.8101\n",
      "Epoch [93/300], Step [24200/27733], Loss: 2.8476\n",
      "Epoch [93/300], Step [24300/27733], Loss: 2.6484\n",
      "Epoch [93/300], Step [24400/27733], Loss: 3.4148\n",
      "Epoch [93/300], Step [24500/27733], Loss: 2.8570\n",
      "Epoch [93/300], Step [24600/27733], Loss: 3.2459\n",
      "Epoch [93/300], Step [24700/27733], Loss: 2.9208\n",
      "Epoch [93/300], Step [24800/27733], Loss: 3.0315\n",
      "Epoch [93/300], Step [24900/27733], Loss: 2.4668\n",
      "Epoch [93/300], Step [25000/27733], Loss: 2.9061\n",
      "Epoch [93/300], Step [25100/27733], Loss: 2.7448\n",
      "Epoch [93/300], Step [25200/27733], Loss: 2.6416\n",
      "Epoch [93/300], Step [25300/27733], Loss: 2.9177\n",
      "Epoch [93/300], Step [25400/27733], Loss: 3.0192\n",
      "Epoch [93/300], Step [25500/27733], Loss: 2.4206\n",
      "Epoch [93/300], Step [25600/27733], Loss: 2.6544\n",
      "Epoch [93/300], Step [25700/27733], Loss: 2.0982\n",
      "Epoch [93/300], Step [25800/27733], Loss: 3.3266\n",
      "Epoch [93/300], Step [25900/27733], Loss: 2.8806\n",
      "Epoch [93/300], Step [26000/27733], Loss: 2.5213\n",
      "Epoch [93/300], Step [26100/27733], Loss: 3.0104\n",
      "Epoch [93/300], Step [26200/27733], Loss: 2.4052\n",
      "Epoch [93/300], Step [26300/27733], Loss: 2.6900\n",
      "Epoch [93/300], Step [26400/27733], Loss: 2.9098\n",
      "Epoch [93/300], Step [26500/27733], Loss: 2.9267\n",
      "Epoch [93/300], Step [26600/27733], Loss: 2.8538\n",
      "Epoch [93/300], Step [26700/27733], Loss: 2.6813\n",
      "Epoch [93/300], Step [26800/27733], Loss: 2.4227\n",
      "Epoch [93/300], Step [26900/27733], Loss: 2.9840\n",
      "Epoch [93/300], Step [27000/27733], Loss: 2.1401\n",
      "Epoch [93/300], Step [27100/27733], Loss: 2.9103\n",
      "Epoch [93/300], Step [27200/27733], Loss: 2.9284\n",
      "Epoch [93/300], Step [27300/27733], Loss: 3.2754\n",
      "Epoch [93/300], Step [27400/27733], Loss: 3.1564\n",
      "Epoch [93/300], Step [27500/27733], Loss: 2.7029\n",
      "Epoch [93/300], Step [27600/27733], Loss: 2.9257\n",
      "Epoch [93/300], Step [27700/27733], Loss: 3.6962\n",
      "Epoch [94/300], Step [100/27733], Loss: 2.2329\n",
      "Epoch [94/300], Step [200/27733], Loss: 3.0443\n",
      "Epoch [94/300], Step [300/27733], Loss: 2.7648\n",
      "Epoch [94/300], Step [400/27733], Loss: 2.3110\n",
      "Epoch [94/300], Step [500/27733], Loss: 2.7001\n",
      "Epoch [94/300], Step [600/27733], Loss: 2.5189\n",
      "Epoch [94/300], Step [700/27733], Loss: 2.3390\n",
      "Epoch [94/300], Step [800/27733], Loss: 2.5618\n",
      "Epoch [94/300], Step [900/27733], Loss: 3.0712\n",
      "Epoch [94/300], Step [1000/27733], Loss: 2.9129\n",
      "Epoch [94/300], Step [1100/27733], Loss: 2.5143\n",
      "Epoch [94/300], Step [1200/27733], Loss: 2.9716\n",
      "Epoch [94/300], Step [1300/27733], Loss: 2.6617\n",
      "Epoch [94/300], Step [1400/27733], Loss: 2.3976\n",
      "Epoch [94/300], Step [1500/27733], Loss: 1.9611\n",
      "Epoch [94/300], Step [1600/27733], Loss: 2.5729\n",
      "Epoch [94/300], Step [1700/27733], Loss: 2.3272\n",
      "Epoch [94/300], Step [1800/27733], Loss: 2.5749\n",
      "Epoch [94/300], Step [1900/27733], Loss: 1.9411\n",
      "Epoch [94/300], Step [2000/27733], Loss: 2.7432\n",
      "Epoch [94/300], Step [2100/27733], Loss: 1.9275\n",
      "Epoch [94/300], Step [2200/27733], Loss: 2.3806\n",
      "Epoch [94/300], Step [2300/27733], Loss: 2.5566\n",
      "Epoch [94/300], Step [2400/27733], Loss: 3.7013\n",
      "Epoch [94/300], Step [2500/27733], Loss: 2.3403\n",
      "Epoch [94/300], Step [2600/27733], Loss: 2.6334\n",
      "Epoch [94/300], Step [2700/27733], Loss: 2.8003\n",
      "Epoch [94/300], Step [2800/27733], Loss: 2.5309\n",
      "Epoch [94/300], Step [2900/27733], Loss: 2.2035\n",
      "Epoch [94/300], Step [3000/27733], Loss: 2.8287\n",
      "Epoch [94/300], Step [3100/27733], Loss: 2.1119\n",
      "Epoch [94/300], Step [3200/27733], Loss: 2.1021\n",
      "Epoch [94/300], Step [3300/27733], Loss: 1.9049\n",
      "Epoch [94/300], Step [3400/27733], Loss: 1.9390\n",
      "Epoch [94/300], Step [3500/27733], Loss: 2.6452\n",
      "Epoch [94/300], Step [3600/27733], Loss: 2.0083\n",
      "Epoch [94/300], Step [3700/27733], Loss: 2.5151\n",
      "Epoch [94/300], Step [3800/27733], Loss: 2.9767\n",
      "Epoch [94/300], Step [3900/27733], Loss: 2.2427\n",
      "Epoch [94/300], Step [4000/27733], Loss: 2.8276\n",
      "Epoch [94/300], Step [4100/27733], Loss: 1.9296\n",
      "Epoch [94/300], Step [4200/27733], Loss: 2.9054\n",
      "Epoch [94/300], Step [4300/27733], Loss: 3.2179\n",
      "Epoch [94/300], Step [4400/27733], Loss: 2.0908\n",
      "Epoch [94/300], Step [4500/27733], Loss: 2.3941\n",
      "Epoch [94/300], Step [4600/27733], Loss: 2.6446\n",
      "Epoch [94/300], Step [4700/27733], Loss: 2.3891\n",
      "Epoch [94/300], Step [4800/27733], Loss: 2.3676\n",
      "Epoch [94/300], Step [4900/27733], Loss: 2.1407\n",
      "Epoch [94/300], Step [5000/27733], Loss: 2.5102\n",
      "Epoch [94/300], Step [5100/27733], Loss: 2.1306\n",
      "Epoch [94/300], Step [5200/27733], Loss: 3.0227\n",
      "Epoch [94/300], Step [5300/27733], Loss: 1.7590\n",
      "Epoch [94/300], Step [5400/27733], Loss: 2.8311\n",
      "Epoch [94/300], Step [5500/27733], Loss: 3.0892\n",
      "Epoch [94/300], Step [5600/27733], Loss: 2.9303\n",
      "Epoch [94/300], Step [5700/27733], Loss: 2.2443\n",
      "Epoch [94/300], Step [5800/27733], Loss: 2.3366\n",
      "Epoch [94/300], Step [5900/27733], Loss: 3.5517\n",
      "Epoch [94/300], Step [6000/27733], Loss: 1.8924\n",
      "Epoch [94/300], Step [6100/27733], Loss: 2.7202\n",
      "Epoch [94/300], Step [6200/27733], Loss: 3.7530\n",
      "Epoch [94/300], Step [6300/27733], Loss: 2.2713\n",
      "Epoch [94/300], Step [6400/27733], Loss: 2.2789\n",
      "Epoch [94/300], Step [6500/27733], Loss: 2.9408\n",
      "Epoch [94/300], Step [6600/27733], Loss: 2.9267\n",
      "Epoch [94/300], Step [6700/27733], Loss: 2.6418\n",
      "Epoch [94/300], Step [6800/27733], Loss: 2.4357\n",
      "Epoch [94/300], Step [6900/27733], Loss: 2.7213\n",
      "Epoch [94/300], Step [7000/27733], Loss: 2.9619\n",
      "Epoch [94/300], Step [7100/27733], Loss: 3.2052\n",
      "Epoch [94/300], Step [7200/27733], Loss: 2.8113\n",
      "Epoch [94/300], Step [7300/27733], Loss: 2.7387\n",
      "Epoch [94/300], Step [7400/27733], Loss: 2.8847\n",
      "Epoch [94/300], Step [7500/27733], Loss: 3.3195\n",
      "Epoch [94/300], Step [7600/27733], Loss: 3.1842\n",
      "Epoch [94/300], Step [7700/27733], Loss: 2.9622\n",
      "Epoch [94/300], Step [7800/27733], Loss: 3.0212\n",
      "Epoch [94/300], Step [7900/27733], Loss: 2.3181\n",
      "Epoch [94/300], Step [8000/27733], Loss: 3.0847\n",
      "Epoch [94/300], Step [8100/27733], Loss: 2.3889\n",
      "Epoch [94/300], Step [8200/27733], Loss: 2.3782\n",
      "Epoch [94/300], Step [8300/27733], Loss: 2.6956\n",
      "Epoch [94/300], Step [8400/27733], Loss: 2.5218\n",
      "Epoch [94/300], Step [8500/27733], Loss: 2.2791\n",
      "Epoch [94/300], Step [8600/27733], Loss: 1.9092\n",
      "Epoch [94/300], Step [8700/27733], Loss: 2.8817\n",
      "Epoch [94/300], Step [8800/27733], Loss: 2.1888\n",
      "Epoch [94/300], Step [8900/27733], Loss: 2.8707\n",
      "Epoch [94/300], Step [9000/27733], Loss: 2.7705\n",
      "Epoch [94/300], Step [9100/27733], Loss: 2.0168\n",
      "Epoch [94/300], Step [9200/27733], Loss: 3.1883\n",
      "Epoch [94/300], Step [9300/27733], Loss: 2.7310\n",
      "Epoch [94/300], Step [9400/27733], Loss: 2.2438\n",
      "Epoch [94/300], Step [9500/27733], Loss: 3.7320\n",
      "Epoch [94/300], Step [9600/27733], Loss: 3.5658\n",
      "Epoch [94/300], Step [9700/27733], Loss: 2.5713\n",
      "Epoch [94/300], Step [9800/27733], Loss: 3.3684\n",
      "Epoch [94/300], Step [9900/27733], Loss: 2.6733\n",
      "Epoch [94/300], Step [10000/27733], Loss: 2.8763\n",
      "Epoch [94/300], Step [10100/27733], Loss: 2.5602\n",
      "Epoch [94/300], Step [10200/27733], Loss: 3.0149\n",
      "Epoch [94/300], Step [10300/27733], Loss: 2.5684\n",
      "Epoch [94/300], Step [10400/27733], Loss: 2.9967\n",
      "Epoch [94/300], Step [10500/27733], Loss: 3.1574\n",
      "Epoch [94/300], Step [10600/27733], Loss: 2.7699\n",
      "Epoch [94/300], Step [10700/27733], Loss: 2.7330\n",
      "Epoch [94/300], Step [10800/27733], Loss: 2.3173\n",
      "Epoch [94/300], Step [10900/27733], Loss: 3.2825\n",
      "Epoch [94/300], Step [11000/27733], Loss: 3.1950\n",
      "Epoch [94/300], Step [11100/27733], Loss: 1.8334\n",
      "Epoch [94/300], Step [11200/27733], Loss: 2.4746\n",
      "Epoch [94/300], Step [11300/27733], Loss: 3.2562\n",
      "Epoch [94/300], Step [11400/27733], Loss: 2.3022\n",
      "Epoch [94/300], Step [11500/27733], Loss: 2.6684\n",
      "Epoch [94/300], Step [11600/27733], Loss: 3.5478\n",
      "Epoch [94/300], Step [11700/27733], Loss: 2.5561\n",
      "Epoch [94/300], Step [11800/27733], Loss: 2.9136\n",
      "Epoch [94/300], Step [11900/27733], Loss: 2.6329\n",
      "Epoch [94/300], Step [12000/27733], Loss: 3.0348\n",
      "Epoch [94/300], Step [12100/27733], Loss: 3.3258\n",
      "Epoch [94/300], Step [12200/27733], Loss: 4.4578\n",
      "Epoch [94/300], Step [12300/27733], Loss: 2.4213\n",
      "Epoch [94/300], Step [12400/27733], Loss: 2.2437\n",
      "Epoch [94/300], Step [12500/27733], Loss: 2.7967\n",
      "Epoch [94/300], Step [12600/27733], Loss: 3.2348\n",
      "Epoch [94/300], Step [12700/27733], Loss: 2.9013\n",
      "Epoch [94/300], Step [12800/27733], Loss: 3.1069\n",
      "Epoch [94/300], Step [12900/27733], Loss: 2.7881\n",
      "Epoch [94/300], Step [13000/27733], Loss: 2.7760\n",
      "Epoch [94/300], Step [13100/27733], Loss: 3.0992\n",
      "Epoch [94/300], Step [13200/27733], Loss: 2.9337\n",
      "Epoch [94/300], Step [13300/27733], Loss: 2.7409\n",
      "Epoch [94/300], Step [13400/27733], Loss: 2.8065\n",
      "Epoch [94/300], Step [13500/27733], Loss: 3.7976\n",
      "Epoch [94/300], Step [13600/27733], Loss: 3.2682\n",
      "Epoch [94/300], Step [13700/27733], Loss: 2.1533\n",
      "Epoch [94/300], Step [13800/27733], Loss: 3.3038\n",
      "Epoch [94/300], Step [13900/27733], Loss: 3.1960\n",
      "Epoch [94/300], Step [14000/27733], Loss: 3.0491\n",
      "Epoch [94/300], Step [14100/27733], Loss: 2.1036\n",
      "Epoch [94/300], Step [14200/27733], Loss: 2.7430\n",
      "Epoch [94/300], Step [14300/27733], Loss: 2.6902\n",
      "Epoch [94/300], Step [14400/27733], Loss: 2.8515\n",
      "Epoch [94/300], Step [14500/27733], Loss: 2.2083\n",
      "Epoch [94/300], Step [14600/27733], Loss: 3.5373\n",
      "Epoch [94/300], Step [14700/27733], Loss: 2.6972\n",
      "Epoch [94/300], Step [14800/27733], Loss: 3.3330\n",
      "Epoch [94/300], Step [14900/27733], Loss: 3.2159\n",
      "Epoch [94/300], Step [15000/27733], Loss: 2.9805\n",
      "Epoch [94/300], Step [15100/27733], Loss: 2.0717\n",
      "Epoch [94/300], Step [15200/27733], Loss: 3.4748\n",
      "Epoch [94/300], Step [15300/27733], Loss: 2.5462\n",
      "Epoch [94/300], Step [15400/27733], Loss: 3.5376\n",
      "Epoch [94/300], Step [15500/27733], Loss: 3.1676\n",
      "Epoch [94/300], Step [15600/27733], Loss: 2.9993\n",
      "Epoch [94/300], Step [15700/27733], Loss: 2.3873\n",
      "Epoch [94/300], Step [15800/27733], Loss: 3.6768\n",
      "Epoch [94/300], Step [15900/27733], Loss: 2.8815\n",
      "Epoch [94/300], Step [16000/27733], Loss: 2.3006\n",
      "Epoch [94/300], Step [16100/27733], Loss: 2.5740\n",
      "Epoch [94/300], Step [16200/27733], Loss: 3.3069\n",
      "Epoch [94/300], Step [16300/27733], Loss: 2.4268\n",
      "Epoch [94/300], Step [16400/27733], Loss: 3.0715\n",
      "Epoch [94/300], Step [16500/27733], Loss: 2.7301\n",
      "Epoch [94/300], Step [16600/27733], Loss: 2.4380\n",
      "Epoch [94/300], Step [16700/27733], Loss: 2.8142\n",
      "Epoch [94/300], Step [16800/27733], Loss: 2.9337\n",
      "Epoch [94/300], Step [16900/27733], Loss: 1.9472\n",
      "Epoch [94/300], Step [17000/27733], Loss: 2.5729\n",
      "Epoch [94/300], Step [17100/27733], Loss: 2.3419\n",
      "Epoch [94/300], Step [17200/27733], Loss: 3.2867\n",
      "Epoch [94/300], Step [17300/27733], Loss: 2.9987\n",
      "Epoch [94/300], Step [17400/27733], Loss: 3.0377\n",
      "Epoch [94/300], Step [17500/27733], Loss: 3.0012\n",
      "Epoch [94/300], Step [17600/27733], Loss: 3.4292\n",
      "Epoch [94/300], Step [17700/27733], Loss: 2.8631\n",
      "Epoch [94/300], Step [17800/27733], Loss: 2.7965\n",
      "Epoch [94/300], Step [17900/27733], Loss: 3.1217\n",
      "Epoch [94/300], Step [18000/27733], Loss: 2.6790\n",
      "Epoch [94/300], Step [18100/27733], Loss: 3.5203\n",
      "Epoch [94/300], Step [18200/27733], Loss: 2.6881\n",
      "Epoch [94/300], Step [18300/27733], Loss: 2.4745\n",
      "Epoch [94/300], Step [18400/27733], Loss: 2.9859\n",
      "Epoch [94/300], Step [18500/27733], Loss: 1.9501\n",
      "Epoch [94/300], Step [18600/27733], Loss: 2.5339\n",
      "Epoch [94/300], Step [18700/27733], Loss: 3.3051\n",
      "Epoch [94/300], Step [18800/27733], Loss: 3.0667\n",
      "Epoch [94/300], Step [18900/27733], Loss: 2.4066\n",
      "Epoch [94/300], Step [19000/27733], Loss: 2.8546\n",
      "Epoch [94/300], Step [19100/27733], Loss: 2.5351\n",
      "Epoch [94/300], Step [19200/27733], Loss: 3.1973\n",
      "Epoch [94/300], Step [19300/27733], Loss: 2.1865\n",
      "Epoch [94/300], Step [19400/27733], Loss: 2.5854\n",
      "Epoch [94/300], Step [19500/27733], Loss: 2.2936\n",
      "Epoch [94/300], Step [19600/27733], Loss: 2.6408\n",
      "Epoch [94/300], Step [19700/27733], Loss: 3.1312\n",
      "Epoch [94/300], Step [19800/27733], Loss: 2.5853\n",
      "Epoch [94/300], Step [19900/27733], Loss: 2.7569\n",
      "Epoch [94/300], Step [20000/27733], Loss: 2.9909\n",
      "Epoch [94/300], Step [20100/27733], Loss: 3.1893\n",
      "Epoch [94/300], Step [20200/27733], Loss: 2.7258\n",
      "Epoch [94/300], Step [20300/27733], Loss: 3.5556\n",
      "Epoch [94/300], Step [20400/27733], Loss: 2.5675\n",
      "Epoch [94/300], Step [20500/27733], Loss: 2.1208\n",
      "Epoch [94/300], Step [20600/27733], Loss: 2.7664\n",
      "Epoch [94/300], Step [20700/27733], Loss: 2.5081\n",
      "Epoch [94/300], Step [20800/27733], Loss: 2.4315\n",
      "Epoch [94/300], Step [20900/27733], Loss: 3.3702\n",
      "Epoch [94/300], Step [21000/27733], Loss: 2.2837\n",
      "Epoch [94/300], Step [21100/27733], Loss: 2.3894\n",
      "Epoch [94/300], Step [21200/27733], Loss: 2.6569\n",
      "Epoch [94/300], Step [21300/27733], Loss: 2.9053\n",
      "Epoch [94/300], Step [21400/27733], Loss: 2.1939\n",
      "Epoch [94/300], Step [21500/27733], Loss: 2.3037\n",
      "Epoch [94/300], Step [21600/27733], Loss: 2.6523\n",
      "Epoch [94/300], Step [21700/27733], Loss: 2.8141\n",
      "Epoch [94/300], Step [21800/27733], Loss: 3.0671\n",
      "Epoch [94/300], Step [21900/27733], Loss: 2.2481\n",
      "Epoch [94/300], Step [22000/27733], Loss: 3.6866\n",
      "Epoch [94/300], Step [22100/27733], Loss: 3.1119\n",
      "Epoch [94/300], Step [22200/27733], Loss: 2.6814\n",
      "Epoch [94/300], Step [22300/27733], Loss: 3.3889\n",
      "Epoch [94/300], Step [22400/27733], Loss: 2.7352\n",
      "Epoch [94/300], Step [22500/27733], Loss: 3.1290\n",
      "Epoch [94/300], Step [22600/27733], Loss: 3.2930\n",
      "Epoch [94/300], Step [22700/27733], Loss: 1.8807\n",
      "Epoch [94/300], Step [22800/27733], Loss: 4.0074\n",
      "Epoch [94/300], Step [22900/27733], Loss: 3.6260\n",
      "Epoch [94/300], Step [23000/27733], Loss: 3.0918\n",
      "Epoch [94/300], Step [23100/27733], Loss: 2.2090\n",
      "Epoch [94/300], Step [23200/27733], Loss: 3.4980\n",
      "Epoch [94/300], Step [23300/27733], Loss: 2.8807\n",
      "Epoch [94/300], Step [23400/27733], Loss: 3.3679\n",
      "Epoch [94/300], Step [23500/27733], Loss: 2.7654\n",
      "Epoch [94/300], Step [23600/27733], Loss: 3.8308\n",
      "Epoch [94/300], Step [23700/27733], Loss: 4.2750\n",
      "Epoch [94/300], Step [23800/27733], Loss: 2.8494\n",
      "Epoch [94/300], Step [23900/27733], Loss: 2.1064\n",
      "Epoch [94/300], Step [24000/27733], Loss: 2.3858\n",
      "Epoch [94/300], Step [24100/27733], Loss: 2.9524\n",
      "Epoch [94/300], Step [24200/27733], Loss: 1.9488\n",
      "Epoch [94/300], Step [24300/27733], Loss: 2.9403\n",
      "Epoch [94/300], Step [24400/27733], Loss: 3.2896\n",
      "Epoch [94/300], Step [24500/27733], Loss: 2.9603\n",
      "Epoch [94/300], Step [24600/27733], Loss: 3.0507\n",
      "Epoch [94/300], Step [24700/27733], Loss: 3.6838\n",
      "Epoch [94/300], Step [24800/27733], Loss: 3.0862\n",
      "Epoch [94/300], Step [24900/27733], Loss: 2.3812\n",
      "Epoch [94/300], Step [25000/27733], Loss: 2.8607\n",
      "Epoch [94/300], Step [25100/27733], Loss: 2.3866\n",
      "Epoch [94/300], Step [25200/27733], Loss: 3.6256\n",
      "Epoch [94/300], Step [25300/27733], Loss: 3.0963\n",
      "Epoch [94/300], Step [25400/27733], Loss: 2.6479\n",
      "Epoch [94/300], Step [25500/27733], Loss: 3.4948\n",
      "Epoch [94/300], Step [25600/27733], Loss: 3.7954\n",
      "Epoch [94/300], Step [25700/27733], Loss: 2.7969\n",
      "Epoch [94/300], Step [25800/27733], Loss: 3.4522\n",
      "Epoch [94/300], Step [25900/27733], Loss: 3.1071\n",
      "Epoch [94/300], Step [26000/27733], Loss: 2.1578\n",
      "Epoch [94/300], Step [26100/27733], Loss: 2.2432\n",
      "Epoch [94/300], Step [26200/27733], Loss: 3.2509\n",
      "Epoch [94/300], Step [26300/27733], Loss: 2.6728\n",
      "Epoch [94/300], Step [26400/27733], Loss: 3.4630\n",
      "Epoch [94/300], Step [26500/27733], Loss: 2.5667\n",
      "Epoch [94/300], Step [26600/27733], Loss: 3.0682\n",
      "Epoch [94/300], Step [26700/27733], Loss: 2.6355\n",
      "Epoch [94/300], Step [26800/27733], Loss: 2.6340\n",
      "Epoch [94/300], Step [26900/27733], Loss: 2.5279\n",
      "Epoch [94/300], Step [27000/27733], Loss: 3.5342\n",
      "Epoch [94/300], Step [27100/27733], Loss: 2.5499\n",
      "Epoch [94/300], Step [27200/27733], Loss: 3.6597\n",
      "Epoch [94/300], Step [27300/27733], Loss: 2.9541\n",
      "Epoch [94/300], Step [27400/27733], Loss: 2.9266\n",
      "Epoch [94/300], Step [27500/27733], Loss: 3.0469\n",
      "Epoch [94/300], Step [27600/27733], Loss: 3.8161\n",
      "Epoch [94/300], Step [27700/27733], Loss: 2.5991\n",
      "Epoch [95/300], Step [100/27733], Loss: 1.9200\n",
      "Epoch [95/300], Step [200/27733], Loss: 2.7179\n",
      "Epoch [95/300], Step [300/27733], Loss: 2.4393\n",
      "Epoch [95/300], Step [400/27733], Loss: 2.0371\n",
      "Epoch [95/300], Step [500/27733], Loss: 2.6980\n",
      "Epoch [95/300], Step [600/27733], Loss: 2.8918\n",
      "Epoch [95/300], Step [700/27733], Loss: 2.3581\n",
      "Epoch [95/300], Step [800/27733], Loss: 2.1048\n",
      "Epoch [95/300], Step [900/27733], Loss: 2.0791\n",
      "Epoch [95/300], Step [1000/27733], Loss: 2.0265\n",
      "Epoch [95/300], Step [1100/27733], Loss: 2.3594\n",
      "Epoch [95/300], Step [1200/27733], Loss: 2.0569\n",
      "Epoch [95/300], Step [1300/27733], Loss: 2.1251\n",
      "Epoch [95/300], Step [1400/27733], Loss: 2.6713\n",
      "Epoch [95/300], Step [1500/27733], Loss: 2.3551\n",
      "Epoch [95/300], Step [1600/27733], Loss: 2.4061\n",
      "Epoch [95/300], Step [1700/27733], Loss: 2.1075\n",
      "Epoch [95/300], Step [1800/27733], Loss: 2.3758\n",
      "Epoch [95/300], Step [1900/27733], Loss: 2.4726\n",
      "Epoch [95/300], Step [2000/27733], Loss: 1.8931\n",
      "Epoch [95/300], Step [2100/27733], Loss: 1.6188\n",
      "Epoch [95/300], Step [2200/27733], Loss: 2.8832\n",
      "Epoch [95/300], Step [2300/27733], Loss: 2.4398\n",
      "Epoch [95/300], Step [2400/27733], Loss: 2.3418\n",
      "Epoch [95/300], Step [2500/27733], Loss: 3.1151\n",
      "Epoch [95/300], Step [2600/27733], Loss: 2.8812\n",
      "Epoch [95/300], Step [2700/27733], Loss: 1.8203\n",
      "Epoch [95/300], Step [2800/27733], Loss: 2.1798\n",
      "Epoch [95/300], Step [2900/27733], Loss: 2.0340\n",
      "Epoch [95/300], Step [3000/27733], Loss: 2.2345\n",
      "Epoch [95/300], Step [3100/27733], Loss: 2.5890\n",
      "Epoch [95/300], Step [3200/27733], Loss: 2.8273\n",
      "Epoch [95/300], Step [3300/27733], Loss: 3.5700\n",
      "Epoch [95/300], Step [3400/27733], Loss: 1.9750\n",
      "Epoch [95/300], Step [3500/27733], Loss: 2.4712\n",
      "Epoch [95/300], Step [3600/27733], Loss: 2.4733\n",
      "Epoch [95/300], Step [3700/27733], Loss: 1.8750\n",
      "Epoch [95/300], Step [3800/27733], Loss: 2.5276\n",
      "Epoch [95/300], Step [3900/27733], Loss: 3.1307\n",
      "Epoch [95/300], Step [4000/27733], Loss: 2.6991\n",
      "Epoch [95/300], Step [4100/27733], Loss: 3.5488\n",
      "Epoch [95/300], Step [4200/27733], Loss: 2.0651\n",
      "Epoch [95/300], Step [4300/27733], Loss: 2.5331\n",
      "Epoch [95/300], Step [4400/27733], Loss: 2.3162\n",
      "Epoch [95/300], Step [4500/27733], Loss: 2.3222\n",
      "Epoch [95/300], Step [4600/27733], Loss: 2.6026\n",
      "Epoch [95/300], Step [4700/27733], Loss: 1.6758\n",
      "Epoch [95/300], Step [4800/27733], Loss: 2.4892\n",
      "Epoch [95/300], Step [4900/27733], Loss: 3.0108\n",
      "Epoch [95/300], Step [5000/27733], Loss: 2.1379\n",
      "Epoch [95/300], Step [5100/27733], Loss: 2.2528\n",
      "Epoch [95/300], Step [5200/27733], Loss: 1.9164\n",
      "Epoch [95/300], Step [5300/27733], Loss: 2.4557\n",
      "Epoch [95/300], Step [5400/27733], Loss: 2.1721\n",
      "Epoch [95/300], Step [5500/27733], Loss: 2.8481\n",
      "Epoch [95/300], Step [5600/27733], Loss: 3.0908\n",
      "Epoch [95/300], Step [5700/27733], Loss: 2.2488\n",
      "Epoch [95/300], Step [5800/27733], Loss: 2.2591\n",
      "Epoch [95/300], Step [5900/27733], Loss: 2.5188\n",
      "Epoch [95/300], Step [6000/27733], Loss: 2.4776\n",
      "Epoch [95/300], Step [6100/27733], Loss: 2.3577\n",
      "Epoch [95/300], Step [6200/27733], Loss: 3.3489\n",
      "Epoch [95/300], Step [6300/27733], Loss: 1.9263\n",
      "Epoch [95/300], Step [6400/27733], Loss: 2.1007\n",
      "Epoch [95/300], Step [6500/27733], Loss: 2.1651\n",
      "Epoch [95/300], Step [6600/27733], Loss: 2.4222\n",
      "Epoch [95/300], Step [6700/27733], Loss: 2.8988\n",
      "Epoch [95/300], Step [6800/27733], Loss: 2.0608\n",
      "Epoch [95/300], Step [6900/27733], Loss: 2.0708\n",
      "Epoch [95/300], Step [7000/27733], Loss: 2.6191\n",
      "Epoch [95/300], Step [7100/27733], Loss: 2.3196\n",
      "Epoch [95/300], Step [7200/27733], Loss: 2.5784\n",
      "Epoch [95/300], Step [7300/27733], Loss: 2.9604\n",
      "Epoch [95/300], Step [7400/27733], Loss: 2.4224\n",
      "Epoch [95/300], Step [7500/27733], Loss: 4.3979\n",
      "Epoch [95/300], Step [7600/27733], Loss: 2.5741\n",
      "Epoch [95/300], Step [7700/27733], Loss: 2.7002\n",
      "Epoch [95/300], Step [7800/27733], Loss: 3.1960\n",
      "Epoch [95/300], Step [7900/27733], Loss: 3.4285\n",
      "Epoch [95/300], Step [8000/27733], Loss: 2.4144\n",
      "Epoch [95/300], Step [8100/27733], Loss: 2.8783\n",
      "Epoch [95/300], Step [8200/27733], Loss: 2.8645\n",
      "Epoch [95/300], Step [8300/27733], Loss: 2.3946\n",
      "Epoch [95/300], Step [8400/27733], Loss: 1.8400\n",
      "Epoch [95/300], Step [8500/27733], Loss: 2.8034\n",
      "Epoch [95/300], Step [8600/27733], Loss: 3.4495\n",
      "Epoch [95/300], Step [8700/27733], Loss: 1.8522\n",
      "Epoch [95/300], Step [8800/27733], Loss: 2.7268\n",
      "Epoch [95/300], Step [8900/27733], Loss: 2.4207\n",
      "Epoch [95/300], Step [9000/27733], Loss: 2.3220\n",
      "Epoch [95/300], Step [9100/27733], Loss: 2.7446\n",
      "Epoch [95/300], Step [9200/27733], Loss: 2.0828\n",
      "Epoch [95/300], Step [9300/27733], Loss: 2.5282\n",
      "Epoch [95/300], Step [9400/27733], Loss: 2.6682\n",
      "Epoch [95/300], Step [9500/27733], Loss: 2.6500\n",
      "Epoch [95/300], Step [9600/27733], Loss: 2.3441\n",
      "Epoch [95/300], Step [9700/27733], Loss: 2.8789\n",
      "Epoch [95/300], Step [9800/27733], Loss: 2.2003\n",
      "Epoch [95/300], Step [9900/27733], Loss: 2.8069\n",
      "Epoch [95/300], Step [10000/27733], Loss: 2.9571\n",
      "Epoch [95/300], Step [10100/27733], Loss: 2.3027\n",
      "Epoch [95/300], Step [10200/27733], Loss: 2.2537\n",
      "Epoch [95/300], Step [10300/27733], Loss: 2.0900\n",
      "Epoch [95/300], Step [10400/27733], Loss: 1.8486\n",
      "Epoch [95/300], Step [10500/27733], Loss: 1.5140\n",
      "Epoch [95/300], Step [10600/27733], Loss: 2.9551\n",
      "Epoch [95/300], Step [10700/27733], Loss: 3.0502\n",
      "Epoch [95/300], Step [10800/27733], Loss: 3.4140\n",
      "Epoch [95/300], Step [10900/27733], Loss: 2.6954\n",
      "Epoch [95/300], Step [11000/27733], Loss: 2.8110\n",
      "Epoch [95/300], Step [11100/27733], Loss: 2.3185\n",
      "Epoch [95/300], Step [11200/27733], Loss: 3.3841\n",
      "Epoch [95/300], Step [11300/27733], Loss: 2.8720\n",
      "Epoch [95/300], Step [11400/27733], Loss: 2.8335\n",
      "Epoch [95/300], Step [11500/27733], Loss: 2.8992\n",
      "Epoch [95/300], Step [11600/27733], Loss: 2.2016\n",
      "Epoch [95/300], Step [11700/27733], Loss: 2.4024\n",
      "Epoch [95/300], Step [11800/27733], Loss: 2.9480\n",
      "Epoch [95/300], Step [11900/27733], Loss: 2.8133\n",
      "Epoch [95/300], Step [12000/27733], Loss: 2.3324\n",
      "Epoch [95/300], Step [12100/27733], Loss: 3.2602\n",
      "Epoch [95/300], Step [12200/27733], Loss: 3.0277\n",
      "Epoch [95/300], Step [12300/27733], Loss: 2.8593\n",
      "Epoch [95/300], Step [12400/27733], Loss: 2.4566\n",
      "Epoch [95/300], Step [12500/27733], Loss: 3.3696\n",
      "Epoch [95/300], Step [12600/27733], Loss: 3.7009\n",
      "Epoch [95/300], Step [12700/27733], Loss: 2.7809\n",
      "Epoch [95/300], Step [12800/27733], Loss: 3.2604\n",
      "Epoch [95/300], Step [12900/27733], Loss: 2.9625\n",
      "Epoch [95/300], Step [13000/27733], Loss: 3.0654\n",
      "Epoch [95/300], Step [13100/27733], Loss: 2.2087\n",
      "Epoch [95/300], Step [13200/27733], Loss: 2.8770\n",
      "Epoch [95/300], Step [13300/27733], Loss: 3.3967\n",
      "Epoch [95/300], Step [13400/27733], Loss: 3.2586\n",
      "Epoch [95/300], Step [13500/27733], Loss: 3.7763\n",
      "Epoch [95/300], Step [13600/27733], Loss: 2.8989\n",
      "Epoch [95/300], Step [13700/27733], Loss: 3.2467\n",
      "Epoch [95/300], Step [13800/27733], Loss: 3.4657\n",
      "Epoch [95/300], Step [13900/27733], Loss: 2.5965\n",
      "Epoch [95/300], Step [14000/27733], Loss: 2.6959\n",
      "Epoch [95/300], Step [14100/27733], Loss: 3.2820\n",
      "Epoch [95/300], Step [14200/27733], Loss: 2.9300\n",
      "Epoch [95/300], Step [14300/27733], Loss: 3.1968\n",
      "Epoch [95/300], Step [14400/27733], Loss: 3.7576\n",
      "Epoch [95/300], Step [14500/27733], Loss: 2.2420\n",
      "Epoch [95/300], Step [14600/27733], Loss: 2.7504\n",
      "Epoch [95/300], Step [14700/27733], Loss: 2.8947\n",
      "Epoch [95/300], Step [14800/27733], Loss: 2.5325\n",
      "Epoch [95/300], Step [14900/27733], Loss: 2.4392\n",
      "Epoch [95/300], Step [15000/27733], Loss: 3.0836\n",
      "Epoch [95/300], Step [15100/27733], Loss: 2.6074\n",
      "Epoch [95/300], Step [15200/27733], Loss: 2.6810\n",
      "Epoch [95/300], Step [15300/27733], Loss: 2.7464\n",
      "Epoch [95/300], Step [15400/27733], Loss: 2.9348\n",
      "Epoch [95/300], Step [15500/27733], Loss: 1.8578\n",
      "Epoch [95/300], Step [15600/27733], Loss: 2.3197\n",
      "Epoch [95/300], Step [15700/27733], Loss: 1.9253\n",
      "Epoch [95/300], Step [15800/27733], Loss: 2.7119\n",
      "Epoch [95/300], Step [15900/27733], Loss: 3.2787\n",
      "Epoch [95/300], Step [16000/27733], Loss: 2.4622\n",
      "Epoch [95/300], Step [16100/27733], Loss: 2.9604\n",
      "Epoch [95/300], Step [16200/27733], Loss: 2.6374\n",
      "Epoch [95/300], Step [16300/27733], Loss: 2.5161\n",
      "Epoch [95/300], Step [16400/27733], Loss: 2.4584\n",
      "Epoch [95/300], Step [16500/27733], Loss: 3.0564\n",
      "Epoch [95/300], Step [16600/27733], Loss: 2.6171\n",
      "Epoch [95/300], Step [16700/27733], Loss: 3.6708\n",
      "Epoch [95/300], Step [16800/27733], Loss: 3.1569\n",
      "Epoch [95/300], Step [16900/27733], Loss: 2.5060\n",
      "Epoch [95/300], Step [17000/27733], Loss: 3.6806\n",
      "Epoch [95/300], Step [17100/27733], Loss: 2.6946\n",
      "Epoch [95/300], Step [17200/27733], Loss: 2.7204\n",
      "Epoch [95/300], Step [17300/27733], Loss: 3.4797\n",
      "Epoch [95/300], Step [17400/27733], Loss: 2.9258\n",
      "Epoch [95/300], Step [17500/27733], Loss: 3.1548\n",
      "Epoch [95/300], Step [17600/27733], Loss: 3.0176\n",
      "Epoch [95/300], Step [17700/27733], Loss: 2.8306\n",
      "Epoch [95/300], Step [17800/27733], Loss: 3.3376\n",
      "Epoch [95/300], Step [17900/27733], Loss: 2.5441\n",
      "Epoch [95/300], Step [18000/27733], Loss: 2.7503\n",
      "Epoch [95/300], Step [18100/27733], Loss: 2.9999\n",
      "Epoch [95/300], Step [18200/27733], Loss: 2.3508\n",
      "Epoch [95/300], Step [18300/27733], Loss: 3.8838\n",
      "Epoch [95/300], Step [18400/27733], Loss: 4.0943\n",
      "Epoch [95/300], Step [18500/27733], Loss: 2.4784\n",
      "Epoch [95/300], Step [18600/27733], Loss: 2.8711\n",
      "Epoch [95/300], Step [18700/27733], Loss: 3.1042\n",
      "Epoch [95/300], Step [18800/27733], Loss: 2.9373\n",
      "Epoch [95/300], Step [18900/27733], Loss: 2.6716\n",
      "Epoch [95/300], Step [19000/27733], Loss: 3.0990\n",
      "Epoch [95/300], Step [19100/27733], Loss: 2.8028\n",
      "Epoch [95/300], Step [19200/27733], Loss: 2.8470\n",
      "Epoch [95/300], Step [19300/27733], Loss: 3.1421\n",
      "Epoch [95/300], Step [19400/27733], Loss: 2.5919\n",
      "Epoch [95/300], Step [19500/27733], Loss: 2.0438\n",
      "Epoch [95/300], Step [19600/27733], Loss: 2.9532\n",
      "Epoch [95/300], Step [19700/27733], Loss: 3.2357\n",
      "Epoch [95/300], Step [19800/27733], Loss: 2.0659\n",
      "Epoch [95/300], Step [19900/27733], Loss: 3.2968\n",
      "Epoch [95/300], Step [20000/27733], Loss: 2.0788\n",
      "Epoch [95/300], Step [20100/27733], Loss: 2.7217\n",
      "Epoch [95/300], Step [20200/27733], Loss: 2.6111\n",
      "Epoch [95/300], Step [20300/27733], Loss: 2.8918\n",
      "Epoch [95/300], Step [20400/27733], Loss: 3.6049\n",
      "Epoch [95/300], Step [20500/27733], Loss: 3.0696\n",
      "Epoch [95/300], Step [20600/27733], Loss: 2.6575\n",
      "Epoch [95/300], Step [20700/27733], Loss: 2.7105\n",
      "Epoch [95/300], Step [20800/27733], Loss: 3.2415\n",
      "Epoch [95/300], Step [20900/27733], Loss: 2.3125\n",
      "Epoch [95/300], Step [21000/27733], Loss: 2.2402\n",
      "Epoch [95/300], Step [21100/27733], Loss: 2.1340\n",
      "Epoch [95/300], Step [21200/27733], Loss: 3.0669\n",
      "Epoch [95/300], Step [21300/27733], Loss: 2.2419\n",
      "Epoch [95/300], Step [21400/27733], Loss: 2.4484\n",
      "Epoch [95/300], Step [21500/27733], Loss: 2.1792\n",
      "Epoch [95/300], Step [21600/27733], Loss: 3.2785\n",
      "Epoch [95/300], Step [21700/27733], Loss: 2.5941\n",
      "Epoch [95/300], Step [21800/27733], Loss: 2.6449\n",
      "Epoch [95/300], Step [21900/27733], Loss: 3.3128\n",
      "Epoch [95/300], Step [22000/27733], Loss: 3.2521\n",
      "Epoch [95/300], Step [22100/27733], Loss: 2.6247\n",
      "Epoch [95/300], Step [22200/27733], Loss: 3.0865\n",
      "Epoch [95/300], Step [22300/27733], Loss: 2.7190\n",
      "Epoch [95/300], Step [22400/27733], Loss: 3.3353\n",
      "Epoch [95/300], Step [22500/27733], Loss: 2.4601\n",
      "Epoch [95/300], Step [22600/27733], Loss: 2.9825\n",
      "Epoch [95/300], Step [22700/27733], Loss: 2.6703\n",
      "Epoch [95/300], Step [22800/27733], Loss: 2.5033\n",
      "Epoch [95/300], Step [22900/27733], Loss: 3.8673\n",
      "Epoch [95/300], Step [23000/27733], Loss: 2.8568\n",
      "Epoch [95/300], Step [23100/27733], Loss: 2.4219\n",
      "Epoch [95/300], Step [23200/27733], Loss: 2.6471\n",
      "Epoch [95/300], Step [23300/27733], Loss: 3.4396\n",
      "Epoch [95/300], Step [23400/27733], Loss: 2.6811\n",
      "Epoch [95/300], Step [23500/27733], Loss: 2.6368\n",
      "Epoch [95/300], Step [23600/27733], Loss: 2.8186\n",
      "Epoch [95/300], Step [23700/27733], Loss: 3.0010\n",
      "Epoch [95/300], Step [23800/27733], Loss: 3.2769\n",
      "Epoch [95/300], Step [23900/27733], Loss: 3.0882\n",
      "Epoch [95/300], Step [24000/27733], Loss: 3.2173\n",
      "Epoch [95/300], Step [24100/27733], Loss: 2.6348\n",
      "Epoch [95/300], Step [24200/27733], Loss: 2.2834\n",
      "Epoch [95/300], Step [24300/27733], Loss: 3.2282\n",
      "Epoch [95/300], Step [24400/27733], Loss: 2.7508\n",
      "Epoch [95/300], Step [24500/27733], Loss: 3.1447\n",
      "Epoch [95/300], Step [24600/27733], Loss: 3.2257\n",
      "Epoch [95/300], Step [24700/27733], Loss: 2.7139\n",
      "Epoch [95/300], Step [24800/27733], Loss: 3.2394\n",
      "Epoch [95/300], Step [24900/27733], Loss: 2.4973\n",
      "Epoch [95/300], Step [25000/27733], Loss: 3.8492\n",
      "Epoch [95/300], Step [25100/27733], Loss: 3.3468\n",
      "Epoch [95/300], Step [25200/27733], Loss: 2.9896\n",
      "Epoch [95/300], Step [25300/27733], Loss: 3.0229\n",
      "Epoch [95/300], Step [25400/27733], Loss: 2.6570\n",
      "Epoch [95/300], Step [25500/27733], Loss: 2.9619\n",
      "Epoch [95/300], Step [25600/27733], Loss: 3.0370\n",
      "Epoch [95/300], Step [25700/27733], Loss: 2.4377\n",
      "Epoch [95/300], Step [25800/27733], Loss: 2.5227\n",
      "Epoch [95/300], Step [25900/27733], Loss: 3.1144\n",
      "Epoch [95/300], Step [26000/27733], Loss: 2.5497\n",
      "Epoch [95/300], Step [26100/27733], Loss: 2.8305\n",
      "Epoch [95/300], Step [26200/27733], Loss: 3.8873\n",
      "Epoch [95/300], Step [26300/27733], Loss: 4.2592\n",
      "Epoch [95/300], Step [26400/27733], Loss: 2.1568\n",
      "Epoch [95/300], Step [26500/27733], Loss: 2.2092\n",
      "Epoch [95/300], Step [26600/27733], Loss: 2.7453\n",
      "Epoch [95/300], Step [26700/27733], Loss: 3.3064\n",
      "Epoch [95/300], Step [26800/27733], Loss: 3.4141\n",
      "Epoch [95/300], Step [26900/27733], Loss: 3.7014\n",
      "Epoch [95/300], Step [27000/27733], Loss: 2.8893\n",
      "Epoch [95/300], Step [27100/27733], Loss: 2.4619\n",
      "Epoch [95/300], Step [27200/27733], Loss: 3.1810\n",
      "Epoch [95/300], Step [27300/27733], Loss: 2.8246\n",
      "Epoch [95/300], Step [27400/27733], Loss: 3.3928\n",
      "Epoch [95/300], Step [27500/27733], Loss: 2.9034\n",
      "Epoch [95/300], Step [27600/27733], Loss: 3.2265\n",
      "Epoch [95/300], Step [27700/27733], Loss: 3.3482\n",
      "Epoch [96/300], Step [100/27733], Loss: 2.0561\n",
      "Epoch [96/300], Step [200/27733], Loss: 2.1875\n",
      "Epoch [96/300], Step [300/27733], Loss: 2.1046\n",
      "Epoch [96/300], Step [400/27733], Loss: 2.0673\n",
      "Epoch [96/300], Step [500/27733], Loss: 2.3720\n",
      "Epoch [96/300], Step [600/27733], Loss: 3.4273\n",
      "Epoch [96/300], Step [700/27733], Loss: 1.9843\n",
      "Epoch [96/300], Step [800/27733], Loss: 2.7498\n",
      "Epoch [96/300], Step [900/27733], Loss: 2.8256\n",
      "Epoch [96/300], Step [1000/27733], Loss: 2.9128\n",
      "Epoch [96/300], Step [1100/27733], Loss: 2.2745\n",
      "Epoch [96/300], Step [1200/27733], Loss: 2.4020\n",
      "Epoch [96/300], Step [1300/27733], Loss: 2.3953\n",
      "Epoch [96/300], Step [1400/27733], Loss: 2.3582\n",
      "Epoch [96/300], Step [1500/27733], Loss: 2.1758\n",
      "Epoch [96/300], Step [1600/27733], Loss: 3.6063\n",
      "Epoch [96/300], Step [1700/27733], Loss: 2.2907\n",
      "Epoch [96/300], Step [1800/27733], Loss: 2.9252\n",
      "Epoch [96/300], Step [1900/27733], Loss: 2.8984\n",
      "Epoch [96/300], Step [2000/27733], Loss: 3.1343\n",
      "Epoch [96/300], Step [2100/27733], Loss: 2.1227\n",
      "Epoch [96/300], Step [2200/27733], Loss: 2.6900\n",
      "Epoch [96/300], Step [2300/27733], Loss: 2.2829\n",
      "Epoch [96/300], Step [2400/27733], Loss: 2.8552\n",
      "Epoch [96/300], Step [2500/27733], Loss: 2.8292\n",
      "Epoch [96/300], Step [2600/27733], Loss: 2.7775\n",
      "Epoch [96/300], Step [2700/27733], Loss: 1.9562\n",
      "Epoch [96/300], Step [2800/27733], Loss: 2.0277\n",
      "Epoch [96/300], Step [2900/27733], Loss: 3.4973\n",
      "Epoch [96/300], Step [3000/27733], Loss: 3.1707\n",
      "Epoch [96/300], Step [3100/27733], Loss: 2.5192\n",
      "Epoch [96/300], Step [3200/27733], Loss: 2.6851\n",
      "Epoch [96/300], Step [3300/27733], Loss: 2.7487\n",
      "Epoch [96/300], Step [3400/27733], Loss: 2.6060\n",
      "Epoch [96/300], Step [3500/27733], Loss: 2.7477\n",
      "Epoch [96/300], Step [3600/27733], Loss: 2.3520\n",
      "Epoch [96/300], Step [3700/27733], Loss: 2.8377\n",
      "Epoch [96/300], Step [3800/27733], Loss: 2.1797\n",
      "Epoch [96/300], Step [3900/27733], Loss: 3.2704\n",
      "Epoch [96/300], Step [4000/27733], Loss: 2.5672\n",
      "Epoch [96/300], Step [4100/27733], Loss: 3.1145\n",
      "Epoch [96/300], Step [4200/27733], Loss: 2.5509\n",
      "Epoch [96/300], Step [4300/27733], Loss: 2.7432\n",
      "Epoch [96/300], Step [4400/27733], Loss: 2.4962\n",
      "Epoch [96/300], Step [4500/27733], Loss: 1.8670\n",
      "Epoch [96/300], Step [4600/27733], Loss: 3.0533\n",
      "Epoch [96/300], Step [4700/27733], Loss: 2.1532\n",
      "Epoch [96/300], Step [4800/27733], Loss: 3.3509\n",
      "Epoch [96/300], Step [4900/27733], Loss: 2.7583\n",
      "Epoch [96/300], Step [5000/27733], Loss: 2.5778\n",
      "Epoch [96/300], Step [5100/27733], Loss: 2.8666\n",
      "Epoch [96/300], Step [5200/27733], Loss: 3.1567\n",
      "Epoch [96/300], Step [5300/27733], Loss: 3.0024\n",
      "Epoch [96/300], Step [5400/27733], Loss: 2.0720\n",
      "Epoch [96/300], Step [5500/27733], Loss: 3.5768\n",
      "Epoch [96/300], Step [5600/27733], Loss: 2.4632\n",
      "Epoch [96/300], Step [5700/27733], Loss: 2.3456\n",
      "Epoch [96/300], Step [5800/27733], Loss: 3.0362\n",
      "Epoch [96/300], Step [5900/27733], Loss: 2.8569\n",
      "Epoch [96/300], Step [6000/27733], Loss: 3.0065\n",
      "Epoch [96/300], Step [6100/27733], Loss: 2.4685\n",
      "Epoch [96/300], Step [6200/27733], Loss: 2.9601\n",
      "Epoch [96/300], Step [6300/27733], Loss: 2.2633\n",
      "Epoch [96/300], Step [6400/27733], Loss: 3.0508\n",
      "Epoch [96/300], Step [6500/27733], Loss: 2.4471\n",
      "Epoch [96/300], Step [6600/27733], Loss: 2.1329\n",
      "Epoch [96/300], Step [6700/27733], Loss: 2.5443\n",
      "Epoch [96/300], Step [6800/27733], Loss: 2.2134\n",
      "Epoch [96/300], Step [6900/27733], Loss: 1.9815\n",
      "Epoch [96/300], Step [7000/27733], Loss: 3.1841\n",
      "Epoch [96/300], Step [7100/27733], Loss: 1.7800\n",
      "Epoch [96/300], Step [7200/27733], Loss: 2.4602\n",
      "Epoch [96/300], Step [7300/27733], Loss: 3.7687\n",
      "Epoch [96/300], Step [7400/27733], Loss: 2.6057\n",
      "Epoch [96/300], Step [7500/27733], Loss: 2.2869\n",
      "Epoch [96/300], Step [7600/27733], Loss: 1.6246\n",
      "Epoch [96/300], Step [7700/27733], Loss: 3.1818\n",
      "Epoch [96/300], Step [7800/27733], Loss: 1.8859\n",
      "Epoch [96/300], Step [7900/27733], Loss: 3.0817\n",
      "Epoch [96/300], Step [8000/27733], Loss: 2.7631\n",
      "Epoch [96/300], Step [8100/27733], Loss: 2.0264\n",
      "Epoch [96/300], Step [8200/27733], Loss: 3.4839\n",
      "Epoch [96/300], Step [8300/27733], Loss: 2.5704\n",
      "Epoch [96/300], Step [8400/27733], Loss: 3.4362\n",
      "Epoch [96/300], Step [8500/27733], Loss: 3.1090\n",
      "Epoch [96/300], Step [8600/27733], Loss: 1.8360\n",
      "Epoch [96/300], Step [8700/27733], Loss: 2.4962\n",
      "Epoch [96/300], Step [8800/27733], Loss: 2.7420\n",
      "Epoch [96/300], Step [8900/27733], Loss: 2.6851\n",
      "Epoch [96/300], Step [9000/27733], Loss: 3.1864\n",
      "Epoch [96/300], Step [9100/27733], Loss: 2.1369\n",
      "Epoch [96/300], Step [9200/27733], Loss: 3.5367\n",
      "Epoch [96/300], Step [9300/27733], Loss: 2.8660\n",
      "Epoch [96/300], Step [9400/27733], Loss: 3.0368\n",
      "Epoch [96/300], Step [9500/27733], Loss: 2.1918\n",
      "Epoch [96/300], Step [9600/27733], Loss: 2.5968\n",
      "Epoch [96/300], Step [9700/27733], Loss: 3.0486\n",
      "Epoch [96/300], Step [9800/27733], Loss: 2.9669\n",
      "Epoch [96/300], Step [9900/27733], Loss: 2.3812\n",
      "Epoch [96/300], Step [10000/27733], Loss: 2.2850\n",
      "Epoch [96/300], Step [10100/27733], Loss: 3.4505\n",
      "Epoch [96/300], Step [10200/27733], Loss: 2.9818\n",
      "Epoch [96/300], Step [10300/27733], Loss: 3.2893\n",
      "Epoch [96/300], Step [10400/27733], Loss: 3.2505\n",
      "Epoch [96/300], Step [10500/27733], Loss: 2.3378\n",
      "Epoch [96/300], Step [10600/27733], Loss: 2.7572\n",
      "Epoch [96/300], Step [10700/27733], Loss: 3.2098\n",
      "Epoch [96/300], Step [10800/27733], Loss: 2.7859\n",
      "Epoch [96/300], Step [10900/27733], Loss: 2.4705\n",
      "Epoch [96/300], Step [11000/27733], Loss: 3.3516\n",
      "Epoch [96/300], Step [11100/27733], Loss: 2.7130\n",
      "Epoch [96/300], Step [11200/27733], Loss: 3.2504\n",
      "Epoch [96/300], Step [11300/27733], Loss: 2.5133\n",
      "Epoch [96/300], Step [11400/27733], Loss: 3.5477\n",
      "Epoch [96/300], Step [11500/27733], Loss: 2.6205\n",
      "Epoch [96/300], Step [11600/27733], Loss: 2.5665\n",
      "Epoch [96/300], Step [11700/27733], Loss: 4.2550\n",
      "Epoch [96/300], Step [11800/27733], Loss: 2.8423\n",
      "Epoch [96/300], Step [11900/27733], Loss: 2.3504\n",
      "Epoch [96/300], Step [12000/27733], Loss: 2.7090\n",
      "Epoch [96/300], Step [12100/27733], Loss: 2.4490\n",
      "Epoch [96/300], Step [12200/27733], Loss: 2.7192\n",
      "Epoch [96/300], Step [12300/27733], Loss: 2.1713\n",
      "Epoch [96/300], Step [12400/27733], Loss: 3.0017\n",
      "Epoch [96/300], Step [12500/27733], Loss: 2.8580\n",
      "Epoch [96/300], Step [12600/27733], Loss: 2.0581\n",
      "Epoch [96/300], Step [12700/27733], Loss: 3.1294\n",
      "Epoch [96/300], Step [12800/27733], Loss: 2.3186\n",
      "Epoch [96/300], Step [12900/27733], Loss: 2.2436\n",
      "Epoch [96/300], Step [13000/27733], Loss: 2.8800\n",
      "Epoch [96/300], Step [13100/27733], Loss: 2.7116\n",
      "Epoch [96/300], Step [13200/27733], Loss: 2.2297\n",
      "Epoch [96/300], Step [13300/27733], Loss: 2.6149\n",
      "Epoch [96/300], Step [13400/27733], Loss: 2.9664\n",
      "Epoch [96/300], Step [13500/27733], Loss: 3.0068\n",
      "Epoch [96/300], Step [13600/27733], Loss: 2.1831\n",
      "Epoch [96/300], Step [13700/27733], Loss: 3.4411\n",
      "Epoch [96/300], Step [13800/27733], Loss: 2.5262\n",
      "Epoch [96/300], Step [13900/27733], Loss: 2.5396\n",
      "Epoch [96/300], Step [14000/27733], Loss: 3.0114\n",
      "Epoch [96/300], Step [14100/27733], Loss: 2.3481\n",
      "Epoch [96/300], Step [14200/27733], Loss: 2.4797\n",
      "Epoch [96/300], Step [14300/27733], Loss: 3.0803\n",
      "Epoch [96/300], Step [14400/27733], Loss: 3.3522\n",
      "Epoch [96/300], Step [14500/27733], Loss: 2.8705\n",
      "Epoch [96/300], Step [14600/27733], Loss: 2.9902\n",
      "Epoch [96/300], Step [14700/27733], Loss: 3.1061\n",
      "Epoch [96/300], Step [14800/27733], Loss: 2.4998\n",
      "Epoch [96/300], Step [14900/27733], Loss: 3.1171\n",
      "Epoch [96/300], Step [15000/27733], Loss: 2.7658\n",
      "Epoch [96/300], Step [15100/27733], Loss: 3.4341\n",
      "Epoch [96/300], Step [15200/27733], Loss: 3.9298\n",
      "Epoch [96/300], Step [15300/27733], Loss: 2.0935\n",
      "Epoch [96/300], Step [15400/27733], Loss: 3.4719\n",
      "Epoch [96/300], Step [15500/27733], Loss: 2.1331\n",
      "Epoch [96/300], Step [15600/27733], Loss: 2.6012\n",
      "Epoch [96/300], Step [15700/27733], Loss: 2.9168\n",
      "Epoch [96/300], Step [15800/27733], Loss: 2.9275\n",
      "Epoch [96/300], Step [15900/27733], Loss: 3.0313\n",
      "Epoch [96/300], Step [16000/27733], Loss: 2.5819\n",
      "Epoch [96/300], Step [16100/27733], Loss: 2.7264\n",
      "Epoch [96/300], Step [16200/27733], Loss: 2.6547\n",
      "Epoch [96/300], Step [16300/27733], Loss: 2.5704\n",
      "Epoch [96/300], Step [16400/27733], Loss: 3.1246\n",
      "Epoch [96/300], Step [16500/27733], Loss: 3.1701\n",
      "Epoch [96/300], Step [16600/27733], Loss: 2.6244\n",
      "Epoch [96/300], Step [16700/27733], Loss: 3.2242\n",
      "Epoch [96/300], Step [16800/27733], Loss: 3.1478\n",
      "Epoch [96/300], Step [16900/27733], Loss: 2.9899\n",
      "Epoch [96/300], Step [17000/27733], Loss: 2.1432\n",
      "Epoch [96/300], Step [17100/27733], Loss: 2.4730\n",
      "Epoch [96/300], Step [17200/27733], Loss: 3.7867\n",
      "Epoch [96/300], Step [17300/27733], Loss: 2.9663\n",
      "Epoch [96/300], Step [17400/27733], Loss: 2.8631\n",
      "Epoch [96/300], Step [17500/27733], Loss: 2.2638\n",
      "Epoch [96/300], Step [17600/27733], Loss: 2.1948\n",
      "Epoch [96/300], Step [17700/27733], Loss: 2.7943\n",
      "Epoch [96/300], Step [17800/27733], Loss: 3.7331\n",
      "Epoch [96/300], Step [17900/27733], Loss: 2.7293\n",
      "Epoch [96/300], Step [18000/27733], Loss: 3.2723\n",
      "Epoch [96/300], Step [18100/27733], Loss: 2.7867\n",
      "Epoch [96/300], Step [18200/27733], Loss: 2.9520\n",
      "Epoch [96/300], Step [18300/27733], Loss: 2.0892\n",
      "Epoch [96/300], Step [18400/27733], Loss: 3.4062\n",
      "Epoch [96/300], Step [18500/27733], Loss: 2.6844\n",
      "Epoch [96/300], Step [18600/27733], Loss: 3.1063\n",
      "Epoch [96/300], Step [18700/27733], Loss: 2.9226\n",
      "Epoch [96/300], Step [18800/27733], Loss: 2.4245\n",
      "Epoch [96/300], Step [18900/27733], Loss: 3.2006\n",
      "Epoch [96/300], Step [19000/27733], Loss: 3.0121\n",
      "Epoch [96/300], Step [19100/27733], Loss: 2.6649\n",
      "Epoch [96/300], Step [19200/27733], Loss: 3.0060\n",
      "Epoch [96/300], Step [19300/27733], Loss: 3.5365\n",
      "Epoch [96/300], Step [19400/27733], Loss: 2.2595\n",
      "Epoch [96/300], Step [19500/27733], Loss: 2.5867\n",
      "Epoch [96/300], Step [19600/27733], Loss: 2.1170\n",
      "Epoch [96/300], Step [19700/27733], Loss: 2.6100\n",
      "Epoch [96/300], Step [19800/27733], Loss: 2.6916\n",
      "Epoch [96/300], Step [19900/27733], Loss: 3.0907\n",
      "Epoch [96/300], Step [20000/27733], Loss: 3.3052\n",
      "Epoch [96/300], Step [20100/27733], Loss: 3.1196\n",
      "Epoch [96/300], Step [20200/27733], Loss: 3.0730\n",
      "Epoch [96/300], Step [20300/27733], Loss: 2.7974\n",
      "Epoch [96/300], Step [20400/27733], Loss: 2.4469\n",
      "Epoch [96/300], Step [20500/27733], Loss: 2.9012\n",
      "Epoch [96/300], Step [20600/27733], Loss: 2.7879\n",
      "Epoch [96/300], Step [20700/27733], Loss: 2.7829\n",
      "Epoch [96/300], Step [20800/27733], Loss: 3.1693\n",
      "Epoch [96/300], Step [20900/27733], Loss: 3.2678\n",
      "Epoch [96/300], Step [21000/27733], Loss: 2.8313\n",
      "Epoch [96/300], Step [21100/27733], Loss: 2.2989\n",
      "Epoch [96/300], Step [21200/27733], Loss: 2.7344\n",
      "Epoch [96/300], Step [21300/27733], Loss: 2.6772\n",
      "Epoch [96/300], Step [21400/27733], Loss: 2.9724\n",
      "Epoch [96/300], Step [21500/27733], Loss: 3.0171\n",
      "Epoch [96/300], Step [21600/27733], Loss: 2.4242\n",
      "Epoch [96/300], Step [21700/27733], Loss: 2.6057\n",
      "Epoch [96/300], Step [21800/27733], Loss: 3.9344\n",
      "Epoch [96/300], Step [21900/27733], Loss: 3.4815\n",
      "Epoch [96/300], Step [22000/27733], Loss: 2.9353\n",
      "Epoch [96/300], Step [22100/27733], Loss: 2.3906\n",
      "Epoch [96/300], Step [22200/27733], Loss: 2.2051\n",
      "Epoch [96/300], Step [22300/27733], Loss: 1.7969\n",
      "Epoch [96/300], Step [22400/27733], Loss: 2.9085\n",
      "Epoch [96/300], Step [22500/27733], Loss: 3.1718\n",
      "Epoch [96/300], Step [22600/27733], Loss: 4.1786\n",
      "Epoch [96/300], Step [22700/27733], Loss: 3.3498\n",
      "Epoch [96/300], Step [22800/27733], Loss: 2.2658\n",
      "Epoch [96/300], Step [22900/27733], Loss: 3.1872\n",
      "Epoch [96/300], Step [23000/27733], Loss: 2.7665\n",
      "Epoch [96/300], Step [23100/27733], Loss: 2.9729\n",
      "Epoch [96/300], Step [23200/27733], Loss: 3.1508\n",
      "Epoch [96/300], Step [23300/27733], Loss: 3.1350\n",
      "Epoch [96/300], Step [23400/27733], Loss: 3.5235\n",
      "Epoch [96/300], Step [23500/27733], Loss: 2.5941\n",
      "Epoch [96/300], Step [23600/27733], Loss: 2.9502\n",
      "Epoch [96/300], Step [23700/27733], Loss: 3.2842\n",
      "Epoch [96/300], Step [23800/27733], Loss: 2.7575\n",
      "Epoch [96/300], Step [23900/27733], Loss: 3.1710\n",
      "Epoch [96/300], Step [24000/27733], Loss: 2.9586\n",
      "Epoch [96/300], Step [24100/27733], Loss: 2.4463\n",
      "Epoch [96/300], Step [24200/27733], Loss: 3.7370\n",
      "Epoch [96/300], Step [24300/27733], Loss: 2.8809\n",
      "Epoch [96/300], Step [24400/27733], Loss: 2.6994\n",
      "Epoch [96/300], Step [24500/27733], Loss: 2.0737\n",
      "Epoch [96/300], Step [24600/27733], Loss: 2.9357\n",
      "Epoch [96/300], Step [24700/27733], Loss: 2.5052\n",
      "Epoch [96/300], Step [24800/27733], Loss: 3.6246\n",
      "Epoch [96/300], Step [24900/27733], Loss: 2.9759\n",
      "Epoch [96/300], Step [25000/27733], Loss: 3.3113\n",
      "Epoch [96/300], Step [25100/27733], Loss: 2.8018\n",
      "Epoch [96/300], Step [25200/27733], Loss: 3.1901\n",
      "Epoch [96/300], Step [25300/27733], Loss: 2.6054\n",
      "Epoch [96/300], Step [25400/27733], Loss: 2.8632\n",
      "Epoch [96/300], Step [25500/27733], Loss: 2.6296\n",
      "Epoch [96/300], Step [25600/27733], Loss: 2.7558\n",
      "Epoch [96/300], Step [25700/27733], Loss: 2.7005\n",
      "Epoch [96/300], Step [25800/27733], Loss: 3.0120\n",
      "Epoch [96/300], Step [25900/27733], Loss: 2.8219\n",
      "Epoch [96/300], Step [26000/27733], Loss: 3.3868\n",
      "Epoch [96/300], Step [26100/27733], Loss: 4.2084\n",
      "Epoch [96/300], Step [26200/27733], Loss: 3.5290\n",
      "Epoch [96/300], Step [26300/27733], Loss: 4.0142\n",
      "Epoch [96/300], Step [26400/27733], Loss: 1.7159\n",
      "Epoch [96/300], Step [26500/27733], Loss: 3.1440\n",
      "Epoch [96/300], Step [26600/27733], Loss: 2.4422\n",
      "Epoch [96/300], Step [26700/27733], Loss: 3.0635\n",
      "Epoch [96/300], Step [26800/27733], Loss: 3.1338\n",
      "Epoch [96/300], Step [26900/27733], Loss: 3.3625\n",
      "Epoch [96/300], Step [27000/27733], Loss: 3.0083\n",
      "Epoch [96/300], Step [27100/27733], Loss: 2.1238\n",
      "Epoch [96/300], Step [27200/27733], Loss: 3.2085\n",
      "Epoch [96/300], Step [27300/27733], Loss: 3.2734\n",
      "Epoch [96/300], Step [27400/27733], Loss: 2.7015\n",
      "Epoch [96/300], Step [27500/27733], Loss: 3.3303\n",
      "Epoch [96/300], Step [27600/27733], Loss: 3.0529\n",
      "Epoch [96/300], Step [27700/27733], Loss: 3.4692\n",
      "Epoch [97/300], Step [100/27733], Loss: 1.9558\n",
      "Epoch [97/300], Step [200/27733], Loss: 2.7246\n",
      "Epoch [97/300], Step [300/27733], Loss: 3.0399\n",
      "Epoch [97/300], Step [400/27733], Loss: 2.3177\n",
      "Epoch [97/300], Step [500/27733], Loss: 2.0632\n",
      "Epoch [97/300], Step [600/27733], Loss: 2.6108\n",
      "Epoch [97/300], Step [700/27733], Loss: 1.8610\n",
      "Epoch [97/300], Step [800/27733], Loss: 2.6360\n",
      "Epoch [97/300], Step [900/27733], Loss: 2.4055\n",
      "Epoch [97/300], Step [1000/27733], Loss: 2.8279\n",
      "Epoch [97/300], Step [1100/27733], Loss: 2.2948\n",
      "Epoch [97/300], Step [1200/27733], Loss: 2.7862\n",
      "Epoch [97/300], Step [1300/27733], Loss: 2.0802\n",
      "Epoch [97/300], Step [1400/27733], Loss: 1.6215\n",
      "Epoch [97/300], Step [1500/27733], Loss: 2.5489\n",
      "Epoch [97/300], Step [1600/27733], Loss: 2.6320\n",
      "Epoch [97/300], Step [1700/27733], Loss: 2.2602\n",
      "Epoch [97/300], Step [1800/27733], Loss: 1.9553\n",
      "Epoch [97/300], Step [1900/27733], Loss: 2.3888\n",
      "Epoch [97/300], Step [2000/27733], Loss: 2.5480\n",
      "Epoch [97/300], Step [2100/27733], Loss: 2.6022\n",
      "Epoch [97/300], Step [2200/27733], Loss: 2.6392\n",
      "Epoch [97/300], Step [2300/27733], Loss: 1.8814\n",
      "Epoch [97/300], Step [2400/27733], Loss: 2.8972\n",
      "Epoch [97/300], Step [2500/27733], Loss: 2.3342\n",
      "Epoch [97/300], Step [2600/27733], Loss: 2.3226\n",
      "Epoch [97/300], Step [2700/27733], Loss: 2.2086\n",
      "Epoch [97/300], Step [2800/27733], Loss: 2.4506\n",
      "Epoch [97/300], Step [2900/27733], Loss: 2.8483\n",
      "Epoch [97/300], Step [3000/27733], Loss: 2.5156\n",
      "Epoch [97/300], Step [3100/27733], Loss: 2.1127\n",
      "Epoch [97/300], Step [3200/27733], Loss: 3.3432\n",
      "Epoch [97/300], Step [3300/27733], Loss: 2.5049\n",
      "Epoch [97/300], Step [3400/27733], Loss: 2.5338\n",
      "Epoch [97/300], Step [3500/27733], Loss: 2.3117\n",
      "Epoch [97/300], Step [3600/27733], Loss: 1.7764\n",
      "Epoch [97/300], Step [3700/27733], Loss: 2.5822\n",
      "Epoch [97/300], Step [3800/27733], Loss: 3.0020\n",
      "Epoch [97/300], Step [3900/27733], Loss: 2.4550\n",
      "Epoch [97/300], Step [4000/27733], Loss: 2.0130\n",
      "Epoch [97/300], Step [4100/27733], Loss: 2.1472\n",
      "Epoch [97/300], Step [4200/27733], Loss: 2.7612\n",
      "Epoch [97/300], Step [4300/27733], Loss: 2.0240\n",
      "Epoch [97/300], Step [4400/27733], Loss: 2.8690\n",
      "Epoch [97/300], Step [4500/27733], Loss: 2.8046\n",
      "Epoch [97/300], Step [4600/27733], Loss: 2.4177\n",
      "Epoch [97/300], Step [4700/27733], Loss: 2.9461\n",
      "Epoch [97/300], Step [4800/27733], Loss: 2.4086\n",
      "Epoch [97/300], Step [4900/27733], Loss: 2.9915\n",
      "Epoch [97/300], Step [5000/27733], Loss: 2.2786\n",
      "Epoch [97/300], Step [5100/27733], Loss: 3.6758\n",
      "Epoch [97/300], Step [5200/27733], Loss: 3.0141\n",
      "Epoch [97/300], Step [5300/27733], Loss: 2.7617\n",
      "Epoch [97/300], Step [5400/27733], Loss: 2.6504\n",
      "Epoch [97/300], Step [5500/27733], Loss: 2.4099\n",
      "Epoch [97/300], Step [5600/27733], Loss: 2.3371\n",
      "Epoch [97/300], Step [5700/27733], Loss: 2.6887\n",
      "Epoch [97/300], Step [5800/27733], Loss: 2.8328\n",
      "Epoch [97/300], Step [5900/27733], Loss: 2.7145\n",
      "Epoch [97/300], Step [6000/27733], Loss: 3.1568\n",
      "Epoch [97/300], Step [6100/27733], Loss: 2.4995\n",
      "Epoch [97/300], Step [6200/27733], Loss: 2.5520\n",
      "Epoch [97/300], Step [6300/27733], Loss: 2.7514\n",
      "Epoch [97/300], Step [6400/27733], Loss: 2.8774\n",
      "Epoch [97/300], Step [6500/27733], Loss: 3.1399\n",
      "Epoch [97/300], Step [6600/27733], Loss: 1.7490\n",
      "Epoch [97/300], Step [6700/27733], Loss: 3.1836\n",
      "Epoch [97/300], Step [6800/27733], Loss: 2.5716\n",
      "Epoch [97/300], Step [6900/27733], Loss: 2.0548\n",
      "Epoch [97/300], Step [7000/27733], Loss: 3.3712\n",
      "Epoch [97/300], Step [7100/27733], Loss: 2.0254\n",
      "Epoch [97/300], Step [7200/27733], Loss: 2.7061\n",
      "Epoch [97/300], Step [7300/27733], Loss: 1.9220\n",
      "Epoch [97/300], Step [7400/27733], Loss: 2.9023\n",
      "Epoch [97/300], Step [7500/27733], Loss: 2.9590\n",
      "Epoch [97/300], Step [7600/27733], Loss: 2.2907\n",
      "Epoch [97/300], Step [7700/27733], Loss: 2.0249\n",
      "Epoch [97/300], Step [7800/27733], Loss: 2.5638\n",
      "Epoch [97/300], Step [7900/27733], Loss: 2.4075\n",
      "Epoch [97/300], Step [8000/27733], Loss: 2.6952\n",
      "Epoch [97/300], Step [8100/27733], Loss: 2.7119\n",
      "Epoch [97/300], Step [8200/27733], Loss: 2.6208\n",
      "Epoch [97/300], Step [8300/27733], Loss: 2.6724\n",
      "Epoch [97/300], Step [8400/27733], Loss: 2.6935\n",
      "Epoch [97/300], Step [8500/27733], Loss: 2.4865\n",
      "Epoch [97/300], Step [8600/27733], Loss: 2.6042\n",
      "Epoch [97/300], Step [8700/27733], Loss: 2.6124\n",
      "Epoch [97/300], Step [8800/27733], Loss: 2.2840\n",
      "Epoch [97/300], Step [8900/27733], Loss: 3.3257\n",
      "Epoch [97/300], Step [9000/27733], Loss: 3.1906\n",
      "Epoch [97/300], Step [9100/27733], Loss: 3.2952\n",
      "Epoch [97/300], Step [9200/27733], Loss: 2.7714\n",
      "Epoch [97/300], Step [9300/27733], Loss: 2.1270\n",
      "Epoch [97/300], Step [9400/27733], Loss: 3.8120\n",
      "Epoch [97/300], Step [9500/27733], Loss: 3.0829\n",
      "Epoch [97/300], Step [9600/27733], Loss: 2.9084\n",
      "Epoch [97/300], Step [9700/27733], Loss: 2.8296\n",
      "Epoch [97/300], Step [9800/27733], Loss: 2.3600\n",
      "Epoch [97/300], Step [9900/27733], Loss: 3.0882\n",
      "Epoch [97/300], Step [10000/27733], Loss: 2.9743\n",
      "Epoch [97/300], Step [10100/27733], Loss: 2.9247\n",
      "Epoch [97/300], Step [10200/27733], Loss: 2.8749\n",
      "Epoch [97/300], Step [10300/27733], Loss: 2.9625\n",
      "Epoch [97/300], Step [10400/27733], Loss: 2.6216\n",
      "Epoch [97/300], Step [10500/27733], Loss: 2.4774\n",
      "Epoch [97/300], Step [10600/27733], Loss: 2.6286\n",
      "Epoch [97/300], Step [10700/27733], Loss: 2.7766\n",
      "Epoch [97/300], Step [10800/27733], Loss: 3.7911\n",
      "Epoch [97/300], Step [10900/27733], Loss: 2.0336\n",
      "Epoch [97/300], Step [11000/27733], Loss: 3.3687\n",
      "Epoch [97/300], Step [11100/27733], Loss: 3.1953\n",
      "Epoch [97/300], Step [11200/27733], Loss: 2.5076\n",
      "Epoch [97/300], Step [11300/27733], Loss: 2.5615\n",
      "Epoch [97/300], Step [11400/27733], Loss: 2.5743\n",
      "Epoch [97/300], Step [11500/27733], Loss: 3.0650\n",
      "Epoch [97/300], Step [11600/27733], Loss: 2.8935\n",
      "Epoch [97/300], Step [11700/27733], Loss: 2.9651\n",
      "Epoch [97/300], Step [11800/27733], Loss: 3.2112\n",
      "Epoch [97/300], Step [11900/27733], Loss: 2.5936\n",
      "Epoch [97/300], Step [12000/27733], Loss: 2.9348\n",
      "Epoch [97/300], Step [12100/27733], Loss: 2.8467\n",
      "Epoch [97/300], Step [12200/27733], Loss: 2.9100\n",
      "Epoch [97/300], Step [12300/27733], Loss: 2.8558\n",
      "Epoch [97/300], Step [12400/27733], Loss: 2.7447\n",
      "Epoch [97/300], Step [12500/27733], Loss: 3.2555\n",
      "Epoch [97/300], Step [12600/27733], Loss: 2.5191\n",
      "Epoch [97/300], Step [12700/27733], Loss: 3.0853\n",
      "Epoch [97/300], Step [12800/27733], Loss: 2.6552\n",
      "Epoch [97/300], Step [12900/27733], Loss: 3.2193\n",
      "Epoch [97/300], Step [13000/27733], Loss: 2.5952\n",
      "Epoch [97/300], Step [13100/27733], Loss: 2.0038\n",
      "Epoch [97/300], Step [13200/27733], Loss: 2.1541\n",
      "Epoch [97/300], Step [13300/27733], Loss: 2.6785\n",
      "Epoch [97/300], Step [13400/27733], Loss: 2.9328\n",
      "Epoch [97/300], Step [13500/27733], Loss: 3.2833\n",
      "Epoch [97/300], Step [13600/27733], Loss: 3.0614\n",
      "Epoch [97/300], Step [13700/27733], Loss: 1.9821\n",
      "Epoch [97/300], Step [13800/27733], Loss: 3.7890\n",
      "Epoch [97/300], Step [13900/27733], Loss: 3.0243\n",
      "Epoch [97/300], Step [14000/27733], Loss: 2.3517\n",
      "Epoch [97/300], Step [14100/27733], Loss: 2.6295\n",
      "Epoch [97/300], Step [14200/27733], Loss: 2.8182\n",
      "Epoch [97/300], Step [14300/27733], Loss: 2.4086\n",
      "Epoch [97/300], Step [14400/27733], Loss: 2.3688\n",
      "Epoch [97/300], Step [14500/27733], Loss: 2.6203\n",
      "Epoch [97/300], Step [14600/27733], Loss: 2.3811\n",
      "Epoch [97/300], Step [14700/27733], Loss: 2.8297\n",
      "Epoch [97/300], Step [14800/27733], Loss: 2.9207\n",
      "Epoch [97/300], Step [14900/27733], Loss: 3.0395\n",
      "Epoch [97/300], Step [15000/27733], Loss: 2.5943\n",
      "Epoch [97/300], Step [15100/27733], Loss: 3.0293\n",
      "Epoch [97/300], Step [15200/27733], Loss: 2.6109\n",
      "Epoch [97/300], Step [15300/27733], Loss: 3.1497\n",
      "Epoch [97/300], Step [15400/27733], Loss: 3.4978\n",
      "Epoch [97/300], Step [15500/27733], Loss: 2.4443\n",
      "Epoch [97/300], Step [15600/27733], Loss: 2.4071\n",
      "Epoch [97/300], Step [15700/27733], Loss: 2.8848\n",
      "Epoch [97/300], Step [15800/27733], Loss: 3.4679\n",
      "Epoch [97/300], Step [15900/27733], Loss: 3.1020\n",
      "Epoch [97/300], Step [16000/27733], Loss: 3.0209\n",
      "Epoch [97/300], Step [16100/27733], Loss: 2.3462\n",
      "Epoch [97/300], Step [16200/27733], Loss: 2.6995\n",
      "Epoch [97/300], Step [16300/27733], Loss: 2.9904\n",
      "Epoch [97/300], Step [16400/27733], Loss: 3.2349\n",
      "Epoch [97/300], Step [16500/27733], Loss: 3.2509\n",
      "Epoch [97/300], Step [16600/27733], Loss: 2.6396\n",
      "Epoch [97/300], Step [16700/27733], Loss: 2.0321\n",
      "Epoch [97/300], Step [16800/27733], Loss: 2.1152\n",
      "Epoch [97/300], Step [16900/27733], Loss: 3.0540\n",
      "Epoch [97/300], Step [17000/27733], Loss: 3.1304\n",
      "Epoch [97/300], Step [17100/27733], Loss: 3.8669\n",
      "Epoch [97/300], Step [17200/27733], Loss: 2.5651\n",
      "Epoch [97/300], Step [17300/27733], Loss: 2.9388\n",
      "Epoch [97/300], Step [17400/27733], Loss: 2.7710\n",
      "Epoch [97/300], Step [17500/27733], Loss: 2.8723\n",
      "Epoch [97/300], Step [17600/27733], Loss: 2.6426\n",
      "Epoch [97/300], Step [17700/27733], Loss: 2.4583\n",
      "Epoch [97/300], Step [17800/27733], Loss: 3.3437\n",
      "Epoch [97/300], Step [17900/27733], Loss: 2.2959\n",
      "Epoch [97/300], Step [18000/27733], Loss: 2.5808\n",
      "Epoch [97/300], Step [18100/27733], Loss: 2.6143\n",
      "Epoch [97/300], Step [18200/27733], Loss: 2.9492\n",
      "Epoch [97/300], Step [18300/27733], Loss: 2.5278\n",
      "Epoch [97/300], Step [18400/27733], Loss: 2.2650\n",
      "Epoch [97/300], Step [18500/27733], Loss: 3.2433\n",
      "Epoch [97/300], Step [18600/27733], Loss: 3.4425\n",
      "Epoch [97/300], Step [18700/27733], Loss: 3.1673\n",
      "Epoch [97/300], Step [18800/27733], Loss: 3.3786\n",
      "Epoch [97/300], Step [18900/27733], Loss: 2.9556\n",
      "Epoch [97/300], Step [19000/27733], Loss: 2.4780\n",
      "Epoch [97/300], Step [19100/27733], Loss: 2.5910\n",
      "Epoch [97/300], Step [19200/27733], Loss: 2.3818\n",
      "Epoch [97/300], Step [19300/27733], Loss: 2.9127\n",
      "Epoch [97/300], Step [19400/27733], Loss: 2.5689\n",
      "Epoch [97/300], Step [19500/27733], Loss: 3.6604\n",
      "Epoch [97/300], Step [19600/27733], Loss: 2.3099\n",
      "Epoch [97/300], Step [19700/27733], Loss: 2.3178\n",
      "Epoch [97/300], Step [19800/27733], Loss: 3.2577\n",
      "Epoch [97/300], Step [19900/27733], Loss: 3.2852\n",
      "Epoch [97/300], Step [20000/27733], Loss: 3.1190\n",
      "Epoch [97/300], Step [20100/27733], Loss: 2.0932\n",
      "Epoch [97/300], Step [20200/27733], Loss: 2.8790\n",
      "Epoch [97/300], Step [20300/27733], Loss: 2.4579\n",
      "Epoch [97/300], Step [20400/27733], Loss: 2.5200\n",
      "Epoch [97/300], Step [20500/27733], Loss: 2.1311\n",
      "Epoch [97/300], Step [20600/27733], Loss: 3.4515\n",
      "Epoch [97/300], Step [20700/27733], Loss: 3.0546\n",
      "Epoch [97/300], Step [20800/27733], Loss: 3.5175\n",
      "Epoch [97/300], Step [20900/27733], Loss: 2.2987\n",
      "Epoch [97/300], Step [21000/27733], Loss: 2.9773\n",
      "Epoch [97/300], Step [21100/27733], Loss: 2.5636\n",
      "Epoch [97/300], Step [21200/27733], Loss: 4.0357\n",
      "Epoch [97/300], Step [21300/27733], Loss: 3.0084\n",
      "Epoch [97/300], Step [21400/27733], Loss: 3.0400\n",
      "Epoch [97/300], Step [21500/27733], Loss: 2.4162\n",
      "Epoch [97/300], Step [21600/27733], Loss: 3.1070\n",
      "Epoch [97/300], Step [21700/27733], Loss: 2.8622\n",
      "Epoch [97/300], Step [21800/27733], Loss: 3.1073\n",
      "Epoch [97/300], Step [21900/27733], Loss: 2.9222\n",
      "Epoch [97/300], Step [22000/27733], Loss: 3.3216\n",
      "Epoch [97/300], Step [22100/27733], Loss: 2.9259\n",
      "Epoch [97/300], Step [22200/27733], Loss: 3.1184\n",
      "Epoch [97/300], Step [22300/27733], Loss: 3.2763\n",
      "Epoch [97/300], Step [22400/27733], Loss: 3.5249\n",
      "Epoch [97/300], Step [22500/27733], Loss: 3.0545\n",
      "Epoch [97/300], Step [22600/27733], Loss: 2.8974\n",
      "Epoch [97/300], Step [22700/27733], Loss: 2.8862\n",
      "Epoch [97/300], Step [22800/27733], Loss: 2.8262\n",
      "Epoch [97/300], Step [22900/27733], Loss: 2.7125\n",
      "Epoch [97/300], Step [23000/27733], Loss: 4.1191\n",
      "Epoch [97/300], Step [23100/27733], Loss: 4.1521\n",
      "Epoch [97/300], Step [23200/27733], Loss: 4.4087\n",
      "Epoch [97/300], Step [23300/27733], Loss: 3.0824\n",
      "Epoch [97/300], Step [23400/27733], Loss: 2.0277\n",
      "Epoch [97/300], Step [23500/27733], Loss: 2.5045\n",
      "Epoch [97/300], Step [23600/27733], Loss: 3.0413\n",
      "Epoch [97/300], Step [23700/27733], Loss: 3.5547\n",
      "Epoch [97/300], Step [23800/27733], Loss: 2.6291\n",
      "Epoch [97/300], Step [23900/27733], Loss: 3.1994\n",
      "Epoch [97/300], Step [24000/27733], Loss: 3.3198\n",
      "Epoch [97/300], Step [24100/27733], Loss: 2.1796\n",
      "Epoch [97/300], Step [24200/27733], Loss: 4.1458\n",
      "Epoch [97/300], Step [24300/27733], Loss: 2.5159\n",
      "Epoch [97/300], Step [24400/27733], Loss: 2.9736\n",
      "Epoch [97/300], Step [24500/27733], Loss: 3.3635\n",
      "Epoch [97/300], Step [24600/27733], Loss: 2.8238\n",
      "Epoch [97/300], Step [24700/27733], Loss: 3.2467\n",
      "Epoch [97/300], Step [24800/27733], Loss: 3.0907\n",
      "Epoch [97/300], Step [24900/27733], Loss: 2.9037\n",
      "Epoch [97/300], Step [25000/27733], Loss: 3.2517\n",
      "Epoch [97/300], Step [25100/27733], Loss: 1.9323\n",
      "Epoch [97/300], Step [25200/27733], Loss: 3.0757\n",
      "Epoch [97/300], Step [25300/27733], Loss: 2.6063\n",
      "Epoch [97/300], Step [25400/27733], Loss: 2.6523\n",
      "Epoch [97/300], Step [25500/27733], Loss: 2.7990\n",
      "Epoch [97/300], Step [25600/27733], Loss: 2.9021\n",
      "Epoch [97/300], Step [25700/27733], Loss: 2.4363\n",
      "Epoch [97/300], Step [25800/27733], Loss: 3.1997\n",
      "Epoch [97/300], Step [25900/27733], Loss: 3.7682\n",
      "Epoch [97/300], Step [26000/27733], Loss: 1.9670\n",
      "Epoch [97/300], Step [26100/27733], Loss: 2.9395\n",
      "Epoch [97/300], Step [26200/27733], Loss: 2.6978\n",
      "Epoch [97/300], Step [26300/27733], Loss: 3.7691\n",
      "Epoch [97/300], Step [26400/27733], Loss: 2.5959\n",
      "Epoch [97/300], Step [26500/27733], Loss: 3.3355\n",
      "Epoch [97/300], Step [26600/27733], Loss: 2.9667\n",
      "Epoch [97/300], Step [26700/27733], Loss: 2.6867\n",
      "Epoch [97/300], Step [26800/27733], Loss: 2.9741\n",
      "Epoch [97/300], Step [26900/27733], Loss: 2.8359\n",
      "Epoch [97/300], Step [27000/27733], Loss: 3.2454\n",
      "Epoch [97/300], Step [27100/27733], Loss: 3.0897\n",
      "Epoch [97/300], Step [27200/27733], Loss: 2.7571\n",
      "Epoch [97/300], Step [27300/27733], Loss: 3.2730\n",
      "Epoch [97/300], Step [27400/27733], Loss: 3.7611\n",
      "Epoch [97/300], Step [27500/27733], Loss: 3.2607\n",
      "Epoch [97/300], Step [27600/27733], Loss: 3.6351\n",
      "Epoch [97/300], Step [27700/27733], Loss: 4.0229\n",
      "Epoch [98/300], Step [100/27733], Loss: 2.0394\n",
      "Epoch [98/300], Step [200/27733], Loss: 2.2139\n",
      "Epoch [98/300], Step [300/27733], Loss: 2.6875\n",
      "Epoch [98/300], Step [400/27733], Loss: 2.9009\n",
      "Epoch [98/300], Step [500/27733], Loss: 2.0829\n",
      "Epoch [98/300], Step [600/27733], Loss: 1.9226\n",
      "Epoch [98/300], Step [700/27733], Loss: 2.0358\n",
      "Epoch [98/300], Step [800/27733], Loss: 2.6069\n",
      "Epoch [98/300], Step [900/27733], Loss: 2.7878\n",
      "Epoch [98/300], Step [1000/27733], Loss: 2.7435\n",
      "Epoch [98/300], Step [1100/27733], Loss: 2.2552\n",
      "Epoch [98/300], Step [1200/27733], Loss: 2.2015\n",
      "Epoch [98/300], Step [1300/27733], Loss: 1.5200\n",
      "Epoch [98/300], Step [1400/27733], Loss: 2.5312\n",
      "Epoch [98/300], Step [1500/27733], Loss: 2.8874\n",
      "Epoch [98/300], Step [1600/27733], Loss: 2.6306\n",
      "Epoch [98/300], Step [1700/27733], Loss: 2.5004\n",
      "Epoch [98/300], Step [1800/27733], Loss: 2.3771\n",
      "Epoch [98/300], Step [1900/27733], Loss: 2.7922\n",
      "Epoch [98/300], Step [2000/27733], Loss: 1.3947\n",
      "Epoch [98/300], Step [2100/27733], Loss: 2.5054\n",
      "Epoch [98/300], Step [2200/27733], Loss: 1.8972\n",
      "Epoch [98/300], Step [2300/27733], Loss: 2.0269\n",
      "Epoch [98/300], Step [2400/27733], Loss: 3.5370\n",
      "Epoch [98/300], Step [2500/27733], Loss: 2.9445\n",
      "Epoch [98/300], Step [2600/27733], Loss: 2.5910\n",
      "Epoch [98/300], Step [2700/27733], Loss: 2.5104\n",
      "Epoch [98/300], Step [2800/27733], Loss: 2.2340\n",
      "Epoch [98/300], Step [2900/27733], Loss: 2.9525\n",
      "Epoch [98/300], Step [3000/27733], Loss: 2.4810\n",
      "Epoch [98/300], Step [3100/27733], Loss: 2.4804\n",
      "Epoch [98/300], Step [3200/27733], Loss: 2.2468\n",
      "Epoch [98/300], Step [3300/27733], Loss: 2.1477\n",
      "Epoch [98/300], Step [3400/27733], Loss: 2.5826\n",
      "Epoch [98/300], Step [3500/27733], Loss: 3.1667\n",
      "Epoch [98/300], Step [3600/27733], Loss: 2.1857\n",
      "Epoch [98/300], Step [3700/27733], Loss: 2.1593\n",
      "Epoch [98/300], Step [3800/27733], Loss: 1.8842\n",
      "Epoch [98/300], Step [3900/27733], Loss: 2.7555\n",
      "Epoch [98/300], Step [4000/27733], Loss: 2.8432\n",
      "Epoch [98/300], Step [4100/27733], Loss: 3.7269\n",
      "Epoch [98/300], Step [4200/27733], Loss: 2.8203\n",
      "Epoch [98/300], Step [4300/27733], Loss: 2.7960\n",
      "Epoch [98/300], Step [4400/27733], Loss: 2.2533\n",
      "Epoch [98/300], Step [4500/27733], Loss: 2.2382\n",
      "Epoch [98/300], Step [4600/27733], Loss: 2.4194\n",
      "Epoch [98/300], Step [4700/27733], Loss: 2.7542\n",
      "Epoch [98/300], Step [4800/27733], Loss: 3.1238\n",
      "Epoch [98/300], Step [4900/27733], Loss: 2.1945\n",
      "Epoch [98/300], Step [5000/27733], Loss: 1.6930\n",
      "Epoch [98/300], Step [5100/27733], Loss: 2.6757\n",
      "Epoch [98/300], Step [5200/27733], Loss: 2.2709\n",
      "Epoch [98/300], Step [5300/27733], Loss: 2.6712\n",
      "Epoch [98/300], Step [5400/27733], Loss: 2.0672\n",
      "Epoch [98/300], Step [5500/27733], Loss: 1.5926\n",
      "Epoch [98/300], Step [5600/27733], Loss: 2.6630\n",
      "Epoch [98/300], Step [5700/27733], Loss: 2.3110\n",
      "Epoch [98/300], Step [5800/27733], Loss: 2.7417\n",
      "Epoch [98/300], Step [5900/27733], Loss: 2.8086\n",
      "Epoch [98/300], Step [6000/27733], Loss: 2.9641\n",
      "Epoch [98/300], Step [6100/27733], Loss: 2.1516\n",
      "Epoch [98/300], Step [6200/27733], Loss: 2.0070\n",
      "Epoch [98/300], Step [6300/27733], Loss: 2.3613\n",
      "Epoch [98/300], Step [6400/27733], Loss: 2.5237\n",
      "Epoch [98/300], Step [6500/27733], Loss: 3.2115\n",
      "Epoch [98/300], Step [6600/27733], Loss: 3.0213\n",
      "Epoch [98/300], Step [6700/27733], Loss: 3.4711\n",
      "Epoch [98/300], Step [6800/27733], Loss: 2.5211\n",
      "Epoch [98/300], Step [6900/27733], Loss: 2.0244\n",
      "Epoch [98/300], Step [7000/27733], Loss: 3.0244\n",
      "Epoch [98/300], Step [7100/27733], Loss: 2.9565\n",
      "Epoch [98/300], Step [7200/27733], Loss: 2.9412\n",
      "Epoch [98/300], Step [7300/27733], Loss: 1.3080\n",
      "Epoch [98/300], Step [7400/27733], Loss: 2.5328\n",
      "Epoch [98/300], Step [7500/27733], Loss: 2.6550\n",
      "Epoch [98/300], Step [7600/27733], Loss: 2.6729\n",
      "Epoch [98/300], Step [7700/27733], Loss: 2.8138\n",
      "Epoch [98/300], Step [7800/27733], Loss: 2.4057\n",
      "Epoch [98/300], Step [7900/27733], Loss: 2.8964\n",
      "Epoch [98/300], Step [8000/27733], Loss: 1.8706\n",
      "Epoch [98/300], Step [8100/27733], Loss: 3.0668\n",
      "Epoch [98/300], Step [8200/27733], Loss: 3.4278\n",
      "Epoch [98/300], Step [8300/27733], Loss: 2.2332\n",
      "Epoch [98/300], Step [8400/27733], Loss: 2.4869\n",
      "Epoch [98/300], Step [8500/27733], Loss: 2.5597\n",
      "Epoch [98/300], Step [8600/27733], Loss: 2.7190\n",
      "Epoch [98/300], Step [8700/27733], Loss: 3.1667\n",
      "Epoch [98/300], Step [8800/27733], Loss: 2.7279\n",
      "Epoch [98/300], Step [8900/27733], Loss: 2.6999\n",
      "Epoch [98/300], Step [9000/27733], Loss: 2.3307\n",
      "Epoch [98/300], Step [9100/27733], Loss: 2.2102\n",
      "Epoch [98/300], Step [9200/27733], Loss: 2.4105\n",
      "Epoch [98/300], Step [9300/27733], Loss: 2.7815\n",
      "Epoch [98/300], Step [9400/27733], Loss: 2.5814\n",
      "Epoch [98/300], Step [9500/27733], Loss: 2.7693\n",
      "Epoch [98/300], Step [9600/27733], Loss: 3.1336\n",
      "Epoch [98/300], Step [9700/27733], Loss: 2.9120\n",
      "Epoch [98/300], Step [9800/27733], Loss: 2.5321\n",
      "Epoch [98/300], Step [9900/27733], Loss: 2.3501\n",
      "Epoch [98/300], Step [10000/27733], Loss: 3.7507\n",
      "Epoch [98/300], Step [10100/27733], Loss: 2.3798\n",
      "Epoch [98/300], Step [10200/27733], Loss: 2.8994\n",
      "Epoch [98/300], Step [10300/27733], Loss: 2.3452\n",
      "Epoch [98/300], Step [10400/27733], Loss: 2.2815\n",
      "Epoch [98/300], Step [10500/27733], Loss: 2.6299\n",
      "Epoch [98/300], Step [10600/27733], Loss: 3.8638\n",
      "Epoch [98/300], Step [10700/27733], Loss: 2.3440\n",
      "Epoch [98/300], Step [10800/27733], Loss: 2.6486\n",
      "Epoch [98/300], Step [10900/27733], Loss: 2.8750\n",
      "Epoch [98/300], Step [11000/27733], Loss: 2.4932\n",
      "Epoch [98/300], Step [11100/27733], Loss: 2.8844\n",
      "Epoch [98/300], Step [11200/27733], Loss: 1.9423\n",
      "Epoch [98/300], Step [11300/27733], Loss: 3.7678\n",
      "Epoch [98/300], Step [11400/27733], Loss: 2.5977\n",
      "Epoch [98/300], Step [11500/27733], Loss: 1.9008\n",
      "Epoch [98/300], Step [11600/27733], Loss: 3.0071\n",
      "Epoch [98/300], Step [11700/27733], Loss: 2.1202\n",
      "Epoch [98/300], Step [11800/27733], Loss: 2.7846\n",
      "Epoch [98/300], Step [11900/27733], Loss: 3.5155\n",
      "Epoch [98/300], Step [12000/27733], Loss: 3.6416\n",
      "Epoch [98/300], Step [12100/27733], Loss: 3.8374\n",
      "Epoch [98/300], Step [12200/27733], Loss: 3.4650\n",
      "Epoch [98/300], Step [12300/27733], Loss: 2.0564\n",
      "Epoch [98/300], Step [12400/27733], Loss: 2.6693\n",
      "Epoch [98/300], Step [12500/27733], Loss: 2.4751\n",
      "Epoch [98/300], Step [12600/27733], Loss: 2.3673\n",
      "Epoch [98/300], Step [12700/27733], Loss: 2.3415\n",
      "Epoch [98/300], Step [12800/27733], Loss: 3.2864\n",
      "Epoch [98/300], Step [12900/27733], Loss: 2.2291\n",
      "Epoch [98/300], Step [13000/27733], Loss: 2.4843\n",
      "Epoch [98/300], Step [13100/27733], Loss: 2.4875\n",
      "Epoch [98/300], Step [13200/27733], Loss: 3.5716\n",
      "Epoch [98/300], Step [13300/27733], Loss: 2.8456\n",
      "Epoch [98/300], Step [13400/27733], Loss: 2.1248\n",
      "Epoch [98/300], Step [13500/27733], Loss: 2.6976\n",
      "Epoch [98/300], Step [13600/27733], Loss: 2.9608\n",
      "Epoch [98/300], Step [13700/27733], Loss: 2.9993\n",
      "Epoch [98/300], Step [13800/27733], Loss: 2.2716\n",
      "Epoch [98/300], Step [13900/27733], Loss: 2.7757\n",
      "Epoch [98/300], Step [14000/27733], Loss: 2.0050\n",
      "Epoch [98/300], Step [14100/27733], Loss: 2.7219\n",
      "Epoch [98/300], Step [14200/27733], Loss: 2.4256\n",
      "Epoch [98/300], Step [14300/27733], Loss: 3.3566\n",
      "Epoch [98/300], Step [14400/27733], Loss: 2.6835\n",
      "Epoch [98/300], Step [14500/27733], Loss: 2.8840\n",
      "Epoch [98/300], Step [14600/27733], Loss: 2.4267\n",
      "Epoch [98/300], Step [14700/27733], Loss: 2.9190\n",
      "Epoch [98/300], Step [14800/27733], Loss: 2.9396\n",
      "Epoch [98/300], Step [14900/27733], Loss: 2.3909\n",
      "Epoch [98/300], Step [15000/27733], Loss: 2.7185\n",
      "Epoch [98/300], Step [15100/27733], Loss: 2.9415\n",
      "Epoch [98/300], Step [15200/27733], Loss: 2.1578\n",
      "Epoch [98/300], Step [15300/27733], Loss: 2.1051\n",
      "Epoch [98/300], Step [15400/27733], Loss: 2.9734\n",
      "Epoch [98/300], Step [15500/27733], Loss: 2.6864\n",
      "Epoch [98/300], Step [15600/27733], Loss: 2.1253\n",
      "Epoch [98/300], Step [15700/27733], Loss: 2.5941\n",
      "Epoch [98/300], Step [15800/27733], Loss: 1.8964\n",
      "Epoch [98/300], Step [15900/27733], Loss: 2.4396\n",
      "Epoch [98/300], Step [16000/27733], Loss: 2.0762\n",
      "Epoch [98/300], Step [16100/27733], Loss: 2.9513\n",
      "Epoch [98/300], Step [16200/27733], Loss: 2.5712\n",
      "Epoch [98/300], Step [16300/27733], Loss: 2.5907\n",
      "Epoch [98/300], Step [16400/27733], Loss: 3.1783\n",
      "Epoch [98/300], Step [16500/27733], Loss: 2.4728\n",
      "Epoch [98/300], Step [16600/27733], Loss: 2.8607\n",
      "Epoch [98/300], Step [16700/27733], Loss: 2.0240\n",
      "Epoch [98/300], Step [16800/27733], Loss: 2.9996\n",
      "Epoch [98/300], Step [16900/27733], Loss: 2.3887\n",
      "Epoch [98/300], Step [17000/27733], Loss: 3.1270\n",
      "Epoch [98/300], Step [17100/27733], Loss: 3.6566\n",
      "Epoch [98/300], Step [17200/27733], Loss: 2.4162\n",
      "Epoch [98/300], Step [17300/27733], Loss: 2.4177\n",
      "Epoch [98/300], Step [17400/27733], Loss: 2.8374\n",
      "Epoch [98/300], Step [17500/27733], Loss: 3.1996\n",
      "Epoch [98/300], Step [17600/27733], Loss: 2.4122\n",
      "Epoch [98/300], Step [17700/27733], Loss: 3.4611\n",
      "Epoch [98/300], Step [17800/27733], Loss: 2.6212\n",
      "Epoch [98/300], Step [17900/27733], Loss: 2.3948\n",
      "Epoch [98/300], Step [18000/27733], Loss: 1.8945\n",
      "Epoch [98/300], Step [18100/27733], Loss: 2.0830\n",
      "Epoch [98/300], Step [18200/27733], Loss: 2.5207\n",
      "Epoch [98/300], Step [18300/27733], Loss: 2.8134\n",
      "Epoch [98/300], Step [18400/27733], Loss: 2.6887\n",
      "Epoch [98/300], Step [18500/27733], Loss: 2.6313\n",
      "Epoch [98/300], Step [18600/27733], Loss: 3.0410\n",
      "Epoch [98/300], Step [18700/27733], Loss: 2.2149\n",
      "Epoch [98/300], Step [18800/27733], Loss: 2.6099\n",
      "Epoch [98/300], Step [18900/27733], Loss: 3.8668\n",
      "Epoch [98/300], Step [19000/27733], Loss: 2.6901\n",
      "Epoch [98/300], Step [19100/27733], Loss: 3.7740\n",
      "Epoch [98/300], Step [19200/27733], Loss: 2.6026\n",
      "Epoch [98/300], Step [19300/27733], Loss: 3.0259\n",
      "Epoch [98/300], Step [19400/27733], Loss: 2.7257\n",
      "Epoch [98/300], Step [19500/27733], Loss: 3.2842\n",
      "Epoch [98/300], Step [19600/27733], Loss: 2.4051\n",
      "Epoch [98/300], Step [19700/27733], Loss: 2.3549\n",
      "Epoch [98/300], Step [19800/27733], Loss: 2.2941\n",
      "Epoch [98/300], Step [19900/27733], Loss: 2.9226\n",
      "Epoch [98/300], Step [20000/27733], Loss: 3.0691\n",
      "Epoch [98/300], Step [20100/27733], Loss: 2.2459\n",
      "Epoch [98/300], Step [20200/27733], Loss: 3.0511\n",
      "Epoch [98/300], Step [20300/27733], Loss: 3.3854\n",
      "Epoch [98/300], Step [20400/27733], Loss: 2.5131\n",
      "Epoch [98/300], Step [20500/27733], Loss: 3.0907\n",
      "Epoch [98/300], Step [20600/27733], Loss: 2.8152\n",
      "Epoch [98/300], Step [20700/27733], Loss: 3.5952\n",
      "Epoch [98/300], Step [20800/27733], Loss: 3.0946\n",
      "Epoch [98/300], Step [20900/27733], Loss: 2.5360\n",
      "Epoch [98/300], Step [21000/27733], Loss: 2.9749\n",
      "Epoch [98/300], Step [21100/27733], Loss: 3.2155\n",
      "Epoch [98/300], Step [21200/27733], Loss: 2.7854\n",
      "Epoch [98/300], Step [21300/27733], Loss: 3.1995\n",
      "Epoch [98/300], Step [21400/27733], Loss: 2.8293\n",
      "Epoch [98/300], Step [21500/27733], Loss: 2.9641\n",
      "Epoch [98/300], Step [21600/27733], Loss: 3.1381\n",
      "Epoch [98/300], Step [21700/27733], Loss: 1.6438\n",
      "Epoch [98/300], Step [21800/27733], Loss: 2.7153\n",
      "Epoch [98/300], Step [21900/27733], Loss: 3.2795\n",
      "Epoch [98/300], Step [22000/27733], Loss: 2.7797\n",
      "Epoch [98/300], Step [22100/27733], Loss: 1.9532\n",
      "Epoch [98/300], Step [22200/27733], Loss: 2.6312\n",
      "Epoch [98/300], Step [22300/27733], Loss: 3.6677\n",
      "Epoch [98/300], Step [22400/27733], Loss: 2.3966\n",
      "Epoch [98/300], Step [22500/27733], Loss: 3.0812\n",
      "Epoch [98/300], Step [22600/27733], Loss: 3.1518\n",
      "Epoch [98/300], Step [22700/27733], Loss: 2.7025\n",
      "Epoch [98/300], Step [22800/27733], Loss: 3.2890\n",
      "Epoch [98/300], Step [22900/27733], Loss: 2.7493\n",
      "Epoch [98/300], Step [23000/27733], Loss: 2.7250\n",
      "Epoch [98/300], Step [23100/27733], Loss: 3.8451\n",
      "Epoch [98/300], Step [23200/27733], Loss: 2.6738\n",
      "Epoch [98/300], Step [23300/27733], Loss: 3.3063\n",
      "Epoch [98/300], Step [23400/27733], Loss: 2.8664\n",
      "Epoch [98/300], Step [23500/27733], Loss: 2.7562\n",
      "Epoch [98/300], Step [23600/27733], Loss: 2.6771\n",
      "Epoch [98/300], Step [23700/27733], Loss: 2.8327\n",
      "Epoch [98/300], Step [23800/27733], Loss: 2.8912\n",
      "Epoch [98/300], Step [23900/27733], Loss: 2.8335\n",
      "Epoch [98/300], Step [24000/27733], Loss: 3.0997\n",
      "Epoch [98/300], Step [24100/27733], Loss: 2.5284\n",
      "Epoch [98/300], Step [24200/27733], Loss: 3.7132\n",
      "Epoch [98/300], Step [24300/27733], Loss: 2.4198\n",
      "Epoch [98/300], Step [24400/27733], Loss: 2.6411\n",
      "Epoch [98/300], Step [24500/27733], Loss: 3.2063\n",
      "Epoch [98/300], Step [24600/27733], Loss: 2.8276\n",
      "Epoch [98/300], Step [24700/27733], Loss: 2.7176\n",
      "Epoch [98/300], Step [24800/27733], Loss: 3.4696\n",
      "Epoch [98/300], Step [24900/27733], Loss: 3.0773\n",
      "Epoch [98/300], Step [25000/27733], Loss: 2.6023\n",
      "Epoch [98/300], Step [25100/27733], Loss: 2.3497\n",
      "Epoch [98/300], Step [25200/27733], Loss: 2.5904\n",
      "Epoch [98/300], Step [25300/27733], Loss: 3.3426\n",
      "Epoch [98/300], Step [25400/27733], Loss: 3.3900\n",
      "Epoch [98/300], Step [25500/27733], Loss: 3.7549\n",
      "Epoch [98/300], Step [25600/27733], Loss: 2.9459\n",
      "Epoch [98/300], Step [25700/27733], Loss: 3.8807\n",
      "Epoch [98/300], Step [25800/27733], Loss: 3.1276\n",
      "Epoch [98/300], Step [25900/27733], Loss: 2.7974\n",
      "Epoch [98/300], Step [26000/27733], Loss: 2.7496\n",
      "Epoch [98/300], Step [26100/27733], Loss: 3.2585\n",
      "Epoch [98/300], Step [26200/27733], Loss: 3.5648\n",
      "Epoch [98/300], Step [26300/27733], Loss: 2.7307\n",
      "Epoch [98/300], Step [26400/27733], Loss: 2.4238\n",
      "Epoch [98/300], Step [26500/27733], Loss: 2.0057\n",
      "Epoch [98/300], Step [26600/27733], Loss: 3.1558\n",
      "Epoch [98/300], Step [26700/27733], Loss: 2.6053\n",
      "Epoch [98/300], Step [26800/27733], Loss: 3.0704\n",
      "Epoch [98/300], Step [26900/27733], Loss: 3.4272\n",
      "Epoch [98/300], Step [27000/27733], Loss: 3.4578\n",
      "Epoch [98/300], Step [27100/27733], Loss: 3.2887\n",
      "Epoch [98/300], Step [27200/27733], Loss: 2.8000\n",
      "Epoch [98/300], Step [27300/27733], Loss: 3.1972\n",
      "Epoch [98/300], Step [27400/27733], Loss: 2.0934\n",
      "Epoch [98/300], Step [27500/27733], Loss: 2.7701\n",
      "Epoch [98/300], Step [27600/27733], Loss: 2.7715\n",
      "Epoch [98/300], Step [27700/27733], Loss: 2.2955\n",
      "Epoch [99/300], Step [100/27733], Loss: 2.7838\n",
      "Epoch [99/300], Step [200/27733], Loss: 3.4425\n",
      "Epoch [99/300], Step [300/27733], Loss: 3.0116\n",
      "Epoch [99/300], Step [400/27733], Loss: 2.8250\n",
      "Epoch [99/300], Step [500/27733], Loss: 1.9986\n",
      "Epoch [99/300], Step [600/27733], Loss: 2.6703\n",
      "Epoch [99/300], Step [700/27733], Loss: 2.5728\n",
      "Epoch [99/300], Step [800/27733], Loss: 2.7727\n",
      "Epoch [99/300], Step [900/27733], Loss: 2.3936\n",
      "Epoch [99/300], Step [1000/27733], Loss: 3.1766\n",
      "Epoch [99/300], Step [1100/27733], Loss: 2.1487\n",
      "Epoch [99/300], Step [1200/27733], Loss: 2.2371\n",
      "Epoch [99/300], Step [1300/27733], Loss: 2.8670\n",
      "Epoch [99/300], Step [1400/27733], Loss: 2.3940\n",
      "Epoch [99/300], Step [1500/27733], Loss: 2.4972\n",
      "Epoch [99/300], Step [1600/27733], Loss: 2.6058\n",
      "Epoch [99/300], Step [1700/27733], Loss: 2.3353\n",
      "Epoch [99/300], Step [1800/27733], Loss: 2.8122\n",
      "Epoch [99/300], Step [1900/27733], Loss: 2.0589\n",
      "Epoch [99/300], Step [2000/27733], Loss: 2.2324\n",
      "Epoch [99/300], Step [2100/27733], Loss: 2.4289\n",
      "Epoch [99/300], Step [2200/27733], Loss: 3.2053\n",
      "Epoch [99/300], Step [2300/27733], Loss: 2.5328\n",
      "Epoch [99/300], Step [2400/27733], Loss: 2.3238\n",
      "Epoch [99/300], Step [2500/27733], Loss: 2.4878\n",
      "Epoch [99/300], Step [2600/27733], Loss: 2.8972\n",
      "Epoch [99/300], Step [2700/27733], Loss: 2.9010\n",
      "Epoch [99/300], Step [2800/27733], Loss: 2.4677\n",
      "Epoch [99/300], Step [2900/27733], Loss: 2.7064\n",
      "Epoch [99/300], Step [3000/27733], Loss: 2.5570\n",
      "Epoch [99/300], Step [3100/27733], Loss: 2.8119\n",
      "Epoch [99/300], Step [3200/27733], Loss: 2.2841\n",
      "Epoch [99/300], Step [3300/27733], Loss: 3.0910\n",
      "Epoch [99/300], Step [3400/27733], Loss: 2.6971\n",
      "Epoch [99/300], Step [3500/27733], Loss: 2.6602\n",
      "Epoch [99/300], Step [3600/27733], Loss: 2.5403\n",
      "Epoch [99/300], Step [3700/27733], Loss: 2.2412\n",
      "Epoch [99/300], Step [3800/27733], Loss: 1.6349\n",
      "Epoch [99/300], Step [3900/27733], Loss: 2.4069\n",
      "Epoch [99/300], Step [4000/27733], Loss: 3.3951\n",
      "Epoch [99/300], Step [4100/27733], Loss: 3.9435\n",
      "Epoch [99/300], Step [4200/27733], Loss: 3.0804\n",
      "Epoch [99/300], Step [4300/27733], Loss: 2.6664\n",
      "Epoch [99/300], Step [4400/27733], Loss: 3.2748\n",
      "Epoch [99/300], Step [4500/27733], Loss: 2.6836\n",
      "Epoch [99/300], Step [4600/27733], Loss: 3.0819\n",
      "Epoch [99/300], Step [4700/27733], Loss: 2.5528\n",
      "Epoch [99/300], Step [4800/27733], Loss: 2.4485\n",
      "Epoch [99/300], Step [4900/27733], Loss: 2.9878\n",
      "Epoch [99/300], Step [5000/27733], Loss: 2.6655\n",
      "Epoch [99/300], Step [5100/27733], Loss: 3.3266\n",
      "Epoch [99/300], Step [5200/27733], Loss: 1.6401\n",
      "Epoch [99/300], Step [5300/27733], Loss: 2.4227\n",
      "Epoch [99/300], Step [5400/27733], Loss: 2.4609\n",
      "Epoch [99/300], Step [5500/27733], Loss: 2.7370\n",
      "Epoch [99/300], Step [5600/27733], Loss: 2.3845\n",
      "Epoch [99/300], Step [5700/27733], Loss: 1.9833\n",
      "Epoch [99/300], Step [5800/27733], Loss: 2.9359\n",
      "Epoch [99/300], Step [5900/27733], Loss: 3.0838\n",
      "Epoch [99/300], Step [6000/27733], Loss: 1.9656\n",
      "Epoch [99/300], Step [6100/27733], Loss: 2.4166\n",
      "Epoch [99/300], Step [6200/27733], Loss: 2.2759\n",
      "Epoch [99/300], Step [6300/27733], Loss: 2.1039\n",
      "Epoch [99/300], Step [6400/27733], Loss: 2.0209\n",
      "Epoch [99/300], Step [6500/27733], Loss: 1.8954\n",
      "Epoch [99/300], Step [6600/27733], Loss: 2.3955\n",
      "Epoch [99/300], Step [6700/27733], Loss: 2.4036\n",
      "Epoch [99/300], Step [6800/27733], Loss: 2.8397\n",
      "Epoch [99/300], Step [6900/27733], Loss: 2.8175\n",
      "Epoch [99/300], Step [7000/27733], Loss: 3.3391\n",
      "Epoch [99/300], Step [7100/27733], Loss: 2.9959\n",
      "Epoch [99/300], Step [7200/27733], Loss: 2.5141\n",
      "Epoch [99/300], Step [7300/27733], Loss: 2.5242\n",
      "Epoch [99/300], Step [7400/27733], Loss: 2.3984\n",
      "Epoch [99/300], Step [7500/27733], Loss: 2.9610\n",
      "Epoch [99/300], Step [7600/27733], Loss: 3.1731\n",
      "Epoch [99/300], Step [7700/27733], Loss: 3.4367\n",
      "Epoch [99/300], Step [7800/27733], Loss: 2.2810\n",
      "Epoch [99/300], Step [7900/27733], Loss: 2.7340\n",
      "Epoch [99/300], Step [8000/27733], Loss: 2.2244\n",
      "Epoch [99/300], Step [8100/27733], Loss: 3.5648\n",
      "Epoch [99/300], Step [8200/27733], Loss: 2.8688\n",
      "Epoch [99/300], Step [8300/27733], Loss: 3.3715\n",
      "Epoch [99/300], Step [8400/27733], Loss: 2.7280\n",
      "Epoch [99/300], Step [8500/27733], Loss: 2.2278\n",
      "Epoch [99/300], Step [8600/27733], Loss: 3.9116\n",
      "Epoch [99/300], Step [8700/27733], Loss: 2.3938\n",
      "Epoch [99/300], Step [8800/27733], Loss: 1.9671\n",
      "Epoch [99/300], Step [8900/27733], Loss: 2.8272\n",
      "Epoch [99/300], Step [9000/27733], Loss: 2.9159\n",
      "Epoch [99/300], Step [9100/27733], Loss: 2.2875\n",
      "Epoch [99/300], Step [9200/27733], Loss: 2.6739\n",
      "Epoch [99/300], Step [9300/27733], Loss: 3.0135\n",
      "Epoch [99/300], Step [9400/27733], Loss: 2.3106\n",
      "Epoch [99/300], Step [9500/27733], Loss: 2.9594\n",
      "Epoch [99/300], Step [9600/27733], Loss: 2.5640\n",
      "Epoch [99/300], Step [9700/27733], Loss: 2.4201\n",
      "Epoch [99/300], Step [9800/27733], Loss: 3.1834\n",
      "Epoch [99/300], Step [9900/27733], Loss: 2.4985\n",
      "Epoch [99/300], Step [10000/27733], Loss: 4.0055\n",
      "Epoch [99/300], Step [10100/27733], Loss: 2.3271\n",
      "Epoch [99/300], Step [10200/27733], Loss: 1.9256\n",
      "Epoch [99/300], Step [10300/27733], Loss: 2.3507\n",
      "Epoch [99/300], Step [10400/27733], Loss: 3.0364\n",
      "Epoch [99/300], Step [10500/27733], Loss: 2.6513\n",
      "Epoch [99/300], Step [10600/27733], Loss: 3.7811\n",
      "Epoch [99/300], Step [10700/27733], Loss: 2.5104\n",
      "Epoch [99/300], Step [10800/27733], Loss: 3.1102\n",
      "Epoch [99/300], Step [10900/27733], Loss: 2.0027\n",
      "Epoch [99/300], Step [11000/27733], Loss: 2.6713\n",
      "Epoch [99/300], Step [11100/27733], Loss: 2.7716\n",
      "Epoch [99/300], Step [11200/27733], Loss: 3.0801\n",
      "Epoch [99/300], Step [11300/27733], Loss: 3.5054\n",
      "Epoch [99/300], Step [11400/27733], Loss: 1.6347\n",
      "Epoch [99/300], Step [11500/27733], Loss: 3.7869\n",
      "Epoch [99/300], Step [11600/27733], Loss: 1.3340\n",
      "Epoch [99/300], Step [11700/27733], Loss: 2.9387\n",
      "Epoch [99/300], Step [11800/27733], Loss: 2.3692\n",
      "Epoch [99/300], Step [11900/27733], Loss: 2.4201\n",
      "Epoch [99/300], Step [12000/27733], Loss: 2.3849\n",
      "Epoch [99/300], Step [12100/27733], Loss: 2.3287\n",
      "Epoch [99/300], Step [12200/27733], Loss: 2.6451\n",
      "Epoch [99/300], Step [12300/27733], Loss: 3.0255\n",
      "Epoch [99/300], Step [12400/27733], Loss: 3.2725\n",
      "Epoch [99/300], Step [12500/27733], Loss: 3.2491\n",
      "Epoch [99/300], Step [12600/27733], Loss: 2.8704\n",
      "Epoch [99/300], Step [12700/27733], Loss: 3.1380\n",
      "Epoch [99/300], Step [12800/27733], Loss: 3.0936\n",
      "Epoch [99/300], Step [12900/27733], Loss: 2.6330\n",
      "Epoch [99/300], Step [13000/27733], Loss: 2.6111\n",
      "Epoch [99/300], Step [13100/27733], Loss: 2.6430\n",
      "Epoch [99/300], Step [13200/27733], Loss: 2.2362\n",
      "Epoch [99/300], Step [13300/27733], Loss: 2.4537\n",
      "Epoch [99/300], Step [13400/27733], Loss: 2.0449\n",
      "Epoch [99/300], Step [13500/27733], Loss: 4.0551\n",
      "Epoch [99/300], Step [13600/27733], Loss: 2.8102\n",
      "Epoch [99/300], Step [13700/27733], Loss: 3.0778\n",
      "Epoch [99/300], Step [13800/27733], Loss: 3.6713\n",
      "Epoch [99/300], Step [13900/27733], Loss: 2.6645\n",
      "Epoch [99/300], Step [14000/27733], Loss: 2.7896\n",
      "Epoch [99/300], Step [14100/27733], Loss: 3.3614\n",
      "Epoch [99/300], Step [14200/27733], Loss: 2.9334\n",
      "Epoch [99/300], Step [14300/27733], Loss: 2.2622\n",
      "Epoch [99/300], Step [14400/27733], Loss: 3.1683\n",
      "Epoch [99/300], Step [14500/27733], Loss: 2.5432\n",
      "Epoch [99/300], Step [14600/27733], Loss: 3.8032\n",
      "Epoch [99/300], Step [14700/27733], Loss: 2.7865\n",
      "Epoch [99/300], Step [14800/27733], Loss: 2.5916\n",
      "Epoch [99/300], Step [14900/27733], Loss: 2.6786\n",
      "Epoch [99/300], Step [15000/27733], Loss: 2.4071\n",
      "Epoch [99/300], Step [15100/27733], Loss: 2.6852\n",
      "Epoch [99/300], Step [15200/27733], Loss: 3.5658\n",
      "Epoch [99/300], Step [15300/27733], Loss: 2.9809\n",
      "Epoch [99/300], Step [15400/27733], Loss: 2.9429\n",
      "Epoch [99/300], Step [15500/27733], Loss: 3.1656\n",
      "Epoch [99/300], Step [15600/27733], Loss: 2.5771\n",
      "Epoch [99/300], Step [15700/27733], Loss: 2.7720\n",
      "Epoch [99/300], Step [15800/27733], Loss: 2.6959\n",
      "Epoch [99/300], Step [15900/27733], Loss: 3.1380\n",
      "Epoch [99/300], Step [16000/27733], Loss: 2.7276\n",
      "Epoch [99/300], Step [16100/27733], Loss: 3.0487\n",
      "Epoch [99/300], Step [16200/27733], Loss: 2.2746\n",
      "Epoch [99/300], Step [16300/27733], Loss: 1.7479\n",
      "Epoch [99/300], Step [16400/27733], Loss: 4.1579\n",
      "Epoch [99/300], Step [16500/27733], Loss: 2.5674\n",
      "Epoch [99/300], Step [16600/27733], Loss: 2.7981\n",
      "Epoch [99/300], Step [16700/27733], Loss: 2.7165\n",
      "Epoch [99/300], Step [16800/27733], Loss: 3.2999\n",
      "Epoch [99/300], Step [16900/27733], Loss: 2.5656\n",
      "Epoch [99/300], Step [17000/27733], Loss: 2.2755\n",
      "Epoch [99/300], Step [17100/27733], Loss: 3.1747\n",
      "Epoch [99/300], Step [17200/27733], Loss: 2.8766\n",
      "Epoch [99/300], Step [17300/27733], Loss: 3.0218\n",
      "Epoch [99/300], Step [17400/27733], Loss: 2.7230\n",
      "Epoch [99/300], Step [17500/27733], Loss: 2.8789\n",
      "Epoch [99/300], Step [17600/27733], Loss: 2.8453\n",
      "Epoch [99/300], Step [17700/27733], Loss: 1.7259\n",
      "Epoch [99/300], Step [17800/27733], Loss: 2.4907\n",
      "Epoch [99/300], Step [17900/27733], Loss: 3.6991\n",
      "Epoch [99/300], Step [18000/27733], Loss: 2.7626\n",
      "Epoch [99/300], Step [18100/27733], Loss: 3.3936\n",
      "Epoch [99/300], Step [18200/27733], Loss: 3.1002\n",
      "Epoch [99/300], Step [18300/27733], Loss: 2.6412\n",
      "Epoch [99/300], Step [18400/27733], Loss: 3.1271\n",
      "Epoch [99/300], Step [18500/27733], Loss: 2.5224\n",
      "Epoch [99/300], Step [18600/27733], Loss: 2.8655\n",
      "Epoch [99/300], Step [18700/27733], Loss: 2.7061\n",
      "Epoch [99/300], Step [18800/27733], Loss: 3.2632\n",
      "Epoch [99/300], Step [18900/27733], Loss: 2.4961\n",
      "Epoch [99/300], Step [19000/27733], Loss: 2.8001\n",
      "Epoch [99/300], Step [19100/27733], Loss: 2.6000\n",
      "Epoch [99/300], Step [19200/27733], Loss: 2.5084\n",
      "Epoch [99/300], Step [19300/27733], Loss: 2.6978\n",
      "Epoch [99/300], Step [19400/27733], Loss: 2.1803\n",
      "Epoch [99/300], Step [19500/27733], Loss: 2.6119\n",
      "Epoch [99/300], Step [19600/27733], Loss: 2.6877\n",
      "Epoch [99/300], Step [19700/27733], Loss: 2.9505\n",
      "Epoch [99/300], Step [19800/27733], Loss: 2.2919\n",
      "Epoch [99/300], Step [19900/27733], Loss: 2.2922\n",
      "Epoch [99/300], Step [20000/27733], Loss: 3.8523\n",
      "Epoch [99/300], Step [20100/27733], Loss: 3.1125\n",
      "Epoch [99/300], Step [20200/27733], Loss: 3.0824\n",
      "Epoch [99/300], Step [20300/27733], Loss: 2.3858\n",
      "Epoch [99/300], Step [20400/27733], Loss: 2.7039\n",
      "Epoch [99/300], Step [20500/27733], Loss: 3.0811\n",
      "Epoch [99/300], Step [20600/27733], Loss: 1.7203\n",
      "Epoch [99/300], Step [20700/27733], Loss: 2.9127\n",
      "Epoch [99/300], Step [20800/27733], Loss: 3.0843\n",
      "Epoch [99/300], Step [20900/27733], Loss: 3.6866\n",
      "Epoch [99/300], Step [21000/27733], Loss: 1.8645\n",
      "Epoch [99/300], Step [21100/27733], Loss: 3.0441\n",
      "Epoch [99/300], Step [21200/27733], Loss: 2.2324\n",
      "Epoch [99/300], Step [21300/27733], Loss: 3.1333\n",
      "Epoch [99/300], Step [21400/27733], Loss: 2.9332\n",
      "Epoch [99/300], Step [21500/27733], Loss: 3.6150\n",
      "Epoch [99/300], Step [21600/27733], Loss: 2.9438\n",
      "Epoch [99/300], Step [21700/27733], Loss: 3.3586\n",
      "Epoch [99/300], Step [21800/27733], Loss: 3.0688\n",
      "Epoch [99/300], Step [21900/27733], Loss: 3.2098\n",
      "Epoch [99/300], Step [22000/27733], Loss: 2.9128\n",
      "Epoch [99/300], Step [22100/27733], Loss: 2.8561\n",
      "Epoch [99/300], Step [22200/27733], Loss: 3.7426\n",
      "Epoch [99/300], Step [22300/27733], Loss: 3.2023\n",
      "Epoch [99/300], Step [22400/27733], Loss: 2.8619\n",
      "Epoch [99/300], Step [22500/27733], Loss: 3.3064\n",
      "Epoch [99/300], Step [22600/27733], Loss: 3.8828\n",
      "Epoch [99/300], Step [22700/27733], Loss: 3.1186\n",
      "Epoch [99/300], Step [22800/27733], Loss: 3.0274\n",
      "Epoch [99/300], Step [22900/27733], Loss: 2.4971\n",
      "Epoch [99/300], Step [23000/27733], Loss: 2.8403\n",
      "Epoch [99/300], Step [23100/27733], Loss: 2.9587\n",
      "Epoch [99/300], Step [23200/27733], Loss: 2.7479\n",
      "Epoch [99/300], Step [23300/27733], Loss: 3.5662\n",
      "Epoch [99/300], Step [23400/27733], Loss: 2.7349\n",
      "Epoch [99/300], Step [23500/27733], Loss: 3.5734\n",
      "Epoch [99/300], Step [23600/27733], Loss: 2.9658\n",
      "Epoch [99/300], Step [23700/27733], Loss: 2.1487\n",
      "Epoch [99/300], Step [23800/27733], Loss: 2.3857\n",
      "Epoch [99/300], Step [23900/27733], Loss: 2.5363\n",
      "Epoch [99/300], Step [24000/27733], Loss: 2.5332\n",
      "Epoch [99/300], Step [24100/27733], Loss: 2.6471\n",
      "Epoch [99/300], Step [24200/27733], Loss: 2.9153\n",
      "Epoch [99/300], Step [24300/27733], Loss: 2.4331\n",
      "Epoch [99/300], Step [24400/27733], Loss: 2.7188\n",
      "Epoch [99/300], Step [24500/27733], Loss: 2.9639\n",
      "Epoch [99/300], Step [24600/27733], Loss: 2.7693\n",
      "Epoch [99/300], Step [24700/27733], Loss: 2.6770\n",
      "Epoch [99/300], Step [24800/27733], Loss: 2.7340\n",
      "Epoch [99/300], Step [24900/27733], Loss: 2.3540\n",
      "Epoch [99/300], Step [25000/27733], Loss: 2.9235\n",
      "Epoch [99/300], Step [25100/27733], Loss: 2.7145\n",
      "Epoch [99/300], Step [25200/27733], Loss: 2.3828\n",
      "Epoch [99/300], Step [25300/27733], Loss: 3.2624\n",
      "Epoch [99/300], Step [25400/27733], Loss: 3.4076\n",
      "Epoch [99/300], Step [25500/27733], Loss: 3.5495\n",
      "Epoch [99/300], Step [25600/27733], Loss: 3.3236\n",
      "Epoch [99/300], Step [25700/27733], Loss: 3.1266\n",
      "Epoch [99/300], Step [25800/27733], Loss: 2.3220\n",
      "Epoch [99/300], Step [25900/27733], Loss: 3.2726\n",
      "Epoch [99/300], Step [26000/27733], Loss: 3.0084\n",
      "Epoch [99/300], Step [26100/27733], Loss: 2.9362\n",
      "Epoch [99/300], Step [26200/27733], Loss: 2.0949\n",
      "Epoch [99/300], Step [26300/27733], Loss: 3.4646\n",
      "Epoch [99/300], Step [26400/27733], Loss: 2.5567\n",
      "Epoch [99/300], Step [26500/27733], Loss: 2.5587\n",
      "Epoch [99/300], Step [26600/27733], Loss: 2.6413\n",
      "Epoch [99/300], Step [26700/27733], Loss: 3.4678\n",
      "Epoch [99/300], Step [26800/27733], Loss: 3.2872\n",
      "Epoch [99/300], Step [26900/27733], Loss: 3.4710\n",
      "Epoch [99/300], Step [27000/27733], Loss: 2.9199\n",
      "Epoch [99/300], Step [27100/27733], Loss: 3.5365\n",
      "Epoch [99/300], Step [27200/27733], Loss: 2.8428\n",
      "Epoch [99/300], Step [27300/27733], Loss: 3.1120\n",
      "Epoch [99/300], Step [27400/27733], Loss: 2.7773\n",
      "Epoch [99/300], Step [27500/27733], Loss: 2.9318\n",
      "Epoch [99/300], Step [27600/27733], Loss: 3.0334\n",
      "Epoch [99/300], Step [27700/27733], Loss: 2.1597\n",
      "Epoch [100/300], Step [100/27733], Loss: 2.7983\n",
      "Epoch [100/300], Step [200/27733], Loss: 2.1990\n",
      "Epoch [100/300], Step [300/27733], Loss: 2.9736\n",
      "Epoch [100/300], Step [400/27733], Loss: 1.7965\n",
      "Epoch [100/300], Step [500/27733], Loss: 1.8981\n",
      "Epoch [100/300], Step [600/27733], Loss: 2.2205\n",
      "Epoch [100/300], Step [700/27733], Loss: 2.3399\n",
      "Epoch [100/300], Step [800/27733], Loss: 2.1949\n",
      "Epoch [100/300], Step [900/27733], Loss: 1.5247\n",
      "Epoch [100/300], Step [1000/27733], Loss: 1.8649\n",
      "Epoch [100/300], Step [1100/27733], Loss: 2.9002\n",
      "Epoch [100/300], Step [1200/27733], Loss: 2.0642\n",
      "Epoch [100/300], Step [1300/27733], Loss: 2.3880\n",
      "Epoch [100/300], Step [1400/27733], Loss: 2.0648\n",
      "Epoch [100/300], Step [1500/27733], Loss: 2.5582\n",
      "Epoch [100/300], Step [1600/27733], Loss: 2.9711\n",
      "Epoch [100/300], Step [1700/27733], Loss: 2.1421\n",
      "Epoch [100/300], Step [1800/27733], Loss: 3.1795\n",
      "Epoch [100/300], Step [1900/27733], Loss: 1.9377\n",
      "Epoch [100/300], Step [2000/27733], Loss: 2.6739\n",
      "Epoch [100/300], Step [2100/27733], Loss: 1.8141\n",
      "Epoch [100/300], Step [2200/27733], Loss: 2.8780\n",
      "Epoch [100/300], Step [2300/27733], Loss: 2.1313\n",
      "Epoch [100/300], Step [2400/27733], Loss: 2.3789\n",
      "Epoch [100/300], Step [2500/27733], Loss: 2.7669\n",
      "Epoch [100/300], Step [2600/27733], Loss: 2.7725\n",
      "Epoch [100/300], Step [2700/27733], Loss: 3.1732\n",
      "Epoch [100/300], Step [2800/27733], Loss: 2.6880\n",
      "Epoch [100/300], Step [2900/27733], Loss: 2.9389\n",
      "Epoch [100/300], Step [3000/27733], Loss: 2.0357\n",
      "Epoch [100/300], Step [3100/27733], Loss: 3.1215\n",
      "Epoch [100/300], Step [3200/27733], Loss: 2.2375\n",
      "Epoch [100/300], Step [3300/27733], Loss: 3.4014\n",
      "Epoch [100/300], Step [3400/27733], Loss: 2.2648\n",
      "Epoch [100/300], Step [3500/27733], Loss: 2.7092\n",
      "Epoch [100/300], Step [3600/27733], Loss: 1.9725\n",
      "Epoch [100/300], Step [3700/27733], Loss: 2.9096\n",
      "Epoch [100/300], Step [3800/27733], Loss: 2.6706\n",
      "Epoch [100/300], Step [3900/27733], Loss: 3.3223\n",
      "Epoch [100/300], Step [4000/27733], Loss: 2.3204\n",
      "Epoch [100/300], Step [4100/27733], Loss: 2.6508\n",
      "Epoch [100/300], Step [4200/27733], Loss: 2.5196\n",
      "Epoch [100/300], Step [4300/27733], Loss: 2.8790\n",
      "Epoch [100/300], Step [4400/27733], Loss: 2.0988\n",
      "Epoch [100/300], Step [4500/27733], Loss: 2.1165\n",
      "Epoch [100/300], Step [4600/27733], Loss: 3.9306\n",
      "Epoch [100/300], Step [4700/27733], Loss: 2.4744\n",
      "Epoch [100/300], Step [4800/27733], Loss: 2.5685\n",
      "Epoch [100/300], Step [4900/27733], Loss: 2.3897\n",
      "Epoch [100/300], Step [5000/27733], Loss: 2.0901\n",
      "Epoch [100/300], Step [5100/27733], Loss: 2.7927\n",
      "Epoch [100/300], Step [5200/27733], Loss: 2.5374\n",
      "Epoch [100/300], Step [5300/27733], Loss: 1.9803\n",
      "Epoch [100/300], Step [5400/27733], Loss: 2.2447\n",
      "Epoch [100/300], Step [5500/27733], Loss: 3.9251\n",
      "Epoch [100/300], Step [5600/27733], Loss: 2.5242\n",
      "Epoch [100/300], Step [5700/27733], Loss: 2.6328\n",
      "Epoch [100/300], Step [5800/27733], Loss: 2.4941\n",
      "Epoch [100/300], Step [5900/27733], Loss: 2.4309\n",
      "Epoch [100/300], Step [6000/27733], Loss: 2.5041\n",
      "Epoch [100/300], Step [6100/27733], Loss: 3.5439\n",
      "Epoch [100/300], Step [6200/27733], Loss: 2.9457\n",
      "Epoch [100/300], Step [6300/27733], Loss: 2.0791\n",
      "Epoch [100/300], Step [6400/27733], Loss: 2.2088\n",
      "Epoch [100/300], Step [6500/27733], Loss: 2.6316\n",
      "Epoch [100/300], Step [6600/27733], Loss: 3.1559\n",
      "Epoch [100/300], Step [6700/27733], Loss: 2.1949\n",
      "Epoch [100/300], Step [6800/27733], Loss: 3.3995\n",
      "Epoch [100/300], Step [6900/27733], Loss: 3.1941\n",
      "Epoch [100/300], Step [7000/27733], Loss: 3.3139\n",
      "Epoch [100/300], Step [7100/27733], Loss: 2.7170\n",
      "Epoch [100/300], Step [7200/27733], Loss: 2.8169\n",
      "Epoch [100/300], Step [7300/27733], Loss: 2.4413\n",
      "Epoch [100/300], Step [7400/27733], Loss: 3.0045\n",
      "Epoch [100/300], Step [7500/27733], Loss: 2.7104\n",
      "Epoch [100/300], Step [7600/27733], Loss: 1.7051\n",
      "Epoch [100/300], Step [7700/27733], Loss: 2.4839\n",
      "Epoch [100/300], Step [7800/27733], Loss: 2.1551\n",
      "Epoch [100/300], Step [7900/27733], Loss: 2.3733\n",
      "Epoch [100/300], Step [8000/27733], Loss: 2.5201\n",
      "Epoch [100/300], Step [8100/27733], Loss: 2.5553\n",
      "Epoch [100/300], Step [8200/27733], Loss: 3.0715\n",
      "Epoch [100/300], Step [8300/27733], Loss: 2.5406\n",
      "Epoch [100/300], Step [8400/27733], Loss: 2.8652\n",
      "Epoch [100/300], Step [8500/27733], Loss: 2.8027\n",
      "Epoch [100/300], Step [8600/27733], Loss: 2.5412\n",
      "Epoch [100/300], Step [8700/27733], Loss: 3.3666\n",
      "Epoch [100/300], Step [8800/27733], Loss: 2.4493\n",
      "Epoch [100/300], Step [8900/27733], Loss: 2.8319\n",
      "Epoch [100/300], Step [9000/27733], Loss: 2.9373\n",
      "Epoch [100/300], Step [9100/27733], Loss: 2.8788\n",
      "Epoch [100/300], Step [9200/27733], Loss: 2.6549\n",
      "Epoch [100/300], Step [9300/27733], Loss: 2.5636\n",
      "Epoch [100/300], Step [9400/27733], Loss: 2.1571\n",
      "Epoch [100/300], Step [9500/27733], Loss: 2.8966\n",
      "Epoch [100/300], Step [9600/27733], Loss: 2.5413\n",
      "Epoch [100/300], Step [9700/27733], Loss: 3.0920\n",
      "Epoch [100/300], Step [9800/27733], Loss: 2.4061\n",
      "Epoch [100/300], Step [9900/27733], Loss: 2.6685\n",
      "Epoch [100/300], Step [10000/27733], Loss: 2.5311\n",
      "Epoch [100/300], Step [10100/27733], Loss: 2.2481\n",
      "Epoch [100/300], Step [10200/27733], Loss: 2.4259\n",
      "Epoch [100/300], Step [10300/27733], Loss: 2.0099\n",
      "Epoch [100/300], Step [10400/27733], Loss: 2.6509\n",
      "Epoch [100/300], Step [10500/27733], Loss: 3.0096\n",
      "Epoch [100/300], Step [10600/27733], Loss: 2.7683\n",
      "Epoch [100/300], Step [10700/27733], Loss: 2.8620\n",
      "Epoch [100/300], Step [10800/27733], Loss: 2.5886\n",
      "Epoch [100/300], Step [10900/27733], Loss: 2.4108\n",
      "Epoch [100/300], Step [11000/27733], Loss: 3.1931\n",
      "Epoch [100/300], Step [11100/27733], Loss: 2.4675\n",
      "Epoch [100/300], Step [11200/27733], Loss: 2.7735\n",
      "Epoch [100/300], Step [11300/27733], Loss: 2.0709\n",
      "Epoch [100/300], Step [11400/27733], Loss: 3.4712\n",
      "Epoch [100/300], Step [11500/27733], Loss: 3.1614\n",
      "Epoch [100/300], Step [11600/27733], Loss: 2.8705\n",
      "Epoch [100/300], Step [11700/27733], Loss: 2.3347\n",
      "Epoch [100/300], Step [11800/27733], Loss: 2.6343\n",
      "Epoch [100/300], Step [11900/27733], Loss: 2.4280\n",
      "Epoch [100/300], Step [12000/27733], Loss: 3.6592\n",
      "Epoch [100/300], Step [12100/27733], Loss: 2.9917\n",
      "Epoch [100/300], Step [12200/27733], Loss: 2.8834\n",
      "Epoch [100/300], Step [12300/27733], Loss: 3.1920\n",
      "Epoch [100/300], Step [12400/27733], Loss: 2.0605\n",
      "Epoch [100/300], Step [12500/27733], Loss: 2.6710\n",
      "Epoch [100/300], Step [12600/27733], Loss: 3.1258\n",
      "Epoch [100/300], Step [12700/27733], Loss: 2.5820\n",
      "Epoch [100/300], Step [12800/27733], Loss: 2.7923\n",
      "Epoch [100/300], Step [12900/27733], Loss: 2.7284\n",
      "Epoch [100/300], Step [13000/27733], Loss: 2.5643\n",
      "Epoch [100/300], Step [13100/27733], Loss: 2.6513\n",
      "Epoch [100/300], Step [13200/27733], Loss: 2.8929\n",
      "Epoch [100/300], Step [13300/27733], Loss: 3.3258\n",
      "Epoch [100/300], Step [13400/27733], Loss: 3.2389\n",
      "Epoch [100/300], Step [13500/27733], Loss: 2.3320\n",
      "Epoch [100/300], Step [13600/27733], Loss: 3.8348\n",
      "Epoch [100/300], Step [13700/27733], Loss: 2.8979\n",
      "Epoch [100/300], Step [13800/27733], Loss: 2.5716\n",
      "Epoch [100/300], Step [13900/27733], Loss: 2.9637\n",
      "Epoch [100/300], Step [14000/27733], Loss: 2.1106\n",
      "Epoch [100/300], Step [14100/27733], Loss: 3.8303\n",
      "Epoch [100/300], Step [14200/27733], Loss: 2.8821\n",
      "Epoch [100/300], Step [14300/27733], Loss: 2.4192\n",
      "Epoch [100/300], Step [14400/27733], Loss: 2.4543\n",
      "Epoch [100/300], Step [14500/27733], Loss: 2.5157\n",
      "Epoch [100/300], Step [14600/27733], Loss: 3.0832\n",
      "Epoch [100/300], Step [14700/27733], Loss: 3.4731\n",
      "Epoch [100/300], Step [14800/27733], Loss: 2.1086\n",
      "Epoch [100/300], Step [14900/27733], Loss: 2.4487\n",
      "Epoch [100/300], Step [15000/27733], Loss: 2.4003\n",
      "Epoch [100/300], Step [15100/27733], Loss: 2.5925\n",
      "Epoch [100/300], Step [15200/27733], Loss: 2.2628\n",
      "Epoch [100/300], Step [15300/27733], Loss: 2.2899\n",
      "Epoch [100/300], Step [15400/27733], Loss: 2.8824\n",
      "Epoch [100/300], Step [15500/27733], Loss: 2.7936\n",
      "Epoch [100/300], Step [15600/27733], Loss: 2.8594\n",
      "Epoch [100/300], Step [15700/27733], Loss: 2.5417\n",
      "Epoch [100/300], Step [15800/27733], Loss: 2.2324\n",
      "Epoch [100/300], Step [15900/27733], Loss: 2.7177\n",
      "Epoch [100/300], Step [16000/27733], Loss: 2.6838\n",
      "Epoch [100/300], Step [16100/27733], Loss: 2.4431\n",
      "Epoch [100/300], Step [16200/27733], Loss: 3.5196\n",
      "Epoch [100/300], Step [16300/27733], Loss: 3.0528\n",
      "Epoch [100/300], Step [16400/27733], Loss: 2.8504\n",
      "Epoch [100/300], Step [16500/27733], Loss: 2.3178\n",
      "Epoch [100/300], Step [16600/27733], Loss: 2.9536\n",
      "Epoch [100/300], Step [16700/27733], Loss: 2.5943\n",
      "Epoch [100/300], Step [16800/27733], Loss: 3.0716\n",
      "Epoch [100/300], Step [16900/27733], Loss: 3.0269\n",
      "Epoch [100/300], Step [17000/27733], Loss: 2.5615\n",
      "Epoch [100/300], Step [17100/27733], Loss: 3.2157\n",
      "Epoch [100/300], Step [17200/27733], Loss: 3.7816\n",
      "Epoch [100/300], Step [17300/27733], Loss: 3.4450\n",
      "Epoch [100/300], Step [17400/27733], Loss: 3.9449\n",
      "Epoch [100/300], Step [17500/27733], Loss: 3.1628\n",
      "Epoch [100/300], Step [17600/27733], Loss: 2.2730\n",
      "Epoch [100/300], Step [17700/27733], Loss: 3.2365\n",
      "Epoch [100/300], Step [17800/27733], Loss: 3.2833\n",
      "Epoch [100/300], Step [17900/27733], Loss: 2.7057\n",
      "Epoch [100/300], Step [18000/27733], Loss: 3.2739\n",
      "Epoch [100/300], Step [18100/27733], Loss: 3.0223\n",
      "Epoch [100/300], Step [18200/27733], Loss: 3.1936\n",
      "Epoch [100/300], Step [18300/27733], Loss: 2.5677\n",
      "Epoch [100/300], Step [18400/27733], Loss: 2.9353\n",
      "Epoch [100/300], Step [18500/27733], Loss: 2.8705\n",
      "Epoch [100/300], Step [18600/27733], Loss: 2.1715\n",
      "Epoch [100/300], Step [18700/27733], Loss: 3.3224\n",
      "Epoch [100/300], Step [18800/27733], Loss: 3.1689\n",
      "Epoch [100/300], Step [18900/27733], Loss: 2.4567\n",
      "Epoch [100/300], Step [19000/27733], Loss: 2.2719\n",
      "Epoch [100/300], Step [19100/27733], Loss: 2.2847\n",
      "Epoch [100/300], Step [19200/27733], Loss: 3.0979\n",
      "Epoch [100/300], Step [19300/27733], Loss: 2.6361\n",
      "Epoch [100/300], Step [19400/27733], Loss: 2.9574\n",
      "Epoch [100/300], Step [19500/27733], Loss: 3.5668\n",
      "Epoch [100/300], Step [19600/27733], Loss: 3.1599\n",
      "Epoch [100/300], Step [19700/27733], Loss: 3.5369\n",
      "Epoch [100/300], Step [19800/27733], Loss: 3.5665\n",
      "Epoch [100/300], Step [19900/27733], Loss: 3.5921\n",
      "Epoch [100/300], Step [20000/27733], Loss: 2.7778\n",
      "Epoch [100/300], Step [20100/27733], Loss: 3.0693\n",
      "Epoch [100/300], Step [20200/27733], Loss: 3.2088\n",
      "Epoch [100/300], Step [20300/27733], Loss: 3.3150\n",
      "Epoch [100/300], Step [20400/27733], Loss: 3.0396\n",
      "Epoch [100/300], Step [20500/27733], Loss: 3.3060\n",
      "Epoch [100/300], Step [20600/27733], Loss: 2.1551\n",
      "Epoch [100/300], Step [20700/27733], Loss: 2.8687\n",
      "Epoch [100/300], Step [20800/27733], Loss: 3.9991\n",
      "Epoch [100/300], Step [20900/27733], Loss: 4.2846\n",
      "Epoch [100/300], Step [21000/27733], Loss: 3.2045\n",
      "Epoch [100/300], Step [21100/27733], Loss: 2.9987\n",
      "Epoch [100/300], Step [21200/27733], Loss: 2.8499\n",
      "Epoch [100/300], Step [21300/27733], Loss: 2.7827\n",
      "Epoch [100/300], Step [21400/27733], Loss: 2.6391\n",
      "Epoch [100/300], Step [21500/27733], Loss: 3.4683\n",
      "Epoch [100/300], Step [21600/27733], Loss: 2.6518\n",
      "Epoch [100/300], Step [21700/27733], Loss: 3.4984\n",
      "Epoch [100/300], Step [21800/27733], Loss: 2.0141\n",
      "Epoch [100/300], Step [21900/27733], Loss: 2.2154\n",
      "Epoch [100/300], Step [22000/27733], Loss: 3.2143\n",
      "Epoch [100/300], Step [22100/27733], Loss: 2.9965\n",
      "Epoch [100/300], Step [22200/27733], Loss: 2.6014\n",
      "Epoch [100/300], Step [22300/27733], Loss: 3.1699\n",
      "Epoch [100/300], Step [22400/27733], Loss: 3.5466\n",
      "Epoch [100/300], Step [22500/27733], Loss: 3.5233\n",
      "Epoch [100/300], Step [22600/27733], Loss: 3.5304\n",
      "Epoch [100/300], Step [22700/27733], Loss: 3.1396\n",
      "Epoch [100/300], Step [22800/27733], Loss: 1.9868\n",
      "Epoch [100/300], Step [22900/27733], Loss: 2.6636\n",
      "Epoch [100/300], Step [23000/27733], Loss: 3.6049\n",
      "Epoch [100/300], Step [23100/27733], Loss: 3.2418\n",
      "Epoch [100/300], Step [23200/27733], Loss: 3.3230\n",
      "Epoch [100/300], Step [23300/27733], Loss: 2.7577\n",
      "Epoch [100/300], Step [23400/27733], Loss: 3.1794\n",
      "Epoch [100/300], Step [23500/27733], Loss: 3.3120\n",
      "Epoch [100/300], Step [23600/27733], Loss: 3.0695\n",
      "Epoch [100/300], Step [23700/27733], Loss: 2.9545\n",
      "Epoch [100/300], Step [23800/27733], Loss: 3.5325\n",
      "Epoch [100/300], Step [23900/27733], Loss: 2.1801\n",
      "Epoch [100/300], Step [24000/27733], Loss: 2.8498\n",
      "Epoch [100/300], Step [24100/27733], Loss: 2.8547\n",
      "Epoch [100/300], Step [24200/27733], Loss: 2.9360\n",
      "Epoch [100/300], Step [24300/27733], Loss: 3.1554\n",
      "Epoch [100/300], Step [24400/27733], Loss: 3.7933\n",
      "Epoch [100/300], Step [24500/27733], Loss: 2.2477\n",
      "Epoch [100/300], Step [24600/27733], Loss: 3.0436\n",
      "Epoch [100/300], Step [24700/27733], Loss: 3.4929\n",
      "Epoch [100/300], Step [24800/27733], Loss: 3.6298\n",
      "Epoch [100/300], Step [24900/27733], Loss: 2.8559\n",
      "Epoch [100/300], Step [25000/27733], Loss: 2.6633\n",
      "Epoch [100/300], Step [25100/27733], Loss: 2.6444\n",
      "Epoch [100/300], Step [25200/27733], Loss: 2.9556\n",
      "Epoch [100/300], Step [25300/27733], Loss: 3.3019\n",
      "Epoch [100/300], Step [25400/27733], Loss: 2.7203\n",
      "Epoch [100/300], Step [25500/27733], Loss: 2.8929\n",
      "Epoch [100/300], Step [25600/27733], Loss: 3.0750\n",
      "Epoch [100/300], Step [25700/27733], Loss: 3.2606\n",
      "Epoch [100/300], Step [25800/27733], Loss: 2.7925\n",
      "Epoch [100/300], Step [25900/27733], Loss: 3.3305\n",
      "Epoch [100/300], Step [26000/27733], Loss: 2.4730\n",
      "Epoch [100/300], Step [26100/27733], Loss: 3.7191\n",
      "Epoch [100/300], Step [26200/27733], Loss: 3.6016\n",
      "Epoch [100/300], Step [26300/27733], Loss: 3.1976\n",
      "Epoch [100/300], Step [26400/27733], Loss: 3.2546\n",
      "Epoch [100/300], Step [26500/27733], Loss: 2.5300\n",
      "Epoch [100/300], Step [26600/27733], Loss: 2.8190\n",
      "Epoch [100/300], Step [26700/27733], Loss: 3.8756\n",
      "Epoch [100/300], Step [26800/27733], Loss: 3.9662\n",
      "Epoch [100/300], Step [26900/27733], Loss: 3.2164\n",
      "Epoch [100/300], Step [27000/27733], Loss: 3.2989\n",
      "Epoch [100/300], Step [27100/27733], Loss: 3.1080\n",
      "Epoch [100/300], Step [27200/27733], Loss: 3.0060\n",
      "Epoch [100/300], Step [27300/27733], Loss: 2.0846\n",
      "Epoch [100/300], Step [27400/27733], Loss: 2.9712\n",
      "Epoch [100/300], Step [27500/27733], Loss: 3.1316\n",
      "Epoch [100/300], Step [27600/27733], Loss: 2.7259\n",
      "Epoch [100/300], Step [27700/27733], Loss: 3.0783\n",
      "Epoch [101/300], Step [100/27733], Loss: 2.9246\n",
      "Epoch [101/300], Step [200/27733], Loss: 2.1569\n",
      "Epoch [101/300], Step [300/27733], Loss: 1.5398\n",
      "Epoch [101/300], Step [400/27733], Loss: 2.2673\n",
      "Epoch [101/300], Step [500/27733], Loss: 2.6674\n",
      "Epoch [101/300], Step [600/27733], Loss: 2.1394\n",
      "Epoch [101/300], Step [700/27733], Loss: 2.4141\n",
      "Epoch [101/300], Step [800/27733], Loss: 3.0909\n",
      "Epoch [101/300], Step [900/27733], Loss: 3.3660\n",
      "Epoch [101/300], Step [1000/27733], Loss: 2.4697\n",
      "Epoch [101/300], Step [1100/27733], Loss: 2.4164\n",
      "Epoch [101/300], Step [1200/27733], Loss: 2.1986\n",
      "Epoch [101/300], Step [1300/27733], Loss: 2.4330\n",
      "Epoch [101/300], Step [1400/27733], Loss: 1.5476\n",
      "Epoch [101/300], Step [1500/27733], Loss: 2.6228\n",
      "Epoch [101/300], Step [1600/27733], Loss: 2.2292\n",
      "Epoch [101/300], Step [1700/27733], Loss: 2.5134\n",
      "Epoch [101/300], Step [1800/27733], Loss: 2.3461\n",
      "Epoch [101/300], Step [1900/27733], Loss: 2.5425\n",
      "Epoch [101/300], Step [2000/27733], Loss: 2.3895\n",
      "Epoch [101/300], Step [2100/27733], Loss: 2.2717\n",
      "Epoch [101/300], Step [2200/27733], Loss: 2.4935\n",
      "Epoch [101/300], Step [2300/27733], Loss: 2.4439\n",
      "Epoch [101/300], Step [2400/27733], Loss: 2.3661\n",
      "Epoch [101/300], Step [2500/27733], Loss: 2.2112\n",
      "Epoch [101/300], Step [2600/27733], Loss: 2.7606\n",
      "Epoch [101/300], Step [2700/27733], Loss: 2.3521\n",
      "Epoch [101/300], Step [2800/27733], Loss: 2.3414\n",
      "Epoch [101/300], Step [2900/27733], Loss: 2.0260\n",
      "Epoch [101/300], Step [3000/27733], Loss: 3.8658\n",
      "Epoch [101/300], Step [3100/27733], Loss: 3.0680\n",
      "Epoch [101/300], Step [3200/27733], Loss: 3.0931\n",
      "Epoch [101/300], Step [3300/27733], Loss: 2.8807\n",
      "Epoch [101/300], Step [3400/27733], Loss: 1.9495\n",
      "Epoch [101/300], Step [3500/27733], Loss: 2.7327\n",
      "Epoch [101/300], Step [3600/27733], Loss: 2.4419\n",
      "Epoch [101/300], Step [3700/27733], Loss: 2.0485\n",
      "Epoch [101/300], Step [3800/27733], Loss: 2.9834\n",
      "Epoch [101/300], Step [3900/27733], Loss: 2.1180\n",
      "Epoch [101/300], Step [4000/27733], Loss: 2.6100\n",
      "Epoch [101/300], Step [4100/27733], Loss: 3.6944\n",
      "Epoch [101/300], Step [4200/27733], Loss: 2.6043\n",
      "Epoch [101/300], Step [4300/27733], Loss: 2.3968\n",
      "Epoch [101/300], Step [4400/27733], Loss: 2.3731\n",
      "Epoch [101/300], Step [4500/27733], Loss: 3.0470\n",
      "Epoch [101/300], Step [4600/27733], Loss: 1.9597\n",
      "Epoch [101/300], Step [4700/27733], Loss: 2.6631\n",
      "Epoch [101/300], Step [4800/27733], Loss: 2.6371\n",
      "Epoch [101/300], Step [4900/27733], Loss: 2.7155\n",
      "Epoch [101/300], Step [5000/27733], Loss: 2.4613\n",
      "Epoch [101/300], Step [5100/27733], Loss: 3.2789\n",
      "Epoch [101/300], Step [5200/27733], Loss: 2.3398\n",
      "Epoch [101/300], Step [5300/27733], Loss: 2.4925\n",
      "Epoch [101/300], Step [5400/27733], Loss: 2.7051\n",
      "Epoch [101/300], Step [5500/27733], Loss: 3.3625\n",
      "Epoch [101/300], Step [5600/27733], Loss: 2.3474\n",
      "Epoch [101/300], Step [5700/27733], Loss: 2.6115\n",
      "Epoch [101/300], Step [5800/27733], Loss: 3.2798\n",
      "Epoch [101/300], Step [5900/27733], Loss: 3.0279\n",
      "Epoch [101/300], Step [6000/27733], Loss: 2.4832\n",
      "Epoch [101/300], Step [6100/27733], Loss: 3.3191\n",
      "Epoch [101/300], Step [6200/27733], Loss: 2.2041\n",
      "Epoch [101/300], Step [6300/27733], Loss: 3.0642\n",
      "Epoch [101/300], Step [6400/27733], Loss: 3.2573\n",
      "Epoch [101/300], Step [6500/27733], Loss: 2.7875\n",
      "Epoch [101/300], Step [6600/27733], Loss: 2.3648\n",
      "Epoch [101/300], Step [6700/27733], Loss: 2.8354\n",
      "Epoch [101/300], Step [6800/27733], Loss: 2.3433\n",
      "Epoch [101/300], Step [6900/27733], Loss: 2.2749\n",
      "Epoch [101/300], Step [7000/27733], Loss: 2.5879\n",
      "Epoch [101/300], Step [7100/27733], Loss: 3.3305\n",
      "Epoch [101/300], Step [7200/27733], Loss: 2.4937\n",
      "Epoch [101/300], Step [7300/27733], Loss: 2.9682\n",
      "Epoch [101/300], Step [7400/27733], Loss: 2.2253\n",
      "Epoch [101/300], Step [7500/27733], Loss: 2.4025\n",
      "Epoch [101/300], Step [7600/27733], Loss: 2.6465\n",
      "Epoch [101/300], Step [7700/27733], Loss: 2.6793\n",
      "Epoch [101/300], Step [7800/27733], Loss: 2.8898\n",
      "Epoch [101/300], Step [7900/27733], Loss: 2.7203\n",
      "Epoch [101/300], Step [8000/27733], Loss: 2.4862\n",
      "Epoch [101/300], Step [8100/27733], Loss: 3.3551\n",
      "Epoch [101/300], Step [8200/27733], Loss: 2.9187\n",
      "Epoch [101/300], Step [8300/27733], Loss: 2.7202\n",
      "Epoch [101/300], Step [8400/27733], Loss: 2.2951\n",
      "Epoch [101/300], Step [8500/27733], Loss: 2.9283\n",
      "Epoch [101/300], Step [8600/27733], Loss: 3.9746\n",
      "Epoch [101/300], Step [8700/27733], Loss: 3.3325\n",
      "Epoch [101/300], Step [8800/27733], Loss: 2.1950\n",
      "Epoch [101/300], Step [8900/27733], Loss: 2.4362\n",
      "Epoch [101/300], Step [9000/27733], Loss: 2.2442\n",
      "Epoch [101/300], Step [9100/27733], Loss: 2.7967\n",
      "Epoch [101/300], Step [9200/27733], Loss: 2.7436\n",
      "Epoch [101/300], Step [9300/27733], Loss: 2.5021\n",
      "Epoch [101/300], Step [9400/27733], Loss: 2.6220\n",
      "Epoch [101/300], Step [9500/27733], Loss: 2.9168\n",
      "Epoch [101/300], Step [9600/27733], Loss: 2.4700\n",
      "Epoch [101/300], Step [9700/27733], Loss: 3.2791\n",
      "Epoch [101/300], Step [9800/27733], Loss: 3.2217\n",
      "Epoch [101/300], Step [9900/27733], Loss: 2.9173\n",
      "Epoch [101/300], Step [10000/27733], Loss: 2.2022\n",
      "Epoch [101/300], Step [10100/27733], Loss: 3.5777\n",
      "Epoch [101/300], Step [10200/27733], Loss: 3.2974\n",
      "Epoch [101/300], Step [10300/27733], Loss: 2.8964\n",
      "Epoch [101/300], Step [10400/27733], Loss: 2.8932\n",
      "Epoch [101/300], Step [10500/27733], Loss: 3.1912\n",
      "Epoch [101/300], Step [10600/27733], Loss: 2.9681\n",
      "Epoch [101/300], Step [10700/27733], Loss: 2.3129\n",
      "Epoch [101/300], Step [10800/27733], Loss: 2.7107\n",
      "Epoch [101/300], Step [10900/27733], Loss: 2.4071\n",
      "Epoch [101/300], Step [11000/27733], Loss: 2.4705\n",
      "Epoch [101/300], Step [11100/27733], Loss: 2.7841\n",
      "Epoch [101/300], Step [11200/27733], Loss: 2.6752\n",
      "Epoch [101/300], Step [11300/27733], Loss: 2.0680\n",
      "Epoch [101/300], Step [11400/27733], Loss: 2.3333\n",
      "Epoch [101/300], Step [11500/27733], Loss: 2.5225\n",
      "Epoch [101/300], Step [11600/27733], Loss: 2.7541\n",
      "Epoch [101/300], Step [11700/27733], Loss: 3.1012\n",
      "Epoch [101/300], Step [11800/27733], Loss: 3.2834\n",
      "Epoch [101/300], Step [11900/27733], Loss: 2.9382\n",
      "Epoch [101/300], Step [12000/27733], Loss: 3.5557\n",
      "Epoch [101/300], Step [12100/27733], Loss: 2.9813\n",
      "Epoch [101/300], Step [12200/27733], Loss: 1.9494\n",
      "Epoch [101/300], Step [12300/27733], Loss: 2.7373\n",
      "Epoch [101/300], Step [12400/27733], Loss: 1.7635\n",
      "Epoch [101/300], Step [12500/27733], Loss: 2.5594\n",
      "Epoch [101/300], Step [12600/27733], Loss: 2.4856\n",
      "Epoch [101/300], Step [12700/27733], Loss: 3.8973\n",
      "Epoch [101/300], Step [12800/27733], Loss: 2.5566\n",
      "Epoch [101/300], Step [12900/27733], Loss: 2.9474\n",
      "Epoch [101/300], Step [13000/27733], Loss: 2.4400\n",
      "Epoch [101/300], Step [13100/27733], Loss: 2.0794\n",
      "Epoch [101/300], Step [13200/27733], Loss: 3.0672\n",
      "Epoch [101/300], Step [13300/27733], Loss: 2.6142\n",
      "Epoch [101/300], Step [13400/27733], Loss: 2.5335\n",
      "Epoch [101/300], Step [13500/27733], Loss: 2.4390\n",
      "Epoch [101/300], Step [13600/27733], Loss: 2.7346\n",
      "Epoch [101/300], Step [13700/27733], Loss: 2.3408\n",
      "Epoch [101/300], Step [13800/27733], Loss: 2.6530\n",
      "Epoch [101/300], Step [13900/27733], Loss: 2.9615\n",
      "Epoch [101/300], Step [14000/27733], Loss: 1.8221\n",
      "Epoch [101/300], Step [14100/27733], Loss: 2.2757\n",
      "Epoch [101/300], Step [14200/27733], Loss: 2.1227\n",
      "Epoch [101/300], Step [14300/27733], Loss: 3.3749\n",
      "Epoch [101/300], Step [14400/27733], Loss: 2.9880\n",
      "Epoch [101/300], Step [14500/27733], Loss: 3.0600\n",
      "Epoch [101/300], Step [14600/27733], Loss: 2.8687\n",
      "Epoch [101/300], Step [14700/27733], Loss: 2.7548\n",
      "Epoch [101/300], Step [14800/27733], Loss: 2.8026\n",
      "Epoch [101/300], Step [14900/27733], Loss: 3.2790\n",
      "Epoch [101/300], Step [15000/27733], Loss: 2.4174\n",
      "Epoch [101/300], Step [15100/27733], Loss: 2.4933\n",
      "Epoch [101/300], Step [15200/27733], Loss: 3.5912\n",
      "Epoch [101/300], Step [15300/27733], Loss: 2.4989\n",
      "Epoch [101/300], Step [15400/27733], Loss: 1.5801\n",
      "Epoch [101/300], Step [15500/27733], Loss: 2.9360\n",
      "Epoch [101/300], Step [15600/27733], Loss: 2.9959\n",
      "Epoch [101/300], Step [15700/27733], Loss: 2.8843\n",
      "Epoch [101/300], Step [15800/27733], Loss: 2.4477\n",
      "Epoch [101/300], Step [15900/27733], Loss: 2.5850\n",
      "Epoch [101/300], Step [16000/27733], Loss: 2.7978\n",
      "Epoch [101/300], Step [16100/27733], Loss: 3.1292\n",
      "Epoch [101/300], Step [16200/27733], Loss: 2.1755\n",
      "Epoch [101/300], Step [16300/27733], Loss: 3.0822\n",
      "Epoch [101/300], Step [16400/27733], Loss: 3.4275\n",
      "Epoch [101/300], Step [16500/27733], Loss: 2.8709\n",
      "Epoch [101/300], Step [16600/27733], Loss: 3.1713\n",
      "Epoch [101/300], Step [16700/27733], Loss: 2.9135\n",
      "Epoch [101/300], Step [16800/27733], Loss: 3.4377\n",
      "Epoch [101/300], Step [16900/27733], Loss: 2.6009\n",
      "Epoch [101/300], Step [17000/27733], Loss: 3.8188\n",
      "Epoch [101/300], Step [17100/27733], Loss: 2.9330\n",
      "Epoch [101/300], Step [17200/27733], Loss: 2.9216\n",
      "Epoch [101/300], Step [17300/27733], Loss: 2.8741\n",
      "Epoch [101/300], Step [17400/27733], Loss: 3.1643\n",
      "Epoch [101/300], Step [17500/27733], Loss: 2.4276\n",
      "Epoch [101/300], Step [17600/27733], Loss: 2.4157\n",
      "Epoch [101/300], Step [17700/27733], Loss: 3.2954\n",
      "Epoch [101/300], Step [17800/27733], Loss: 2.6227\n",
      "Epoch [101/300], Step [17900/27733], Loss: 2.9024\n",
      "Epoch [101/300], Step [18000/27733], Loss: 3.1099\n",
      "Epoch [101/300], Step [18100/27733], Loss: 2.7877\n",
      "Epoch [101/300], Step [18200/27733], Loss: 2.8921\n",
      "Epoch [101/300], Step [18300/27733], Loss: 2.3824\n",
      "Epoch [101/300], Step [18400/27733], Loss: 3.6395\n",
      "Epoch [101/300], Step [18500/27733], Loss: 2.4344\n",
      "Epoch [101/300], Step [18600/27733], Loss: 2.9790\n",
      "Epoch [101/300], Step [18700/27733], Loss: 2.9737\n",
      "Epoch [101/300], Step [18800/27733], Loss: 2.2436\n",
      "Epoch [101/300], Step [18900/27733], Loss: 3.4964\n",
      "Epoch [101/300], Step [19000/27733], Loss: 3.2475\n",
      "Epoch [101/300], Step [19100/27733], Loss: 3.1994\n",
      "Epoch [101/300], Step [19200/27733], Loss: 2.1581\n",
      "Epoch [101/300], Step [19300/27733], Loss: 2.5111\n",
      "Epoch [101/300], Step [19400/27733], Loss: 3.6582\n",
      "Epoch [101/300], Step [19500/27733], Loss: 2.6310\n",
      "Epoch [101/300], Step [19600/27733], Loss: 2.1988\n",
      "Epoch [101/300], Step [19700/27733], Loss: 2.9445\n",
      "Epoch [101/300], Step [19800/27733], Loss: 2.9872\n",
      "Epoch [101/300], Step [19900/27733], Loss: 2.9297\n",
      "Epoch [101/300], Step [20000/27733], Loss: 3.1724\n",
      "Epoch [101/300], Step [20100/27733], Loss: 2.1892\n",
      "Epoch [101/300], Step [20200/27733], Loss: 2.7612\n",
      "Epoch [101/300], Step [20300/27733], Loss: 2.5249\n",
      "Epoch [101/300], Step [20400/27733], Loss: 2.8035\n",
      "Epoch [101/300], Step [20500/27733], Loss: 3.6944\n",
      "Epoch [101/300], Step [20600/27733], Loss: 3.2492\n",
      "Epoch [101/300], Step [20700/27733], Loss: 3.0140\n",
      "Epoch [101/300], Step [20800/27733], Loss: 2.8649\n",
      "Epoch [101/300], Step [20900/27733], Loss: 2.4905\n",
      "Epoch [101/300], Step [21000/27733], Loss: 3.0248\n",
      "Epoch [101/300], Step [21100/27733], Loss: 2.5941\n",
      "Epoch [101/300], Step [21200/27733], Loss: 2.6638\n",
      "Epoch [101/300], Step [21300/27733], Loss: 2.4595\n",
      "Epoch [101/300], Step [21400/27733], Loss: 2.2368\n",
      "Epoch [101/300], Step [21500/27733], Loss: 3.8205\n",
      "Epoch [101/300], Step [21600/27733], Loss: 3.0501\n",
      "Epoch [101/300], Step [21700/27733], Loss: 3.3022\n",
      "Epoch [101/300], Step [21800/27733], Loss: 2.1018\n",
      "Epoch [101/300], Step [21900/27733], Loss: 2.7295\n",
      "Epoch [101/300], Step [22000/27733], Loss: 2.7188\n",
      "Epoch [101/300], Step [22100/27733], Loss: 2.4555\n",
      "Epoch [101/300], Step [22200/27733], Loss: 2.1800\n",
      "Epoch [101/300], Step [22300/27733], Loss: 3.7035\n",
      "Epoch [101/300], Step [22400/27733], Loss: 1.9962\n",
      "Epoch [101/300], Step [22500/27733], Loss: 1.9535\n",
      "Epoch [101/300], Step [22600/27733], Loss: 3.2706\n",
      "Epoch [101/300], Step [22700/27733], Loss: 1.7610\n",
      "Epoch [101/300], Step [22800/27733], Loss: 2.5965\n",
      "Epoch [101/300], Step [22900/27733], Loss: 3.0269\n",
      "Epoch [101/300], Step [23000/27733], Loss: 2.4727\n",
      "Epoch [101/300], Step [23100/27733], Loss: 3.1642\n",
      "Epoch [101/300], Step [23200/27733], Loss: 2.9766\n",
      "Epoch [101/300], Step [23300/27733], Loss: 3.4934\n",
      "Epoch [101/300], Step [23400/27733], Loss: 2.7262\n",
      "Epoch [101/300], Step [23500/27733], Loss: 2.5185\n",
      "Epoch [101/300], Step [23600/27733], Loss: 2.4194\n",
      "Epoch [101/300], Step [23700/27733], Loss: 1.9410\n",
      "Epoch [101/300], Step [23800/27733], Loss: 3.1217\n",
      "Epoch [101/300], Step [23900/27733], Loss: 3.3881\n",
      "Epoch [101/300], Step [24000/27733], Loss: 2.4062\n",
      "Epoch [101/300], Step [24100/27733], Loss: 3.0344\n",
      "Epoch [101/300], Step [24200/27733], Loss: 2.9187\n",
      "Epoch [101/300], Step [24300/27733], Loss: 3.0666\n",
      "Epoch [101/300], Step [24400/27733], Loss: 2.8625\n",
      "Epoch [101/300], Step [24500/27733], Loss: 3.8730\n",
      "Epoch [101/300], Step [24600/27733], Loss: 2.8676\n",
      "Epoch [101/300], Step [24700/27733], Loss: 3.2148\n",
      "Epoch [101/300], Step [24800/27733], Loss: 3.0121\n",
      "Epoch [101/300], Step [24900/27733], Loss: 2.5893\n",
      "Epoch [101/300], Step [25000/27733], Loss: 2.6536\n",
      "Epoch [101/300], Step [25100/27733], Loss: 3.4369\n",
      "Epoch [101/300], Step [25200/27733], Loss: 2.2411\n",
      "Epoch [101/300], Step [25300/27733], Loss: 2.7122\n",
      "Epoch [101/300], Step [25400/27733], Loss: 2.9216\n",
      "Epoch [101/300], Step [25500/27733], Loss: 2.0044\n",
      "Epoch [101/300], Step [25600/27733], Loss: 2.7658\n",
      "Epoch [101/300], Step [25700/27733], Loss: 4.1184\n",
      "Epoch [101/300], Step [25800/27733], Loss: 2.7642\n",
      "Epoch [101/300], Step [25900/27733], Loss: 2.7803\n",
      "Epoch [101/300], Step [26000/27733], Loss: 3.2028\n",
      "Epoch [101/300], Step [26100/27733], Loss: 2.6523\n",
      "Epoch [101/300], Step [26200/27733], Loss: 3.4114\n",
      "Epoch [101/300], Step [26300/27733], Loss: 3.4156\n",
      "Epoch [101/300], Step [26400/27733], Loss: 3.1317\n",
      "Epoch [101/300], Step [26500/27733], Loss: 2.5915\n",
      "Epoch [101/300], Step [26600/27733], Loss: 2.6010\n",
      "Epoch [101/300], Step [26700/27733], Loss: 2.2511\n",
      "Epoch [101/300], Step [26800/27733], Loss: 3.3109\n",
      "Epoch [101/300], Step [26900/27733], Loss: 3.1412\n",
      "Epoch [101/300], Step [27000/27733], Loss: 3.4140\n",
      "Epoch [101/300], Step [27100/27733], Loss: 2.7372\n",
      "Epoch [101/300], Step [27200/27733], Loss: 2.9671\n",
      "Epoch [101/300], Step [27300/27733], Loss: 2.5896\n",
      "Epoch [101/300], Step [27400/27733], Loss: 3.7877\n",
      "Epoch [101/300], Step [27500/27733], Loss: 2.8470\n",
      "Epoch [101/300], Step [27600/27733], Loss: 2.5859\n",
      "Epoch [101/300], Step [27700/27733], Loss: 3.8230\n",
      "Epoch [102/300], Step [100/27733], Loss: 2.4528\n",
      "Epoch [102/300], Step [200/27733], Loss: 2.7891\n",
      "Epoch [102/300], Step [300/27733], Loss: 2.4258\n",
      "Epoch [102/300], Step [400/27733], Loss: 2.3747\n",
      "Epoch [102/300], Step [500/27733], Loss: 3.0288\n",
      "Epoch [102/300], Step [600/27733], Loss: 2.2443\n",
      "Epoch [102/300], Step [700/27733], Loss: 2.5733\n",
      "Epoch [102/300], Step [800/27733], Loss: 3.2689\n",
      "Epoch [102/300], Step [900/27733], Loss: 2.1363\n",
      "Epoch [102/300], Step [1000/27733], Loss: 2.4328\n",
      "Epoch [102/300], Step [1100/27733], Loss: 2.5937\n",
      "Epoch [102/300], Step [1200/27733], Loss: 2.5443\n",
      "Epoch [102/300], Step [1300/27733], Loss: 2.6414\n",
      "Epoch [102/300], Step [1400/27733], Loss: 2.4253\n",
      "Epoch [102/300], Step [1500/27733], Loss: 3.0217\n",
      "Epoch [102/300], Step [1600/27733], Loss: 2.3694\n",
      "Epoch [102/300], Step [1700/27733], Loss: 1.9521\n",
      "Epoch [102/300], Step [1800/27733], Loss: 2.3405\n",
      "Epoch [102/300], Step [1900/27733], Loss: 2.7118\n",
      "Epoch [102/300], Step [2000/27733], Loss: 2.6616\n",
      "Epoch [102/300], Step [2100/27733], Loss: 2.1804\n",
      "Epoch [102/300], Step [2200/27733], Loss: 2.7511\n",
      "Epoch [102/300], Step [2300/27733], Loss: 3.2111\n",
      "Epoch [102/300], Step [2400/27733], Loss: 2.2304\n",
      "Epoch [102/300], Step [2500/27733], Loss: 2.0487\n",
      "Epoch [102/300], Step [2600/27733], Loss: 2.9791\n",
      "Epoch [102/300], Step [2700/27733], Loss: 2.4487\n",
      "Epoch [102/300], Step [2800/27733], Loss: 2.3667\n",
      "Epoch [102/300], Step [2900/27733], Loss: 2.5054\n",
      "Epoch [102/300], Step [3000/27733], Loss: 2.0880\n",
      "Epoch [102/300], Step [3100/27733], Loss: 2.6365\n",
      "Epoch [102/300], Step [3200/27733], Loss: 2.6284\n",
      "Epoch [102/300], Step [3300/27733], Loss: 3.0875\n",
      "Epoch [102/300], Step [3400/27733], Loss: 2.6773\n",
      "Epoch [102/300], Step [3500/27733], Loss: 2.6311\n",
      "Epoch [102/300], Step [3600/27733], Loss: 2.7431\n",
      "Epoch [102/300], Step [3700/27733], Loss: 2.3185\n",
      "Epoch [102/300], Step [3800/27733], Loss: 2.2896\n",
      "Epoch [102/300], Step [3900/27733], Loss: 2.2226\n",
      "Epoch [102/300], Step [4000/27733], Loss: 2.2236\n",
      "Epoch [102/300], Step [4100/27733], Loss: 3.5674\n",
      "Epoch [102/300], Step [4200/27733], Loss: 2.8063\n",
      "Epoch [102/300], Step [4300/27733], Loss: 2.4240\n",
      "Epoch [102/300], Step [4400/27733], Loss: 2.7162\n",
      "Epoch [102/300], Step [4500/27733], Loss: 2.4057\n",
      "Epoch [102/300], Step [4600/27733], Loss: 2.7238\n",
      "Epoch [102/300], Step [4700/27733], Loss: 2.6263\n",
      "Epoch [102/300], Step [4800/27733], Loss: 2.6217\n",
      "Epoch [102/300], Step [4900/27733], Loss: 2.6420\n",
      "Epoch [102/300], Step [5000/27733], Loss: 2.8682\n",
      "Epoch [102/300], Step [5100/27733], Loss: 2.6737\n",
      "Epoch [102/300], Step [5200/27733], Loss: 2.8825\n",
      "Epoch [102/300], Step [5300/27733], Loss: 3.8286\n",
      "Epoch [102/300], Step [5400/27733], Loss: 2.9748\n",
      "Epoch [102/300], Step [5500/27733], Loss: 2.6452\n",
      "Epoch [102/300], Step [5600/27733], Loss: 2.4081\n",
      "Epoch [102/300], Step [5700/27733], Loss: 2.9062\n",
      "Epoch [102/300], Step [5800/27733], Loss: 2.3941\n",
      "Epoch [102/300], Step [5900/27733], Loss: 3.3445\n",
      "Epoch [102/300], Step [6000/27733], Loss: 2.9485\n",
      "Epoch [102/300], Step [6100/27733], Loss: 1.9123\n",
      "Epoch [102/300], Step [6200/27733], Loss: 2.0796\n",
      "Epoch [102/300], Step [6300/27733], Loss: 2.8437\n",
      "Epoch [102/300], Step [6400/27733], Loss: 3.6942\n",
      "Epoch [102/300], Step [6500/27733], Loss: 2.3066\n",
      "Epoch [102/300], Step [6600/27733], Loss: 2.0963\n",
      "Epoch [102/300], Step [6700/27733], Loss: 2.3169\n",
      "Epoch [102/300], Step [6800/27733], Loss: 1.4659\n",
      "Epoch [102/300], Step [6900/27733], Loss: 2.9008\n",
      "Epoch [102/300], Step [7000/27733], Loss: 3.3686\n",
      "Epoch [102/300], Step [7100/27733], Loss: 3.1927\n",
      "Epoch [102/300], Step [7200/27733], Loss: 2.2112\n",
      "Epoch [102/300], Step [7300/27733], Loss: 2.7546\n",
      "Epoch [102/300], Step [7400/27733], Loss: 3.0428\n",
      "Epoch [102/300], Step [7500/27733], Loss: 2.4817\n",
      "Epoch [102/300], Step [7600/27733], Loss: 2.7216\n",
      "Epoch [102/300], Step [7700/27733], Loss: 2.4387\n",
      "Epoch [102/300], Step [7800/27733], Loss: 2.7122\n",
      "Epoch [102/300], Step [7900/27733], Loss: 2.8864\n",
      "Epoch [102/300], Step [8000/27733], Loss: 3.2478\n",
      "Epoch [102/300], Step [8100/27733], Loss: 2.7197\n",
      "Epoch [102/300], Step [8200/27733], Loss: 2.1495\n",
      "Epoch [102/300], Step [8300/27733], Loss: 3.3303\n",
      "Epoch [102/300], Step [8400/27733], Loss: 2.5980\n",
      "Epoch [102/300], Step [8500/27733], Loss: 2.3818\n",
      "Epoch [102/300], Step [8600/27733], Loss: 3.4890\n",
      "Epoch [102/300], Step [8700/27733], Loss: 2.6459\n",
      "Epoch [102/300], Step [8800/27733], Loss: 2.1311\n",
      "Epoch [102/300], Step [8900/27733], Loss: 2.2758\n",
      "Epoch [102/300], Step [9000/27733], Loss: 2.7466\n",
      "Epoch [102/300], Step [9100/27733], Loss: 2.7047\n",
      "Epoch [102/300], Step [9200/27733], Loss: 3.4915\n",
      "Epoch [102/300], Step [9300/27733], Loss: 2.9075\n",
      "Epoch [102/300], Step [9400/27733], Loss: 2.0122\n",
      "Epoch [102/300], Step [9500/27733], Loss: 2.6391\n",
      "Epoch [102/300], Step [9600/27733], Loss: 3.2081\n",
      "Epoch [102/300], Step [9700/27733], Loss: 2.8783\n",
      "Epoch [102/300], Step [9800/27733], Loss: 2.8112\n",
      "Epoch [102/300], Step [9900/27733], Loss: 2.2349\n",
      "Epoch [102/300], Step [10000/27733], Loss: 2.9893\n",
      "Epoch [102/300], Step [10100/27733], Loss: 2.5746\n",
      "Epoch [102/300], Step [10200/27733], Loss: 2.5367\n",
      "Epoch [102/300], Step [10300/27733], Loss: 2.3809\n",
      "Epoch [102/300], Step [10400/27733], Loss: 2.7443\n",
      "Epoch [102/300], Step [10500/27733], Loss: 2.8181\n",
      "Epoch [102/300], Step [10600/27733], Loss: 2.8418\n",
      "Epoch [102/300], Step [10700/27733], Loss: 2.1981\n",
      "Epoch [102/300], Step [10800/27733], Loss: 3.2766\n",
      "Epoch [102/300], Step [10900/27733], Loss: 3.4984\n",
      "Epoch [102/300], Step [11000/27733], Loss: 2.7730\n",
      "Epoch [102/300], Step [11100/27733], Loss: 3.8950\n",
      "Epoch [102/300], Step [11200/27733], Loss: 2.9063\n",
      "Epoch [102/300], Step [11300/27733], Loss: 3.5080\n",
      "Epoch [102/300], Step [11400/27733], Loss: 2.0034\n",
      "Epoch [102/300], Step [11500/27733], Loss: 3.1358\n",
      "Epoch [102/300], Step [11600/27733], Loss: 2.8950\n",
      "Epoch [102/300], Step [11700/27733], Loss: 2.2781\n",
      "Epoch [102/300], Step [11800/27733], Loss: 2.5814\n",
      "Epoch [102/300], Step [11900/27733], Loss: 3.3830\n",
      "Epoch [102/300], Step [12000/27733], Loss: 3.1860\n",
      "Epoch [102/300], Step [12100/27733], Loss: 2.5439\n",
      "Epoch [102/300], Step [12200/27733], Loss: 2.6124\n",
      "Epoch [102/300], Step [12300/27733], Loss: 2.4678\n",
      "Epoch [102/300], Step [12400/27733], Loss: 3.0028\n",
      "Epoch [102/300], Step [12500/27733], Loss: 2.9374\n",
      "Epoch [102/300], Step [12600/27733], Loss: 2.7298\n",
      "Epoch [102/300], Step [12700/27733], Loss: 3.0587\n",
      "Epoch [102/300], Step [12800/27733], Loss: 3.0702\n",
      "Epoch [102/300], Step [12900/27733], Loss: 3.4015\n",
      "Epoch [102/300], Step [13000/27733], Loss: 2.9804\n",
      "Epoch [102/300], Step [13100/27733], Loss: 3.6479\n",
      "Epoch [102/300], Step [13200/27733], Loss: 2.5128\n",
      "Epoch [102/300], Step [13300/27733], Loss: 2.2626\n",
      "Epoch [102/300], Step [13400/27733], Loss: 2.6652\n",
      "Epoch [102/300], Step [13500/27733], Loss: 2.8099\n",
      "Epoch [102/300], Step [13600/27733], Loss: 2.0819\n",
      "Epoch [102/300], Step [13700/27733], Loss: 3.2913\n",
      "Epoch [102/300], Step [13800/27733], Loss: 2.9260\n",
      "Epoch [102/300], Step [13900/27733], Loss: 3.4155\n",
      "Epoch [102/300], Step [14000/27733], Loss: 2.2155\n",
      "Epoch [102/300], Step [14100/27733], Loss: 3.1148\n",
      "Epoch [102/300], Step [14200/27733], Loss: 3.0018\n",
      "Epoch [102/300], Step [14300/27733], Loss: 3.3995\n",
      "Epoch [102/300], Step [14400/27733], Loss: 3.8707\n",
      "Epoch [102/300], Step [14500/27733], Loss: 2.5082\n",
      "Epoch [102/300], Step [14600/27733], Loss: 2.4646\n",
      "Epoch [102/300], Step [14700/27733], Loss: 2.3455\n",
      "Epoch [102/300], Step [14800/27733], Loss: 2.0705\n",
      "Epoch [102/300], Step [14900/27733], Loss: 2.6266\n",
      "Epoch [102/300], Step [15000/27733], Loss: 3.4500\n",
      "Epoch [102/300], Step [15100/27733], Loss: 2.4142\n",
      "Epoch [102/300], Step [15200/27733], Loss: 2.2319\n",
      "Epoch [102/300], Step [15300/27733], Loss: 2.2774\n",
      "Epoch [102/300], Step [15400/27733], Loss: 2.8268\n",
      "Epoch [102/300], Step [15500/27733], Loss: 2.7849\n",
      "Epoch [102/300], Step [15600/27733], Loss: 2.3940\n",
      "Epoch [102/300], Step [15700/27733], Loss: 3.0664\n",
      "Epoch [102/300], Step [15800/27733], Loss: 2.5973\n",
      "Epoch [102/300], Step [15900/27733], Loss: 3.5366\n",
      "Epoch [102/300], Step [16000/27733], Loss: 3.5732\n",
      "Epoch [102/300], Step [16100/27733], Loss: 2.8733\n",
      "Epoch [102/300], Step [16200/27733], Loss: 3.2142\n",
      "Epoch [102/300], Step [16300/27733], Loss: 3.0085\n",
      "Epoch [102/300], Step [16400/27733], Loss: 2.8814\n",
      "Epoch [102/300], Step [16500/27733], Loss: 2.7446\n",
      "Epoch [102/300], Step [16600/27733], Loss: 2.3844\n",
      "Epoch [102/300], Step [16700/27733], Loss: 2.8677\n",
      "Epoch [102/300], Step [16800/27733], Loss: 3.4044\n",
      "Epoch [102/300], Step [16900/27733], Loss: 3.4529\n",
      "Epoch [102/300], Step [17000/27733], Loss: 3.8180\n",
      "Epoch [102/300], Step [17100/27733], Loss: 3.8962\n",
      "Epoch [102/300], Step [17200/27733], Loss: 2.1092\n",
      "Epoch [102/300], Step [17300/27733], Loss: 3.3576\n",
      "Epoch [102/300], Step [17400/27733], Loss: 3.0255\n",
      "Epoch [102/300], Step [17500/27733], Loss: 2.5802\n",
      "Epoch [102/300], Step [17600/27733], Loss: 2.3868\n",
      "Epoch [102/300], Step [17700/27733], Loss: 3.5326\n",
      "Epoch [102/300], Step [17800/27733], Loss: 3.6205\n",
      "Epoch [102/300], Step [17900/27733], Loss: 3.4248\n",
      "Epoch [102/300], Step [18000/27733], Loss: 2.8900\n",
      "Epoch [102/300], Step [18100/27733], Loss: 3.0442\n",
      "Epoch [102/300], Step [18200/27733], Loss: 2.7026\n",
      "Epoch [102/300], Step [18300/27733], Loss: 2.6470\n",
      "Epoch [102/300], Step [18400/27733], Loss: 3.3741\n",
      "Epoch [102/300], Step [18500/27733], Loss: 2.2355\n",
      "Epoch [102/300], Step [18600/27733], Loss: 2.5704\n",
      "Epoch [102/300], Step [18700/27733], Loss: 3.1310\n",
      "Epoch [102/300], Step [18800/27733], Loss: 2.8082\n",
      "Epoch [102/300], Step [18900/27733], Loss: 2.7354\n",
      "Epoch [102/300], Step [19000/27733], Loss: 3.2299\n",
      "Epoch [102/300], Step [19100/27733], Loss: 2.9443\n",
      "Epoch [102/300], Step [19200/27733], Loss: 2.4108\n",
      "Epoch [102/300], Step [19300/27733], Loss: 2.8539\n",
      "Epoch [102/300], Step [19400/27733], Loss: 3.2313\n",
      "Epoch [102/300], Step [19500/27733], Loss: 2.6245\n",
      "Epoch [102/300], Step [19600/27733], Loss: 3.0658\n",
      "Epoch [102/300], Step [19700/27733], Loss: 2.6572\n",
      "Epoch [102/300], Step [19800/27733], Loss: 2.4522\n",
      "Epoch [102/300], Step [19900/27733], Loss: 2.7038\n",
      "Epoch [102/300], Step [20000/27733], Loss: 3.0809\n",
      "Epoch [102/300], Step [20100/27733], Loss: 2.9542\n",
      "Epoch [102/300], Step [20200/27733], Loss: 2.3400\n",
      "Epoch [102/300], Step [20300/27733], Loss: 3.5288\n",
      "Epoch [102/300], Step [20400/27733], Loss: 3.2146\n",
      "Epoch [102/300], Step [20500/27733], Loss: 3.7202\n",
      "Epoch [102/300], Step [20600/27733], Loss: 2.3612\n",
      "Epoch [102/300], Step [20700/27733], Loss: 3.6952\n",
      "Epoch [102/300], Step [20800/27733], Loss: 3.4398\n",
      "Epoch [102/300], Step [20900/27733], Loss: 2.6142\n",
      "Epoch [102/300], Step [21000/27733], Loss: 3.3233\n",
      "Epoch [102/300], Step [21100/27733], Loss: 2.5191\n",
      "Epoch [102/300], Step [21200/27733], Loss: 2.0337\n",
      "Epoch [102/300], Step [21300/27733], Loss: 2.3362\n",
      "Epoch [102/300], Step [21400/27733], Loss: 2.7794\n",
      "Epoch [102/300], Step [21500/27733], Loss: 3.3554\n",
      "Epoch [102/300], Step [21600/27733], Loss: 3.0536\n",
      "Epoch [102/300], Step [21700/27733], Loss: 2.7073\n",
      "Epoch [102/300], Step [21800/27733], Loss: 1.9142\n",
      "Epoch [102/300], Step [21900/27733], Loss: 3.1599\n",
      "Epoch [102/300], Step [22000/27733], Loss: 3.2654\n",
      "Epoch [102/300], Step [22100/27733], Loss: 2.1070\n",
      "Epoch [102/300], Step [22200/27733], Loss: 2.6885\n",
      "Epoch [102/300], Step [22300/27733], Loss: 2.5688\n",
      "Epoch [102/300], Step [22400/27733], Loss: 2.1899\n",
      "Epoch [102/300], Step [22500/27733], Loss: 3.6176\n",
      "Epoch [102/300], Step [22600/27733], Loss: 2.9437\n",
      "Epoch [102/300], Step [22700/27733], Loss: 3.0568\n",
      "Epoch [102/300], Step [22800/27733], Loss: 3.0451\n",
      "Epoch [102/300], Step [22900/27733], Loss: 2.6861\n",
      "Epoch [102/300], Step [23000/27733], Loss: 2.8293\n",
      "Epoch [102/300], Step [23100/27733], Loss: 3.2546\n",
      "Epoch [102/300], Step [23200/27733], Loss: 3.7424\n",
      "Epoch [102/300], Step [23300/27733], Loss: 2.7213\n",
      "Epoch [102/300], Step [23400/27733], Loss: 2.0562\n",
      "Epoch [102/300], Step [23500/27733], Loss: 2.6098\n",
      "Epoch [102/300], Step [23600/27733], Loss: 2.4269\n",
      "Epoch [102/300], Step [23700/27733], Loss: 3.1751\n",
      "Epoch [102/300], Step [23800/27733], Loss: 3.7535\n",
      "Epoch [102/300], Step [23900/27733], Loss: 2.2240\n",
      "Epoch [102/300], Step [24000/27733], Loss: 2.9423\n",
      "Epoch [102/300], Step [24100/27733], Loss: 3.1544\n",
      "Epoch [102/300], Step [24200/27733], Loss: 2.7466\n",
      "Epoch [102/300], Step [24300/27733], Loss: 3.5929\n",
      "Epoch [102/300], Step [24400/27733], Loss: 2.8278\n",
      "Epoch [102/300], Step [24500/27733], Loss: 2.9175\n",
      "Epoch [102/300], Step [24600/27733], Loss: 3.0088\n",
      "Epoch [102/300], Step [24700/27733], Loss: 3.2323\n",
      "Epoch [102/300], Step [24800/27733], Loss: 3.2733\n",
      "Epoch [102/300], Step [24900/27733], Loss: 3.4270\n",
      "Epoch [102/300], Step [25000/27733], Loss: 3.3168\n",
      "Epoch [102/300], Step [25100/27733], Loss: 3.3236\n",
      "Epoch [102/300], Step [25200/27733], Loss: 3.6506\n",
      "Epoch [102/300], Step [25300/27733], Loss: 2.0407\n",
      "Epoch [102/300], Step [25400/27733], Loss: 3.3821\n",
      "Epoch [102/300], Step [25500/27733], Loss: 2.3185\n",
      "Epoch [102/300], Step [25600/27733], Loss: 3.1937\n",
      "Epoch [102/300], Step [25700/27733], Loss: 2.9476\n",
      "Epoch [102/300], Step [25800/27733], Loss: 2.3465\n",
      "Epoch [102/300], Step [25900/27733], Loss: 2.9983\n",
      "Epoch [102/300], Step [26000/27733], Loss: 3.0900\n",
      "Epoch [102/300], Step [26100/27733], Loss: 3.0770\n",
      "Epoch [102/300], Step [26200/27733], Loss: 3.6415\n",
      "Epoch [102/300], Step [26300/27733], Loss: 2.9843\n",
      "Epoch [102/300], Step [26400/27733], Loss: 3.3748\n",
      "Epoch [102/300], Step [26500/27733], Loss: 2.5561\n",
      "Epoch [102/300], Step [26600/27733], Loss: 2.9976\n",
      "Epoch [102/300], Step [26700/27733], Loss: 3.0894\n",
      "Epoch [102/300], Step [26800/27733], Loss: 3.3254\n",
      "Epoch [102/300], Step [26900/27733], Loss: 2.3016\n",
      "Epoch [102/300], Step [27000/27733], Loss: 2.9081\n",
      "Epoch [102/300], Step [27100/27733], Loss: 2.3389\n",
      "Epoch [102/300], Step [27200/27733], Loss: 2.6110\n",
      "Epoch [102/300], Step [27300/27733], Loss: 2.6788\n",
      "Epoch [102/300], Step [27400/27733], Loss: 2.3152\n",
      "Epoch [102/300], Step [27500/27733], Loss: 2.9899\n",
      "Epoch [102/300], Step [27600/27733], Loss: 2.7329\n",
      "Epoch [102/300], Step [27700/27733], Loss: 3.4738\n",
      "Epoch [103/300], Step [100/27733], Loss: 2.4107\n",
      "Epoch [103/300], Step [200/27733], Loss: 2.5598\n",
      "Epoch [103/300], Step [300/27733], Loss: 2.7058\n",
      "Epoch [103/300], Step [400/27733], Loss: 2.1594\n",
      "Epoch [103/300], Step [500/27733], Loss: 1.9961\n",
      "Epoch [103/300], Step [600/27733], Loss: 2.3138\n",
      "Epoch [103/300], Step [700/27733], Loss: 1.7662\n",
      "Epoch [103/300], Step [800/27733], Loss: 2.5611\n",
      "Epoch [103/300], Step [900/27733], Loss: 3.3160\n",
      "Epoch [103/300], Step [1000/27733], Loss: 3.1796\n",
      "Epoch [103/300], Step [1100/27733], Loss: 2.2564\n",
      "Epoch [103/300], Step [1200/27733], Loss: 1.8884\n",
      "Epoch [103/300], Step [1300/27733], Loss: 2.3835\n",
      "Epoch [103/300], Step [1400/27733], Loss: 2.3043\n",
      "Epoch [103/300], Step [1500/27733], Loss: 2.0850\n",
      "Epoch [103/300], Step [1600/27733], Loss: 2.4308\n",
      "Epoch [103/300], Step [1700/27733], Loss: 2.7429\n",
      "Epoch [103/300], Step [1800/27733], Loss: 2.4883\n",
      "Epoch [103/300], Step [1900/27733], Loss: 2.5442\n",
      "Epoch [103/300], Step [2000/27733], Loss: 2.7975\n",
      "Epoch [103/300], Step [2100/27733], Loss: 2.5874\n",
      "Epoch [103/300], Step [2200/27733], Loss: 2.8236\n",
      "Epoch [103/300], Step [2300/27733], Loss: 2.4924\n",
      "Epoch [103/300], Step [2400/27733], Loss: 2.0225\n",
      "Epoch [103/300], Step [2500/27733], Loss: 1.7339\n",
      "Epoch [103/300], Step [2600/27733], Loss: 2.1073\n",
      "Epoch [103/300], Step [2700/27733], Loss: 2.7699\n",
      "Epoch [103/300], Step [2800/27733], Loss: 2.3360\n",
      "Epoch [103/300], Step [2900/27733], Loss: 1.5304\n",
      "Epoch [103/300], Step [3000/27733], Loss: 2.4827\n",
      "Epoch [103/300], Step [3100/27733], Loss: 2.3941\n",
      "Epoch [103/300], Step [3200/27733], Loss: 1.6451\n",
      "Epoch [103/300], Step [3300/27733], Loss: 1.8373\n",
      "Epoch [103/300], Step [3400/27733], Loss: 2.4679\n",
      "Epoch [103/300], Step [3500/27733], Loss: 2.2994\n",
      "Epoch [103/300], Step [3600/27733], Loss: 1.6389\n",
      "Epoch [103/300], Step [3700/27733], Loss: 2.2033\n",
      "Epoch [103/300], Step [3800/27733], Loss: 2.3315\n",
      "Epoch [103/300], Step [3900/27733], Loss: 3.1764\n",
      "Epoch [103/300], Step [4000/27733], Loss: 3.6677\n",
      "Epoch [103/300], Step [4100/27733], Loss: 2.7001\n",
      "Epoch [103/300], Step [4200/27733], Loss: 2.5086\n",
      "Epoch [103/300], Step [4300/27733], Loss: 2.1249\n",
      "Epoch [103/300], Step [4400/27733], Loss: 3.6550\n",
      "Epoch [103/300], Step [4500/27733], Loss: 3.0060\n",
      "Epoch [103/300], Step [4600/27733], Loss: 2.9122\n",
      "Epoch [103/300], Step [4700/27733], Loss: 2.4552\n",
      "Epoch [103/300], Step [4800/27733], Loss: 2.4633\n",
      "Epoch [103/300], Step [4900/27733], Loss: 1.9808\n",
      "Epoch [103/300], Step [5000/27733], Loss: 2.4685\n",
      "Epoch [103/300], Step [5100/27733], Loss: 2.5039\n",
      "Epoch [103/300], Step [5200/27733], Loss: 2.4695\n",
      "Epoch [103/300], Step [5300/27733], Loss: 2.4261\n",
      "Epoch [103/300], Step [5400/27733], Loss: 2.5039\n",
      "Epoch [103/300], Step [5500/27733], Loss: 4.1727\n",
      "Epoch [103/300], Step [5600/27733], Loss: 2.9868\n",
      "Epoch [103/300], Step [5700/27733], Loss: 3.3401\n",
      "Epoch [103/300], Step [5800/27733], Loss: 1.7529\n",
      "Epoch [103/300], Step [5900/27733], Loss: 2.6896\n",
      "Epoch [103/300], Step [6000/27733], Loss: 2.4439\n",
      "Epoch [103/300], Step [6100/27733], Loss: 2.4104\n",
      "Epoch [103/300], Step [6200/27733], Loss: 2.4647\n",
      "Epoch [103/300], Step [6300/27733], Loss: 2.2671\n",
      "Epoch [103/300], Step [6400/27733], Loss: 2.3332\n",
      "Epoch [103/300], Step [6500/27733], Loss: 2.9812\n",
      "Epoch [103/300], Step [6600/27733], Loss: 2.5070\n",
      "Epoch [103/300], Step [6700/27733], Loss: 2.9563\n",
      "Epoch [103/300], Step [6800/27733], Loss: 3.2863\n",
      "Epoch [103/300], Step [6900/27733], Loss: 2.8797\n",
      "Epoch [103/300], Step [7000/27733], Loss: 2.9760\n",
      "Epoch [103/300], Step [7100/27733], Loss: 2.4712\n",
      "Epoch [103/300], Step [7200/27733], Loss: 2.8432\n",
      "Epoch [103/300], Step [7300/27733], Loss: 2.4138\n",
      "Epoch [103/300], Step [7400/27733], Loss: 2.3701\n",
      "Epoch [103/300], Step [7500/27733], Loss: 1.9653\n",
      "Epoch [103/300], Step [7600/27733], Loss: 2.3574\n",
      "Epoch [103/300], Step [7700/27733], Loss: 2.6433\n",
      "Epoch [103/300], Step [7800/27733], Loss: 2.9177\n",
      "Epoch [103/300], Step [7900/27733], Loss: 2.6098\n",
      "Epoch [103/300], Step [8000/27733], Loss: 2.9783\n",
      "Epoch [103/300], Step [8100/27733], Loss: 2.6217\n",
      "Epoch [103/300], Step [8200/27733], Loss: 3.2545\n",
      "Epoch [103/300], Step [8300/27733], Loss: 2.1182\n",
      "Epoch [103/300], Step [8400/27733], Loss: 2.6377\n",
      "Epoch [103/300], Step [8500/27733], Loss: 2.9231\n",
      "Epoch [103/300], Step [8600/27733], Loss: 2.8156\n",
      "Epoch [103/300], Step [8700/27733], Loss: 3.7534\n",
      "Epoch [103/300], Step [8800/27733], Loss: 1.9211\n",
      "Epoch [103/300], Step [8900/27733], Loss: 2.5540\n",
      "Epoch [103/300], Step [9000/27733], Loss: 3.0576\n",
      "Epoch [103/300], Step [9100/27733], Loss: 2.8249\n",
      "Epoch [103/300], Step [9200/27733], Loss: 2.5171\n",
      "Epoch [103/300], Step [9300/27733], Loss: 3.4120\n",
      "Epoch [103/300], Step [9400/27733], Loss: 3.2217\n",
      "Epoch [103/300], Step [9500/27733], Loss: 3.0872\n",
      "Epoch [103/300], Step [9600/27733], Loss: 2.6004\n",
      "Epoch [103/300], Step [9700/27733], Loss: 3.6618\n",
      "Epoch [103/300], Step [9800/27733], Loss: 2.8197\n",
      "Epoch [103/300], Step [9900/27733], Loss: 2.0464\n",
      "Epoch [103/300], Step [10000/27733], Loss: 2.4945\n",
      "Epoch [103/300], Step [10100/27733], Loss: 3.1806\n",
      "Epoch [103/300], Step [10200/27733], Loss: 2.1752\n",
      "Epoch [103/300], Step [10300/27733], Loss: 2.5049\n",
      "Epoch [103/300], Step [10400/27733], Loss: 3.3358\n",
      "Epoch [103/300], Step [10500/27733], Loss: 3.8482\n",
      "Epoch [103/300], Step [10600/27733], Loss: 2.1724\n",
      "Epoch [103/300], Step [10700/27733], Loss: 2.5046\n",
      "Epoch [103/300], Step [10800/27733], Loss: 2.7856\n",
      "Epoch [103/300], Step [10900/27733], Loss: 2.1205\n",
      "Epoch [103/300], Step [11000/27733], Loss: 2.6901\n",
      "Epoch [103/300], Step [11100/27733], Loss: 2.5085\n",
      "Epoch [103/300], Step [11200/27733], Loss: 3.1813\n",
      "Epoch [103/300], Step [11300/27733], Loss: 2.3371\n",
      "Epoch [103/300], Step [11400/27733], Loss: 2.9959\n",
      "Epoch [103/300], Step [11500/27733], Loss: 3.3611\n",
      "Epoch [103/300], Step [11600/27733], Loss: 2.5389\n",
      "Epoch [103/300], Step [11700/27733], Loss: 2.4221\n",
      "Epoch [103/300], Step [11800/27733], Loss: 2.8680\n",
      "Epoch [103/300], Step [11900/27733], Loss: 2.0795\n",
      "Epoch [103/300], Step [12000/27733], Loss: 2.8317\n",
      "Epoch [103/300], Step [12100/27733], Loss: 2.8764\n",
      "Epoch [103/300], Step [12200/27733], Loss: 2.9254\n",
      "Epoch [103/300], Step [12300/27733], Loss: 2.3124\n",
      "Epoch [103/300], Step [12400/27733], Loss: 2.7969\n",
      "Epoch [103/300], Step [12500/27733], Loss: 2.0154\n",
      "Epoch [103/300], Step [12600/27733], Loss: 2.9910\n",
      "Epoch [103/300], Step [12700/27733], Loss: 3.5131\n",
      "Epoch [103/300], Step [12800/27733], Loss: 3.1851\n",
      "Epoch [103/300], Step [12900/27733], Loss: 2.6710\n",
      "Epoch [103/300], Step [13000/27733], Loss: 2.6694\n",
      "Epoch [103/300], Step [13100/27733], Loss: 3.9379\n",
      "Epoch [103/300], Step [13200/27733], Loss: 2.7341\n",
      "Epoch [103/300], Step [13300/27733], Loss: 2.6558\n",
      "Epoch [103/300], Step [13400/27733], Loss: 2.5199\n",
      "Epoch [103/300], Step [13500/27733], Loss: 3.1762\n",
      "Epoch [103/300], Step [13600/27733], Loss: 2.9654\n",
      "Epoch [103/300], Step [13700/27733], Loss: 2.1110\n",
      "Epoch [103/300], Step [13800/27733], Loss: 2.3190\n",
      "Epoch [103/300], Step [13900/27733], Loss: 2.8821\n",
      "Epoch [103/300], Step [14000/27733], Loss: 3.1096\n",
      "Epoch [103/300], Step [14100/27733], Loss: 2.3242\n",
      "Epoch [103/300], Step [14200/27733], Loss: 3.2842\n",
      "Epoch [103/300], Step [14300/27733], Loss: 2.5136\n",
      "Epoch [103/300], Step [14400/27733], Loss: 3.4395\n",
      "Epoch [103/300], Step [14500/27733], Loss: 2.0806\n",
      "Epoch [103/300], Step [14600/27733], Loss: 3.0836\n",
      "Epoch [103/300], Step [14700/27733], Loss: 2.6100\n",
      "Epoch [103/300], Step [14800/27733], Loss: 3.1504\n",
      "Epoch [103/300], Step [14900/27733], Loss: 3.1713\n",
      "Epoch [103/300], Step [15000/27733], Loss: 2.0941\n",
      "Epoch [103/300], Step [15100/27733], Loss: 2.5150\n",
      "Epoch [103/300], Step [15200/27733], Loss: 2.9978\n",
      "Epoch [103/300], Step [15300/27733], Loss: 4.2053\n",
      "Epoch [103/300], Step [15400/27733], Loss: 3.4469\n",
      "Epoch [103/300], Step [15500/27733], Loss: 4.5565\n",
      "Epoch [103/300], Step [15600/27733], Loss: 2.5396\n",
      "Epoch [103/300], Step [15700/27733], Loss: 3.2516\n",
      "Epoch [103/300], Step [15800/27733], Loss: 2.7511\n",
      "Epoch [103/300], Step [15900/27733], Loss: 3.9258\n",
      "Epoch [103/300], Step [16000/27733], Loss: 3.1442\n",
      "Epoch [103/300], Step [16100/27733], Loss: 2.3403\n",
      "Epoch [103/300], Step [16200/27733], Loss: 3.4359\n",
      "Epoch [103/300], Step [16300/27733], Loss: 2.8162\n",
      "Epoch [103/300], Step [16400/27733], Loss: 2.5096\n",
      "Epoch [103/300], Step [16500/27733], Loss: 2.8573\n",
      "Epoch [103/300], Step [16600/27733], Loss: 2.2283\n",
      "Epoch [103/300], Step [16700/27733], Loss: 3.0438\n",
      "Epoch [103/300], Step [16800/27733], Loss: 3.1873\n",
      "Epoch [103/300], Step [16900/27733], Loss: 2.4188\n",
      "Epoch [103/300], Step [17000/27733], Loss: 3.2837\n",
      "Epoch [103/300], Step [17100/27733], Loss: 2.4400\n",
      "Epoch [103/300], Step [17200/27733], Loss: 3.1865\n",
      "Epoch [103/300], Step [17300/27733], Loss: 2.4538\n",
      "Epoch [103/300], Step [17400/27733], Loss: 2.9899\n",
      "Epoch [103/300], Step [17500/27733], Loss: 3.0879\n",
      "Epoch [103/300], Step [17600/27733], Loss: 2.9102\n",
      "Epoch [103/300], Step [17700/27733], Loss: 2.1775\n",
      "Epoch [103/300], Step [17800/27733], Loss: 4.1037\n",
      "Epoch [103/300], Step [17900/27733], Loss: 2.8421\n",
      "Epoch [103/300], Step [18000/27733], Loss: 2.7778\n",
      "Epoch [103/300], Step [18100/27733], Loss: 3.0554\n",
      "Epoch [103/300], Step [18200/27733], Loss: 2.6019\n",
      "Epoch [103/300], Step [18300/27733], Loss: 2.0894\n",
      "Epoch [103/300], Step [18400/27733], Loss: 2.9200\n",
      "Epoch [103/300], Step [18500/27733], Loss: 3.5541\n",
      "Epoch [103/300], Step [18600/27733], Loss: 2.9433\n",
      "Epoch [103/300], Step [18700/27733], Loss: 3.4382\n",
      "Epoch [103/300], Step [18800/27733], Loss: 2.9261\n",
      "Epoch [103/300], Step [18900/27733], Loss: 3.0967\n",
      "Epoch [103/300], Step [19000/27733], Loss: 2.5265\n",
      "Epoch [103/300], Step [19100/27733], Loss: 3.2546\n",
      "Epoch [103/300], Step [19200/27733], Loss: 2.7168\n",
      "Epoch [103/300], Step [19300/27733], Loss: 2.7589\n",
      "Epoch [103/300], Step [19400/27733], Loss: 2.6053\n",
      "Epoch [103/300], Step [19500/27733], Loss: 3.1024\n",
      "Epoch [103/300], Step [19600/27733], Loss: 2.5410\n",
      "Epoch [103/300], Step [19700/27733], Loss: 2.6280\n",
      "Epoch [103/300], Step [19800/27733], Loss: 3.5685\n",
      "Epoch [103/300], Step [19900/27733], Loss: 2.6764\n",
      "Epoch [103/300], Step [20000/27733], Loss: 2.9432\n",
      "Epoch [103/300], Step [20100/27733], Loss: 2.9624\n",
      "Epoch [103/300], Step [20200/27733], Loss: 2.8273\n",
      "Epoch [103/300], Step [20300/27733], Loss: 3.2144\n",
      "Epoch [103/300], Step [20400/27733], Loss: 3.1059\n",
      "Epoch [103/300], Step [20500/27733], Loss: 2.7703\n",
      "Epoch [103/300], Step [20600/27733], Loss: 2.8110\n",
      "Epoch [103/300], Step [20700/27733], Loss: 2.9210\n",
      "Epoch [103/300], Step [20800/27733], Loss: 2.6000\n",
      "Epoch [103/300], Step [20900/27733], Loss: 1.9417\n",
      "Epoch [103/300], Step [21000/27733], Loss: 3.3435\n",
      "Epoch [103/300], Step [21100/27733], Loss: 3.2316\n",
      "Epoch [103/300], Step [21200/27733], Loss: 2.7157\n",
      "Epoch [103/300], Step [21300/27733], Loss: 2.1668\n",
      "Epoch [103/300], Step [21400/27733], Loss: 3.5572\n",
      "Epoch [103/300], Step [21500/27733], Loss: 3.3693\n",
      "Epoch [103/300], Step [21600/27733], Loss: 2.8566\n",
      "Epoch [103/300], Step [21700/27733], Loss: 3.2648\n",
      "Epoch [103/300], Step [21800/27733], Loss: 2.7269\n",
      "Epoch [103/300], Step [21900/27733], Loss: 2.3017\n",
      "Epoch [103/300], Step [22000/27733], Loss: 3.6870\n",
      "Epoch [103/300], Step [22100/27733], Loss: 3.0320\n",
      "Epoch [103/300], Step [22200/27733], Loss: 2.8827\n",
      "Epoch [103/300], Step [22300/27733], Loss: 2.8070\n",
      "Epoch [103/300], Step [22400/27733], Loss: 3.0656\n",
      "Epoch [103/300], Step [22500/27733], Loss: 3.1167\n",
      "Epoch [103/300], Step [22600/27733], Loss: 3.3637\n",
      "Epoch [103/300], Step [22700/27733], Loss: 2.9397\n",
      "Epoch [103/300], Step [22800/27733], Loss: 3.4078\n",
      "Epoch [103/300], Step [22900/27733], Loss: 2.8132\n",
      "Epoch [103/300], Step [23000/27733], Loss: 2.4953\n",
      "Epoch [103/300], Step [23100/27733], Loss: 2.6305\n",
      "Epoch [103/300], Step [23200/27733], Loss: 3.5429\n",
      "Epoch [103/300], Step [23300/27733], Loss: 2.9680\n",
      "Epoch [103/300], Step [23400/27733], Loss: 1.7736\n",
      "Epoch [103/300], Step [23500/27733], Loss: 2.3541\n",
      "Epoch [103/300], Step [23600/27733], Loss: 3.2079\n",
      "Epoch [103/300], Step [23700/27733], Loss: 3.6537\n",
      "Epoch [103/300], Step [23800/27733], Loss: 3.8375\n",
      "Epoch [103/300], Step [23900/27733], Loss: 3.1339\n",
      "Epoch [103/300], Step [24000/27733], Loss: 3.4415\n",
      "Epoch [103/300], Step [24100/27733], Loss: 3.1935\n",
      "Epoch [103/300], Step [24200/27733], Loss: 3.3761\n",
      "Epoch [103/300], Step [24300/27733], Loss: 2.4967\n",
      "Epoch [103/300], Step [24400/27733], Loss: 2.7130\n",
      "Epoch [103/300], Step [24500/27733], Loss: 3.3485\n",
      "Epoch [103/300], Step [24600/27733], Loss: 2.5130\n",
      "Epoch [103/300], Step [24700/27733], Loss: 3.1364\n",
      "Epoch [103/300], Step [24800/27733], Loss: 2.8190\n",
      "Epoch [103/300], Step [24900/27733], Loss: 3.9850\n",
      "Epoch [103/300], Step [25000/27733], Loss: 3.4933\n",
      "Epoch [103/300], Step [25100/27733], Loss: 3.0971\n",
      "Epoch [103/300], Step [25200/27733], Loss: 3.3259\n",
      "Epoch [103/300], Step [25300/27733], Loss: 2.9136\n",
      "Epoch [103/300], Step [25400/27733], Loss: 3.4672\n",
      "Epoch [103/300], Step [25500/27733], Loss: 2.8861\n",
      "Epoch [103/300], Step [25600/27733], Loss: 2.4853\n",
      "Epoch [103/300], Step [25700/27733], Loss: 4.4615\n",
      "Epoch [103/300], Step [25800/27733], Loss: 3.6734\n",
      "Epoch [103/300], Step [25900/27733], Loss: 2.5406\n",
      "Epoch [103/300], Step [26000/27733], Loss: 3.1100\n",
      "Epoch [103/300], Step [26100/27733], Loss: 3.2615\n",
      "Epoch [103/300], Step [26200/27733], Loss: 2.7427\n",
      "Epoch [103/300], Step [26300/27733], Loss: 2.6735\n",
      "Epoch [103/300], Step [26400/27733], Loss: 3.2621\n",
      "Epoch [103/300], Step [26500/27733], Loss: 3.1873\n",
      "Epoch [103/300], Step [26600/27733], Loss: 2.9245\n",
      "Epoch [103/300], Step [26700/27733], Loss: 3.1122\n",
      "Epoch [103/300], Step [26800/27733], Loss: 3.5499\n",
      "Epoch [103/300], Step [26900/27733], Loss: 2.5784\n",
      "Epoch [103/300], Step [27000/27733], Loss: 2.9952\n",
      "Epoch [103/300], Step [27100/27733], Loss: 3.0932\n",
      "Epoch [103/300], Step [27200/27733], Loss: 1.9179\n",
      "Epoch [103/300], Step [27300/27733], Loss: 2.1537\n",
      "Epoch [103/300], Step [27400/27733], Loss: 2.5948\n",
      "Epoch [103/300], Step [27500/27733], Loss: 4.1321\n",
      "Epoch [103/300], Step [27600/27733], Loss: 2.7780\n",
      "Epoch [103/300], Step [27700/27733], Loss: 3.4615\n",
      "Epoch [104/300], Step [100/27733], Loss: 2.9053\n",
      "Epoch [104/300], Step [200/27733], Loss: 2.8832\n",
      "Epoch [104/300], Step [300/27733], Loss: 1.7756\n",
      "Epoch [104/300], Step [400/27733], Loss: 2.5701\n",
      "Epoch [104/300], Step [500/27733], Loss: 2.7725\n",
      "Epoch [104/300], Step [600/27733], Loss: 2.5973\n",
      "Epoch [104/300], Step [700/27733], Loss: 3.2519\n",
      "Epoch [104/300], Step [800/27733], Loss: 1.8511\n",
      "Epoch [104/300], Step [900/27733], Loss: 2.5242\n",
      "Epoch [104/300], Step [1000/27733], Loss: 2.4609\n",
      "Epoch [104/300], Step [1100/27733], Loss: 2.5129\n",
      "Epoch [104/300], Step [1200/27733], Loss: 2.8459\n",
      "Epoch [104/300], Step [1300/27733], Loss: 3.2201\n",
      "Epoch [104/300], Step [1400/27733], Loss: 2.4084\n",
      "Epoch [104/300], Step [1500/27733], Loss: 2.6246\n",
      "Epoch [104/300], Step [1600/27733], Loss: 1.8970\n",
      "Epoch [104/300], Step [1700/27733], Loss: 1.6656\n",
      "Epoch [104/300], Step [1800/27733], Loss: 2.0254\n",
      "Epoch [104/300], Step [1900/27733], Loss: 2.5629\n",
      "Epoch [104/300], Step [2000/27733], Loss: 2.0021\n",
      "Epoch [104/300], Step [2100/27733], Loss: 2.3818\n",
      "Epoch [104/300], Step [2200/27733], Loss: 2.3536\n",
      "Epoch [104/300], Step [2300/27733], Loss: 3.0066\n",
      "Epoch [104/300], Step [2400/27733], Loss: 2.5726\n",
      "Epoch [104/300], Step [2500/27733], Loss: 2.4317\n",
      "Epoch [104/300], Step [2600/27733], Loss: 2.7096\n",
      "Epoch [104/300], Step [2700/27733], Loss: 2.5459\n",
      "Epoch [104/300], Step [2800/27733], Loss: 3.0808\n",
      "Epoch [104/300], Step [2900/27733], Loss: 2.1083\n",
      "Epoch [104/300], Step [3000/27733], Loss: 2.3460\n",
      "Epoch [104/300], Step [3100/27733], Loss: 2.4168\n",
      "Epoch [104/300], Step [3200/27733], Loss: 3.1077\n",
      "Epoch [104/300], Step [3300/27733], Loss: 2.4077\n",
      "Epoch [104/300], Step [3400/27733], Loss: 2.4785\n",
      "Epoch [104/300], Step [3500/27733], Loss: 2.4596\n",
      "Epoch [104/300], Step [3600/27733], Loss: 2.5798\n",
      "Epoch [104/300], Step [3700/27733], Loss: 2.5872\n",
      "Epoch [104/300], Step [3800/27733], Loss: 3.7295\n",
      "Epoch [104/300], Step [3900/27733], Loss: 3.7029\n",
      "Epoch [104/300], Step [4000/27733], Loss: 1.6957\n",
      "Epoch [104/300], Step [4100/27733], Loss: 2.2843\n",
      "Epoch [104/300], Step [4200/27733], Loss: 2.3673\n",
      "Epoch [104/300], Step [4300/27733], Loss: 2.5099\n",
      "Epoch [104/300], Step [4400/27733], Loss: 2.1974\n",
      "Epoch [104/300], Step [4500/27733], Loss: 2.5477\n",
      "Epoch [104/300], Step [4600/27733], Loss: 2.6798\n",
      "Epoch [104/300], Step [4700/27733], Loss: 2.6769\n",
      "Epoch [104/300], Step [4800/27733], Loss: 3.2601\n",
      "Epoch [104/300], Step [4900/27733], Loss: 2.3305\n",
      "Epoch [104/300], Step [5000/27733], Loss: 2.2946\n",
      "Epoch [104/300], Step [5100/27733], Loss: 2.5912\n",
      "Epoch [104/300], Step [5200/27733], Loss: 3.3629\n",
      "Epoch [104/300], Step [5300/27733], Loss: 2.3221\n",
      "Epoch [104/300], Step [5400/27733], Loss: 2.3715\n",
      "Epoch [104/300], Step [5500/27733], Loss: 3.1460\n",
      "Epoch [104/300], Step [5600/27733], Loss: 2.6407\n",
      "Epoch [104/300], Step [5700/27733], Loss: 2.9610\n",
      "Epoch [104/300], Step [5800/27733], Loss: 2.1012\n",
      "Epoch [104/300], Step [5900/27733], Loss: 1.9526\n",
      "Epoch [104/300], Step [6000/27733], Loss: 2.9233\n",
      "Epoch [104/300], Step [6100/27733], Loss: 2.6000\n",
      "Epoch [104/300], Step [6200/27733], Loss: 2.6744\n",
      "Epoch [104/300], Step [6300/27733], Loss: 2.6274\n",
      "Epoch [104/300], Step [6400/27733], Loss: 2.3551\n",
      "Epoch [104/300], Step [6500/27733], Loss: 2.2791\n",
      "Epoch [104/300], Step [6600/27733], Loss: 2.6059\n",
      "Epoch [104/300], Step [6700/27733], Loss: 3.5779\n",
      "Epoch [104/300], Step [6800/27733], Loss: 2.2728\n",
      "Epoch [104/300], Step [6900/27733], Loss: 2.6187\n",
      "Epoch [104/300], Step [7000/27733], Loss: 2.8193\n",
      "Epoch [104/300], Step [7100/27733], Loss: 2.8763\n",
      "Epoch [104/300], Step [7200/27733], Loss: 3.0924\n",
      "Epoch [104/300], Step [7300/27733], Loss: 3.0294\n",
      "Epoch [104/300], Step [7400/27733], Loss: 2.3863\n",
      "Epoch [104/300], Step [7500/27733], Loss: 2.3831\n",
      "Epoch [104/300], Step [7600/27733], Loss: 2.5449\n",
      "Epoch [104/300], Step [7700/27733], Loss: 2.0736\n",
      "Epoch [104/300], Step [7800/27733], Loss: 2.0870\n",
      "Epoch [104/300], Step [7900/27733], Loss: 3.4994\n",
      "Epoch [104/300], Step [8000/27733], Loss: 3.4949\n",
      "Epoch [104/300], Step [8100/27733], Loss: 3.2919\n",
      "Epoch [104/300], Step [8200/27733], Loss: 3.3238\n",
      "Epoch [104/300], Step [8300/27733], Loss: 2.9887\n",
      "Epoch [104/300], Step [8400/27733], Loss: 2.6644\n",
      "Epoch [104/300], Step [8500/27733], Loss: 2.9476\n",
      "Epoch [104/300], Step [8600/27733], Loss: 2.9099\n",
      "Epoch [104/300], Step [8700/27733], Loss: 2.3604\n",
      "Epoch [104/300], Step [8800/27733], Loss: 2.7177\n",
      "Epoch [104/300], Step [8900/27733], Loss: 2.4516\n",
      "Epoch [104/300], Step [9000/27733], Loss: 3.1930\n",
      "Epoch [104/300], Step [9100/27733], Loss: 2.1713\n",
      "Epoch [104/300], Step [9200/27733], Loss: 2.6082\n",
      "Epoch [104/300], Step [9300/27733], Loss: 2.3982\n",
      "Epoch [104/300], Step [9400/27733], Loss: 3.6291\n",
      "Epoch [104/300], Step [9500/27733], Loss: 2.2576\n",
      "Epoch [104/300], Step [9600/27733], Loss: 3.3256\n",
      "Epoch [104/300], Step [9700/27733], Loss: 2.6984\n",
      "Epoch [104/300], Step [9800/27733], Loss: 3.5832\n",
      "Epoch [104/300], Step [9900/27733], Loss: 2.7168\n",
      "Epoch [104/300], Step [10000/27733], Loss: 2.1825\n",
      "Epoch [104/300], Step [10100/27733], Loss: 2.9109\n",
      "Epoch [104/300], Step [10200/27733], Loss: 2.9440\n",
      "Epoch [104/300], Step [10300/27733], Loss: 2.6257\n",
      "Epoch [104/300], Step [10400/27733], Loss: 3.0340\n",
      "Epoch [104/300], Step [10500/27733], Loss: 3.0207\n",
      "Epoch [104/300], Step [10600/27733], Loss: 2.0710\n",
      "Epoch [104/300], Step [10700/27733], Loss: 2.3682\n",
      "Epoch [104/300], Step [10800/27733], Loss: 2.4524\n",
      "Epoch [104/300], Step [10900/27733], Loss: 2.5910\n",
      "Epoch [104/300], Step [11000/27733], Loss: 2.8248\n",
      "Epoch [104/300], Step [11100/27733], Loss: 2.5960\n",
      "Epoch [104/300], Step [11200/27733], Loss: 2.6342\n",
      "Epoch [104/300], Step [11300/27733], Loss: 3.0719\n",
      "Epoch [104/300], Step [11400/27733], Loss: 2.7746\n",
      "Epoch [104/300], Step [11500/27733], Loss: 3.0745\n",
      "Epoch [104/300], Step [11600/27733], Loss: 2.9255\n",
      "Epoch [104/300], Step [11700/27733], Loss: 2.7215\n",
      "Epoch [104/300], Step [11800/27733], Loss: 1.9536\n",
      "Epoch [104/300], Step [11900/27733], Loss: 2.7803\n",
      "Epoch [104/300], Step [12000/27733], Loss: 2.2969\n",
      "Epoch [104/300], Step [12100/27733], Loss: 2.2425\n",
      "Epoch [104/300], Step [12200/27733], Loss: 2.7395\n",
      "Epoch [104/300], Step [12300/27733], Loss: 3.6872\n",
      "Epoch [104/300], Step [12400/27733], Loss: 2.8708\n",
      "Epoch [104/300], Step [12500/27733], Loss: 2.7436\n",
      "Epoch [104/300], Step [12600/27733], Loss: 2.6742\n",
      "Epoch [104/300], Step [12700/27733], Loss: 2.8430\n",
      "Epoch [104/300], Step [12800/27733], Loss: 3.1211\n",
      "Epoch [104/300], Step [12900/27733], Loss: 2.6023\n",
      "Epoch [104/300], Step [13000/27733], Loss: 2.3625\n",
      "Epoch [104/300], Step [13100/27733], Loss: 1.9793\n",
      "Epoch [104/300], Step [13200/27733], Loss: 2.3232\n",
      "Epoch [104/300], Step [13300/27733], Loss: 2.8730\n",
      "Epoch [104/300], Step [13400/27733], Loss: 2.7792\n",
      "Epoch [104/300], Step [13500/27733], Loss: 2.8423\n",
      "Epoch [104/300], Step [13600/27733], Loss: 3.1198\n",
      "Epoch [104/300], Step [13700/27733], Loss: 2.6117\n",
      "Epoch [104/300], Step [13800/27733], Loss: 2.7477\n",
      "Epoch [104/300], Step [13900/27733], Loss: 1.9570\n",
      "Epoch [104/300], Step [14000/27733], Loss: 3.4858\n",
      "Epoch [104/300], Step [14100/27733], Loss: 3.0418\n",
      "Epoch [104/300], Step [14200/27733], Loss: 2.6754\n",
      "Epoch [104/300], Step [14300/27733], Loss: 2.3257\n",
      "Epoch [104/300], Step [14400/27733], Loss: 2.9287\n",
      "Epoch [104/300], Step [14500/27733], Loss: 3.2878\n",
      "Epoch [104/300], Step [14600/27733], Loss: 2.9292\n",
      "Epoch [104/300], Step [14700/27733], Loss: 2.4618\n",
      "Epoch [104/300], Step [14800/27733], Loss: 3.5314\n",
      "Epoch [104/300], Step [14900/27733], Loss: 2.7579\n",
      "Epoch [104/300], Step [15000/27733], Loss: 3.4855\n",
      "Epoch [104/300], Step [15100/27733], Loss: 1.7815\n",
      "Epoch [104/300], Step [15200/27733], Loss: 3.8197\n",
      "Epoch [104/300], Step [15300/27733], Loss: 2.5517\n",
      "Epoch [104/300], Step [15400/27733], Loss: 2.8654\n",
      "Epoch [104/300], Step [15500/27733], Loss: 2.9458\n",
      "Epoch [104/300], Step [15600/27733], Loss: 2.3192\n",
      "Epoch [104/300], Step [15700/27733], Loss: 3.0880\n",
      "Epoch [104/300], Step [15800/27733], Loss: 2.6513\n",
      "Epoch [104/300], Step [15900/27733], Loss: 3.6689\n",
      "Epoch [104/300], Step [16000/27733], Loss: 3.4499\n",
      "Epoch [104/300], Step [16100/27733], Loss: 2.4046\n",
      "Epoch [104/300], Step [16200/27733], Loss: 2.8424\n",
      "Epoch [104/300], Step [16300/27733], Loss: 3.3256\n",
      "Epoch [104/300], Step [16400/27733], Loss: 3.1028\n",
      "Epoch [104/300], Step [16500/27733], Loss: 3.5610\n",
      "Epoch [104/300], Step [16600/27733], Loss: 2.9668\n",
      "Epoch [104/300], Step [16700/27733], Loss: 2.7168\n",
      "Epoch [104/300], Step [16800/27733], Loss: 2.4527\n",
      "Epoch [104/300], Step [16900/27733], Loss: 2.3192\n",
      "Epoch [104/300], Step [17000/27733], Loss: 1.9216\n",
      "Epoch [104/300], Step [17100/27733], Loss: 3.4966\n",
      "Epoch [104/300], Step [17200/27733], Loss: 2.4528\n",
      "Epoch [104/300], Step [17300/27733], Loss: 2.5715\n",
      "Epoch [104/300], Step [17400/27733], Loss: 3.2260\n",
      "Epoch [104/300], Step [17500/27733], Loss: 3.8216\n",
      "Epoch [104/300], Step [17600/27733], Loss: 3.5113\n",
      "Epoch [104/300], Step [17700/27733], Loss: 2.8806\n",
      "Epoch [104/300], Step [17800/27733], Loss: 3.2298\n",
      "Epoch [104/300], Step [17900/27733], Loss: 2.4159\n",
      "Epoch [104/300], Step [18000/27733], Loss: 3.7207\n",
      "Epoch [104/300], Step [18100/27733], Loss: 3.4434\n",
      "Epoch [104/300], Step [18200/27733], Loss: 3.0447\n",
      "Epoch [104/300], Step [18300/27733], Loss: 3.8826\n",
      "Epoch [104/300], Step [18400/27733], Loss: 3.0803\n",
      "Epoch [104/300], Step [18500/27733], Loss: 2.5536\n",
      "Epoch [104/300], Step [18600/27733], Loss: 2.5276\n",
      "Epoch [104/300], Step [18700/27733], Loss: 2.4052\n",
      "Epoch [104/300], Step [18800/27733], Loss: 2.3964\n",
      "Epoch [104/300], Step [18900/27733], Loss: 4.1025\n",
      "Epoch [104/300], Step [19000/27733], Loss: 2.5404\n",
      "Epoch [104/300], Step [19100/27733], Loss: 3.7098\n",
      "Epoch [104/300], Step [19200/27733], Loss: 2.0165\n",
      "Epoch [104/300], Step [19300/27733], Loss: 2.8601\n",
      "Epoch [104/300], Step [19400/27733], Loss: 2.6366\n",
      "Epoch [104/300], Step [19500/27733], Loss: 3.7399\n",
      "Epoch [104/300], Step [19600/27733], Loss: 3.3283\n",
      "Epoch [104/300], Step [19700/27733], Loss: 3.1759\n",
      "Epoch [104/300], Step [19800/27733], Loss: 2.5500\n",
      "Epoch [104/300], Step [19900/27733], Loss: 2.9202\n",
      "Epoch [104/300], Step [20000/27733], Loss: 2.5738\n",
      "Epoch [104/300], Step [20100/27733], Loss: 3.1821\n",
      "Epoch [104/300], Step [20200/27733], Loss: 3.5490\n",
      "Epoch [104/300], Step [20300/27733], Loss: 3.0760\n",
      "Epoch [104/300], Step [20400/27733], Loss: 2.9704\n",
      "Epoch [104/300], Step [20500/27733], Loss: 2.8031\n",
      "Epoch [104/300], Step [20600/27733], Loss: 2.8045\n",
      "Epoch [104/300], Step [20700/27733], Loss: 2.7498\n",
      "Epoch [104/300], Step [20800/27733], Loss: 3.3436\n",
      "Epoch [104/300], Step [20900/27733], Loss: 2.4009\n",
      "Epoch [104/300], Step [21000/27733], Loss: 3.1970\n",
      "Epoch [104/300], Step [21100/27733], Loss: 2.1935\n",
      "Epoch [104/300], Step [21200/27733], Loss: 2.7612\n",
      "Epoch [104/300], Step [21300/27733], Loss: 2.6696\n",
      "Epoch [104/300], Step [21400/27733], Loss: 2.8305\n",
      "Epoch [104/300], Step [21500/27733], Loss: 3.2368\n",
      "Epoch [104/300], Step [21600/27733], Loss: 3.1483\n",
      "Epoch [104/300], Step [21700/27733], Loss: 3.1505\n",
      "Epoch [104/300], Step [21800/27733], Loss: 2.2067\n",
      "Epoch [104/300], Step [21900/27733], Loss: 3.4776\n",
      "Epoch [104/300], Step [22000/27733], Loss: 3.3540\n",
      "Epoch [104/300], Step [22100/27733], Loss: 2.9580\n",
      "Epoch [104/300], Step [22200/27733], Loss: 2.6387\n",
      "Epoch [104/300], Step [22300/27733], Loss: 2.7733\n",
      "Epoch [104/300], Step [22400/27733], Loss: 2.9692\n",
      "Epoch [104/300], Step [22500/27733], Loss: 2.5740\n",
      "Epoch [104/300], Step [22600/27733], Loss: 2.6736\n",
      "Epoch [104/300], Step [22700/27733], Loss: 3.4396\n",
      "Epoch [104/300], Step [22800/27733], Loss: 2.4671\n",
      "Epoch [104/300], Step [22900/27733], Loss: 3.0454\n",
      "Epoch [104/300], Step [23000/27733], Loss: 3.0852\n",
      "Epoch [104/300], Step [23100/27733], Loss: 2.4440\n",
      "Epoch [104/300], Step [23200/27733], Loss: 2.8558\n",
      "Epoch [104/300], Step [23300/27733], Loss: 2.1829\n",
      "Epoch [104/300], Step [23400/27733], Loss: 2.7495\n",
      "Epoch [104/300], Step [23500/27733], Loss: 3.6610\n",
      "Epoch [104/300], Step [23600/27733], Loss: 2.2729\n",
      "Epoch [104/300], Step [23700/27733], Loss: 3.2273\n",
      "Epoch [104/300], Step [23800/27733], Loss: 2.2463\n",
      "Epoch [104/300], Step [23900/27733], Loss: 2.6830\n",
      "Epoch [104/300], Step [24000/27733], Loss: 3.4842\n",
      "Epoch [104/300], Step [24100/27733], Loss: 2.7140\n",
      "Epoch [104/300], Step [24200/27733], Loss: 2.6286\n",
      "Epoch [104/300], Step [24300/27733], Loss: 2.3901\n",
      "Epoch [104/300], Step [24400/27733], Loss: 3.8319\n",
      "Epoch [104/300], Step [24500/27733], Loss: 3.3837\n",
      "Epoch [104/300], Step [24600/27733], Loss: 2.7761\n",
      "Epoch [104/300], Step [24700/27733], Loss: 2.2398\n",
      "Epoch [104/300], Step [24800/27733], Loss: 3.0298\n",
      "Epoch [104/300], Step [24900/27733], Loss: 2.6161\n",
      "Epoch [104/300], Step [25000/27733], Loss: 2.2493\n",
      "Epoch [104/300], Step [25100/27733], Loss: 3.6499\n",
      "Epoch [104/300], Step [25200/27733], Loss: 2.3272\n",
      "Epoch [104/300], Step [25300/27733], Loss: 2.7784\n",
      "Epoch [104/300], Step [25400/27733], Loss: 3.9826\n",
      "Epoch [104/300], Step [25500/27733], Loss: 2.5837\n",
      "Epoch [104/300], Step [25600/27733], Loss: 2.6569\n",
      "Epoch [104/300], Step [25700/27733], Loss: 3.1684\n",
      "Epoch [104/300], Step [25800/27733], Loss: 3.0337\n",
      "Epoch [104/300], Step [25900/27733], Loss: 2.1633\n",
      "Epoch [104/300], Step [26000/27733], Loss: 3.3533\n",
      "Epoch [104/300], Step [26100/27733], Loss: 3.2442\n",
      "Epoch [104/300], Step [26200/27733], Loss: 4.2359\n",
      "Epoch [104/300], Step [26300/27733], Loss: 2.6059\n",
      "Epoch [104/300], Step [26400/27733], Loss: 2.7537\n",
      "Epoch [104/300], Step [26500/27733], Loss: 2.6653\n",
      "Epoch [104/300], Step [26600/27733], Loss: 2.9498\n",
      "Epoch [104/300], Step [26700/27733], Loss: 2.8159\n",
      "Epoch [104/300], Step [26800/27733], Loss: 2.8672\n",
      "Epoch [104/300], Step [26900/27733], Loss: 3.1091\n",
      "Epoch [104/300], Step [27000/27733], Loss: 2.6851\n",
      "Epoch [104/300], Step [27100/27733], Loss: 2.4348\n",
      "Epoch [104/300], Step [27200/27733], Loss: 2.1365\n",
      "Epoch [104/300], Step [27300/27733], Loss: 3.1645\n",
      "Epoch [104/300], Step [27400/27733], Loss: 2.6423\n",
      "Epoch [104/300], Step [27500/27733], Loss: 2.6878\n",
      "Epoch [104/300], Step [27600/27733], Loss: 3.3156\n",
      "Epoch [104/300], Step [27700/27733], Loss: 3.0856\n",
      "Epoch [105/300], Step [100/27733], Loss: 2.4441\n",
      "Epoch [105/300], Step [200/27733], Loss: 2.1210\n",
      "Epoch [105/300], Step [300/27733], Loss: 2.9615\n",
      "Epoch [105/300], Step [400/27733], Loss: 2.1911\n",
      "Epoch [105/300], Step [500/27733], Loss: 3.0418\n",
      "Epoch [105/300], Step [600/27733], Loss: 2.2853\n",
      "Epoch [105/300], Step [700/27733], Loss: 2.4467\n",
      "Epoch [105/300], Step [800/27733], Loss: 2.5018\n",
      "Epoch [105/300], Step [900/27733], Loss: 2.0378\n",
      "Epoch [105/300], Step [1000/27733], Loss: 2.3738\n",
      "Epoch [105/300], Step [1100/27733], Loss: 2.1642\n",
      "Epoch [105/300], Step [1200/27733], Loss: 2.1936\n",
      "Epoch [105/300], Step [1300/27733], Loss: 2.4620\n",
      "Epoch [105/300], Step [1400/27733], Loss: 2.4707\n",
      "Epoch [105/300], Step [1500/27733], Loss: 2.3277\n",
      "Epoch [105/300], Step [1600/27733], Loss: 2.1653\n",
      "Epoch [105/300], Step [1700/27733], Loss: 3.6701\n",
      "Epoch [105/300], Step [1800/27733], Loss: 2.7216\n",
      "Epoch [105/300], Step [1900/27733], Loss: 2.6905\n",
      "Epoch [105/300], Step [2000/27733], Loss: 1.7511\n",
      "Epoch [105/300], Step [2100/27733], Loss: 2.0629\n",
      "Epoch [105/300], Step [2200/27733], Loss: 1.9652\n",
      "Epoch [105/300], Step [2300/27733], Loss: 2.5480\n",
      "Epoch [105/300], Step [2400/27733], Loss: 1.9090\n",
      "Epoch [105/300], Step [2500/27733], Loss: 2.5087\n",
      "Epoch [105/300], Step [2600/27733], Loss: 2.4889\n",
      "Epoch [105/300], Step [2700/27733], Loss: 2.1097\n",
      "Epoch [105/300], Step [2800/27733], Loss: 2.6720\n",
      "Epoch [105/300], Step [2900/27733], Loss: 2.0081\n",
      "Epoch [105/300], Step [3000/27733], Loss: 2.4130\n",
      "Epoch [105/300], Step [3100/27733], Loss: 2.4422\n",
      "Epoch [105/300], Step [3200/27733], Loss: 2.3176\n",
      "Epoch [105/300], Step [3300/27733], Loss: 1.7273\n",
      "Epoch [105/300], Step [3400/27733], Loss: 2.5674\n",
      "Epoch [105/300], Step [3500/27733], Loss: 2.5179\n",
      "Epoch [105/300], Step [3600/27733], Loss: 2.4311\n",
      "Epoch [105/300], Step [3700/27733], Loss: 3.4458\n",
      "Epoch [105/300], Step [3800/27733], Loss: 2.8605\n",
      "Epoch [105/300], Step [3900/27733], Loss: 2.4410\n",
      "Epoch [105/300], Step [4000/27733], Loss: 2.9611\n",
      "Epoch [105/300], Step [4100/27733], Loss: 2.1888\n",
      "Epoch [105/300], Step [4200/27733], Loss: 3.0527\n",
      "Epoch [105/300], Step [4300/27733], Loss: 3.4598\n",
      "Epoch [105/300], Step [4400/27733], Loss: 2.7105\n",
      "Epoch [105/300], Step [4500/27733], Loss: 1.9221\n",
      "Epoch [105/300], Step [4600/27733], Loss: 2.5170\n",
      "Epoch [105/300], Step [4700/27733], Loss: 2.6729\n",
      "Epoch [105/300], Step [4800/27733], Loss: 2.7075\n",
      "Epoch [105/300], Step [4900/27733], Loss: 2.1158\n",
      "Epoch [105/300], Step [5000/27733], Loss: 2.9192\n",
      "Epoch [105/300], Step [5100/27733], Loss: 2.4029\n",
      "Epoch [105/300], Step [5200/27733], Loss: 2.5359\n",
      "Epoch [105/300], Step [5300/27733], Loss: 2.7147\n",
      "Epoch [105/300], Step [5400/27733], Loss: 2.5985\n",
      "Epoch [105/300], Step [5500/27733], Loss: 2.0710\n",
      "Epoch [105/300], Step [5600/27733], Loss: 2.1616\n",
      "Epoch [105/300], Step [5700/27733], Loss: 2.3375\n",
      "Epoch [105/300], Step [5800/27733], Loss: 2.8593\n",
      "Epoch [105/300], Step [5900/27733], Loss: 2.8065\n",
      "Epoch [105/300], Step [6000/27733], Loss: 2.2136\n",
      "Epoch [105/300], Step [6100/27733], Loss: 1.8180\n",
      "Epoch [105/300], Step [6200/27733], Loss: 2.7753\n",
      "Epoch [105/300], Step [6300/27733], Loss: 2.3343\n",
      "Epoch [105/300], Step [6400/27733], Loss: 2.1115\n",
      "Epoch [105/300], Step [6500/27733], Loss: 2.8737\n",
      "Epoch [105/300], Step [6600/27733], Loss: 2.8779\n",
      "Epoch [105/300], Step [6700/27733], Loss: 2.8100\n",
      "Epoch [105/300], Step [6800/27733], Loss: 2.9405\n",
      "Epoch [105/300], Step [6900/27733], Loss: 2.6855\n",
      "Epoch [105/300], Step [7000/27733], Loss: 3.6613\n",
      "Epoch [105/300], Step [7100/27733], Loss: 2.5680\n",
      "Epoch [105/300], Step [7200/27733], Loss: 2.1555\n",
      "Epoch [105/300], Step [7300/27733], Loss: 2.4437\n",
      "Epoch [105/300], Step [7400/27733], Loss: 2.7931\n",
      "Epoch [105/300], Step [7500/27733], Loss: 2.9653\n",
      "Epoch [105/300], Step [7600/27733], Loss: 3.0041\n",
      "Epoch [105/300], Step [7700/27733], Loss: 3.7720\n",
      "Epoch [105/300], Step [7800/27733], Loss: 2.3840\n",
      "Epoch [105/300], Step [7900/27733], Loss: 2.5188\n",
      "Epoch [105/300], Step [8000/27733], Loss: 3.3713\n",
      "Epoch [105/300], Step [8100/27733], Loss: 2.4237\n",
      "Epoch [105/300], Step [8200/27733], Loss: 2.5941\n",
      "Epoch [105/300], Step [8300/27733], Loss: 2.7775\n",
      "Epoch [105/300], Step [8400/27733], Loss: 2.9788\n",
      "Epoch [105/300], Step [8500/27733], Loss: 3.3731\n",
      "Epoch [105/300], Step [8600/27733], Loss: 2.3029\n",
      "Epoch [105/300], Step [8700/27733], Loss: 2.8835\n",
      "Epoch [105/300], Step [8800/27733], Loss: 2.7531\n",
      "Epoch [105/300], Step [8900/27733], Loss: 1.9198\n",
      "Epoch [105/300], Step [9000/27733], Loss: 2.4873\n",
      "Epoch [105/300], Step [9100/27733], Loss: 2.9233\n",
      "Epoch [105/300], Step [9200/27733], Loss: 2.5025\n",
      "Epoch [105/300], Step [9300/27733], Loss: 2.1367\n",
      "Epoch [105/300], Step [9400/27733], Loss: 4.0837\n",
      "Epoch [105/300], Step [9500/27733], Loss: 3.5652\n",
      "Epoch [105/300], Step [9600/27733], Loss: 2.5389\n",
      "Epoch [105/300], Step [9700/27733], Loss: 3.0616\n",
      "Epoch [105/300], Step [9800/27733], Loss: 2.9110\n",
      "Epoch [105/300], Step [9900/27733], Loss: 2.4547\n",
      "Epoch [105/300], Step [10000/27733], Loss: 2.7246\n",
      "Epoch [105/300], Step [10100/27733], Loss: 2.5276\n",
      "Epoch [105/300], Step [10200/27733], Loss: 3.0150\n",
      "Epoch [105/300], Step [10300/27733], Loss: 3.2915\n",
      "Epoch [105/300], Step [10400/27733], Loss: 3.0773\n",
      "Epoch [105/300], Step [10500/27733], Loss: 2.2206\n",
      "Epoch [105/300], Step [10600/27733], Loss: 2.7806\n",
      "Epoch [105/300], Step [10700/27733], Loss: 2.9778\n",
      "Epoch [105/300], Step [10800/27733], Loss: 2.5280\n",
      "Epoch [105/300], Step [10900/27733], Loss: 3.0089\n",
      "Epoch [105/300], Step [11000/27733], Loss: 2.1708\n",
      "Epoch [105/300], Step [11100/27733], Loss: 2.6952\n",
      "Epoch [105/300], Step [11200/27733], Loss: 2.9327\n",
      "Epoch [105/300], Step [11300/27733], Loss: 2.8591\n",
      "Epoch [105/300], Step [11400/27733], Loss: 3.6633\n",
      "Epoch [105/300], Step [11500/27733], Loss: 3.0985\n",
      "Epoch [105/300], Step [11600/27733], Loss: 2.9577\n",
      "Epoch [105/300], Step [11700/27733], Loss: 2.5820\n",
      "Epoch [105/300], Step [11800/27733], Loss: 3.0249\n",
      "Epoch [105/300], Step [11900/27733], Loss: 2.5452\n",
      "Epoch [105/300], Step [12000/27733], Loss: 2.3450\n",
      "Epoch [105/300], Step [12100/27733], Loss: 3.1718\n",
      "Epoch [105/300], Step [12200/27733], Loss: 3.0271\n",
      "Epoch [105/300], Step [12300/27733], Loss: 3.0752\n",
      "Epoch [105/300], Step [12400/27733], Loss: 2.7416\n",
      "Epoch [105/300], Step [12500/27733], Loss: 3.3172\n",
      "Epoch [105/300], Step [12600/27733], Loss: 2.7778\n",
      "Epoch [105/300], Step [12700/27733], Loss: 1.9835\n",
      "Epoch [105/300], Step [12800/27733], Loss: 2.6275\n",
      "Epoch [105/300], Step [12900/27733], Loss: 2.5236\n",
      "Epoch [105/300], Step [13000/27733], Loss: 2.8895\n",
      "Epoch [105/300], Step [13100/27733], Loss: 2.8783\n",
      "Epoch [105/300], Step [13200/27733], Loss: 2.7356\n",
      "Epoch [105/300], Step [13300/27733], Loss: 3.8305\n",
      "Epoch [105/300], Step [13400/27733], Loss: 2.5821\n",
      "Epoch [105/300], Step [13500/27733], Loss: 2.4828\n",
      "Epoch [105/300], Step [13600/27733], Loss: 3.0304\n",
      "Epoch [105/300], Step [13700/27733], Loss: 2.4768\n",
      "Epoch [105/300], Step [13800/27733], Loss: 3.5318\n",
      "Epoch [105/300], Step [13900/27733], Loss: 2.4841\n",
      "Epoch [105/300], Step [14000/27733], Loss: 3.0686\n",
      "Epoch [105/300], Step [14100/27733], Loss: 3.0344\n",
      "Epoch [105/300], Step [14200/27733], Loss: 2.2112\n",
      "Epoch [105/300], Step [14300/27733], Loss: 2.4516\n",
      "Epoch [105/300], Step [14400/27733], Loss: 3.0529\n",
      "Epoch [105/300], Step [14500/27733], Loss: 3.9163\n",
      "Epoch [105/300], Step [14600/27733], Loss: 2.6299\n",
      "Epoch [105/300], Step [14700/27733], Loss: 2.6348\n",
      "Epoch [105/300], Step [14800/27733], Loss: 3.4131\n",
      "Epoch [105/300], Step [14900/27733], Loss: 2.2754\n",
      "Epoch [105/300], Step [15000/27733], Loss: 2.5967\n",
      "Epoch [105/300], Step [15100/27733], Loss: 3.8136\n",
      "Epoch [105/300], Step [15200/27733], Loss: 2.7192\n",
      "Epoch [105/300], Step [15300/27733], Loss: 2.3504\n",
      "Epoch [105/300], Step [15400/27733], Loss: 2.9743\n",
      "Epoch [105/300], Step [15500/27733], Loss: 2.5198\n",
      "Epoch [105/300], Step [15600/27733], Loss: 2.6644\n",
      "Epoch [105/300], Step [15700/27733], Loss: 3.2815\n",
      "Epoch [105/300], Step [15800/27733], Loss: 2.2707\n",
      "Epoch [105/300], Step [15900/27733], Loss: 2.3956\n",
      "Epoch [105/300], Step [16000/27733], Loss: 3.0541\n",
      "Epoch [105/300], Step [16100/27733], Loss: 3.8197\n",
      "Epoch [105/300], Step [16200/27733], Loss: 2.1196\n",
      "Epoch [105/300], Step [16300/27733], Loss: 2.4818\n",
      "Epoch [105/300], Step [16400/27733], Loss: 2.3016\n",
      "Epoch [105/300], Step [16500/27733], Loss: 3.5303\n",
      "Epoch [105/300], Step [16600/27733], Loss: 2.4102\n",
      "Epoch [105/300], Step [16700/27733], Loss: 2.8710\n",
      "Epoch [105/300], Step [16800/27733], Loss: 3.0595\n",
      "Epoch [105/300], Step [16900/27733], Loss: 2.1314\n",
      "Epoch [105/300], Step [17000/27733], Loss: 3.2663\n",
      "Epoch [105/300], Step [17100/27733], Loss: 2.6394\n",
      "Epoch [105/300], Step [17200/27733], Loss: 3.3644\n",
      "Epoch [105/300], Step [17300/27733], Loss: 2.3235\n",
      "Epoch [105/300], Step [17400/27733], Loss: 2.8689\n",
      "Epoch [105/300], Step [17500/27733], Loss: 3.0986\n",
      "Epoch [105/300], Step [17600/27733], Loss: 3.3379\n",
      "Epoch [105/300], Step [17700/27733], Loss: 2.6684\n",
      "Epoch [105/300], Step [17800/27733], Loss: 3.0895\n",
      "Epoch [105/300], Step [17900/27733], Loss: 2.6965\n",
      "Epoch [105/300], Step [18000/27733], Loss: 3.6443\n",
      "Epoch [105/300], Step [18100/27733], Loss: 2.4390\n",
      "Epoch [105/300], Step [18200/27733], Loss: 3.5512\n",
      "Epoch [105/300], Step [18300/27733], Loss: 3.5740\n",
      "Epoch [105/300], Step [18400/27733], Loss: 3.1478\n",
      "Epoch [105/300], Step [18500/27733], Loss: 3.9763\n",
      "Epoch [105/300], Step [18600/27733], Loss: 2.7990\n",
      "Epoch [105/300], Step [18700/27733], Loss: 2.9997\n",
      "Epoch [105/300], Step [18800/27733], Loss: 2.8080\n",
      "Epoch [105/300], Step [18900/27733], Loss: 2.6466\n",
      "Epoch [105/300], Step [19000/27733], Loss: 3.0034\n",
      "Epoch [105/300], Step [19100/27733], Loss: 2.7754\n",
      "Epoch [105/300], Step [19200/27733], Loss: 3.2180\n",
      "Epoch [105/300], Step [19300/27733], Loss: 2.8949\n",
      "Epoch [105/300], Step [19400/27733], Loss: 3.2256\n",
      "Epoch [105/300], Step [19500/27733], Loss: 3.1796\n",
      "Epoch [105/300], Step [19600/27733], Loss: 2.7116\n",
      "Epoch [105/300], Step [19700/27733], Loss: 3.5301\n",
      "Epoch [105/300], Step [19800/27733], Loss: 2.9566\n",
      "Epoch [105/300], Step [19900/27733], Loss: 2.7272\n",
      "Epoch [105/300], Step [20000/27733], Loss: 2.7921\n",
      "Epoch [105/300], Step [20100/27733], Loss: 3.9055\n",
      "Epoch [105/300], Step [20200/27733], Loss: 2.7891\n",
      "Epoch [105/300], Step [20300/27733], Loss: 2.8950\n",
      "Epoch [105/300], Step [20400/27733], Loss: 3.8428\n",
      "Epoch [105/300], Step [20500/27733], Loss: 3.6098\n",
      "Epoch [105/300], Step [20600/27733], Loss: 2.7810\n",
      "Epoch [105/300], Step [20700/27733], Loss: 3.2737\n",
      "Epoch [105/300], Step [20800/27733], Loss: 3.6298\n",
      "Epoch [105/300], Step [20900/27733], Loss: 2.2919\n",
      "Epoch [105/300], Step [21000/27733], Loss: 2.6675\n",
      "Epoch [105/300], Step [21100/27733], Loss: 2.9606\n",
      "Epoch [105/300], Step [21200/27733], Loss: 2.5078\n",
      "Epoch [105/300], Step [21300/27733], Loss: 3.4436\n",
      "Epoch [105/300], Step [21400/27733], Loss: 2.1891\n",
      "Epoch [105/300], Step [21500/27733], Loss: 2.8609\n",
      "Epoch [105/300], Step [21600/27733], Loss: 2.6524\n",
      "Epoch [105/300], Step [21700/27733], Loss: 2.6478\n",
      "Epoch [105/300], Step [21800/27733], Loss: 2.8714\n",
      "Epoch [105/300], Step [21900/27733], Loss: 2.5511\n",
      "Epoch [105/300], Step [22000/27733], Loss: 2.5890\n",
      "Epoch [105/300], Step [22100/27733], Loss: 3.1392\n",
      "Epoch [105/300], Step [22200/27733], Loss: 2.4965\n",
      "Epoch [105/300], Step [22300/27733], Loss: 3.0646\n",
      "Epoch [105/300], Step [22400/27733], Loss: 2.7439\n",
      "Epoch [105/300], Step [22500/27733], Loss: 2.8302\n",
      "Epoch [105/300], Step [22600/27733], Loss: 3.2858\n",
      "Epoch [105/300], Step [22700/27733], Loss: 2.4399\n",
      "Epoch [105/300], Step [22800/27733], Loss: 2.8828\n",
      "Epoch [105/300], Step [22900/27733], Loss: 2.9266\n",
      "Epoch [105/300], Step [23000/27733], Loss: 2.4975\n",
      "Epoch [105/300], Step [23100/27733], Loss: 2.9578\n",
      "Epoch [105/300], Step [23200/27733], Loss: 2.2727\n",
      "Epoch [105/300], Step [23300/27733], Loss: 3.3043\n",
      "Epoch [105/300], Step [23400/27733], Loss: 3.7930\n",
      "Epoch [105/300], Step [23500/27733], Loss: 2.9965\n",
      "Epoch [105/300], Step [23600/27733], Loss: 2.7860\n",
      "Epoch [105/300], Step [23700/27733], Loss: 2.5944\n",
      "Epoch [105/300], Step [23800/27733], Loss: 3.3655\n",
      "Epoch [105/300], Step [23900/27733], Loss: 3.2829\n",
      "Epoch [105/300], Step [24000/27733], Loss: 2.3605\n",
      "Epoch [105/300], Step [24100/27733], Loss: 2.6707\n",
      "Epoch [105/300], Step [24200/27733], Loss: 3.6311\n",
      "Epoch [105/300], Step [24300/27733], Loss: 3.2461\n",
      "Epoch [105/300], Step [24400/27733], Loss: 2.9183\n",
      "Epoch [105/300], Step [24500/27733], Loss: 3.6423\n",
      "Epoch [105/300], Step [24600/27733], Loss: 2.6715\n",
      "Epoch [105/300], Step [24700/27733], Loss: 3.3764\n",
      "Epoch [105/300], Step [24800/27733], Loss: 2.5246\n",
      "Epoch [105/300], Step [24900/27733], Loss: 2.1126\n",
      "Epoch [105/300], Step [25000/27733], Loss: 3.3026\n",
      "Epoch [105/300], Step [25100/27733], Loss: 2.7348\n",
      "Epoch [105/300], Step [25200/27733], Loss: 3.4709\n",
      "Epoch [105/300], Step [25300/27733], Loss: 2.4864\n",
      "Epoch [105/300], Step [25400/27733], Loss: 2.5786\n",
      "Epoch [105/300], Step [25500/27733], Loss: 2.6620\n",
      "Epoch [105/300], Step [25600/27733], Loss: 3.4420\n",
      "Epoch [105/300], Step [25700/27733], Loss: 3.3253\n",
      "Epoch [105/300], Step [25800/27733], Loss: 2.8175\n",
      "Epoch [105/300], Step [25900/27733], Loss: 3.2620\n",
      "Epoch [105/300], Step [26000/27733], Loss: 3.1389\n",
      "Epoch [105/300], Step [26100/27733], Loss: 2.6344\n",
      "Epoch [105/300], Step [26200/27733], Loss: 2.8958\n",
      "Epoch [105/300], Step [26300/27733], Loss: 3.1674\n",
      "Epoch [105/300], Step [26400/27733], Loss: 3.3153\n",
      "Epoch [105/300], Step [26500/27733], Loss: 3.0018\n",
      "Epoch [105/300], Step [26600/27733], Loss: 2.6709\n",
      "Epoch [105/300], Step [26700/27733], Loss: 3.3192\n",
      "Epoch [105/300], Step [26800/27733], Loss: 2.7548\n",
      "Epoch [105/300], Step [26900/27733], Loss: 3.3542\n",
      "Epoch [105/300], Step [27000/27733], Loss: 2.9146\n",
      "Epoch [105/300], Step [27100/27733], Loss: 3.2460\n",
      "Epoch [105/300], Step [27200/27733], Loss: 2.9419\n",
      "Epoch [105/300], Step [27300/27733], Loss: 3.3052\n",
      "Epoch [105/300], Step [27400/27733], Loss: 3.4064\n",
      "Epoch [105/300], Step [27500/27733], Loss: 3.4967\n",
      "Epoch [105/300], Step [27600/27733], Loss: 2.4339\n",
      "Epoch [105/300], Step [27700/27733], Loss: 3.3944\n",
      "Epoch [106/300], Step [100/27733], Loss: 2.5408\n",
      "Epoch [106/300], Step [200/27733], Loss: 2.2265\n",
      "Epoch [106/300], Step [300/27733], Loss: 2.6452\n",
      "Epoch [106/300], Step [400/27733], Loss: 1.9981\n",
      "Epoch [106/300], Step [500/27733], Loss: 2.7299\n",
      "Epoch [106/300], Step [600/27733], Loss: 1.9838\n",
      "Epoch [106/300], Step [700/27733], Loss: 3.0025\n",
      "Epoch [106/300], Step [800/27733], Loss: 2.7746\n",
      "Epoch [106/300], Step [900/27733], Loss: 2.7317\n",
      "Epoch [106/300], Step [1000/27733], Loss: 1.8261\n",
      "Epoch [106/300], Step [1100/27733], Loss: 2.7293\n",
      "Epoch [106/300], Step [1200/27733], Loss: 2.4004\n",
      "Epoch [106/300], Step [1300/27733], Loss: 1.7658\n",
      "Epoch [106/300], Step [1400/27733], Loss: 2.0614\n",
      "Epoch [106/300], Step [1500/27733], Loss: 3.0296\n",
      "Epoch [106/300], Step [1600/27733], Loss: 2.3457\n",
      "Epoch [106/300], Step [1700/27733], Loss: 1.8854\n",
      "Epoch [106/300], Step [1800/27733], Loss: 2.8531\n",
      "Epoch [106/300], Step [1900/27733], Loss: 2.6800\n",
      "Epoch [106/300], Step [2000/27733], Loss: 2.1215\n",
      "Epoch [106/300], Step [2100/27733], Loss: 3.2545\n",
      "Epoch [106/300], Step [2200/27733], Loss: 3.0649\n",
      "Epoch [106/300], Step [2300/27733], Loss: 2.5484\n",
      "Epoch [106/300], Step [2400/27733], Loss: 3.1985\n",
      "Epoch [106/300], Step [2500/27733], Loss: 3.2168\n",
      "Epoch [106/300], Step [2600/27733], Loss: 2.9831\n",
      "Epoch [106/300], Step [2700/27733], Loss: 3.3936\n",
      "Epoch [106/300], Step [2800/27733], Loss: 2.8205\n",
      "Epoch [106/300], Step [2900/27733], Loss: 2.3637\n",
      "Epoch [106/300], Step [3000/27733], Loss: 2.4765\n",
      "Epoch [106/300], Step [3100/27733], Loss: 2.8779\n",
      "Epoch [106/300], Step [3200/27733], Loss: 2.6799\n",
      "Epoch [106/300], Step [3300/27733], Loss: 1.9549\n",
      "Epoch [106/300], Step [3400/27733], Loss: 2.5698\n",
      "Epoch [106/300], Step [3500/27733], Loss: 1.9964\n",
      "Epoch [106/300], Step [3600/27733], Loss: 2.6503\n",
      "Epoch [106/300], Step [3700/27733], Loss: 2.8685\n",
      "Epoch [106/300], Step [3800/27733], Loss: 3.2082\n",
      "Epoch [106/300], Step [3900/27733], Loss: 2.4333\n",
      "Epoch [106/300], Step [4000/27733], Loss: 2.7710\n",
      "Epoch [106/300], Step [4100/27733], Loss: 2.4128\n",
      "Epoch [106/300], Step [4200/27733], Loss: 3.3956\n",
      "Epoch [106/300], Step [4300/27733], Loss: 2.7512\n",
      "Epoch [106/300], Step [4400/27733], Loss: 1.8318\n",
      "Epoch [106/300], Step [4500/27733], Loss: 2.4726\n",
      "Epoch [106/300], Step [4600/27733], Loss: 2.4064\n",
      "Epoch [106/300], Step [4700/27733], Loss: 2.3301\n",
      "Epoch [106/300], Step [4800/27733], Loss: 1.4940\n",
      "Epoch [106/300], Step [4900/27733], Loss: 2.8978\n",
      "Epoch [106/300], Step [5000/27733], Loss: 2.5864\n",
      "Epoch [106/300], Step [5100/27733], Loss: 3.3274\n",
      "Epoch [106/300], Step [5200/27733], Loss: 2.7483\n",
      "Epoch [106/300], Step [5300/27733], Loss: 2.1604\n",
      "Epoch [106/300], Step [5400/27733], Loss: 3.3626\n",
      "Epoch [106/300], Step [5500/27733], Loss: 2.6960\n",
      "Epoch [106/300], Step [5600/27733], Loss: 2.5984\n",
      "Epoch [106/300], Step [5700/27733], Loss: 2.6606\n",
      "Epoch [106/300], Step [5800/27733], Loss: 2.0461\n",
      "Epoch [106/300], Step [5900/27733], Loss: 1.7476\n",
      "Epoch [106/300], Step [6000/27733], Loss: 2.6974\n",
      "Epoch [106/300], Step [6100/27733], Loss: 2.6202\n",
      "Epoch [106/300], Step [6200/27733], Loss: 2.6929\n",
      "Epoch [106/300], Step [6300/27733], Loss: 2.2685\n",
      "Epoch [106/300], Step [6400/27733], Loss: 2.3224\n",
      "Epoch [106/300], Step [6500/27733], Loss: 2.5229\n",
      "Epoch [106/300], Step [6600/27733], Loss: 2.2254\n",
      "Epoch [106/300], Step [6700/27733], Loss: 3.1259\n",
      "Epoch [106/300], Step [6800/27733], Loss: 2.3435\n",
      "Epoch [106/300], Step [6900/27733], Loss: 1.8080\n",
      "Epoch [106/300], Step [7000/27733], Loss: 3.3072\n",
      "Epoch [106/300], Step [7100/27733], Loss: 3.0602\n",
      "Epoch [106/300], Step [7200/27733], Loss: 3.4141\n",
      "Epoch [106/300], Step [7300/27733], Loss: 1.9144\n",
      "Epoch [106/300], Step [7400/27733], Loss: 2.5150\n",
      "Epoch [106/300], Step [7500/27733], Loss: 2.7098\n",
      "Epoch [106/300], Step [7600/27733], Loss: 2.1838\n",
      "Epoch [106/300], Step [7700/27733], Loss: 2.1583\n",
      "Epoch [106/300], Step [7800/27733], Loss: 2.3010\n",
      "Epoch [106/300], Step [7900/27733], Loss: 2.3674\n",
      "Epoch [106/300], Step [8000/27733], Loss: 2.6595\n",
      "Epoch [106/300], Step [8100/27733], Loss: 2.0420\n",
      "Epoch [106/300], Step [8200/27733], Loss: 2.8479\n",
      "Epoch [106/300], Step [8300/27733], Loss: 2.9946\n",
      "Epoch [106/300], Step [8400/27733], Loss: 3.3044\n",
      "Epoch [106/300], Step [8500/27733], Loss: 1.9114\n",
      "Epoch [106/300], Step [8600/27733], Loss: 2.4046\n",
      "Epoch [106/300], Step [8700/27733], Loss: 2.1159\n",
      "Epoch [106/300], Step [8800/27733], Loss: 2.3143\n",
      "Epoch [106/300], Step [8900/27733], Loss: 2.8451\n",
      "Epoch [106/300], Step [9000/27733], Loss: 2.7499\n",
      "Epoch [106/300], Step [9100/27733], Loss: 2.6401\n",
      "Epoch [106/300], Step [9200/27733], Loss: 2.4777\n",
      "Epoch [106/300], Step [9300/27733], Loss: 2.8835\n",
      "Epoch [106/300], Step [9400/27733], Loss: 3.0647\n",
      "Epoch [106/300], Step [9500/27733], Loss: 2.3385\n",
      "Epoch [106/300], Step [9600/27733], Loss: 3.1835\n",
      "Epoch [106/300], Step [9700/27733], Loss: 2.1986\n",
      "Epoch [106/300], Step [9800/27733], Loss: 2.9454\n",
      "Epoch [106/300], Step [9900/27733], Loss: 2.9716\n",
      "Epoch [106/300], Step [10000/27733], Loss: 2.1139\n",
      "Epoch [106/300], Step [10100/27733], Loss: 2.8165\n",
      "Epoch [106/300], Step [10200/27733], Loss: 2.2111\n",
      "Epoch [106/300], Step [10300/27733], Loss: 2.7764\n",
      "Epoch [106/300], Step [10400/27733], Loss: 2.2215\n",
      "Epoch [106/300], Step [10500/27733], Loss: 3.1187\n",
      "Epoch [106/300], Step [10600/27733], Loss: 3.5240\n",
      "Epoch [106/300], Step [10700/27733], Loss: 2.9919\n",
      "Epoch [106/300], Step [10800/27733], Loss: 2.9077\n",
      "Epoch [106/300], Step [10900/27733], Loss: 3.2242\n",
      "Epoch [106/300], Step [11000/27733], Loss: 2.7195\n",
      "Epoch [106/300], Step [11100/27733], Loss: 2.7563\n",
      "Epoch [106/300], Step [11200/27733], Loss: 2.7754\n",
      "Epoch [106/300], Step [11300/27733], Loss: 2.8281\n",
      "Epoch [106/300], Step [11400/27733], Loss: 3.0077\n",
      "Epoch [106/300], Step [11500/27733], Loss: 4.1643\n",
      "Epoch [106/300], Step [11600/27733], Loss: 3.4662\n",
      "Epoch [106/300], Step [11700/27733], Loss: 2.6955\n",
      "Epoch [106/300], Step [11800/27733], Loss: 3.1841\n",
      "Epoch [106/300], Step [11900/27733], Loss: 2.4185\n",
      "Epoch [106/300], Step [12000/27733], Loss: 3.0314\n",
      "Epoch [106/300], Step [12100/27733], Loss: 2.0043\n",
      "Epoch [106/300], Step [12200/27733], Loss: 3.5719\n",
      "Epoch [106/300], Step [12300/27733], Loss: 2.4658\n",
      "Epoch [106/300], Step [12400/27733], Loss: 3.0540\n",
      "Epoch [106/300], Step [12500/27733], Loss: 2.7186\n",
      "Epoch [106/300], Step [12600/27733], Loss: 2.3103\n",
      "Epoch [106/300], Step [12700/27733], Loss: 3.3690\n",
      "Epoch [106/300], Step [12800/27733], Loss: 2.8940\n",
      "Epoch [106/300], Step [12900/27733], Loss: 2.9145\n",
      "Epoch [106/300], Step [13000/27733], Loss: 2.8041\n",
      "Epoch [106/300], Step [13100/27733], Loss: 2.3603\n",
      "Epoch [106/300], Step [13200/27733], Loss: 2.4390\n",
      "Epoch [106/300], Step [13300/27733], Loss: 2.5802\n",
      "Epoch [106/300], Step [13400/27733], Loss: 2.7195\n",
      "Epoch [106/300], Step [13500/27733], Loss: 3.6804\n",
      "Epoch [106/300], Step [13600/27733], Loss: 3.3170\n",
      "Epoch [106/300], Step [13700/27733], Loss: 2.6277\n",
      "Epoch [106/300], Step [13800/27733], Loss: 3.3129\n",
      "Epoch [106/300], Step [13900/27733], Loss: 2.9864\n",
      "Epoch [106/300], Step [14000/27733], Loss: 2.7703\n",
      "Epoch [106/300], Step [14100/27733], Loss: 2.1046\n",
      "Epoch [106/300], Step [14200/27733], Loss: 3.2872\n",
      "Epoch [106/300], Step [14300/27733], Loss: 3.2538\n",
      "Epoch [106/300], Step [14400/27733], Loss: 2.3015\n",
      "Epoch [106/300], Step [14500/27733], Loss: 2.2869\n",
      "Epoch [106/300], Step [14600/27733], Loss: 2.4500\n",
      "Epoch [106/300], Step [14700/27733], Loss: 2.6899\n",
      "Epoch [106/300], Step [14800/27733], Loss: 2.8460\n",
      "Epoch [106/300], Step [14900/27733], Loss: 3.6111\n",
      "Epoch [106/300], Step [15000/27733], Loss: 2.1287\n",
      "Epoch [106/300], Step [15100/27733], Loss: 2.9262\n",
      "Epoch [106/300], Step [15200/27733], Loss: 3.1450\n",
      "Epoch [106/300], Step [15300/27733], Loss: 3.4216\n",
      "Epoch [106/300], Step [15400/27733], Loss: 3.0512\n",
      "Epoch [106/300], Step [15500/27733], Loss: 2.3376\n",
      "Epoch [106/300], Step [15600/27733], Loss: 2.9220\n",
      "Epoch [106/300], Step [15700/27733], Loss: 2.8799\n",
      "Epoch [106/300], Step [15800/27733], Loss: 2.7422\n",
      "Epoch [106/300], Step [15900/27733], Loss: 3.9500\n",
      "Epoch [106/300], Step [16000/27733], Loss: 2.6849\n",
      "Epoch [106/300], Step [16100/27733], Loss: 3.0722\n",
      "Epoch [106/300], Step [16200/27733], Loss: 2.5287\n",
      "Epoch [106/300], Step [16300/27733], Loss: 2.5218\n",
      "Epoch [106/300], Step [16400/27733], Loss: 3.1127\n",
      "Epoch [106/300], Step [16500/27733], Loss: 2.3885\n",
      "Epoch [106/300], Step [16600/27733], Loss: 2.6293\n",
      "Epoch [106/300], Step [16700/27733], Loss: 3.2129\n",
      "Epoch [106/300], Step [16800/27733], Loss: 2.7200\n",
      "Epoch [106/300], Step [16900/27733], Loss: 2.6557\n",
      "Epoch [106/300], Step [17000/27733], Loss: 2.4402\n",
      "Epoch [106/300], Step [17100/27733], Loss: 2.3839\n",
      "Epoch [106/300], Step [17200/27733], Loss: 2.7541\n",
      "Epoch [106/300], Step [17300/27733], Loss: 2.2743\n",
      "Epoch [106/300], Step [17400/27733], Loss: 2.7897\n",
      "Epoch [106/300], Step [17500/27733], Loss: 2.9459\n",
      "Epoch [106/300], Step [17600/27733], Loss: 1.8807\n",
      "Epoch [106/300], Step [17700/27733], Loss: 2.6830\n",
      "Epoch [106/300], Step [17800/27733], Loss: 2.8586\n",
      "Epoch [106/300], Step [17900/27733], Loss: 3.2664\n",
      "Epoch [106/300], Step [18000/27733], Loss: 2.1566\n",
      "Epoch [106/300], Step [18100/27733], Loss: 2.6924\n",
      "Epoch [106/300], Step [18200/27733], Loss: 2.6890\n",
      "Epoch [106/300], Step [18300/27733], Loss: 2.3837\n",
      "Epoch [106/300], Step [18400/27733], Loss: 2.7488\n",
      "Epoch [106/300], Step [18500/27733], Loss: 2.5281\n",
      "Epoch [106/300], Step [18600/27733], Loss: 3.3318\n",
      "Epoch [106/300], Step [18700/27733], Loss: 2.5825\n",
      "Epoch [106/300], Step [18800/27733], Loss: 2.9846\n",
      "Epoch [106/300], Step [18900/27733], Loss: 3.1851\n",
      "Epoch [106/300], Step [19000/27733], Loss: 3.6761\n",
      "Epoch [106/300], Step [19100/27733], Loss: 3.5297\n",
      "Epoch [106/300], Step [19200/27733], Loss: 2.5641\n",
      "Epoch [106/300], Step [19300/27733], Loss: 3.5742\n",
      "Epoch [106/300], Step [19400/27733], Loss: 3.5843\n",
      "Epoch [106/300], Step [19500/27733], Loss: 2.5451\n",
      "Epoch [106/300], Step [19600/27733], Loss: 3.0691\n",
      "Epoch [106/300], Step [19700/27733], Loss: 2.1563\n",
      "Epoch [106/300], Step [19800/27733], Loss: 3.4838\n",
      "Epoch [106/300], Step [19900/27733], Loss: 2.0985\n",
      "Epoch [106/300], Step [20000/27733], Loss: 3.9986\n",
      "Epoch [106/300], Step [20100/27733], Loss: 2.8296\n",
      "Epoch [106/300], Step [20200/27733], Loss: 2.4358\n",
      "Epoch [106/300], Step [20300/27733], Loss: 3.4340\n",
      "Epoch [106/300], Step [20400/27733], Loss: 3.1911\n",
      "Epoch [106/300], Step [20500/27733], Loss: 2.9775\n",
      "Epoch [106/300], Step [20600/27733], Loss: 3.2053\n",
      "Epoch [106/300], Step [20700/27733], Loss: 2.6518\n",
      "Epoch [106/300], Step [20800/27733], Loss: 3.4339\n",
      "Epoch [106/300], Step [20900/27733], Loss: 3.1153\n",
      "Epoch [106/300], Step [21000/27733], Loss: 2.7482\n",
      "Epoch [106/300], Step [21100/27733], Loss: 3.0081\n",
      "Epoch [106/300], Step [21200/27733], Loss: 3.9890\n",
      "Epoch [106/300], Step [21300/27733], Loss: 3.4622\n",
      "Epoch [106/300], Step [21400/27733], Loss: 2.6170\n",
      "Epoch [106/300], Step [21500/27733], Loss: 2.7842\n",
      "Epoch [106/300], Step [21600/27733], Loss: 2.7524\n",
      "Epoch [106/300], Step [21700/27733], Loss: 2.6386\n",
      "Epoch [106/300], Step [21800/27733], Loss: 2.8885\n",
      "Epoch [106/300], Step [21900/27733], Loss: 3.2563\n",
      "Epoch [106/300], Step [22000/27733], Loss: 3.4962\n",
      "Epoch [106/300], Step [22100/27733], Loss: 3.2259\n",
      "Epoch [106/300], Step [22200/27733], Loss: 2.7761\n",
      "Epoch [106/300], Step [22300/27733], Loss: 3.3702\n",
      "Epoch [106/300], Step [22400/27733], Loss: 3.1813\n",
      "Epoch [106/300], Step [22500/27733], Loss: 3.3583\n",
      "Epoch [106/300], Step [22600/27733], Loss: 2.9254\n",
      "Epoch [106/300], Step [22700/27733], Loss: 3.2198\n",
      "Epoch [106/300], Step [22800/27733], Loss: 2.8299\n",
      "Epoch [106/300], Step [22900/27733], Loss: 2.5828\n",
      "Epoch [106/300], Step [23000/27733], Loss: 2.7125\n",
      "Epoch [106/300], Step [23100/27733], Loss: 2.9505\n",
      "Epoch [106/300], Step [23200/27733], Loss: 2.7180\n",
      "Epoch [106/300], Step [23300/27733], Loss: 2.4729\n",
      "Epoch [106/300], Step [23400/27733], Loss: 3.3651\n",
      "Epoch [106/300], Step [23500/27733], Loss: 3.0840\n",
      "Epoch [106/300], Step [23600/27733], Loss: 2.5840\n",
      "Epoch [106/300], Step [23700/27733], Loss: 3.1797\n",
      "Epoch [106/300], Step [23800/27733], Loss: 3.4923\n",
      "Epoch [106/300], Step [23900/27733], Loss: 2.3721\n",
      "Epoch [106/300], Step [24000/27733], Loss: 3.3792\n",
      "Epoch [106/300], Step [24100/27733], Loss: 3.3772\n",
      "Epoch [106/300], Step [24200/27733], Loss: 2.6343\n",
      "Epoch [106/300], Step [24300/27733], Loss: 3.7780\n",
      "Epoch [106/300], Step [24400/27733], Loss: 2.6471\n",
      "Epoch [106/300], Step [24500/27733], Loss: 2.8315\n",
      "Epoch [106/300], Step [24600/27733], Loss: 3.2603\n",
      "Epoch [106/300], Step [24700/27733], Loss: 3.2013\n",
      "Epoch [106/300], Step [24800/27733], Loss: 2.7103\n",
      "Epoch [106/300], Step [24900/27733], Loss: 2.9145\n",
      "Epoch [106/300], Step [25000/27733], Loss: 3.4784\n",
      "Epoch [106/300], Step [25100/27733], Loss: 3.3955\n",
      "Epoch [106/300], Step [25200/27733], Loss: 3.2033\n",
      "Epoch [106/300], Step [25300/27733], Loss: 2.8998\n",
      "Epoch [106/300], Step [25400/27733], Loss: 3.5769\n",
      "Epoch [106/300], Step [25500/27733], Loss: 3.0731\n",
      "Epoch [106/300], Step [25600/27733], Loss: 2.2936\n",
      "Epoch [106/300], Step [25700/27733], Loss: 3.6373\n",
      "Epoch [106/300], Step [25800/27733], Loss: 2.6739\n",
      "Epoch [106/300], Step [25900/27733], Loss: 2.1809\n",
      "Epoch [106/300], Step [26000/27733], Loss: 3.6902\n",
      "Epoch [106/300], Step [26100/27733], Loss: 2.7302\n",
      "Epoch [106/300], Step [26200/27733], Loss: 2.8597\n",
      "Epoch [106/300], Step [26300/27733], Loss: 2.3592\n",
      "Epoch [106/300], Step [26400/27733], Loss: 3.0217\n",
      "Epoch [106/300], Step [26500/27733], Loss: 2.7757\n",
      "Epoch [106/300], Step [26600/27733], Loss: 1.6620\n",
      "Epoch [106/300], Step [26700/27733], Loss: 3.8918\n",
      "Epoch [106/300], Step [26800/27733], Loss: 2.7628\n",
      "Epoch [106/300], Step [26900/27733], Loss: 3.1322\n",
      "Epoch [106/300], Step [27000/27733], Loss: 3.0598\n",
      "Epoch [106/300], Step [27100/27733], Loss: 2.5822\n",
      "Epoch [106/300], Step [27200/27733], Loss: 3.4476\n",
      "Epoch [106/300], Step [27300/27733], Loss: 3.2787\n",
      "Epoch [106/300], Step [27400/27733], Loss: 2.8962\n",
      "Epoch [106/300], Step [27500/27733], Loss: 4.0144\n",
      "Epoch [106/300], Step [27600/27733], Loss: 3.1578\n",
      "Epoch [106/300], Step [27700/27733], Loss: 2.4961\n",
      "Epoch [107/300], Step [100/27733], Loss: 2.8913\n",
      "Epoch [107/300], Step [200/27733], Loss: 2.1932\n",
      "Epoch [107/300], Step [300/27733], Loss: 2.1742\n",
      "Epoch [107/300], Step [400/27733], Loss: 2.5078\n",
      "Epoch [107/300], Step [500/27733], Loss: 2.9660\n",
      "Epoch [107/300], Step [600/27733], Loss: 1.5536\n",
      "Epoch [107/300], Step [700/27733], Loss: 3.1833\n",
      "Epoch [107/300], Step [800/27733], Loss: 2.1794\n",
      "Epoch [107/300], Step [900/27733], Loss: 2.3083\n",
      "Epoch [107/300], Step [1000/27733], Loss: 2.3287\n",
      "Epoch [107/300], Step [1100/27733], Loss: 1.9756\n",
      "Epoch [107/300], Step [1200/27733], Loss: 2.2775\n",
      "Epoch [107/300], Step [1300/27733], Loss: 2.3928\n",
      "Epoch [107/300], Step [1400/27733], Loss: 2.4351\n",
      "Epoch [107/300], Step [1500/27733], Loss: 2.4513\n",
      "Epoch [107/300], Step [1600/27733], Loss: 2.9631\n",
      "Epoch [107/300], Step [1700/27733], Loss: 2.9230\n",
      "Epoch [107/300], Step [1800/27733], Loss: 2.4327\n",
      "Epoch [107/300], Step [1900/27733], Loss: 2.0175\n",
      "Epoch [107/300], Step [2000/27733], Loss: 2.7175\n",
      "Epoch [107/300], Step [2100/27733], Loss: 2.5965\n",
      "Epoch [107/300], Step [2200/27733], Loss: 1.9204\n",
      "Epoch [107/300], Step [2300/27733], Loss: 2.0408\n",
      "Epoch [107/300], Step [2400/27733], Loss: 2.5716\n",
      "Epoch [107/300], Step [2500/27733], Loss: 2.2046\n",
      "Epoch [107/300], Step [2600/27733], Loss: 2.3074\n",
      "Epoch [107/300], Step [2700/27733], Loss: 2.1926\n",
      "Epoch [107/300], Step [2800/27733], Loss: 2.2798\n",
      "Epoch [107/300], Step [2900/27733], Loss: 3.2715\n",
      "Epoch [107/300], Step [3000/27733], Loss: 2.6104\n",
      "Epoch [107/300], Step [3100/27733], Loss: 2.8158\n",
      "Epoch [107/300], Step [3200/27733], Loss: 2.7923\n",
      "Epoch [107/300], Step [3300/27733], Loss: 1.8309\n",
      "Epoch [107/300], Step [3400/27733], Loss: 3.1428\n",
      "Epoch [107/300], Step [3500/27733], Loss: 2.2702\n",
      "Epoch [107/300], Step [3600/27733], Loss: 2.5198\n",
      "Epoch [107/300], Step [3700/27733], Loss: 2.6089\n",
      "Epoch [107/300], Step [3800/27733], Loss: 1.5616\n",
      "Epoch [107/300], Step [3900/27733], Loss: 2.2506\n",
      "Epoch [107/300], Step [4000/27733], Loss: 2.1010\n",
      "Epoch [107/300], Step [4100/27733], Loss: 3.1730\n",
      "Epoch [107/300], Step [4200/27733], Loss: 3.3000\n",
      "Epoch [107/300], Step [4300/27733], Loss: 1.8387\n",
      "Epoch [107/300], Step [4400/27733], Loss: 2.3796\n",
      "Epoch [107/300], Step [4500/27733], Loss: 3.6047\n",
      "Epoch [107/300], Step [4600/27733], Loss: 3.1882\n",
      "Epoch [107/300], Step [4700/27733], Loss: 1.7282\n",
      "Epoch [107/300], Step [4800/27733], Loss: 1.5479\n",
      "Epoch [107/300], Step [4900/27733], Loss: 2.6473\n",
      "Epoch [107/300], Step [5000/27733], Loss: 3.5845\n",
      "Epoch [107/300], Step [5100/27733], Loss: 2.3463\n",
      "Epoch [107/300], Step [5200/27733], Loss: 3.5582\n",
      "Epoch [107/300], Step [5300/27733], Loss: 3.1517\n",
      "Epoch [107/300], Step [5400/27733], Loss: 3.8293\n",
      "Epoch [107/300], Step [5500/27733], Loss: 2.7507\n",
      "Epoch [107/300], Step [5600/27733], Loss: 2.4856\n",
      "Epoch [107/300], Step [5700/27733], Loss: 2.8307\n",
      "Epoch [107/300], Step [5800/27733], Loss: 3.5874\n",
      "Epoch [107/300], Step [5900/27733], Loss: 2.1426\n",
      "Epoch [107/300], Step [6000/27733], Loss: 3.5781\n",
      "Epoch [107/300], Step [6100/27733], Loss: 3.5673\n",
      "Epoch [107/300], Step [6200/27733], Loss: 2.6866\n",
      "Epoch [107/300], Step [6300/27733], Loss: 3.2334\n",
      "Epoch [107/300], Step [6400/27733], Loss: 3.4496\n",
      "Epoch [107/300], Step [6500/27733], Loss: 2.2713\n",
      "Epoch [107/300], Step [6600/27733], Loss: 2.3217\n",
      "Epoch [107/300], Step [6700/27733], Loss: 2.4558\n",
      "Epoch [107/300], Step [6800/27733], Loss: 3.3766\n",
      "Epoch [107/300], Step [6900/27733], Loss: 3.4310\n",
      "Epoch [107/300], Step [7000/27733], Loss: 2.2348\n",
      "Epoch [107/300], Step [7100/27733], Loss: 3.2714\n",
      "Epoch [107/300], Step [7200/27733], Loss: 2.6034\n",
      "Epoch [107/300], Step [7300/27733], Loss: 2.7895\n",
      "Epoch [107/300], Step [7400/27733], Loss: 2.5914\n",
      "Epoch [107/300], Step [7500/27733], Loss: 2.0872\n",
      "Epoch [107/300], Step [7600/27733], Loss: 2.6566\n",
      "Epoch [107/300], Step [7700/27733], Loss: 2.0038\n",
      "Epoch [107/300], Step [7800/27733], Loss: 2.5045\n",
      "Epoch [107/300], Step [7900/27733], Loss: 3.2347\n",
      "Epoch [107/300], Step [8000/27733], Loss: 2.6545\n",
      "Epoch [107/300], Step [8100/27733], Loss: 2.4614\n",
      "Epoch [107/300], Step [8200/27733], Loss: 3.0884\n",
      "Epoch [107/300], Step [8300/27733], Loss: 3.4064\n",
      "Epoch [107/300], Step [8400/27733], Loss: 2.6871\n",
      "Epoch [107/300], Step [8500/27733], Loss: 2.8156\n",
      "Epoch [107/300], Step [8600/27733], Loss: 3.9428\n",
      "Epoch [107/300], Step [8700/27733], Loss: 2.7301\n",
      "Epoch [107/300], Step [8800/27733], Loss: 2.8846\n",
      "Epoch [107/300], Step [8900/27733], Loss: 2.3328\n",
      "Epoch [107/300], Step [9000/27733], Loss: 2.8820\n",
      "Epoch [107/300], Step [9100/27733], Loss: 3.2056\n",
      "Epoch [107/300], Step [9200/27733], Loss: 3.9030\n",
      "Epoch [107/300], Step [9300/27733], Loss: 3.4991\n",
      "Epoch [107/300], Step [9400/27733], Loss: 3.9476\n",
      "Epoch [107/300], Step [9500/27733], Loss: 3.4642\n",
      "Epoch [107/300], Step [9600/27733], Loss: 2.6658\n",
      "Epoch [107/300], Step [9700/27733], Loss: 2.2668\n",
      "Epoch [107/300], Step [9800/27733], Loss: 3.1633\n",
      "Epoch [107/300], Step [9900/27733], Loss: 1.9207\n",
      "Epoch [107/300], Step [10000/27733], Loss: 2.9087\n",
      "Epoch [107/300], Step [10100/27733], Loss: 3.2265\n",
      "Epoch [107/300], Step [10200/27733], Loss: 2.1209\n",
      "Epoch [107/300], Step [10300/27733], Loss: 3.0849\n",
      "Epoch [107/300], Step [10400/27733], Loss: 2.7343\n",
      "Epoch [107/300], Step [10500/27733], Loss: 2.4411\n",
      "Epoch [107/300], Step [10600/27733], Loss: 3.4609\n",
      "Epoch [107/300], Step [10700/27733], Loss: 3.1035\n",
      "Epoch [107/300], Step [10800/27733], Loss: 2.6571\n",
      "Epoch [107/300], Step [10900/27733], Loss: 2.5656\n",
      "Epoch [107/300], Step [11000/27733], Loss: 2.7024\n",
      "Epoch [107/300], Step [11100/27733], Loss: 2.3182\n",
      "Epoch [107/300], Step [11200/27733], Loss: 2.5874\n",
      "Epoch [107/300], Step [11300/27733], Loss: 2.5397\n",
      "Epoch [107/300], Step [11400/27733], Loss: 2.1593\n",
      "Epoch [107/300], Step [11500/27733], Loss: 2.8007\n",
      "Epoch [107/300], Step [11600/27733], Loss: 2.4200\n",
      "Epoch [107/300], Step [11700/27733], Loss: 3.1876\n",
      "Epoch [107/300], Step [11800/27733], Loss: 2.7246\n",
      "Epoch [107/300], Step [11900/27733], Loss: 2.6022\n",
      "Epoch [107/300], Step [12000/27733], Loss: 3.2917\n",
      "Epoch [107/300], Step [12100/27733], Loss: 3.1237\n",
      "Epoch [107/300], Step [12200/27733], Loss: 3.7409\n",
      "Epoch [107/300], Step [12300/27733], Loss: 3.1202\n",
      "Epoch [107/300], Step [12400/27733], Loss: 2.9824\n",
      "Epoch [107/300], Step [12500/27733], Loss: 2.1318\n",
      "Epoch [107/300], Step [12600/27733], Loss: 3.4257\n",
      "Epoch [107/300], Step [12700/27733], Loss: 2.1816\n",
      "Epoch [107/300], Step [12800/27733], Loss: 2.6623\n",
      "Epoch [107/300], Step [12900/27733], Loss: 2.6450\n",
      "Epoch [107/300], Step [13000/27733], Loss: 2.7724\n",
      "Epoch [107/300], Step [13100/27733], Loss: 2.2402\n",
      "Epoch [107/300], Step [13200/27733], Loss: 3.6023\n",
      "Epoch [107/300], Step [13300/27733], Loss: 2.6627\n",
      "Epoch [107/300], Step [13400/27733], Loss: 2.5528\n",
      "Epoch [107/300], Step [13500/27733], Loss: 2.3794\n",
      "Epoch [107/300], Step [13600/27733], Loss: 2.6337\n",
      "Epoch [107/300], Step [13700/27733], Loss: 2.9822\n",
      "Epoch [107/300], Step [13800/27733], Loss: 1.7544\n",
      "Epoch [107/300], Step [13900/27733], Loss: 2.1856\n",
      "Epoch [107/300], Step [14000/27733], Loss: 2.4957\n",
      "Epoch [107/300], Step [14100/27733], Loss: 2.5887\n",
      "Epoch [107/300], Step [14200/27733], Loss: 2.4059\n",
      "Epoch [107/300], Step [14300/27733], Loss: 3.0129\n",
      "Epoch [107/300], Step [14400/27733], Loss: 3.5549\n",
      "Epoch [107/300], Step [14500/27733], Loss: 3.1997\n",
      "Epoch [107/300], Step [14600/27733], Loss: 3.2926\n",
      "Epoch [107/300], Step [14700/27733], Loss: 2.5345\n",
      "Epoch [107/300], Step [14800/27733], Loss: 3.0295\n",
      "Epoch [107/300], Step [14900/27733], Loss: 2.7669\n",
      "Epoch [107/300], Step [15000/27733], Loss: 3.4622\n",
      "Epoch [107/300], Step [15100/27733], Loss: 2.3940\n",
      "Epoch [107/300], Step [15200/27733], Loss: 2.6886\n",
      "Epoch [107/300], Step [15300/27733], Loss: 2.2871\n",
      "Epoch [107/300], Step [15400/27733], Loss: 2.6743\n",
      "Epoch [107/300], Step [15500/27733], Loss: 2.9886\n",
      "Epoch [107/300], Step [15600/27733], Loss: 2.5641\n",
      "Epoch [107/300], Step [15700/27733], Loss: 2.6586\n",
      "Epoch [107/300], Step [15800/27733], Loss: 2.6068\n",
      "Epoch [107/300], Step [15900/27733], Loss: 3.1958\n",
      "Epoch [107/300], Step [16000/27733], Loss: 2.8824\n",
      "Epoch [107/300], Step [16100/27733], Loss: 3.9997\n",
      "Epoch [107/300], Step [16200/27733], Loss: 2.2708\n",
      "Epoch [107/300], Step [16300/27733], Loss: 3.3419\n",
      "Epoch [107/300], Step [16400/27733], Loss: 3.5620\n",
      "Epoch [107/300], Step [16500/27733], Loss: 3.6042\n",
      "Epoch [107/300], Step [16600/27733], Loss: 3.1019\n",
      "Epoch [107/300], Step [16700/27733], Loss: 2.8775\n",
      "Epoch [107/300], Step [16800/27733], Loss: 3.8357\n",
      "Epoch [107/300], Step [16900/27733], Loss: 2.5845\n",
      "Epoch [107/300], Step [17000/27733], Loss: 2.2509\n",
      "Epoch [107/300], Step [17100/27733], Loss: 2.9469\n",
      "Epoch [107/300], Step [17200/27733], Loss: 2.5877\n",
      "Epoch [107/300], Step [17300/27733], Loss: 3.1419\n",
      "Epoch [107/300], Step [17400/27733], Loss: 2.8447\n",
      "Epoch [107/300], Step [17500/27733], Loss: 3.8477\n",
      "Epoch [107/300], Step [17600/27733], Loss: 2.8481\n",
      "Epoch [107/300], Step [17700/27733], Loss: 3.8685\n",
      "Epoch [107/300], Step [17800/27733], Loss: 2.7664\n",
      "Epoch [107/300], Step [17900/27733], Loss: 2.6793\n",
      "Epoch [107/300], Step [18000/27733], Loss: 2.8857\n",
      "Epoch [107/300], Step [18100/27733], Loss: 2.8249\n",
      "Epoch [107/300], Step [18200/27733], Loss: 3.1504\n",
      "Epoch [107/300], Step [18300/27733], Loss: 2.7295\n",
      "Epoch [107/300], Step [18400/27733], Loss: 2.9503\n",
      "Epoch [107/300], Step [18500/27733], Loss: 2.5109\n",
      "Epoch [107/300], Step [18600/27733], Loss: 2.3933\n",
      "Epoch [107/300], Step [18700/27733], Loss: 2.2929\n",
      "Epoch [107/300], Step [18800/27733], Loss: 3.0923\n",
      "Epoch [107/300], Step [18900/27733], Loss: 2.0184\n",
      "Epoch [107/300], Step [19000/27733], Loss: 3.0107\n",
      "Epoch [107/300], Step [19100/27733], Loss: 2.4752\n",
      "Epoch [107/300], Step [19200/27733], Loss: 3.7348\n",
      "Epoch [107/300], Step [19300/27733], Loss: 3.0908\n",
      "Epoch [107/300], Step [19400/27733], Loss: 3.4490\n",
      "Epoch [107/300], Step [19500/27733], Loss: 2.5728\n",
      "Epoch [107/300], Step [19600/27733], Loss: 3.1372\n",
      "Epoch [107/300], Step [19700/27733], Loss: 3.0080\n",
      "Epoch [107/300], Step [19800/27733], Loss: 3.4047\n",
      "Epoch [107/300], Step [19900/27733], Loss: 2.9071\n",
      "Epoch [107/300], Step [20000/27733], Loss: 3.2342\n",
      "Epoch [107/300], Step [20100/27733], Loss: 3.6282\n",
      "Epoch [107/300], Step [20200/27733], Loss: 2.5050\n",
      "Epoch [107/300], Step [20300/27733], Loss: 2.6884\n",
      "Epoch [107/300], Step [20400/27733], Loss: 3.3216\n",
      "Epoch [107/300], Step [20500/27733], Loss: 3.1714\n",
      "Epoch [107/300], Step [20600/27733], Loss: 2.8374\n",
      "Epoch [107/300], Step [20700/27733], Loss: 2.9502\n",
      "Epoch [107/300], Step [20800/27733], Loss: 4.2095\n",
      "Epoch [107/300], Step [20900/27733], Loss: 3.1354\n",
      "Epoch [107/300], Step [21000/27733], Loss: 1.9912\n",
      "Epoch [107/300], Step [21100/27733], Loss: 3.1671\n",
      "Epoch [107/300], Step [21200/27733], Loss: 2.5026\n",
      "Epoch [107/300], Step [21300/27733], Loss: 3.8634\n",
      "Epoch [107/300], Step [21400/27733], Loss: 2.7977\n",
      "Epoch [107/300], Step [21500/27733], Loss: 3.2198\n",
      "Epoch [107/300], Step [21600/27733], Loss: 3.5671\n",
      "Epoch [107/300], Step [21700/27733], Loss: 2.3777\n",
      "Epoch [107/300], Step [21800/27733], Loss: 3.1960\n",
      "Epoch [107/300], Step [21900/27733], Loss: 2.4704\n",
      "Epoch [107/300], Step [22000/27733], Loss: 2.8420\n",
      "Epoch [107/300], Step [22100/27733], Loss: 2.6266\n",
      "Epoch [107/300], Step [22200/27733], Loss: 3.1528\n",
      "Epoch [107/300], Step [22300/27733], Loss: 3.1094\n",
      "Epoch [107/300], Step [22400/27733], Loss: 2.5601\n",
      "Epoch [107/300], Step [22500/27733], Loss: 2.8841\n",
      "Epoch [107/300], Step [22600/27733], Loss: 2.1805\n",
      "Epoch [107/300], Step [22700/27733], Loss: 3.3735\n",
      "Epoch [107/300], Step [22800/27733], Loss: 2.5450\n",
      "Epoch [107/300], Step [22900/27733], Loss: 2.5872\n",
      "Epoch [107/300], Step [23000/27733], Loss: 3.2507\n",
      "Epoch [107/300], Step [23100/27733], Loss: 2.7614\n",
      "Epoch [107/300], Step [23200/27733], Loss: 2.5993\n",
      "Epoch [107/300], Step [23300/27733], Loss: 3.0147\n",
      "Epoch [107/300], Step [23400/27733], Loss: 3.1660\n",
      "Epoch [107/300], Step [23500/27733], Loss: 3.9466\n",
      "Epoch [107/300], Step [23600/27733], Loss: 2.5626\n",
      "Epoch [107/300], Step [23700/27733], Loss: 3.3174\n",
      "Epoch [107/300], Step [23800/27733], Loss: 3.2860\n",
      "Epoch [107/300], Step [23900/27733], Loss: 2.6026\n",
      "Epoch [107/300], Step [24000/27733], Loss: 3.1257\n",
      "Epoch [107/300], Step [24100/27733], Loss: 3.1541\n",
      "Epoch [107/300], Step [24200/27733], Loss: 3.3306\n",
      "Epoch [107/300], Step [24300/27733], Loss: 2.1394\n",
      "Epoch [107/300], Step [24400/27733], Loss: 2.9329\n",
      "Epoch [107/300], Step [24500/27733], Loss: 3.0771\n",
      "Epoch [107/300], Step [24600/27733], Loss: 3.1000\n",
      "Epoch [107/300], Step [24700/27733], Loss: 2.7776\n",
      "Epoch [107/300], Step [24800/27733], Loss: 3.3363\n",
      "Epoch [107/300], Step [24900/27733], Loss: 2.8787\n",
      "Epoch [107/300], Step [25000/27733], Loss: 2.8966\n",
      "Epoch [107/300], Step [25100/27733], Loss: 3.2184\n",
      "Epoch [107/300], Step [25200/27733], Loss: 2.4829\n",
      "Epoch [107/300], Step [25300/27733], Loss: 3.1034\n",
      "Epoch [107/300], Step [25400/27733], Loss: 2.7370\n",
      "Epoch [107/300], Step [25500/27733], Loss: 3.8760\n",
      "Epoch [107/300], Step [25600/27733], Loss: 3.4360\n",
      "Epoch [107/300], Step [25700/27733], Loss: 3.3903\n",
      "Epoch [107/300], Step [25800/27733], Loss: 2.6259\n",
      "Epoch [107/300], Step [25900/27733], Loss: 2.5210\n",
      "Epoch [107/300], Step [26000/27733], Loss: 2.9079\n",
      "Epoch [107/300], Step [26100/27733], Loss: 3.7266\n",
      "Epoch [107/300], Step [26200/27733], Loss: 2.8240\n",
      "Epoch [107/300], Step [26300/27733], Loss: 3.0466\n",
      "Epoch [107/300], Step [26400/27733], Loss: 2.9106\n",
      "Epoch [107/300], Step [26500/27733], Loss: 2.8737\n",
      "Epoch [107/300], Step [26600/27733], Loss: 3.5018\n",
      "Epoch [107/300], Step [26700/27733], Loss: 3.0932\n",
      "Epoch [107/300], Step [26800/27733], Loss: 3.6989\n",
      "Epoch [107/300], Step [26900/27733], Loss: 2.9102\n",
      "Epoch [107/300], Step [27000/27733], Loss: 3.3557\n",
      "Epoch [107/300], Step [27100/27733], Loss: 3.9060\n",
      "Epoch [107/300], Step [27200/27733], Loss: 2.8753\n",
      "Epoch [107/300], Step [27300/27733], Loss: 4.0974\n",
      "Epoch [107/300], Step [27400/27733], Loss: 3.5997\n",
      "Epoch [107/300], Step [27500/27733], Loss: 2.7977\n",
      "Epoch [107/300], Step [27600/27733], Loss: 3.4026\n",
      "Epoch [107/300], Step [27700/27733], Loss: 3.0427\n",
      "Epoch [108/300], Step [100/27733], Loss: 2.8844\n",
      "Epoch [108/300], Step [200/27733], Loss: 1.7946\n",
      "Epoch [108/300], Step [300/27733], Loss: 2.6680\n",
      "Epoch [108/300], Step [400/27733], Loss: 2.9841\n",
      "Epoch [108/300], Step [500/27733], Loss: 3.5409\n",
      "Epoch [108/300], Step [600/27733], Loss: 1.6095\n",
      "Epoch [108/300], Step [700/27733], Loss: 2.1557\n",
      "Epoch [108/300], Step [800/27733], Loss: 1.9500\n",
      "Epoch [108/300], Step [900/27733], Loss: 3.6752\n",
      "Epoch [108/300], Step [1000/27733], Loss: 1.7277\n",
      "Epoch [108/300], Step [1100/27733], Loss: 2.2934\n",
      "Epoch [108/300], Step [1200/27733], Loss: 2.4118\n",
      "Epoch [108/300], Step [1300/27733], Loss: 2.5809\n",
      "Epoch [108/300], Step [1400/27733], Loss: 2.2668\n",
      "Epoch [108/300], Step [1500/27733], Loss: 2.7378\n",
      "Epoch [108/300], Step [1600/27733], Loss: 2.2802\n",
      "Epoch [108/300], Step [1700/27733], Loss: 3.2349\n",
      "Epoch [108/300], Step [1800/27733], Loss: 2.4987\n",
      "Epoch [108/300], Step [1900/27733], Loss: 3.2492\n",
      "Epoch [108/300], Step [2000/27733], Loss: 1.8478\n",
      "Epoch [108/300], Step [2100/27733], Loss: 2.2995\n",
      "Epoch [108/300], Step [2200/27733], Loss: 2.0015\n",
      "Epoch [108/300], Step [2300/27733], Loss: 2.3295\n",
      "Epoch [108/300], Step [2400/27733], Loss: 2.5772\n",
      "Epoch [108/300], Step [2500/27733], Loss: 3.0786\n",
      "Epoch [108/300], Step [2600/27733], Loss: 2.6917\n",
      "Epoch [108/300], Step [2700/27733], Loss: 1.9520\n",
      "Epoch [108/300], Step [2800/27733], Loss: 2.8344\n",
      "Epoch [108/300], Step [2900/27733], Loss: 2.3742\n",
      "Epoch [108/300], Step [3000/27733], Loss: 2.7082\n",
      "Epoch [108/300], Step [3100/27733], Loss: 3.6135\n",
      "Epoch [108/300], Step [3200/27733], Loss: 2.4399\n",
      "Epoch [108/300], Step [3300/27733], Loss: 2.4608\n",
      "Epoch [108/300], Step [3400/27733], Loss: 2.4701\n",
      "Epoch [108/300], Step [3500/27733], Loss: 2.0891\n",
      "Epoch [108/300], Step [3600/27733], Loss: 3.5402\n",
      "Epoch [108/300], Step [3700/27733], Loss: 2.8831\n",
      "Epoch [108/300], Step [3800/27733], Loss: 2.7756\n",
      "Epoch [108/300], Step [3900/27733], Loss: 3.0555\n",
      "Epoch [108/300], Step [4000/27733], Loss: 2.5668\n",
      "Epoch [108/300], Step [4100/27733], Loss: 3.0174\n",
      "Epoch [108/300], Step [4200/27733], Loss: 2.6733\n",
      "Epoch [108/300], Step [4300/27733], Loss: 2.2409\n",
      "Epoch [108/300], Step [4400/27733], Loss: 2.2925\n",
      "Epoch [108/300], Step [4500/27733], Loss: 2.8184\n",
      "Epoch [108/300], Step [4600/27733], Loss: 4.1894\n",
      "Epoch [108/300], Step [4700/27733], Loss: 3.1129\n",
      "Epoch [108/300], Step [4800/27733], Loss: 2.6225\n",
      "Epoch [108/300], Step [4900/27733], Loss: 2.7214\n",
      "Epoch [108/300], Step [5000/27733], Loss: 2.5662\n",
      "Epoch [108/300], Step [5100/27733], Loss: 2.4959\n",
      "Epoch [108/300], Step [5200/27733], Loss: 2.6723\n",
      "Epoch [108/300], Step [5300/27733], Loss: 2.6737\n",
      "Epoch [108/300], Step [5400/27733], Loss: 3.5197\n",
      "Epoch [108/300], Step [5500/27733], Loss: 1.9566\n",
      "Epoch [108/300], Step [5600/27733], Loss: 3.2524\n",
      "Epoch [108/300], Step [5700/27733], Loss: 2.6676\n",
      "Epoch [108/300], Step [5800/27733], Loss: 2.8846\n",
      "Epoch [108/300], Step [5900/27733], Loss: 2.9901\n",
      "Epoch [108/300], Step [6000/27733], Loss: 2.5446\n",
      "Epoch [108/300], Step [6100/27733], Loss: 2.9672\n",
      "Epoch [108/300], Step [6200/27733], Loss: 2.6810\n",
      "Epoch [108/300], Step [6300/27733], Loss: 3.0121\n",
      "Epoch [108/300], Step [6400/27733], Loss: 2.9970\n",
      "Epoch [108/300], Step [6500/27733], Loss: 2.4432\n",
      "Epoch [108/300], Step [6600/27733], Loss: 2.6387\n",
      "Epoch [108/300], Step [6700/27733], Loss: 1.8829\n",
      "Epoch [108/300], Step [6800/27733], Loss: 2.0638\n",
      "Epoch [108/300], Step [6900/27733], Loss: 2.3407\n",
      "Epoch [108/300], Step [7000/27733], Loss: 2.5181\n",
      "Epoch [108/300], Step [7100/27733], Loss: 2.3208\n",
      "Epoch [108/300], Step [7200/27733], Loss: 2.6121\n",
      "Epoch [108/300], Step [7300/27733], Loss: 2.7890\n",
      "Epoch [108/300], Step [7400/27733], Loss: 3.0157\n",
      "Epoch [108/300], Step [7500/27733], Loss: 3.4688\n",
      "Epoch [108/300], Step [7600/27733], Loss: 3.0823\n",
      "Epoch [108/300], Step [7700/27733], Loss: 2.7400\n",
      "Epoch [108/300], Step [7800/27733], Loss: 2.4693\n",
      "Epoch [108/300], Step [7900/27733], Loss: 1.8842\n",
      "Epoch [108/300], Step [8000/27733], Loss: 3.2714\n",
      "Epoch [108/300], Step [8100/27733], Loss: 3.0304\n",
      "Epoch [108/300], Step [8200/27733], Loss: 2.5649\n",
      "Epoch [108/300], Step [8300/27733], Loss: 2.1697\n",
      "Epoch [108/300], Step [8400/27733], Loss: 2.8212\n",
      "Epoch [108/300], Step [8500/27733], Loss: 3.0074\n",
      "Epoch [108/300], Step [8600/27733], Loss: 2.6757\n",
      "Epoch [108/300], Step [8700/27733], Loss: 2.7501\n",
      "Epoch [108/300], Step [8800/27733], Loss: 2.2905\n",
      "Epoch [108/300], Step [8900/27733], Loss: 3.2593\n",
      "Epoch [108/300], Step [9000/27733], Loss: 2.9159\n",
      "Epoch [108/300], Step [9100/27733], Loss: 2.5347\n",
      "Epoch [108/300], Step [9200/27733], Loss: 2.2708\n",
      "Epoch [108/300], Step [9300/27733], Loss: 3.0851\n",
      "Epoch [108/300], Step [9400/27733], Loss: 2.6837\n",
      "Epoch [108/300], Step [9500/27733], Loss: 2.6147\n",
      "Epoch [108/300], Step [9600/27733], Loss: 3.1502\n",
      "Epoch [108/300], Step [9700/27733], Loss: 2.8616\n",
      "Epoch [108/300], Step [9800/27733], Loss: 3.7651\n",
      "Epoch [108/300], Step [9900/27733], Loss: 2.8896\n",
      "Epoch [108/300], Step [10000/27733], Loss: 3.0350\n",
      "Epoch [108/300], Step [10100/27733], Loss: 2.6086\n",
      "Epoch [108/300], Step [10200/27733], Loss: 2.6850\n",
      "Epoch [108/300], Step [10300/27733], Loss: 2.1117\n",
      "Epoch [108/300], Step [10400/27733], Loss: 3.0483\n",
      "Epoch [108/300], Step [10500/27733], Loss: 2.6034\n",
      "Epoch [108/300], Step [10600/27733], Loss: 2.8284\n",
      "Epoch [108/300], Step [10700/27733], Loss: 2.8680\n",
      "Epoch [108/300], Step [10800/27733], Loss: 2.7097\n",
      "Epoch [108/300], Step [10900/27733], Loss: 2.0878\n",
      "Epoch [108/300], Step [11000/27733], Loss: 2.5909\n",
      "Epoch [108/300], Step [11100/27733], Loss: 3.2259\n",
      "Epoch [108/300], Step [11200/27733], Loss: 2.6678\n",
      "Epoch [108/300], Step [11300/27733], Loss: 2.4630\n",
      "Epoch [108/300], Step [11400/27733], Loss: 2.4402\n",
      "Epoch [108/300], Step [11500/27733], Loss: 3.3668\n",
      "Epoch [108/300], Step [11600/27733], Loss: 4.0808\n",
      "Epoch [108/300], Step [11700/27733], Loss: 2.4719\n",
      "Epoch [108/300], Step [11800/27733], Loss: 2.7885\n",
      "Epoch [108/300], Step [11900/27733], Loss: 2.4043\n",
      "Epoch [108/300], Step [12000/27733], Loss: 3.2502\n",
      "Epoch [108/300], Step [12100/27733], Loss: 2.2550\n",
      "Epoch [108/300], Step [12200/27733], Loss: 2.9743\n",
      "Epoch [108/300], Step [12300/27733], Loss: 2.4688\n",
      "Epoch [108/300], Step [12400/27733], Loss: 2.8565\n",
      "Epoch [108/300], Step [12500/27733], Loss: 1.5525\n",
      "Epoch [108/300], Step [12600/27733], Loss: 3.5428\n",
      "Epoch [108/300], Step [12700/27733], Loss: 2.6955\n",
      "Epoch [108/300], Step [12800/27733], Loss: 1.9431\n",
      "Epoch [108/300], Step [12900/27733], Loss: 2.9632\n",
      "Epoch [108/300], Step [13000/27733], Loss: 2.8359\n",
      "Epoch [108/300], Step [13100/27733], Loss: 3.0379\n",
      "Epoch [108/300], Step [13200/27733], Loss: 2.9626\n",
      "Epoch [108/300], Step [13300/27733], Loss: 2.8031\n",
      "Epoch [108/300], Step [13400/27733], Loss: 2.4595\n",
      "Epoch [108/300], Step [13500/27733], Loss: 3.5385\n",
      "Epoch [108/300], Step [13600/27733], Loss: 2.5528\n",
      "Epoch [108/300], Step [13700/27733], Loss: 3.4497\n",
      "Epoch [108/300], Step [13800/27733], Loss: 2.9646\n",
      "Epoch [108/300], Step [13900/27733], Loss: 3.3264\n",
      "Epoch [108/300], Step [14000/27733], Loss: 2.6775\n",
      "Epoch [108/300], Step [14100/27733], Loss: 3.7887\n",
      "Epoch [108/300], Step [14200/27733], Loss: 2.2894\n",
      "Epoch [108/300], Step [14300/27733], Loss: 2.4879\n",
      "Epoch [108/300], Step [14400/27733], Loss: 2.4563\n",
      "Epoch [108/300], Step [14500/27733], Loss: 2.3282\n",
      "Epoch [108/300], Step [14600/27733], Loss: 2.7358\n",
      "Epoch [108/300], Step [14700/27733], Loss: 3.2572\n",
      "Epoch [108/300], Step [14800/27733], Loss: 2.5930\n",
      "Epoch [108/300], Step [14900/27733], Loss: 2.7776\n",
      "Epoch [108/300], Step [15000/27733], Loss: 3.3812\n",
      "Epoch [108/300], Step [15100/27733], Loss: 2.7366\n",
      "Epoch [108/300], Step [15200/27733], Loss: 3.1702\n",
      "Epoch [108/300], Step [15300/27733], Loss: 2.1511\n",
      "Epoch [108/300], Step [15400/27733], Loss: 3.2948\n",
      "Epoch [108/300], Step [15500/27733], Loss: 1.8163\n",
      "Epoch [108/300], Step [15600/27733], Loss: 2.3580\n",
      "Epoch [108/300], Step [15700/27733], Loss: 2.6460\n",
      "Epoch [108/300], Step [15800/27733], Loss: 3.2658\n",
      "Epoch [108/300], Step [15900/27733], Loss: 3.1996\n",
      "Epoch [108/300], Step [16000/27733], Loss: 2.3555\n",
      "Epoch [108/300], Step [16100/27733], Loss: 3.2741\n",
      "Epoch [108/300], Step [16200/27733], Loss: 2.9724\n",
      "Epoch [108/300], Step [16300/27733], Loss: 3.4639\n",
      "Epoch [108/300], Step [16400/27733], Loss: 3.3128\n",
      "Epoch [108/300], Step [16500/27733], Loss: 2.6633\n",
      "Epoch [108/300], Step [16600/27733], Loss: 2.9876\n",
      "Epoch [108/300], Step [16700/27733], Loss: 2.8996\n",
      "Epoch [108/300], Step [16800/27733], Loss: 3.2310\n",
      "Epoch [108/300], Step [16900/27733], Loss: 3.1693\n",
      "Epoch [108/300], Step [17000/27733], Loss: 3.0990\n",
      "Epoch [108/300], Step [17100/27733], Loss: 3.0464\n",
      "Epoch [108/300], Step [17200/27733], Loss: 3.2676\n",
      "Epoch [108/300], Step [17300/27733], Loss: 2.6784\n",
      "Epoch [108/300], Step [17400/27733], Loss: 2.5722\n",
      "Epoch [108/300], Step [17500/27733], Loss: 2.3032\n",
      "Epoch [108/300], Step [17600/27733], Loss: 3.5191\n",
      "Epoch [108/300], Step [17700/27733], Loss: 3.2216\n",
      "Epoch [108/300], Step [17800/27733], Loss: 2.7140\n",
      "Epoch [108/300], Step [17900/27733], Loss: 2.7326\n",
      "Epoch [108/300], Step [18000/27733], Loss: 3.0023\n",
      "Epoch [108/300], Step [18100/27733], Loss: 2.7666\n",
      "Epoch [108/300], Step [18200/27733], Loss: 3.2826\n",
      "Epoch [108/300], Step [18300/27733], Loss: 2.4381\n",
      "Epoch [108/300], Step [18400/27733], Loss: 2.7015\n",
      "Epoch [108/300], Step [18500/27733], Loss: 3.4150\n",
      "Epoch [108/300], Step [18600/27733], Loss: 2.8350\n",
      "Epoch [108/300], Step [18700/27733], Loss: 2.7063\n",
      "Epoch [108/300], Step [18800/27733], Loss: 3.7661\n",
      "Epoch [108/300], Step [18900/27733], Loss: 3.0252\n",
      "Epoch [108/300], Step [19000/27733], Loss: 2.7461\n",
      "Epoch [108/300], Step [19100/27733], Loss: 3.4426\n",
      "Epoch [108/300], Step [19200/27733], Loss: 2.0990\n",
      "Epoch [108/300], Step [19300/27733], Loss: 3.3312\n",
      "Epoch [108/300], Step [19400/27733], Loss: 3.6101\n",
      "Epoch [108/300], Step [19500/27733], Loss: 3.0682\n",
      "Epoch [108/300], Step [19600/27733], Loss: 2.7653\n",
      "Epoch [108/300], Step [19700/27733], Loss: 2.8388\n",
      "Epoch [108/300], Step [19800/27733], Loss: 2.8156\n",
      "Epoch [108/300], Step [19900/27733], Loss: 3.0093\n",
      "Epoch [108/300], Step [20000/27733], Loss: 3.2659\n",
      "Epoch [108/300], Step [20100/27733], Loss: 2.4521\n",
      "Epoch [108/300], Step [20200/27733], Loss: 2.4592\n",
      "Epoch [108/300], Step [20300/27733], Loss: 3.0296\n",
      "Epoch [108/300], Step [20400/27733], Loss: 3.4649\n",
      "Epoch [108/300], Step [20500/27733], Loss: 2.4588\n",
      "Epoch [108/300], Step [20600/27733], Loss: 2.7657\n",
      "Epoch [108/300], Step [20700/27733], Loss: 2.6110\n",
      "Epoch [108/300], Step [20800/27733], Loss: 2.5680\n",
      "Epoch [108/300], Step [20900/27733], Loss: 4.3922\n",
      "Epoch [108/300], Step [21000/27733], Loss: 2.2819\n",
      "Epoch [108/300], Step [21100/27733], Loss: 2.3809\n",
      "Epoch [108/300], Step [21200/27733], Loss: 3.7890\n",
      "Epoch [108/300], Step [21300/27733], Loss: 2.4107\n",
      "Epoch [108/300], Step [21400/27733], Loss: 2.9353\n",
      "Epoch [108/300], Step [21500/27733], Loss: 2.8065\n",
      "Epoch [108/300], Step [21600/27733], Loss: 3.1068\n",
      "Epoch [108/300], Step [21700/27733], Loss: 2.8434\n",
      "Epoch [108/300], Step [21800/27733], Loss: 2.4235\n",
      "Epoch [108/300], Step [21900/27733], Loss: 2.7403\n",
      "Epoch [108/300], Step [22000/27733], Loss: 2.3044\n",
      "Epoch [108/300], Step [22100/27733], Loss: 2.9143\n",
      "Epoch [108/300], Step [22200/27733], Loss: 2.5607\n",
      "Epoch [108/300], Step [22300/27733], Loss: 3.1111\n",
      "Epoch [108/300], Step [22400/27733], Loss: 3.4109\n",
      "Epoch [108/300], Step [22500/27733], Loss: 3.4369\n",
      "Epoch [108/300], Step [22600/27733], Loss: 3.6420\n",
      "Epoch [108/300], Step [22700/27733], Loss: 2.5271\n",
      "Epoch [108/300], Step [22800/27733], Loss: 2.4736\n",
      "Epoch [108/300], Step [22900/27733], Loss: 2.5100\n",
      "Epoch [108/300], Step [23000/27733], Loss: 2.8499\n",
      "Epoch [108/300], Step [23100/27733], Loss: 2.2275\n",
      "Epoch [108/300], Step [23200/27733], Loss: 2.2083\n",
      "Epoch [108/300], Step [23300/27733], Loss: 3.0542\n",
      "Epoch [108/300], Step [23400/27733], Loss: 3.0121\n",
      "Epoch [108/300], Step [23500/27733], Loss: 2.5151\n",
      "Epoch [108/300], Step [23600/27733], Loss: 2.9306\n",
      "Epoch [108/300], Step [23700/27733], Loss: 2.8260\n",
      "Epoch [108/300], Step [23800/27733], Loss: 3.0035\n",
      "Epoch [108/300], Step [23900/27733], Loss: 2.1607\n",
      "Epoch [108/300], Step [24000/27733], Loss: 4.7975\n",
      "Epoch [108/300], Step [24100/27733], Loss: 2.2082\n",
      "Epoch [108/300], Step [24200/27733], Loss: 3.1859\n",
      "Epoch [108/300], Step [24300/27733], Loss: 2.1307\n",
      "Epoch [108/300], Step [24400/27733], Loss: 3.1320\n",
      "Epoch [108/300], Step [24500/27733], Loss: 2.7133\n",
      "Epoch [108/300], Step [24600/27733], Loss: 3.3133\n",
      "Epoch [108/300], Step [24700/27733], Loss: 3.3370\n",
      "Epoch [108/300], Step [24800/27733], Loss: 2.6667\n",
      "Epoch [108/300], Step [24900/27733], Loss: 2.6156\n",
      "Epoch [108/300], Step [25000/27733], Loss: 3.2598\n",
      "Epoch [108/300], Step [25100/27733], Loss: 3.0710\n",
      "Epoch [108/300], Step [25200/27733], Loss: 2.7648\n",
      "Epoch [108/300], Step [25300/27733], Loss: 3.0110\n",
      "Epoch [108/300], Step [25400/27733], Loss: 2.6021\n",
      "Epoch [108/300], Step [25500/27733], Loss: 2.7969\n",
      "Epoch [108/300], Step [25600/27733], Loss: 3.4226\n",
      "Epoch [108/300], Step [25700/27733], Loss: 3.6967\n",
      "Epoch [108/300], Step [25800/27733], Loss: 2.9796\n",
      "Epoch [108/300], Step [25900/27733], Loss: 3.3236\n",
      "Epoch [108/300], Step [26000/27733], Loss: 3.5160\n",
      "Epoch [108/300], Step [26100/27733], Loss: 3.0406\n",
      "Epoch [108/300], Step [26200/27733], Loss: 2.6047\n",
      "Epoch [108/300], Step [26300/27733], Loss: 3.7511\n",
      "Epoch [108/300], Step [26400/27733], Loss: 3.9768\n",
      "Epoch [108/300], Step [26500/27733], Loss: 2.7026\n",
      "Epoch [108/300], Step [26600/27733], Loss: 2.4735\n",
      "Epoch [108/300], Step [26700/27733], Loss: 3.4676\n",
      "Epoch [108/300], Step [26800/27733], Loss: 3.1609\n",
      "Epoch [108/300], Step [26900/27733], Loss: 2.6658\n",
      "Epoch [108/300], Step [27000/27733], Loss: 2.9819\n",
      "Epoch [108/300], Step [27100/27733], Loss: 2.9481\n",
      "Epoch [108/300], Step [27200/27733], Loss: 4.2307\n",
      "Epoch [108/300], Step [27300/27733], Loss: 3.3700\n",
      "Epoch [108/300], Step [27400/27733], Loss: 3.5700\n",
      "Epoch [108/300], Step [27500/27733], Loss: 3.5464\n",
      "Epoch [108/300], Step [27600/27733], Loss: 3.3467\n",
      "Epoch [108/300], Step [27700/27733], Loss: 3.0702\n",
      "Epoch [109/300], Step [100/27733], Loss: 2.2713\n",
      "Epoch [109/300], Step [200/27733], Loss: 2.3908\n",
      "Epoch [109/300], Step [300/27733], Loss: 2.2715\n",
      "Epoch [109/300], Step [400/27733], Loss: 2.8949\n",
      "Epoch [109/300], Step [500/27733], Loss: 2.5107\n",
      "Epoch [109/300], Step [600/27733], Loss: 2.2568\n",
      "Epoch [109/300], Step [700/27733], Loss: 2.3239\n",
      "Epoch [109/300], Step [800/27733], Loss: 1.8482\n",
      "Epoch [109/300], Step [900/27733], Loss: 2.1331\n",
      "Epoch [109/300], Step [1000/27733], Loss: 2.6038\n",
      "Epoch [109/300], Step [1100/27733], Loss: 2.8169\n",
      "Epoch [109/300], Step [1200/27733], Loss: 2.9018\n",
      "Epoch [109/300], Step [1300/27733], Loss: 2.4661\n",
      "Epoch [109/300], Step [1400/27733], Loss: 2.1768\n",
      "Epoch [109/300], Step [1500/27733], Loss: 1.9499\n",
      "Epoch [109/300], Step [1600/27733], Loss: 2.2829\n",
      "Epoch [109/300], Step [1700/27733], Loss: 2.0221\n",
      "Epoch [109/300], Step [1800/27733], Loss: 3.3557\n",
      "Epoch [109/300], Step [1900/27733], Loss: 2.5299\n",
      "Epoch [109/300], Step [2000/27733], Loss: 2.1274\n",
      "Epoch [109/300], Step [2100/27733], Loss: 2.5189\n",
      "Epoch [109/300], Step [2200/27733], Loss: 1.4320\n",
      "Epoch [109/300], Step [2300/27733], Loss: 2.4215\n",
      "Epoch [109/300], Step [2400/27733], Loss: 2.3242\n",
      "Epoch [109/300], Step [2500/27733], Loss: 2.7737\n",
      "Epoch [109/300], Step [2600/27733], Loss: 2.4561\n",
      "Epoch [109/300], Step [2700/27733], Loss: 3.2201\n",
      "Epoch [109/300], Step [2800/27733], Loss: 1.9322\n",
      "Epoch [109/300], Step [2900/27733], Loss: 2.7689\n",
      "Epoch [109/300], Step [3000/27733], Loss: 1.6494\n",
      "Epoch [109/300], Step [3100/27733], Loss: 2.9596\n",
      "Epoch [109/300], Step [3200/27733], Loss: 3.2623\n",
      "Epoch [109/300], Step [3300/27733], Loss: 2.9013\n",
      "Epoch [109/300], Step [3400/27733], Loss: 2.8925\n",
      "Epoch [109/300], Step [3500/27733], Loss: 2.1617\n",
      "Epoch [109/300], Step [3600/27733], Loss: 2.9254\n",
      "Epoch [109/300], Step [3700/27733], Loss: 2.5160\n",
      "Epoch [109/300], Step [3800/27733], Loss: 2.8911\n",
      "Epoch [109/300], Step [3900/27733], Loss: 2.7129\n",
      "Epoch [109/300], Step [4000/27733], Loss: 2.9318\n",
      "Epoch [109/300], Step [4100/27733], Loss: 2.7068\n",
      "Epoch [109/300], Step [4200/27733], Loss: 2.8967\n",
      "Epoch [109/300], Step [4300/27733], Loss: 2.8442\n",
      "Epoch [109/300], Step [4400/27733], Loss: 2.6036\n",
      "Epoch [109/300], Step [4500/27733], Loss: 2.4821\n",
      "Epoch [109/300], Step [4600/27733], Loss: 2.1566\n",
      "Epoch [109/300], Step [4700/27733], Loss: 2.8764\n",
      "Epoch [109/300], Step [4800/27733], Loss: 2.4372\n",
      "Epoch [109/300], Step [4900/27733], Loss: 2.5371\n",
      "Epoch [109/300], Step [5000/27733], Loss: 2.5779\n",
      "Epoch [109/300], Step [5100/27733], Loss: 2.7954\n",
      "Epoch [109/300], Step [5200/27733], Loss: 3.2003\n",
      "Epoch [109/300], Step [5300/27733], Loss: 2.5817\n",
      "Epoch [109/300], Step [5400/27733], Loss: 2.8456\n",
      "Epoch [109/300], Step [5500/27733], Loss: 2.4201\n",
      "Epoch [109/300], Step [5600/27733], Loss: 2.2723\n",
      "Epoch [109/300], Step [5700/27733], Loss: 3.0523\n",
      "Epoch [109/300], Step [5800/27733], Loss: 1.8568\n",
      "Epoch [109/300], Step [5900/27733], Loss: 2.8775\n",
      "Epoch [109/300], Step [6000/27733], Loss: 2.2506\n",
      "Epoch [109/300], Step [6100/27733], Loss: 2.2027\n",
      "Epoch [109/300], Step [6200/27733], Loss: 2.8040\n",
      "Epoch [109/300], Step [6300/27733], Loss: 3.0761\n",
      "Epoch [109/300], Step [6400/27733], Loss: 2.3600\n",
      "Epoch [109/300], Step [6500/27733], Loss: 3.1161\n",
      "Epoch [109/300], Step [6600/27733], Loss: 2.1335\n",
      "Epoch [109/300], Step [6700/27733], Loss: 1.8545\n",
      "Epoch [109/300], Step [6800/27733], Loss: 3.0690\n",
      "Epoch [109/300], Step [6900/27733], Loss: 3.0263\n",
      "Epoch [109/300], Step [7000/27733], Loss: 2.3502\n",
      "Epoch [109/300], Step [7100/27733], Loss: 2.2620\n",
      "Epoch [109/300], Step [7200/27733], Loss: 2.7496\n",
      "Epoch [109/300], Step [7300/27733], Loss: 2.5087\n",
      "Epoch [109/300], Step [7400/27733], Loss: 2.3772\n",
      "Epoch [109/300], Step [7500/27733], Loss: 3.4586\n",
      "Epoch [109/300], Step [7600/27733], Loss: 2.2019\n",
      "Epoch [109/300], Step [7700/27733], Loss: 3.0669\n",
      "Epoch [109/300], Step [7800/27733], Loss: 2.5987\n",
      "Epoch [109/300], Step [7900/27733], Loss: 2.4071\n",
      "Epoch [109/300], Step [8000/27733], Loss: 2.7292\n",
      "Epoch [109/300], Step [8100/27733], Loss: 3.7661\n",
      "Epoch [109/300], Step [8200/27733], Loss: 3.1325\n",
      "Epoch [109/300], Step [8300/27733], Loss: 2.9017\n",
      "Epoch [109/300], Step [8400/27733], Loss: 2.4523\n",
      "Epoch [109/300], Step [8500/27733], Loss: 2.2833\n",
      "Epoch [109/300], Step [8600/27733], Loss: 2.9220\n",
      "Epoch [109/300], Step [8700/27733], Loss: 2.4115\n",
      "Epoch [109/300], Step [8800/27733], Loss: 3.6832\n",
      "Epoch [109/300], Step [8900/27733], Loss: 2.3746\n",
      "Epoch [109/300], Step [9000/27733], Loss: 2.1200\n",
      "Epoch [109/300], Step [9100/27733], Loss: 3.2463\n",
      "Epoch [109/300], Step [9200/27733], Loss: 3.4873\n",
      "Epoch [109/300], Step [9300/27733], Loss: 2.9355\n",
      "Epoch [109/300], Step [9400/27733], Loss: 2.8359\n",
      "Epoch [109/300], Step [9500/27733], Loss: 2.4990\n",
      "Epoch [109/300], Step [9600/27733], Loss: 2.3575\n",
      "Epoch [109/300], Step [9700/27733], Loss: 3.3393\n",
      "Epoch [109/300], Step [9800/27733], Loss: 3.2692\n",
      "Epoch [109/300], Step [9900/27733], Loss: 3.7562\n",
      "Epoch [109/300], Step [10000/27733], Loss: 2.1026\n",
      "Epoch [109/300], Step [10100/27733], Loss: 2.2529\n",
      "Epoch [109/300], Step [10200/27733], Loss: 3.5913\n",
      "Epoch [109/300], Step [10300/27733], Loss: 3.4067\n",
      "Epoch [109/300], Step [10400/27733], Loss: 2.0425\n",
      "Epoch [109/300], Step [10500/27733], Loss: 2.3400\n",
      "Epoch [109/300], Step [10600/27733], Loss: 2.3665\n",
      "Epoch [109/300], Step [10700/27733], Loss: 2.9725\n",
      "Epoch [109/300], Step [10800/27733], Loss: 2.7143\n",
      "Epoch [109/300], Step [10900/27733], Loss: 2.9522\n",
      "Epoch [109/300], Step [11000/27733], Loss: 3.1870\n",
      "Epoch [109/300], Step [11100/27733], Loss: 2.6017\n",
      "Epoch [109/300], Step [11200/27733], Loss: 3.2203\n",
      "Epoch [109/300], Step [11300/27733], Loss: 2.5174\n",
      "Epoch [109/300], Step [11400/27733], Loss: 2.7851\n",
      "Epoch [109/300], Step [11500/27733], Loss: 3.6828\n",
      "Epoch [109/300], Step [11600/27733], Loss: 2.9752\n",
      "Epoch [109/300], Step [11700/27733], Loss: 2.3008\n",
      "Epoch [109/300], Step [11800/27733], Loss: 2.5851\n",
      "Epoch [109/300], Step [11900/27733], Loss: 2.9403\n",
      "Epoch [109/300], Step [12000/27733], Loss: 3.5830\n",
      "Epoch [109/300], Step [12100/27733], Loss: 3.2145\n",
      "Epoch [109/300], Step [12200/27733], Loss: 3.8764\n",
      "Epoch [109/300], Step [12300/27733], Loss: 3.0434\n",
      "Epoch [109/300], Step [12400/27733], Loss: 3.1091\n",
      "Epoch [109/300], Step [12500/27733], Loss: 2.4978\n",
      "Epoch [109/300], Step [12600/27733], Loss: 2.9035\n",
      "Epoch [109/300], Step [12700/27733], Loss: 2.4893\n",
      "Epoch [109/300], Step [12800/27733], Loss: 4.2076\n",
      "Epoch [109/300], Step [12900/27733], Loss: 3.0367\n",
      "Epoch [109/300], Step [13000/27733], Loss: 3.0611\n",
      "Epoch [109/300], Step [13100/27733], Loss: 2.6136\n",
      "Epoch [109/300], Step [13200/27733], Loss: 2.6509\n",
      "Epoch [109/300], Step [13300/27733], Loss: 3.4900\n",
      "Epoch [109/300], Step [13400/27733], Loss: 2.5959\n",
      "Epoch [109/300], Step [13500/27733], Loss: 2.6097\n",
      "Epoch [109/300], Step [13600/27733], Loss: 3.0942\n",
      "Epoch [109/300], Step [13700/27733], Loss: 2.7349\n",
      "Epoch [109/300], Step [13800/27733], Loss: 2.3359\n",
      "Epoch [109/300], Step [13900/27733], Loss: 3.0895\n",
      "Epoch [109/300], Step [14000/27733], Loss: 2.4525\n",
      "Epoch [109/300], Step [14100/27733], Loss: 2.6483\n",
      "Epoch [109/300], Step [14200/27733], Loss: 2.8417\n",
      "Epoch [109/300], Step [14300/27733], Loss: 2.6359\n",
      "Epoch [109/300], Step [14400/27733], Loss: 3.1193\n",
      "Epoch [109/300], Step [14500/27733], Loss: 2.5363\n",
      "Epoch [109/300], Step [14600/27733], Loss: 2.7530\n",
      "Epoch [109/300], Step [14700/27733], Loss: 3.0867\n",
      "Epoch [109/300], Step [14800/27733], Loss: 3.0067\n",
      "Epoch [109/300], Step [14900/27733], Loss: 2.7204\n",
      "Epoch [109/300], Step [15000/27733], Loss: 3.1929\n",
      "Epoch [109/300], Step [15100/27733], Loss: 3.0310\n",
      "Epoch [109/300], Step [15200/27733], Loss: 2.9680\n",
      "Epoch [109/300], Step [15300/27733], Loss: 2.8120\n",
      "Epoch [109/300], Step [15400/27733], Loss: 3.2791\n",
      "Epoch [109/300], Step [15500/27733], Loss: 3.1549\n",
      "Epoch [109/300], Step [15600/27733], Loss: 2.7193\n",
      "Epoch [109/300], Step [15700/27733], Loss: 3.3001\n",
      "Epoch [109/300], Step [15800/27733], Loss: 3.1378\n",
      "Epoch [109/300], Step [15900/27733], Loss: 3.2431\n",
      "Epoch [109/300], Step [16000/27733], Loss: 2.5834\n",
      "Epoch [109/300], Step [16100/27733], Loss: 2.8908\n",
      "Epoch [109/300], Step [16200/27733], Loss: 2.4790\n",
      "Epoch [109/300], Step [16300/27733], Loss: 3.2993\n",
      "Epoch [109/300], Step [16400/27733], Loss: 2.6151\n",
      "Epoch [109/300], Step [16500/27733], Loss: 2.4911\n",
      "Epoch [109/300], Step [16600/27733], Loss: 2.9388\n",
      "Epoch [109/300], Step [16700/27733], Loss: 2.2736\n",
      "Epoch [109/300], Step [16800/27733], Loss: 3.1958\n",
      "Epoch [109/300], Step [16900/27733], Loss: 3.4678\n",
      "Epoch [109/300], Step [17000/27733], Loss: 3.6495\n",
      "Epoch [109/300], Step [17100/27733], Loss: 2.7316\n",
      "Epoch [109/300], Step [17200/27733], Loss: 3.0902\n",
      "Epoch [109/300], Step [17300/27733], Loss: 3.0911\n",
      "Epoch [109/300], Step [17400/27733], Loss: 2.9565\n",
      "Epoch [109/300], Step [17500/27733], Loss: 2.4402\n",
      "Epoch [109/300], Step [17600/27733], Loss: 3.4272\n",
      "Epoch [109/300], Step [17700/27733], Loss: 2.6725\n",
      "Epoch [109/300], Step [17800/27733], Loss: 2.6035\n",
      "Epoch [109/300], Step [17900/27733], Loss: 2.3542\n",
      "Epoch [109/300], Step [18000/27733], Loss: 3.8083\n",
      "Epoch [109/300], Step [18100/27733], Loss: 2.3027\n",
      "Epoch [109/300], Step [18200/27733], Loss: 2.6657\n",
      "Epoch [109/300], Step [18300/27733], Loss: 2.9550\n",
      "Epoch [109/300], Step [18400/27733], Loss: 2.2894\n",
      "Epoch [109/300], Step [18500/27733], Loss: 2.6385\n",
      "Epoch [109/300], Step [18600/27733], Loss: 3.4789\n",
      "Epoch [109/300], Step [18700/27733], Loss: 2.8156\n",
      "Epoch [109/300], Step [18800/27733], Loss: 2.7241\n",
      "Epoch [109/300], Step [18900/27733], Loss: 2.3978\n",
      "Epoch [109/300], Step [19000/27733], Loss: 2.8908\n",
      "Epoch [109/300], Step [19100/27733], Loss: 3.0255\n",
      "Epoch [109/300], Step [19200/27733], Loss: 2.4700\n",
      "Epoch [109/300], Step [19300/27733], Loss: 2.7117\n",
      "Epoch [109/300], Step [19400/27733], Loss: 3.4713\n",
      "Epoch [109/300], Step [19500/27733], Loss: 2.7346\n",
      "Epoch [109/300], Step [19600/27733], Loss: 2.2714\n",
      "Epoch [109/300], Step [19700/27733], Loss: 2.3258\n",
      "Epoch [109/300], Step [19800/27733], Loss: 2.6752\n",
      "Epoch [109/300], Step [19900/27733], Loss: 2.3779\n",
      "Epoch [109/300], Step [20000/27733], Loss: 2.5747\n",
      "Epoch [109/300], Step [20100/27733], Loss: 2.5894\n",
      "Epoch [109/300], Step [20200/27733], Loss: 2.8160\n",
      "Epoch [109/300], Step [20300/27733], Loss: 2.6149\n",
      "Epoch [109/300], Step [20400/27733], Loss: 2.7972\n",
      "Epoch [109/300], Step [20500/27733], Loss: 2.3564\n",
      "Epoch [109/300], Step [20600/27733], Loss: 2.7421\n",
      "Epoch [109/300], Step [20700/27733], Loss: 2.5211\n",
      "Epoch [109/300], Step [20800/27733], Loss: 3.3257\n",
      "Epoch [109/300], Step [20900/27733], Loss: 2.9247\n",
      "Epoch [109/300], Step [21000/27733], Loss: 2.9153\n",
      "Epoch [109/300], Step [21100/27733], Loss: 2.9520\n",
      "Epoch [109/300], Step [21200/27733], Loss: 3.4542\n",
      "Epoch [109/300], Step [21300/27733], Loss: 3.1831\n",
      "Epoch [109/300], Step [21400/27733], Loss: 3.0980\n",
      "Epoch [109/300], Step [21500/27733], Loss: 2.8988\n",
      "Epoch [109/300], Step [21600/27733], Loss: 2.8926\n",
      "Epoch [109/300], Step [21700/27733], Loss: 2.2521\n",
      "Epoch [109/300], Step [21800/27733], Loss: 2.7772\n",
      "Epoch [109/300], Step [21900/27733], Loss: 3.2318\n",
      "Epoch [109/300], Step [22000/27733], Loss: 3.0126\n",
      "Epoch [109/300], Step [22100/27733], Loss: 2.3739\n",
      "Epoch [109/300], Step [22200/27733], Loss: 4.3949\n",
      "Epoch [109/300], Step [22300/27733], Loss: 2.8923\n",
      "Epoch [109/300], Step [22400/27733], Loss: 2.8886\n",
      "Epoch [109/300], Step [22500/27733], Loss: 3.0806\n",
      "Epoch [109/300], Step [22600/27733], Loss: 3.6789\n",
      "Epoch [109/300], Step [22700/27733], Loss: 2.8225\n",
      "Epoch [109/300], Step [22800/27733], Loss: 2.1725\n",
      "Epoch [109/300], Step [22900/27733], Loss: 3.3552\n",
      "Epoch [109/300], Step [23000/27733], Loss: 2.8087\n",
      "Epoch [109/300], Step [23100/27733], Loss: 3.4066\n",
      "Epoch [109/300], Step [23200/27733], Loss: 3.0386\n",
      "Epoch [109/300], Step [23300/27733], Loss: 3.5711\n",
      "Epoch [109/300], Step [23400/27733], Loss: 3.5736\n",
      "Epoch [109/300], Step [23500/27733], Loss: 2.8115\n",
      "Epoch [109/300], Step [23600/27733], Loss: 4.0416\n",
      "Epoch [109/300], Step [23700/27733], Loss: 2.3858\n",
      "Epoch [109/300], Step [23800/27733], Loss: 2.5550\n",
      "Epoch [109/300], Step [23900/27733], Loss: 2.6827\n",
      "Epoch [109/300], Step [24000/27733], Loss: 2.4293\n",
      "Epoch [109/300], Step [24100/27733], Loss: 3.1518\n",
      "Epoch [109/300], Step [24200/27733], Loss: 3.1131\n",
      "Epoch [109/300], Step [24300/27733], Loss: 2.6487\n",
      "Epoch [109/300], Step [24400/27733], Loss: 3.4936\n",
      "Epoch [109/300], Step [24500/27733], Loss: 3.3147\n",
      "Epoch [109/300], Step [24600/27733], Loss: 1.7062\n",
      "Epoch [109/300], Step [24700/27733], Loss: 3.2440\n",
      "Epoch [109/300], Step [24800/27733], Loss: 2.8632\n",
      "Epoch [109/300], Step [24900/27733], Loss: 3.8477\n",
      "Epoch [109/300], Step [25000/27733], Loss: 3.5624\n",
      "Epoch [109/300], Step [25100/27733], Loss: 3.7662\n",
      "Epoch [109/300], Step [25200/27733], Loss: 3.8493\n",
      "Epoch [109/300], Step [25300/27733], Loss: 3.0628\n",
      "Epoch [109/300], Step [25400/27733], Loss: 2.6857\n",
      "Epoch [109/300], Step [25500/27733], Loss: 3.7089\n",
      "Epoch [109/300], Step [25600/27733], Loss: 3.3536\n",
      "Epoch [109/300], Step [25700/27733], Loss: 4.6341\n",
      "Epoch [109/300], Step [25800/27733], Loss: 2.4944\n",
      "Epoch [109/300], Step [25900/27733], Loss: 3.5860\n",
      "Epoch [109/300], Step [26000/27733], Loss: 2.7505\n",
      "Epoch [109/300], Step [26100/27733], Loss: 2.6752\n",
      "Epoch [109/300], Step [26200/27733], Loss: 3.6639\n",
      "Epoch [109/300], Step [26300/27733], Loss: 2.9150\n",
      "Epoch [109/300], Step [26400/27733], Loss: 3.7926\n",
      "Epoch [109/300], Step [26500/27733], Loss: 2.8950\n",
      "Epoch [109/300], Step [26600/27733], Loss: 3.5660\n",
      "Epoch [109/300], Step [26700/27733], Loss: 2.6207\n",
      "Epoch [109/300], Step [26800/27733], Loss: 2.6993\n",
      "Epoch [109/300], Step [26900/27733], Loss: 4.3828\n",
      "Epoch [109/300], Step [27000/27733], Loss: 3.3597\n",
      "Epoch [109/300], Step [27100/27733], Loss: 3.7934\n",
      "Epoch [109/300], Step [27200/27733], Loss: 2.9656\n",
      "Epoch [109/300], Step [27300/27733], Loss: 3.5118\n",
      "Epoch [109/300], Step [27400/27733], Loss: 2.4059\n",
      "Epoch [109/300], Step [27500/27733], Loss: 2.4795\n",
      "Epoch [109/300], Step [27600/27733], Loss: 2.5489\n",
      "Epoch [109/300], Step [27700/27733], Loss: 3.6093\n",
      "Epoch [110/300], Step [100/27733], Loss: 2.7708\n",
      "Epoch [110/300], Step [200/27733], Loss: 2.6133\n",
      "Epoch [110/300], Step [300/27733], Loss: 2.3568\n",
      "Epoch [110/300], Step [400/27733], Loss: 2.7256\n",
      "Epoch [110/300], Step [500/27733], Loss: 3.3614\n",
      "Epoch [110/300], Step [600/27733], Loss: 3.2032\n",
      "Epoch [110/300], Step [700/27733], Loss: 2.0958\n",
      "Epoch [110/300], Step [800/27733], Loss: 3.1317\n",
      "Epoch [110/300], Step [900/27733], Loss: 2.6536\n",
      "Epoch [110/300], Step [1000/27733], Loss: 2.3573\n",
      "Epoch [110/300], Step [1100/27733], Loss: 1.8470\n",
      "Epoch [110/300], Step [1200/27733], Loss: 2.1413\n",
      "Epoch [110/300], Step [1300/27733], Loss: 3.3795\n",
      "Epoch [110/300], Step [1400/27733], Loss: 3.2285\n",
      "Epoch [110/300], Step [1500/27733], Loss: 2.8865\n",
      "Epoch [110/300], Step [1600/27733], Loss: 1.4304\n",
      "Epoch [110/300], Step [1700/27733], Loss: 2.2575\n",
      "Epoch [110/300], Step [1800/27733], Loss: 2.0852\n",
      "Epoch [110/300], Step [1900/27733], Loss: 2.4230\n",
      "Epoch [110/300], Step [2000/27733], Loss: 2.4964\n",
      "Epoch [110/300], Step [2100/27733], Loss: 3.2442\n",
      "Epoch [110/300], Step [2200/27733], Loss: 2.2597\n",
      "Epoch [110/300], Step [2300/27733], Loss: 2.2607\n",
      "Epoch [110/300], Step [2400/27733], Loss: 2.6513\n",
      "Epoch [110/300], Step [2500/27733], Loss: 2.4866\n",
      "Epoch [110/300], Step [2600/27733], Loss: 2.2329\n",
      "Epoch [110/300], Step [2700/27733], Loss: 2.2732\n",
      "Epoch [110/300], Step [2800/27733], Loss: 2.5154\n",
      "Epoch [110/300], Step [2900/27733], Loss: 3.1758\n",
      "Epoch [110/300], Step [3000/27733], Loss: 3.7502\n",
      "Epoch [110/300], Step [3100/27733], Loss: 2.5445\n",
      "Epoch [110/300], Step [3200/27733], Loss: 2.7626\n",
      "Epoch [110/300], Step [3300/27733], Loss: 3.6362\n",
      "Epoch [110/300], Step [3400/27733], Loss: 2.8639\n",
      "Epoch [110/300], Step [3500/27733], Loss: 2.3816\n",
      "Epoch [110/300], Step [3600/27733], Loss: 3.4556\n",
      "Epoch [110/300], Step [3700/27733], Loss: 2.2391\n",
      "Epoch [110/300], Step [3800/27733], Loss: 2.6341\n",
      "Epoch [110/300], Step [3900/27733], Loss: 3.0462\n",
      "Epoch [110/300], Step [4000/27733], Loss: 2.7533\n",
      "Epoch [110/300], Step [4100/27733], Loss: 1.7770\n",
      "Epoch [110/300], Step [4200/27733], Loss: 2.6819\n",
      "Epoch [110/300], Step [4300/27733], Loss: 2.5521\n",
      "Epoch [110/300], Step [4400/27733], Loss: 2.9832\n",
      "Epoch [110/300], Step [4500/27733], Loss: 2.8474\n",
      "Epoch [110/300], Step [4600/27733], Loss: 2.4851\n",
      "Epoch [110/300], Step [4700/27733], Loss: 3.0431\n",
      "Epoch [110/300], Step [4800/27733], Loss: 2.9604\n",
      "Epoch [110/300], Step [4900/27733], Loss: 2.6841\n",
      "Epoch [110/300], Step [5000/27733], Loss: 2.3671\n",
      "Epoch [110/300], Step [5100/27733], Loss: 2.4766\n",
      "Epoch [110/300], Step [5200/27733], Loss: 3.3215\n",
      "Epoch [110/300], Step [5300/27733], Loss: 2.2887\n",
      "Epoch [110/300], Step [5400/27733], Loss: 2.1891\n",
      "Epoch [110/300], Step [5500/27733], Loss: 2.4044\n",
      "Epoch [110/300], Step [5600/27733], Loss: 2.7266\n",
      "Epoch [110/300], Step [5700/27733], Loss: 2.1590\n",
      "Epoch [110/300], Step [5800/27733], Loss: 2.0144\n",
      "Epoch [110/300], Step [5900/27733], Loss: 2.2734\n",
      "Epoch [110/300], Step [6000/27733], Loss: 2.2006\n",
      "Epoch [110/300], Step [6100/27733], Loss: 3.2361\n",
      "Epoch [110/300], Step [6200/27733], Loss: 3.5733\n",
      "Epoch [110/300], Step [6300/27733], Loss: 2.1967\n",
      "Epoch [110/300], Step [6400/27733], Loss: 2.4933\n",
      "Epoch [110/300], Step [6500/27733], Loss: 3.4066\n",
      "Epoch [110/300], Step [6600/27733], Loss: 2.5109\n",
      "Epoch [110/300], Step [6700/27733], Loss: 2.5097\n",
      "Epoch [110/300], Step [6800/27733], Loss: 2.6908\n",
      "Epoch [110/300], Step [6900/27733], Loss: 3.5018\n",
      "Epoch [110/300], Step [7000/27733], Loss: 2.0615\n",
      "Epoch [110/300], Step [7100/27733], Loss: 3.5306\n",
      "Epoch [110/300], Step [7200/27733], Loss: 3.5806\n",
      "Epoch [110/300], Step [7300/27733], Loss: 3.0249\n",
      "Epoch [110/300], Step [7400/27733], Loss: 3.1218\n",
      "Epoch [110/300], Step [7500/27733], Loss: 1.9572\n",
      "Epoch [110/300], Step [7600/27733], Loss: 2.0324\n",
      "Epoch [110/300], Step [7700/27733], Loss: 3.2961\n",
      "Epoch [110/300], Step [7800/27733], Loss: 2.3426\n",
      "Epoch [110/300], Step [7900/27733], Loss: 3.2188\n",
      "Epoch [110/300], Step [8000/27733], Loss: 3.0396\n",
      "Epoch [110/300], Step [8100/27733], Loss: 2.4051\n",
      "Epoch [110/300], Step [8200/27733], Loss: 2.5293\n",
      "Epoch [110/300], Step [8300/27733], Loss: 2.4373\n",
      "Epoch [110/300], Step [8400/27733], Loss: 3.0351\n",
      "Epoch [110/300], Step [8500/27733], Loss: 3.2998\n",
      "Epoch [110/300], Step [8600/27733], Loss: 2.7252\n",
      "Epoch [110/300], Step [8700/27733], Loss: 2.3044\n",
      "Epoch [110/300], Step [8800/27733], Loss: 3.2403\n",
      "Epoch [110/300], Step [8900/27733], Loss: 2.6443\n",
      "Epoch [110/300], Step [9000/27733], Loss: 3.1478\n",
      "Epoch [110/300], Step [9100/27733], Loss: 2.7467\n",
      "Epoch [110/300], Step [9200/27733], Loss: 2.4481\n",
      "Epoch [110/300], Step [9300/27733], Loss: 2.1527\n",
      "Epoch [110/300], Step [9400/27733], Loss: 3.6657\n",
      "Epoch [110/300], Step [9500/27733], Loss: 2.3734\n",
      "Epoch [110/300], Step [9600/27733], Loss: 2.4903\n",
      "Epoch [110/300], Step [9700/27733], Loss: 3.3610\n",
      "Epoch [110/300], Step [9800/27733], Loss: 2.8376\n",
      "Epoch [110/300], Step [9900/27733], Loss: 3.4025\n",
      "Epoch [110/300], Step [10000/27733], Loss: 2.7225\n",
      "Epoch [110/300], Step [10100/27733], Loss: 2.7995\n",
      "Epoch [110/300], Step [10200/27733], Loss: 2.9849\n",
      "Epoch [110/300], Step [10300/27733], Loss: 2.5197\n",
      "Epoch [110/300], Step [10400/27733], Loss: 2.6837\n",
      "Epoch [110/300], Step [10500/27733], Loss: 3.0322\n",
      "Epoch [110/300], Step [10600/27733], Loss: 3.0623\n",
      "Epoch [110/300], Step [10700/27733], Loss: 2.7379\n",
      "Epoch [110/300], Step [10800/27733], Loss: 2.7661\n",
      "Epoch [110/300], Step [10900/27733], Loss: 2.7652\n",
      "Epoch [110/300], Step [11000/27733], Loss: 2.4717\n",
      "Epoch [110/300], Step [11100/27733], Loss: 2.0206\n",
      "Epoch [110/300], Step [11200/27733], Loss: 3.3542\n",
      "Epoch [110/300], Step [11300/27733], Loss: 3.3635\n",
      "Epoch [110/300], Step [11400/27733], Loss: 2.1521\n",
      "Epoch [110/300], Step [11500/27733], Loss: 2.5593\n",
      "Epoch [110/300], Step [11600/27733], Loss: 2.9035\n",
      "Epoch [110/300], Step [11700/27733], Loss: 3.1542\n",
      "Epoch [110/300], Step [11800/27733], Loss: 2.2716\n",
      "Epoch [110/300], Step [11900/27733], Loss: 2.3748\n",
      "Epoch [110/300], Step [12000/27733], Loss: 2.7042\n",
      "Epoch [110/300], Step [12100/27733], Loss: 2.5277\n",
      "Epoch [110/300], Step [12200/27733], Loss: 2.9916\n",
      "Epoch [110/300], Step [12300/27733], Loss: 3.3607\n",
      "Epoch [110/300], Step [12400/27733], Loss: 2.9661\n",
      "Epoch [110/300], Step [12500/27733], Loss: 1.6504\n",
      "Epoch [110/300], Step [12600/27733], Loss: 2.6516\n",
      "Epoch [110/300], Step [12700/27733], Loss: 3.1797\n",
      "Epoch [110/300], Step [12800/27733], Loss: 2.3563\n",
      "Epoch [110/300], Step [12900/27733], Loss: 3.3231\n",
      "Epoch [110/300], Step [13000/27733], Loss: 3.4047\n",
      "Epoch [110/300], Step [13100/27733], Loss: 2.3499\n",
      "Epoch [110/300], Step [13200/27733], Loss: 2.8374\n",
      "Epoch [110/300], Step [13300/27733], Loss: 3.1453\n",
      "Epoch [110/300], Step [13400/27733], Loss: 3.2835\n",
      "Epoch [110/300], Step [13500/27733], Loss: 2.6051\n",
      "Epoch [110/300], Step [13600/27733], Loss: 2.5477\n",
      "Epoch [110/300], Step [13700/27733], Loss: 2.8251\n",
      "Epoch [110/300], Step [13800/27733], Loss: 2.1392\n",
      "Epoch [110/300], Step [13900/27733], Loss: 2.1829\n",
      "Epoch [110/300], Step [14000/27733], Loss: 3.1030\n",
      "Epoch [110/300], Step [14100/27733], Loss: 2.4416\n",
      "Epoch [110/300], Step [14200/27733], Loss: 2.5636\n",
      "Epoch [110/300], Step [14300/27733], Loss: 3.7247\n",
      "Epoch [110/300], Step [14400/27733], Loss: 3.4046\n",
      "Epoch [110/300], Step [14500/27733], Loss: 2.6929\n",
      "Epoch [110/300], Step [14600/27733], Loss: 3.1115\n",
      "Epoch [110/300], Step [14700/27733], Loss: 2.6222\n",
      "Epoch [110/300], Step [14800/27733], Loss: 2.7003\n",
      "Epoch [110/300], Step [14900/27733], Loss: 3.0230\n",
      "Epoch [110/300], Step [15000/27733], Loss: 2.7342\n",
      "Epoch [110/300], Step [15100/27733], Loss: 2.8555\n",
      "Epoch [110/300], Step [15200/27733], Loss: 3.2351\n",
      "Epoch [110/300], Step [15300/27733], Loss: 2.6791\n",
      "Epoch [110/300], Step [15400/27733], Loss: 3.1217\n",
      "Epoch [110/300], Step [15500/27733], Loss: 2.1926\n",
      "Epoch [110/300], Step [15600/27733], Loss: 2.9125\n",
      "Epoch [110/300], Step [15700/27733], Loss: 3.6452\n",
      "Epoch [110/300], Step [15800/27733], Loss: 2.6825\n",
      "Epoch [110/300], Step [15900/27733], Loss: 2.6593\n",
      "Epoch [110/300], Step [16000/27733], Loss: 2.6710\n",
      "Epoch [110/300], Step [16100/27733], Loss: 3.7807\n",
      "Epoch [110/300], Step [16200/27733], Loss: 2.8542\n",
      "Epoch [110/300], Step [16300/27733], Loss: 2.4002\n",
      "Epoch [110/300], Step [16400/27733], Loss: 3.1960\n",
      "Epoch [110/300], Step [16500/27733], Loss: 3.1323\n",
      "Epoch [110/300], Step [16600/27733], Loss: 3.9365\n",
      "Epoch [110/300], Step [16700/27733], Loss: 2.6190\n",
      "Epoch [110/300], Step [16800/27733], Loss: 2.8134\n",
      "Epoch [110/300], Step [16900/27733], Loss: 2.5972\n",
      "Epoch [110/300], Step [17000/27733], Loss: 2.7062\n",
      "Epoch [110/300], Step [17100/27733], Loss: 2.8253\n",
      "Epoch [110/300], Step [17200/27733], Loss: 3.2774\n",
      "Epoch [110/300], Step [17300/27733], Loss: 3.9503\n",
      "Epoch [110/300], Step [17400/27733], Loss: 1.8373\n",
      "Epoch [110/300], Step [17500/27733], Loss: 2.9130\n",
      "Epoch [110/300], Step [17600/27733], Loss: 3.0817\n",
      "Epoch [110/300], Step [17700/27733], Loss: 2.6524\n",
      "Epoch [110/300], Step [17800/27733], Loss: 2.3866\n",
      "Epoch [110/300], Step [17900/27733], Loss: 2.7201\n",
      "Epoch [110/300], Step [18000/27733], Loss: 2.3229\n",
      "Epoch [110/300], Step [18100/27733], Loss: 1.5166\n",
      "Epoch [110/300], Step [18200/27733], Loss: 2.9251\n",
      "Epoch [110/300], Step [18300/27733], Loss: 2.4998\n",
      "Epoch [110/300], Step [18400/27733], Loss: 2.5791\n",
      "Epoch [110/300], Step [18500/27733], Loss: 3.1843\n",
      "Epoch [110/300], Step [18600/27733], Loss: 2.9374\n",
      "Epoch [110/300], Step [18700/27733], Loss: 2.7570\n",
      "Epoch [110/300], Step [18800/27733], Loss: 2.5160\n",
      "Epoch [110/300], Step [18900/27733], Loss: 3.4294\n",
      "Epoch [110/300], Step [19000/27733], Loss: 3.2330\n",
      "Epoch [110/300], Step [19100/27733], Loss: 2.9845\n",
      "Epoch [110/300], Step [19200/27733], Loss: 2.7705\n",
      "Epoch [110/300], Step [19300/27733], Loss: 2.7704\n",
      "Epoch [110/300], Step [19400/27733], Loss: 3.5788\n",
      "Epoch [110/300], Step [19500/27733], Loss: 2.3696\n",
      "Epoch [110/300], Step [19600/27733], Loss: 3.3668\n",
      "Epoch [110/300], Step [19700/27733], Loss: 2.7964\n",
      "Epoch [110/300], Step [19800/27733], Loss: 2.2737\n",
      "Epoch [110/300], Step [19900/27733], Loss: 2.6481\n",
      "Epoch [110/300], Step [20000/27733], Loss: 3.4977\n",
      "Epoch [110/300], Step [20100/27733], Loss: 2.5819\n",
      "Epoch [110/300], Step [20200/27733], Loss: 2.2526\n",
      "Epoch [110/300], Step [20300/27733], Loss: 3.3860\n",
      "Epoch [110/300], Step [20400/27733], Loss: 2.3702\n",
      "Epoch [110/300], Step [20500/27733], Loss: 3.2730\n",
      "Epoch [110/300], Step [20600/27733], Loss: 3.4293\n",
      "Epoch [110/300], Step [20700/27733], Loss: 2.6610\n",
      "Epoch [110/300], Step [20800/27733], Loss: 2.5620\n",
      "Epoch [110/300], Step [20900/27733], Loss: 3.2419\n",
      "Epoch [110/300], Step [21000/27733], Loss: 3.2001\n",
      "Epoch [110/300], Step [21100/27733], Loss: 2.7172\n",
      "Epoch [110/300], Step [21200/27733], Loss: 2.6748\n",
      "Epoch [110/300], Step [21300/27733], Loss: 3.6995\n",
      "Epoch [110/300], Step [21400/27733], Loss: 3.5845\n",
      "Epoch [110/300], Step [21500/27733], Loss: 2.6587\n",
      "Epoch [110/300], Step [21600/27733], Loss: 2.9355\n",
      "Epoch [110/300], Step [21700/27733], Loss: 2.9795\n",
      "Epoch [110/300], Step [21800/27733], Loss: 2.5556\n",
      "Epoch [110/300], Step [21900/27733], Loss: 2.5064\n",
      "Epoch [110/300], Step [22000/27733], Loss: 2.8961\n",
      "Epoch [110/300], Step [22100/27733], Loss: 3.5796\n",
      "Epoch [110/300], Step [22200/27733], Loss: 3.2313\n",
      "Epoch [110/300], Step [22300/27733], Loss: 3.6335\n",
      "Epoch [110/300], Step [22400/27733], Loss: 2.3017\n",
      "Epoch [110/300], Step [22500/27733], Loss: 3.0675\n",
      "Epoch [110/300], Step [22600/27733], Loss: 2.8882\n",
      "Epoch [110/300], Step [22700/27733], Loss: 2.9772\n",
      "Epoch [110/300], Step [22800/27733], Loss: 2.4721\n",
      "Epoch [110/300], Step [22900/27733], Loss: 3.1348\n",
      "Epoch [110/300], Step [23000/27733], Loss: 3.0752\n",
      "Epoch [110/300], Step [23100/27733], Loss: 3.5368\n",
      "Epoch [110/300], Step [23200/27733], Loss: 3.4827\n",
      "Epoch [110/300], Step [23300/27733], Loss: 1.9494\n",
      "Epoch [110/300], Step [23400/27733], Loss: 3.0841\n",
      "Epoch [110/300], Step [23500/27733], Loss: 4.3828\n",
      "Epoch [110/300], Step [23600/27733], Loss: 3.2037\n",
      "Epoch [110/300], Step [23700/27733], Loss: 2.9047\n",
      "Epoch [110/300], Step [23800/27733], Loss: 2.5738\n",
      "Epoch [110/300], Step [23900/27733], Loss: 3.2906\n",
      "Epoch [110/300], Step [24000/27733], Loss: 3.2157\n",
      "Epoch [110/300], Step [24100/27733], Loss: 2.6112\n",
      "Epoch [110/300], Step [24200/27733], Loss: 1.6927\n",
      "Epoch [110/300], Step [24300/27733], Loss: 2.9829\n",
      "Epoch [110/300], Step [24400/27733], Loss: 2.9470\n",
      "Epoch [110/300], Step [24500/27733], Loss: 2.6449\n",
      "Epoch [110/300], Step [24600/27733], Loss: 3.0424\n",
      "Epoch [110/300], Step [24700/27733], Loss: 4.1693\n",
      "Epoch [110/300], Step [24800/27733], Loss: 2.9598\n",
      "Epoch [110/300], Step [24900/27733], Loss: 3.4420\n",
      "Epoch [110/300], Step [25000/27733], Loss: 3.3718\n",
      "Epoch [110/300], Step [25100/27733], Loss: 3.8126\n",
      "Epoch [110/300], Step [25200/27733], Loss: 3.2960\n",
      "Epoch [110/300], Step [25300/27733], Loss: 3.6977\n",
      "Epoch [110/300], Step [25400/27733], Loss: 2.8933\n",
      "Epoch [110/300], Step [25500/27733], Loss: 3.2412\n",
      "Epoch [110/300], Step [25600/27733], Loss: 3.0199\n",
      "Epoch [110/300], Step [25700/27733], Loss: 3.0248\n",
      "Epoch [110/300], Step [25800/27733], Loss: 3.0930\n",
      "Epoch [110/300], Step [25900/27733], Loss: 3.2243\n",
      "Epoch [110/300], Step [26000/27733], Loss: 2.7012\n",
      "Epoch [110/300], Step [26100/27733], Loss: 2.4572\n",
      "Epoch [110/300], Step [26200/27733], Loss: 2.3797\n",
      "Epoch [110/300], Step [26300/27733], Loss: 2.2746\n",
      "Epoch [110/300], Step [26400/27733], Loss: 3.0649\n",
      "Epoch [110/300], Step [26500/27733], Loss: 2.8017\n",
      "Epoch [110/300], Step [26600/27733], Loss: 2.7833\n",
      "Epoch [110/300], Step [26700/27733], Loss: 3.1801\n",
      "Epoch [110/300], Step [26800/27733], Loss: 2.5408\n",
      "Epoch [110/300], Step [26900/27733], Loss: 3.6600\n",
      "Epoch [110/300], Step [27000/27733], Loss: 2.6035\n",
      "Epoch [110/300], Step [27100/27733], Loss: 3.1138\n",
      "Epoch [110/300], Step [27200/27733], Loss: 3.2805\n",
      "Epoch [110/300], Step [27300/27733], Loss: 2.7648\n",
      "Epoch [110/300], Step [27400/27733], Loss: 3.2813\n",
      "Epoch [110/300], Step [27500/27733], Loss: 3.4747\n",
      "Epoch [110/300], Step [27600/27733], Loss: 3.4871\n",
      "Epoch [110/300], Step [27700/27733], Loss: 3.6524\n",
      "Epoch [111/300], Step [100/27733], Loss: 2.3432\n",
      "Epoch [111/300], Step [200/27733], Loss: 1.7540\n",
      "Epoch [111/300], Step [300/27733], Loss: 2.3341\n",
      "Epoch [111/300], Step [400/27733], Loss: 1.9493\n",
      "Epoch [111/300], Step [500/27733], Loss: 2.4864\n",
      "Epoch [111/300], Step [600/27733], Loss: 3.3863\n",
      "Epoch [111/300], Step [700/27733], Loss: 1.9426\n",
      "Epoch [111/300], Step [800/27733], Loss: 2.9169\n",
      "Epoch [111/300], Step [900/27733], Loss: 2.3787\n",
      "Epoch [111/300], Step [1000/27733], Loss: 2.3031\n",
      "Epoch [111/300], Step [1100/27733], Loss: 2.4715\n",
      "Epoch [111/300], Step [1200/27733], Loss: 2.3341\n",
      "Epoch [111/300], Step [1300/27733], Loss: 3.4258\n",
      "Epoch [111/300], Step [1400/27733], Loss: 2.3975\n",
      "Epoch [111/300], Step [1500/27733], Loss: 2.8982\n",
      "Epoch [111/300], Step [1600/27733], Loss: 2.1732\n",
      "Epoch [111/300], Step [1700/27733], Loss: 3.0380\n",
      "Epoch [111/300], Step [1800/27733], Loss: 2.2893\n",
      "Epoch [111/300], Step [1900/27733], Loss: 3.5695\n",
      "Epoch [111/300], Step [2000/27733], Loss: 1.9396\n",
      "Epoch [111/300], Step [2100/27733], Loss: 2.9912\n",
      "Epoch [111/300], Step [2200/27733], Loss: 3.1677\n",
      "Epoch [111/300], Step [2300/27733], Loss: 3.5148\n",
      "Epoch [111/300], Step [2400/27733], Loss: 3.0778\n",
      "Epoch [111/300], Step [2500/27733], Loss: 2.9356\n",
      "Epoch [111/300], Step [2600/27733], Loss: 2.4038\n",
      "Epoch [111/300], Step [2700/27733], Loss: 2.9786\n",
      "Epoch [111/300], Step [2800/27733], Loss: 2.4762\n",
      "Epoch [111/300], Step [2900/27733], Loss: 1.7640\n",
      "Epoch [111/300], Step [3000/27733], Loss: 2.9584\n",
      "Epoch [111/300], Step [3100/27733], Loss: 3.1407\n",
      "Epoch [111/300], Step [3200/27733], Loss: 2.9811\n",
      "Epoch [111/300], Step [3300/27733], Loss: 2.7554\n",
      "Epoch [111/300], Step [3400/27733], Loss: 3.0525\n",
      "Epoch [111/300], Step [3500/27733], Loss: 3.0356\n",
      "Epoch [111/300], Step [3600/27733], Loss: 3.1568\n",
      "Epoch [111/300], Step [3700/27733], Loss: 2.9736\n",
      "Epoch [111/300], Step [3800/27733], Loss: 2.7766\n",
      "Epoch [111/300], Step [3900/27733], Loss: 3.4884\n",
      "Epoch [111/300], Step [4000/27733], Loss: 2.5055\n",
      "Epoch [111/300], Step [4100/27733], Loss: 2.2477\n",
      "Epoch [111/300], Step [4200/27733], Loss: 2.4506\n",
      "Epoch [111/300], Step [4300/27733], Loss: 2.6649\n",
      "Epoch [111/300], Step [4400/27733], Loss: 2.3448\n",
      "Epoch [111/300], Step [4500/27733], Loss: 1.8778\n",
      "Epoch [111/300], Step [4600/27733], Loss: 2.8944\n",
      "Epoch [111/300], Step [4700/27733], Loss: 2.6964\n",
      "Epoch [111/300], Step [4800/27733], Loss: 2.2868\n",
      "Epoch [111/300], Step [4900/27733], Loss: 2.3546\n",
      "Epoch [111/300], Step [5000/27733], Loss: 3.3231\n",
      "Epoch [111/300], Step [5100/27733], Loss: 2.3972\n",
      "Epoch [111/300], Step [5200/27733], Loss: 2.9922\n",
      "Epoch [111/300], Step [5300/27733], Loss: 2.8216\n",
      "Epoch [111/300], Step [5400/27733], Loss: 2.5763\n",
      "Epoch [111/300], Step [5500/27733], Loss: 2.6194\n",
      "Epoch [111/300], Step [5600/27733], Loss: 2.1620\n",
      "Epoch [111/300], Step [5700/27733], Loss: 2.7956\n",
      "Epoch [111/300], Step [5800/27733], Loss: 2.1387\n",
      "Epoch [111/300], Step [5900/27733], Loss: 2.9278\n",
      "Epoch [111/300], Step [6000/27733], Loss: 2.4022\n",
      "Epoch [111/300], Step [6100/27733], Loss: 2.7448\n",
      "Epoch [111/300], Step [6200/27733], Loss: 2.7973\n",
      "Epoch [111/300], Step [6300/27733], Loss: 2.8538\n",
      "Epoch [111/300], Step [6400/27733], Loss: 2.7196\n",
      "Epoch [111/300], Step [6500/27733], Loss: 2.2318\n",
      "Epoch [111/300], Step [6600/27733], Loss: 2.3876\n",
      "Epoch [111/300], Step [6700/27733], Loss: 2.7598\n",
      "Epoch [111/300], Step [6800/27733], Loss: 2.7507\n",
      "Epoch [111/300], Step [6900/27733], Loss: 2.4857\n",
      "Epoch [111/300], Step [7000/27733], Loss: 3.0568\n",
      "Epoch [111/300], Step [7100/27733], Loss: 2.5823\n",
      "Epoch [111/300], Step [7200/27733], Loss: 2.5603\n",
      "Epoch [111/300], Step [7300/27733], Loss: 3.4472\n",
      "Epoch [111/300], Step [7400/27733], Loss: 2.5753\n",
      "Epoch [111/300], Step [7500/27733], Loss: 3.2688\n",
      "Epoch [111/300], Step [7600/27733], Loss: 3.6242\n",
      "Epoch [111/300], Step [7700/27733], Loss: 2.7764\n",
      "Epoch [111/300], Step [7800/27733], Loss: 3.3783\n",
      "Epoch [111/300], Step [7900/27733], Loss: 3.4897\n",
      "Epoch [111/300], Step [8000/27733], Loss: 2.8167\n",
      "Epoch [111/300], Step [8100/27733], Loss: 3.3712\n",
      "Epoch [111/300], Step [8200/27733], Loss: 2.7005\n",
      "Epoch [111/300], Step [8300/27733], Loss: 2.5395\n",
      "Epoch [111/300], Step [8400/27733], Loss: 2.1106\n",
      "Epoch [111/300], Step [8500/27733], Loss: 2.6106\n",
      "Epoch [111/300], Step [8600/27733], Loss: 2.9014\n",
      "Epoch [111/300], Step [8700/27733], Loss: 3.3300\n",
      "Epoch [111/300], Step [8800/27733], Loss: 2.1088\n",
      "Epoch [111/300], Step [8900/27733], Loss: 3.3400\n",
      "Epoch [111/300], Step [9000/27733], Loss: 2.3582\n",
      "Epoch [111/300], Step [9100/27733], Loss: 3.2028\n",
      "Epoch [111/300], Step [9200/27733], Loss: 2.2562\n",
      "Epoch [111/300], Step [9300/27733], Loss: 2.6518\n",
      "Epoch [111/300], Step [9400/27733], Loss: 2.7897\n",
      "Epoch [111/300], Step [9500/27733], Loss: 2.4040\n",
      "Epoch [111/300], Step [9600/27733], Loss: 2.7401\n",
      "Epoch [111/300], Step [9700/27733], Loss: 2.9865\n",
      "Epoch [111/300], Step [9800/27733], Loss: 2.3226\n",
      "Epoch [111/300], Step [9900/27733], Loss: 2.9828\n",
      "Epoch [111/300], Step [10000/27733], Loss: 2.6555\n",
      "Epoch [111/300], Step [10100/27733], Loss: 2.8238\n",
      "Epoch [111/300], Step [10200/27733], Loss: 3.7577\n",
      "Epoch [111/300], Step [10300/27733], Loss: 2.2044\n",
      "Epoch [111/300], Step [10400/27733], Loss: 3.1173\n",
      "Epoch [111/300], Step [10500/27733], Loss: 2.7893\n",
      "Epoch [111/300], Step [10600/27733], Loss: 2.8923\n",
      "Epoch [111/300], Step [10700/27733], Loss: 2.3744\n",
      "Epoch [111/300], Step [10800/27733], Loss: 2.9167\n",
      "Epoch [111/300], Step [10900/27733], Loss: 2.6686\n",
      "Epoch [111/300], Step [11000/27733], Loss: 2.7763\n",
      "Epoch [111/300], Step [11100/27733], Loss: 2.8893\n",
      "Epoch [111/300], Step [11200/27733], Loss: 3.0593\n",
      "Epoch [111/300], Step [11300/27733], Loss: 3.3752\n",
      "Epoch [111/300], Step [11400/27733], Loss: 3.2346\n",
      "Epoch [111/300], Step [11500/27733], Loss: 2.9562\n",
      "Epoch [111/300], Step [11600/27733], Loss: 3.2507\n",
      "Epoch [111/300], Step [11700/27733], Loss: 3.2771\n",
      "Epoch [111/300], Step [11800/27733], Loss: 2.5796\n",
      "Epoch [111/300], Step [11900/27733], Loss: 3.0901\n",
      "Epoch [111/300], Step [12000/27733], Loss: 2.6072\n",
      "Epoch [111/300], Step [12100/27733], Loss: 3.9076\n",
      "Epoch [111/300], Step [12200/27733], Loss: 2.9327\n",
      "Epoch [111/300], Step [12300/27733], Loss: 2.8742\n",
      "Epoch [111/300], Step [12400/27733], Loss: 2.7057\n",
      "Epoch [111/300], Step [12500/27733], Loss: 3.2768\n",
      "Epoch [111/300], Step [12600/27733], Loss: 3.7151\n",
      "Epoch [111/300], Step [12700/27733], Loss: 2.6541\n",
      "Epoch [111/300], Step [12800/27733], Loss: 3.1468\n",
      "Epoch [111/300], Step [12900/27733], Loss: 2.1506\n",
      "Epoch [111/300], Step [13000/27733], Loss: 2.8340\n",
      "Epoch [111/300], Step [13100/27733], Loss: 2.6733\n",
      "Epoch [111/300], Step [13200/27733], Loss: 3.1660\n",
      "Epoch [111/300], Step [13300/27733], Loss: 2.5008\n",
      "Epoch [111/300], Step [13400/27733], Loss: 2.0126\n",
      "Epoch [111/300], Step [13500/27733], Loss: 2.7118\n",
      "Epoch [111/300], Step [13600/27733], Loss: 2.5313\n",
      "Epoch [111/300], Step [13700/27733], Loss: 2.9717\n",
      "Epoch [111/300], Step [13800/27733], Loss: 1.9210\n",
      "Epoch [111/300], Step [13900/27733], Loss: 3.5424\n",
      "Epoch [111/300], Step [14000/27733], Loss: 3.0782\n",
      "Epoch [111/300], Step [14100/27733], Loss: 3.1441\n",
      "Epoch [111/300], Step [14200/27733], Loss: 2.5427\n",
      "Epoch [111/300], Step [14300/27733], Loss: 3.2927\n",
      "Epoch [111/300], Step [14400/27733], Loss: 2.9666\n",
      "Epoch [111/300], Step [14500/27733], Loss: 2.8265\n",
      "Epoch [111/300], Step [14600/27733], Loss: 2.8961\n",
      "Epoch [111/300], Step [14700/27733], Loss: 2.8656\n",
      "Epoch [111/300], Step [14800/27733], Loss: 2.2882\n",
      "Epoch [111/300], Step [14900/27733], Loss: 3.1776\n",
      "Epoch [111/300], Step [15000/27733], Loss: 2.8500\n",
      "Epoch [111/300], Step [15100/27733], Loss: 2.6533\n",
      "Epoch [111/300], Step [15200/27733], Loss: 2.8901\n",
      "Epoch [111/300], Step [15300/27733], Loss: 3.0392\n",
      "Epoch [111/300], Step [15400/27733], Loss: 2.4584\n",
      "Epoch [111/300], Step [15500/27733], Loss: 2.8547\n",
      "Epoch [111/300], Step [15600/27733], Loss: 3.9310\n",
      "Epoch [111/300], Step [15700/27733], Loss: 3.8566\n",
      "Epoch [111/300], Step [15800/27733], Loss: 3.0424\n",
      "Epoch [111/300], Step [15900/27733], Loss: 3.0874\n",
      "Epoch [111/300], Step [16000/27733], Loss: 3.7155\n",
      "Epoch [111/300], Step [16100/27733], Loss: 2.4325\n",
      "Epoch [111/300], Step [16200/27733], Loss: 2.8493\n",
      "Epoch [111/300], Step [16300/27733], Loss: 2.7442\n",
      "Epoch [111/300], Step [16400/27733], Loss: 2.4428\n",
      "Epoch [111/300], Step [16500/27733], Loss: 3.2093\n",
      "Epoch [111/300], Step [16600/27733], Loss: 2.9409\n",
      "Epoch [111/300], Step [16700/27733], Loss: 3.4371\n",
      "Epoch [111/300], Step [16800/27733], Loss: 2.9231\n",
      "Epoch [111/300], Step [16900/27733], Loss: 2.8095\n",
      "Epoch [111/300], Step [17000/27733], Loss: 3.7386\n",
      "Epoch [111/300], Step [17100/27733], Loss: 2.9200\n",
      "Epoch [111/300], Step [17200/27733], Loss: 2.3826\n",
      "Epoch [111/300], Step [17300/27733], Loss: 3.0166\n",
      "Epoch [111/300], Step [17400/27733], Loss: 2.9934\n",
      "Epoch [111/300], Step [17500/27733], Loss: 2.7275\n",
      "Epoch [111/300], Step [17600/27733], Loss: 2.4852\n",
      "Epoch [111/300], Step [17700/27733], Loss: 3.1217\n",
      "Epoch [111/300], Step [17800/27733], Loss: 3.5583\n",
      "Epoch [111/300], Step [17900/27733], Loss: 2.0714\n",
      "Epoch [111/300], Step [18000/27733], Loss: 2.9048\n",
      "Epoch [111/300], Step [18100/27733], Loss: 2.3690\n",
      "Epoch [111/300], Step [18200/27733], Loss: 2.7770\n",
      "Epoch [111/300], Step [18300/27733], Loss: 3.1639\n",
      "Epoch [111/300], Step [18400/27733], Loss: 2.1859\n",
      "Epoch [111/300], Step [18500/27733], Loss: 3.4543\n",
      "Epoch [111/300], Step [18600/27733], Loss: 2.6334\n",
      "Epoch [111/300], Step [18700/27733], Loss: 3.2273\n",
      "Epoch [111/300], Step [18800/27733], Loss: 3.8092\n",
      "Epoch [111/300], Step [18900/27733], Loss: 2.1833\n",
      "Epoch [111/300], Step [19000/27733], Loss: 2.3750\n",
      "Epoch [111/300], Step [19100/27733], Loss: 2.0721\n",
      "Epoch [111/300], Step [19200/27733], Loss: 3.0565\n",
      "Epoch [111/300], Step [19300/27733], Loss: 2.9503\n",
      "Epoch [111/300], Step [19400/27733], Loss: 2.6484\n",
      "Epoch [111/300], Step [19500/27733], Loss: 3.1082\n",
      "Epoch [111/300], Step [19600/27733], Loss: 2.8993\n",
      "Epoch [111/300], Step [19700/27733], Loss: 2.8130\n",
      "Epoch [111/300], Step [19800/27733], Loss: 3.3882\n",
      "Epoch [111/300], Step [19900/27733], Loss: 3.2145\n",
      "Epoch [111/300], Step [20000/27733], Loss: 2.6934\n",
      "Epoch [111/300], Step [20100/27733], Loss: 3.1769\n",
      "Epoch [111/300], Step [20200/27733], Loss: 2.8997\n",
      "Epoch [111/300], Step [20300/27733], Loss: 3.1323\n",
      "Epoch [111/300], Step [20400/27733], Loss: 3.1826\n",
      "Epoch [111/300], Step [20500/27733], Loss: 3.5632\n",
      "Epoch [111/300], Step [20600/27733], Loss: 4.3004\n",
      "Epoch [111/300], Step [20700/27733], Loss: 3.1647\n",
      "Epoch [111/300], Step [20800/27733], Loss: 3.2747\n",
      "Epoch [111/300], Step [20900/27733], Loss: 2.9491\n",
      "Epoch [111/300], Step [21000/27733], Loss: 2.7175\n",
      "Epoch [111/300], Step [21100/27733], Loss: 3.5426\n",
      "Epoch [111/300], Step [21200/27733], Loss: 2.7594\n",
      "Epoch [111/300], Step [21300/27733], Loss: 3.3229\n",
      "Epoch [111/300], Step [21400/27733], Loss: 3.4167\n",
      "Epoch [111/300], Step [21500/27733], Loss: 2.8646\n",
      "Epoch [111/300], Step [21600/27733], Loss: 3.2094\n",
      "Epoch [111/300], Step [21700/27733], Loss: 2.9526\n",
      "Epoch [111/300], Step [21800/27733], Loss: 2.7907\n",
      "Epoch [111/300], Step [21900/27733], Loss: 2.5603\n",
      "Epoch [111/300], Step [22000/27733], Loss: 2.2819\n",
      "Epoch [111/300], Step [22100/27733], Loss: 2.9809\n",
      "Epoch [111/300], Step [22200/27733], Loss: 3.3059\n",
      "Epoch [111/300], Step [22300/27733], Loss: 2.0733\n",
      "Epoch [111/300], Step [22400/27733], Loss: 2.7157\n",
      "Epoch [111/300], Step [22500/27733], Loss: 2.5335\n",
      "Epoch [111/300], Step [22600/27733], Loss: 2.6725\n",
      "Epoch [111/300], Step [22700/27733], Loss: 3.3021\n",
      "Epoch [111/300], Step [22800/27733], Loss: 2.9920\n",
      "Epoch [111/300], Step [22900/27733], Loss: 3.2911\n",
      "Epoch [111/300], Step [23000/27733], Loss: 3.1525\n",
      "Epoch [111/300], Step [23100/27733], Loss: 2.5814\n",
      "Epoch [111/300], Step [23200/27733], Loss: 1.9614\n",
      "Epoch [111/300], Step [23300/27733], Loss: 2.8131\n",
      "Epoch [111/300], Step [23400/27733], Loss: 2.5690\n",
      "Epoch [111/300], Step [23500/27733], Loss: 2.8837\n",
      "Epoch [111/300], Step [23600/27733], Loss: 3.7491\n",
      "Epoch [111/300], Step [23700/27733], Loss: 2.8522\n",
      "Epoch [111/300], Step [23800/27733], Loss: 2.3599\n",
      "Epoch [111/300], Step [23900/27733], Loss: 3.1102\n",
      "Epoch [111/300], Step [24000/27733], Loss: 2.6331\n",
      "Epoch [111/300], Step [24100/27733], Loss: 2.7395\n",
      "Epoch [111/300], Step [24200/27733], Loss: 3.4392\n",
      "Epoch [111/300], Step [24300/27733], Loss: 3.1325\n",
      "Epoch [111/300], Step [24400/27733], Loss: 2.9797\n",
      "Epoch [111/300], Step [24500/27733], Loss: 3.5067\n",
      "Epoch [111/300], Step [24600/27733], Loss: 2.8159\n",
      "Epoch [111/300], Step [24700/27733], Loss: 2.7796\n",
      "Epoch [111/300], Step [24800/27733], Loss: 2.3136\n",
      "Epoch [111/300], Step [24900/27733], Loss: 3.0474\n",
      "Epoch [111/300], Step [25000/27733], Loss: 2.9330\n",
      "Epoch [111/300], Step [25100/27733], Loss: 2.7405\n",
      "Epoch [111/300], Step [25200/27733], Loss: 2.6788\n",
      "Epoch [111/300], Step [25300/27733], Loss: 2.8386\n",
      "Epoch [111/300], Step [25400/27733], Loss: 2.9177\n",
      "Epoch [111/300], Step [25500/27733], Loss: 3.4615\n",
      "Epoch [111/300], Step [25600/27733], Loss: 3.6832\n",
      "Epoch [111/300], Step [25700/27733], Loss: 3.2382\n",
      "Epoch [111/300], Step [25800/27733], Loss: 3.7555\n",
      "Epoch [111/300], Step [25900/27733], Loss: 2.5603\n",
      "Epoch [111/300], Step [26000/27733], Loss: 3.2963\n",
      "Epoch [111/300], Step [26100/27733], Loss: 3.6579\n",
      "Epoch [111/300], Step [26200/27733], Loss: 3.2740\n",
      "Epoch [111/300], Step [26300/27733], Loss: 3.3327\n",
      "Epoch [111/300], Step [26400/27733], Loss: 4.1504\n",
      "Epoch [111/300], Step [26500/27733], Loss: 3.1067\n",
      "Epoch [111/300], Step [26600/27733], Loss: 3.2341\n",
      "Epoch [111/300], Step [26700/27733], Loss: 3.3031\n",
      "Epoch [111/300], Step [26800/27733], Loss: 2.8538\n",
      "Epoch [111/300], Step [26900/27733], Loss: 3.2546\n",
      "Epoch [111/300], Step [27000/27733], Loss: 3.6804\n",
      "Epoch [111/300], Step [27100/27733], Loss: 2.5065\n",
      "Epoch [111/300], Step [27200/27733], Loss: 3.3801\n",
      "Epoch [111/300], Step [27300/27733], Loss: 3.1734\n",
      "Epoch [111/300], Step [27400/27733], Loss: 3.3663\n",
      "Epoch [111/300], Step [27500/27733], Loss: 3.0654\n",
      "Epoch [111/300], Step [27600/27733], Loss: 3.2065\n",
      "Epoch [111/300], Step [27700/27733], Loss: 3.8717\n",
      "Epoch [112/300], Step [100/27733], Loss: 2.3642\n",
      "Epoch [112/300], Step [200/27733], Loss: 2.3324\n",
      "Epoch [112/300], Step [300/27733], Loss: 2.4537\n",
      "Epoch [112/300], Step [400/27733], Loss: 2.5131\n",
      "Epoch [112/300], Step [500/27733], Loss: 2.5890\n",
      "Epoch [112/300], Step [600/27733], Loss: 2.6813\n",
      "Epoch [112/300], Step [700/27733], Loss: 2.9885\n",
      "Epoch [112/300], Step [800/27733], Loss: 2.4238\n",
      "Epoch [112/300], Step [900/27733], Loss: 3.1263\n",
      "Epoch [112/300], Step [1000/27733], Loss: 2.7180\n",
      "Epoch [112/300], Step [1100/27733], Loss: 3.4153\n",
      "Epoch [112/300], Step [1200/27733], Loss: 2.5728\n",
      "Epoch [112/300], Step [1300/27733], Loss: 2.2386\n",
      "Epoch [112/300], Step [1400/27733], Loss: 2.2980\n",
      "Epoch [112/300], Step [1500/27733], Loss: 3.0829\n",
      "Epoch [112/300], Step [1600/27733], Loss: 2.8492\n",
      "Epoch [112/300], Step [1700/27733], Loss: 2.4486\n",
      "Epoch [112/300], Step [1800/27733], Loss: 2.3935\n",
      "Epoch [112/300], Step [1900/27733], Loss: 2.9204\n",
      "Epoch [112/300], Step [2000/27733], Loss: 2.4506\n",
      "Epoch [112/300], Step [2100/27733], Loss: 3.1799\n",
      "Epoch [112/300], Step [2200/27733], Loss: 3.3788\n",
      "Epoch [112/300], Step [2300/27733], Loss: 2.8253\n",
      "Epoch [112/300], Step [2400/27733], Loss: 3.4243\n",
      "Epoch [112/300], Step [2500/27733], Loss: 2.8051\n",
      "Epoch [112/300], Step [2600/27733], Loss: 2.2474\n",
      "Epoch [112/300], Step [2700/27733], Loss: 2.8242\n",
      "Epoch [112/300], Step [2800/27733], Loss: 2.7106\n",
      "Epoch [112/300], Step [2900/27733], Loss: 2.5569\n",
      "Epoch [112/300], Step [3000/27733], Loss: 2.7909\n",
      "Epoch [112/300], Step [3100/27733], Loss: 2.6906\n",
      "Epoch [112/300], Step [3200/27733], Loss: 2.8147\n",
      "Epoch [112/300], Step [3300/27733], Loss: 1.8500\n",
      "Epoch [112/300], Step [3400/27733], Loss: 2.6438\n",
      "Epoch [112/300], Step [3500/27733], Loss: 2.2350\n",
      "Epoch [112/300], Step [3600/27733], Loss: 3.2068\n",
      "Epoch [112/300], Step [3700/27733], Loss: 3.3529\n",
      "Epoch [112/300], Step [3800/27733], Loss: 2.5287\n",
      "Epoch [112/300], Step [3900/27733], Loss: 2.8516\n",
      "Epoch [112/300], Step [4000/27733], Loss: 2.4944\n",
      "Epoch [112/300], Step [4100/27733], Loss: 2.2779\n",
      "Epoch [112/300], Step [4200/27733], Loss: 2.9106\n",
      "Epoch [112/300], Step [4300/27733], Loss: 2.5520\n",
      "Epoch [112/300], Step [4400/27733], Loss: 2.4927\n",
      "Epoch [112/300], Step [4500/27733], Loss: 2.9465\n",
      "Epoch [112/300], Step [4600/27733], Loss: 2.6190\n",
      "Epoch [112/300], Step [4700/27733], Loss: 2.9155\n",
      "Epoch [112/300], Step [4800/27733], Loss: 2.6555\n",
      "Epoch [112/300], Step [4900/27733], Loss: 3.0291\n",
      "Epoch [112/300], Step [5000/27733], Loss: 2.3281\n",
      "Epoch [112/300], Step [5100/27733], Loss: 3.2610\n",
      "Epoch [112/300], Step [5200/27733], Loss: 2.8199\n",
      "Epoch [112/300], Step [5300/27733], Loss: 3.1808\n",
      "Epoch [112/300], Step [5400/27733], Loss: 2.7628\n",
      "Epoch [112/300], Step [5500/27733], Loss: 2.7927\n",
      "Epoch [112/300], Step [5600/27733], Loss: 2.8788\n",
      "Epoch [112/300], Step [5700/27733], Loss: 2.9296\n",
      "Epoch [112/300], Step [5800/27733], Loss: 2.1325\n",
      "Epoch [112/300], Step [5900/27733], Loss: 2.7049\n",
      "Epoch [112/300], Step [6000/27733], Loss: 3.4403\n",
      "Epoch [112/300], Step [6100/27733], Loss: 2.2140\n",
      "Epoch [112/300], Step [6200/27733], Loss: 1.5378\n",
      "Epoch [112/300], Step [6300/27733], Loss: 3.1502\n",
      "Epoch [112/300], Step [6400/27733], Loss: 3.3435\n",
      "Epoch [112/300], Step [6500/27733], Loss: 2.2235\n",
      "Epoch [112/300], Step [6600/27733], Loss: 2.4710\n",
      "Epoch [112/300], Step [6700/27733], Loss: 2.9931\n",
      "Epoch [112/300], Step [6800/27733], Loss: 2.2767\n",
      "Epoch [112/300], Step [6900/27733], Loss: 3.3636\n",
      "Epoch [112/300], Step [7000/27733], Loss: 2.4388\n",
      "Epoch [112/300], Step [7100/27733], Loss: 2.8701\n",
      "Epoch [112/300], Step [7200/27733], Loss: 3.0112\n",
      "Epoch [112/300], Step [7300/27733], Loss: 2.7880\n",
      "Epoch [112/300], Step [7400/27733], Loss: 2.3097\n",
      "Epoch [112/300], Step [7500/27733], Loss: 3.1350\n",
      "Epoch [112/300], Step [7600/27733], Loss: 2.4459\n",
      "Epoch [112/300], Step [7700/27733], Loss: 2.5714\n",
      "Epoch [112/300], Step [7800/27733], Loss: 3.8355\n",
      "Epoch [112/300], Step [7900/27733], Loss: 2.7300\n",
      "Epoch [112/300], Step [8000/27733], Loss: 3.3058\n",
      "Epoch [112/300], Step [8100/27733], Loss: 2.3597\n",
      "Epoch [112/300], Step [8200/27733], Loss: 2.1799\n",
      "Epoch [112/300], Step [8300/27733], Loss: 2.4512\n",
      "Epoch [112/300], Step [8400/27733], Loss: 2.3646\n",
      "Epoch [112/300], Step [8500/27733], Loss: 2.2544\n",
      "Epoch [112/300], Step [8600/27733], Loss: 2.5827\n",
      "Epoch [112/300], Step [8700/27733], Loss: 2.4714\n",
      "Epoch [112/300], Step [8800/27733], Loss: 2.7805\n",
      "Epoch [112/300], Step [8900/27733], Loss: 2.1206\n",
      "Epoch [112/300], Step [9000/27733], Loss: 3.0011\n",
      "Epoch [112/300], Step [9100/27733], Loss: 2.0588\n",
      "Epoch [112/300], Step [9200/27733], Loss: 2.6481\n",
      "Epoch [112/300], Step [9300/27733], Loss: 2.0716\n",
      "Epoch [112/300], Step [9400/27733], Loss: 3.2448\n",
      "Epoch [112/300], Step [9500/27733], Loss: 3.7882\n",
      "Epoch [112/300], Step [9600/27733], Loss: 2.7273\n",
      "Epoch [112/300], Step [9700/27733], Loss: 2.3899\n",
      "Epoch [112/300], Step [9800/27733], Loss: 1.9023\n",
      "Epoch [112/300], Step [9900/27733], Loss: 2.3168\n",
      "Epoch [112/300], Step [10000/27733], Loss: 2.3473\n",
      "Epoch [112/300], Step [10100/27733], Loss: 3.3005\n",
      "Epoch [112/300], Step [10200/27733], Loss: 2.1182\n",
      "Epoch [112/300], Step [10300/27733], Loss: 3.2606\n",
      "Epoch [112/300], Step [10400/27733], Loss: 2.7677\n",
      "Epoch [112/300], Step [10500/27733], Loss: 3.7124\n",
      "Epoch [112/300], Step [10600/27733], Loss: 2.5323\n",
      "Epoch [112/300], Step [10700/27733], Loss: 3.6047\n",
      "Epoch [112/300], Step [10800/27733], Loss: 3.3789\n",
      "Epoch [112/300], Step [10900/27733], Loss: 2.2635\n",
      "Epoch [112/300], Step [11000/27733], Loss: 2.6966\n",
      "Epoch [112/300], Step [11100/27733], Loss: 3.1478\n",
      "Epoch [112/300], Step [11200/27733], Loss: 1.9317\n",
      "Epoch [112/300], Step [11300/27733], Loss: 3.0465\n",
      "Epoch [112/300], Step [11400/27733], Loss: 2.8055\n",
      "Epoch [112/300], Step [11500/27733], Loss: 3.4216\n",
      "Epoch [112/300], Step [11600/27733], Loss: 2.4536\n",
      "Epoch [112/300], Step [11700/27733], Loss: 2.2537\n",
      "Epoch [112/300], Step [11800/27733], Loss: 3.3456\n",
      "Epoch [112/300], Step [11900/27733], Loss: 3.4179\n",
      "Epoch [112/300], Step [12000/27733], Loss: 3.5076\n",
      "Epoch [112/300], Step [12100/27733], Loss: 2.4755\n",
      "Epoch [112/300], Step [12200/27733], Loss: 2.5138\n",
      "Epoch [112/300], Step [12300/27733], Loss: 2.4494\n",
      "Epoch [112/300], Step [12400/27733], Loss: 1.4840\n",
      "Epoch [112/300], Step [12500/27733], Loss: 1.9215\n",
      "Epoch [112/300], Step [12600/27733], Loss: 2.9008\n",
      "Epoch [112/300], Step [12700/27733], Loss: 2.2045\n",
      "Epoch [112/300], Step [12800/27733], Loss: 2.8496\n",
      "Epoch [112/300], Step [12900/27733], Loss: 3.1127\n",
      "Epoch [112/300], Step [13000/27733], Loss: 3.4381\n",
      "Epoch [112/300], Step [13100/27733], Loss: 2.7895\n",
      "Epoch [112/300], Step [13200/27733], Loss: 2.2094\n",
      "Epoch [112/300], Step [13300/27733], Loss: 3.2052\n",
      "Epoch [112/300], Step [13400/27733], Loss: 2.3945\n",
      "Epoch [112/300], Step [13500/27733], Loss: 3.4623\n",
      "Epoch [112/300], Step [13600/27733], Loss: 2.4899\n",
      "Epoch [112/300], Step [13700/27733], Loss: 3.0873\n",
      "Epoch [112/300], Step [13800/27733], Loss: 2.7362\n",
      "Epoch [112/300], Step [13900/27733], Loss: 2.8490\n",
      "Epoch [112/300], Step [14000/27733], Loss: 3.4020\n",
      "Epoch [112/300], Step [14100/27733], Loss: 2.8903\n",
      "Epoch [112/300], Step [14200/27733], Loss: 2.6722\n",
      "Epoch [112/300], Step [14300/27733], Loss: 2.8086\n",
      "Epoch [112/300], Step [14400/27733], Loss: 2.8797\n",
      "Epoch [112/300], Step [14500/27733], Loss: 3.1285\n",
      "Epoch [112/300], Step [14600/27733], Loss: 2.5421\n",
      "Epoch [112/300], Step [14700/27733], Loss: 3.2725\n",
      "Epoch [112/300], Step [14800/27733], Loss: 3.4472\n",
      "Epoch [112/300], Step [14900/27733], Loss: 2.9463\n",
      "Epoch [112/300], Step [15000/27733], Loss: 3.1029\n",
      "Epoch [112/300], Step [15100/27733], Loss: 2.3640\n",
      "Epoch [112/300], Step [15200/27733], Loss: 3.0060\n",
      "Epoch [112/300], Step [15300/27733], Loss: 2.8645\n",
      "Epoch [112/300], Step [15400/27733], Loss: 3.2945\n",
      "Epoch [112/300], Step [15500/27733], Loss: 2.6961\n",
      "Epoch [112/300], Step [15600/27733], Loss: 3.8972\n",
      "Epoch [112/300], Step [15700/27733], Loss: 3.0245\n",
      "Epoch [112/300], Step [15800/27733], Loss: 3.2731\n",
      "Epoch [112/300], Step [15900/27733], Loss: 3.5656\n",
      "Epoch [112/300], Step [16000/27733], Loss: 3.3181\n",
      "Epoch [112/300], Step [16100/27733], Loss: 2.9366\n",
      "Epoch [112/300], Step [16200/27733], Loss: 2.7348\n",
      "Epoch [112/300], Step [16300/27733], Loss: 3.2691\n",
      "Epoch [112/300], Step [16400/27733], Loss: 3.1565\n",
      "Epoch [112/300], Step [16500/27733], Loss: 2.5922\n",
      "Epoch [112/300], Step [16600/27733], Loss: 2.5379\n",
      "Epoch [112/300], Step [16700/27733], Loss: 2.6460\n",
      "Epoch [112/300], Step [16800/27733], Loss: 2.8384\n",
      "Epoch [112/300], Step [16900/27733], Loss: 2.9372\n",
      "Epoch [112/300], Step [17000/27733], Loss: 2.7401\n",
      "Epoch [112/300], Step [17100/27733], Loss: 2.9330\n",
      "Epoch [112/300], Step [17200/27733], Loss: 2.8280\n",
      "Epoch [112/300], Step [17300/27733], Loss: 2.4719\n",
      "Epoch [112/300], Step [17400/27733], Loss: 2.0987\n",
      "Epoch [112/300], Step [17500/27733], Loss: 2.9460\n",
      "Epoch [112/300], Step [17600/27733], Loss: 2.2979\n",
      "Epoch [112/300], Step [17700/27733], Loss: 2.9624\n",
      "Epoch [112/300], Step [17800/27733], Loss: 3.4897\n",
      "Epoch [112/300], Step [17900/27733], Loss: 3.1050\n",
      "Epoch [112/300], Step [18000/27733], Loss: 2.9073\n",
      "Epoch [112/300], Step [18100/27733], Loss: 2.8397\n",
      "Epoch [112/300], Step [18200/27733], Loss: 4.1102\n",
      "Epoch [112/300], Step [18300/27733], Loss: 2.4993\n",
      "Epoch [112/300], Step [18400/27733], Loss: 3.5145\n",
      "Epoch [112/300], Step [18500/27733], Loss: 3.0069\n",
      "Epoch [112/300], Step [18600/27733], Loss: 2.4948\n",
      "Epoch [112/300], Step [18700/27733], Loss: 3.8969\n",
      "Epoch [112/300], Step [18800/27733], Loss: 2.8958\n",
      "Epoch [112/300], Step [18900/27733], Loss: 3.1624\n",
      "Epoch [112/300], Step [19000/27733], Loss: 2.3897\n",
      "Epoch [112/300], Step [19100/27733], Loss: 3.7159\n",
      "Epoch [112/300], Step [19200/27733], Loss: 2.5500\n",
      "Epoch [112/300], Step [19300/27733], Loss: 2.6153\n",
      "Epoch [112/300], Step [19400/27733], Loss: 3.1187\n",
      "Epoch [112/300], Step [19500/27733], Loss: 3.3543\n",
      "Epoch [112/300], Step [19600/27733], Loss: 2.9679\n",
      "Epoch [112/300], Step [19700/27733], Loss: 3.7862\n",
      "Epoch [112/300], Step [19800/27733], Loss: 3.6147\n",
      "Epoch [112/300], Step [19900/27733], Loss: 2.4671\n",
      "Epoch [112/300], Step [20000/27733], Loss: 3.2907\n",
      "Epoch [112/300], Step [20100/27733], Loss: 3.0536\n",
      "Epoch [112/300], Step [20200/27733], Loss: 2.4485\n",
      "Epoch [112/300], Step [20300/27733], Loss: 3.2610\n",
      "Epoch [112/300], Step [20400/27733], Loss: 3.6704\n",
      "Epoch [112/300], Step [20500/27733], Loss: 2.5638\n",
      "Epoch [112/300], Step [20600/27733], Loss: 2.8863\n",
      "Epoch [112/300], Step [20700/27733], Loss: 2.2808\n",
      "Epoch [112/300], Step [20800/27733], Loss: 2.8932\n",
      "Epoch [112/300], Step [20900/27733], Loss: 3.1695\n",
      "Epoch [112/300], Step [21000/27733], Loss: 2.4509\n",
      "Epoch [112/300], Step [21100/27733], Loss: 3.5823\n",
      "Epoch [112/300], Step [21200/27733], Loss: 3.0416\n",
      "Epoch [112/300], Step [21300/27733], Loss: 3.3709\n",
      "Epoch [112/300], Step [21400/27733], Loss: 2.6511\n",
      "Epoch [112/300], Step [21500/27733], Loss: 2.6382\n",
      "Epoch [112/300], Step [21600/27733], Loss: 3.2698\n",
      "Epoch [112/300], Step [21700/27733], Loss: 2.7783\n",
      "Epoch [112/300], Step [21800/27733], Loss: 3.2303\n",
      "Epoch [112/300], Step [21900/27733], Loss: 2.5194\n",
      "Epoch [112/300], Step [22000/27733], Loss: 2.7761\n",
      "Epoch [112/300], Step [22100/27733], Loss: 3.4118\n",
      "Epoch [112/300], Step [22200/27733], Loss: 3.0257\n",
      "Epoch [112/300], Step [22300/27733], Loss: 2.6090\n",
      "Epoch [112/300], Step [22400/27733], Loss: 3.2243\n",
      "Epoch [112/300], Step [22500/27733], Loss: 2.7994\n",
      "Epoch [112/300], Step [22600/27733], Loss: 3.3425\n",
      "Epoch [112/300], Step [22700/27733], Loss: 3.3079\n",
      "Epoch [112/300], Step [22800/27733], Loss: 2.9437\n",
      "Epoch [112/300], Step [22900/27733], Loss: 3.1568\n",
      "Epoch [112/300], Step [23000/27733], Loss: 3.1812\n",
      "Epoch [112/300], Step [23100/27733], Loss: 4.0845\n",
      "Epoch [112/300], Step [23200/27733], Loss: 2.9777\n",
      "Epoch [112/300], Step [23300/27733], Loss: 3.0017\n",
      "Epoch [112/300], Step [23400/27733], Loss: 2.1568\n",
      "Epoch [112/300], Step [23500/27733], Loss: 3.6668\n",
      "Epoch [112/300], Step [23600/27733], Loss: 3.0785\n",
      "Epoch [112/300], Step [23700/27733], Loss: 2.5545\n",
      "Epoch [112/300], Step [23800/27733], Loss: 3.6356\n",
      "Epoch [112/300], Step [23900/27733], Loss: 3.0610\n",
      "Epoch [112/300], Step [24000/27733], Loss: 3.1584\n",
      "Epoch [112/300], Step [24100/27733], Loss: 2.1863\n",
      "Epoch [112/300], Step [24200/27733], Loss: 3.2503\n",
      "Epoch [112/300], Step [24300/27733], Loss: 3.4011\n",
      "Epoch [112/300], Step [24400/27733], Loss: 2.8428\n",
      "Epoch [112/300], Step [24500/27733], Loss: 2.4875\n",
      "Epoch [112/300], Step [24600/27733], Loss: 3.3593\n",
      "Epoch [112/300], Step [24700/27733], Loss: 2.4853\n",
      "Epoch [112/300], Step [24800/27733], Loss: 2.4061\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m output, _ \u001b[39m=\u001b[39m model(x, hidden)\n\u001b[0;32m     41\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[1;32m---> 42\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     43\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\xiang\\anaconda3\\envs\\a4\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xiang\\anaconda3\\envs\\a4\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the language model\n",
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(LM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        y = self.embedding(x)\n",
    "        y, _ = self.lstm(y, hidden)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.fc(y)\n",
    "        return y, _\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(DEVICE),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(DEVICE))\n",
    "\n",
    "# Initialize the language model\n",
    "model = LM(len(vocab), 128, 512, 2)\n",
    "model.cuda()\n",
    "\n",
    "writer = SummaryWriter('runs/ptb')\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the language model\n",
    "n_epochs = 300\n",
    "for epoch in range(n_epochs):\n",
    "    hidden = model.init_hidden(32)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(x, hidden)\n",
    "        \n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i + 1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    torch.save(model, f'model_epoch_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best way of exporters and freeze a registered ceiling but the rush is likely to be far less <unk> is projected to add to N hours of gasoline supplies to the housing and effect of young & rubicam said the nation 's total could n't be reached for comment the transportation department said it is difficult to secure within three years to repair programs and <unk> the centers for <unk> programs to address the <unk> of the <unk> <unk> coastal development and <unk> the project is already <unk> by the mexican nuclear refinery houston property is likely to be transcanada pipelines the\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('model_epoch_110.pt')\n",
    "model.eval()\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "sentence = 'The best way'.lower()\n",
    "\n",
    "tokens = [word_to_int[word] for word in sentence.split()]\n",
    "tokens = torch.tensor(tokens, device=DEVICE).long().view(1, -1)\n",
    "\n",
    "for i in range(100):\n",
    "    output, _ = model(tokens, hidden)\n",
    "    prediction = output.argmax(dim=1)\n",
    "    tokens = torch.cat((tokens, prediction.view(1, -1)), dim=1)\n",
    "\n",
    "print(' '.join(decode(tokens[0].tolist())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_epoch_110.pt')\n",
    "model.cpu()\n",
    "torch.save(model, 'model_epoch_110_cpu.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33354ac008e7d66c25b31539028e6805ce41876ceff7ffff18fd20bedbc7b4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
